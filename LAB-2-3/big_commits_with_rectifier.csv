Hash,Message,Filename,Source Code (before),Source Code (current),Diff,LLM Inference,Rectifier
50038c07c815b82d412af43996a122e33eecd385,Matthew Johnson: fix build file issues,build/build_jax.sh,"#!/bin/bash
set -exv

# For a build with CUDA, from the repo root run:
#   bash build/build_jax.sh
# For building without CUDA (CPU-only), instead run:
#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
# To clean intermediate results, run
#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
# To clean everything, run
#   rm -rf /tmp/jax-build

JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}

init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
then
  (>&2 echo ""must be executed from jax repo root"")
  exit 1
fi

tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
mkdir -p ${tmp}

## get bazel
bazel_dir=${tmp}/jax-bazel
if [ ! -d ${bazel_dir}/bin ]
then
    mkdir -p ${bazel_dir}
    case ""$(uname -s)"" in
      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
      *) exit 1;;
    esac
    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
    chmod +x ${installer}
    bash ${installer} --prefix=${bazel_dir}
    rm ${installer}
fi
export PATH=""${bazel_dir}/bin:$PATH""

## get and configure tensorflow for building xla
if [[ ! -d tensorflow ]]
then
  git clone https://github.com/tensorflow/tensorflow.git
fi
pushd tensorflow
export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
export USE_DEFAULT_PYTHON_LIB_PATH=1
if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
then
  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
  export TF_NEED_CUDA=1
else
  export TF_NEED_CUDA=0
fi
export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
export TF_ENABLE_XLA=1
export TF_NEED_MKL=0
export CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
export TF_NEED_IGNITE=1
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_ROCM=0
export TF_NEED_MPI=0
export TF_DOWNLOAD_CLANG=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_CUDA_CLANG=0
export TF_NEED_TENSORRT=0
export TF_NCCL_VERSION=""2""
./configure
popd

## build xla inside tensorflow
mkdir -p ${PYTHON_LIB_PATH}
bazel_output_user_root=${tmp}/jax-bazel-output-user-root
bazel_output_base=${bazel_output_user_root}/output-base
bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
then
  bazel_build_opt=""-c opt --config=cuda""
else
  bazel_build_opt=""-c opt""
fi
bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax

## extract the pieces we need
runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/

## rewrite some imports
sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
sed -i 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' jax/lib/xla_client.py
sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_client.py

## clean up
rm -f bazel-*  # symlinks
rm -rf tensorflow
rm -rf ${bazel_output_user_root}  # clean build results
# rm -rf ${tmp}  # clean everything, including the bazel binary
","#!/bin/bash
set -exv

# For a build with CUDA, from the repo root run:
#   bash build/build_jax.sh
# For building without CUDA (CPU-only), instead run:
#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
# To clean intermediate results, run
#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
# To clean everything, run
#   rm -rf /tmp/jax-build

JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}

init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
then
  (>&2 echo ""must be executed from jax repo root"")
  exit 1
fi

tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
mkdir -p ${tmp}

## get bazel
bazel_dir=${tmp}/jax-bazel
if [ ! -d ${bazel_dir}/bin ]
then
    mkdir -p ${bazel_dir}
    case ""$(uname -s)"" in
      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
      *) exit 1;;
    esac
    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
    chmod +x ${installer}
    bash ${installer} --prefix=${bazel_dir}
    rm ${installer}
fi
export PATH=""${bazel_dir}/bin:$PATH""

## get and configure tensorflow for building xla
if [[ ! -d tensorflow ]]
then
  git clone https://github.com/tensorflow/tensorflow.git
fi
pushd tensorflow
export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
export USE_DEFAULT_PYTHON_LIB_PATH=1
if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
then
  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
  export TF_NCCL_VERSION=2
  export TF_NEED_CUDA=1
else
  export TF_NEED_CUDA=0
fi
export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
export TF_ENABLE_XLA=1
export TF_NEED_MKL=0
export CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
export TF_NEED_IGNITE=1
export TF_NEED_OPENCL=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_ROCM=0
export TF_NEED_MPI=0
export TF_DOWNLOAD_CLANG=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_CUDA_CLANG=0
export TF_NEED_TENSORRT=0
./configure
popd

## build xla inside tensorflow
mkdir -p ${PYTHON_LIB_PATH}
bazel_output_user_root=${tmp}/jax-bazel-output-user-root
bazel_output_base=${bazel_output_user_root}/output-base
bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
then
  bazel_build_opt=""-c opt --config=cuda""
else
  bazel_build_opt=""-c opt""
fi
bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax

## extract the pieces we need
runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/

## rewrite some imports
sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
sed -i 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' jax/lib/xla_client.py
sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_client.py

## clean up
rm -f bazel-*  # symlinks
rm -rf tensorflow
rm -rf ${bazel_output_user_root}  # clean build results
# rm -rf ${tmp}  # clean everything, including the bazel binary
","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229bd97a952fee588dfaa169e669cc5180..0bf8ae06c11bfedc583f8b3bd5a35f0c7eb97d07 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 
",Deadlock / livelock,Move TF_NCCL_VERSION assignment inside CUDA conditional block
50038c07c815b82d412af43996a122e33eecd385,Matthew Johnson: fix build file issues,examples/BUILD,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

licenses([""notice""])  # Apache 2

py_binary(
    name = ""interactive"",
    srcs = [""interactive.py""],
    deps = [""//jax:libjax""],
)

py_library(
    name = ""datasets"",
    srcs = [""datasets.py""],
)

py_binary(
    name = ""mnist_classifier"",
    srcs = [""mnist_classifier.py""],
    deps = [
        "":datasets"",
        ""//jax:libjax"",
    ],
)

py_binary(
    name = ""mnist_vae"",
    srcs = [""mnist_vae.py""],
    deps = [
        "":datasets"",
        "":minmax"",
        "":stax"",
        ""//jax:libjax"",
    ],
)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

licenses([""notice""])  # Apache 2

py_binary(
    name = ""interactive"",
    srcs = [""interactive.py""],
    deps = [""//jax:libjax""],
)

py_library(
    name = ""datasets"",
    srcs = [""datasets.py""],
)

py_binary(
    name = ""mnist_classifier"",
    srcs = [""mnist_classifier.py""],
    deps = [
        "":datasets"",
        ""//jax:libjax"",
    ],
)

py_binary(
    name = ""mnist_vae"",
    srcs = [""mnist_vae.py""],
    deps = [
        "":datasets"",
        ""//jax:libjax"",
        ""//jax:minmax"",
        ""//jax:stax"",
    ],
)
","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df67a06e79da79274f7a972b013f7737e..230e9ceb746cc87285216554f2a13b2d02cef2b6 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )
",Exception handling / swallowed exceptions,Update dependencies for mnist_vae.py to use //jax:minmax and //jax:stax instead of local :minmax
50038c07c815b82d412af43996a122e33eecd385,Matthew Johnson: fix build file issues,jax/BUILD,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# JAX is Autograd and XLA

licenses([""notice""])  # Apache 2

package(default_visibility = [""//visibility:public""])

# top-level EF placeholder

py_library(
    name = ""libjax"",
    srcs = glob(
        [
            ""*.py"",
            ""lib/*.py"",
            ""interpreters/*.py"",
            ""numpy/*.py"",
            ""scipy/*.py"",
        ],
        exclude = [
            ""*_test.py"",
            ""**/*_test.py"",
        ],
    ),
    deps = [""@org_tensorflow//tensorflow/compiler/xla/python:xla_client""],
)

py_library(
    name = ""stax"",
    srcs = [""experimental/stax.py""],
    deps = ["":libjax""],
)

py_library(
    name = ""minmax"",
    srcs = [""experimental/minmax.py""],
    deps = ["":libjax""],
)

py_library(
    name = ""lapax"",
    srcs = [""experimental/lapax.py""],
    deps = ["":libjax""],
)

# this is a dummy target for building purposes
py_binary(
    name = ""build_jax"",
    srcs = [""__init__.py""],
    main = ""__init__.py"",
    deps = ["":libjax""],
)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# JAX is Autograd and XLA

licenses([""notice""])  # Apache 2

package(default_visibility = [""//visibility:public""])

# top-level EF placeholder

py_library(
    name = ""libjax"",
    srcs = glob(
        [
            ""*.py"",
            ""lib/*.py"",
            ""interpreters/*.py"",
            ""numpy/*.py"",
            ""scipy/*.py"",
            ""scipy/stats/*.py"",
        ],
        exclude = [
            ""*_test.py"",
            ""**/*_test.py"",
        ],
    ),
    deps = [""@org_tensorflow//tensorflow/compiler/xla/python:xla_client""],
)

py_library(
    name = ""stax"",
    srcs = [""experimental/stax.py""],
    deps = ["":libjax""],
)

py_library(
    name = ""minmax"",
    srcs = [""experimental/minmax.py""],
    deps = ["":libjax""],
)

py_library(
    name = ""lapax"",
    srcs = [""experimental/lapax.py""],
    deps = ["":libjax""],
)

# this is a dummy target for building purposes
py_binary(
    name = ""build_jax"",
    srcs = [""__init__.py""],
    main = ""__init__.py"",
    deps = ["":libjax""],
)
","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6dc38b1e4e1c8c14ac13a9050cb0822a..c9754372f5fa55582df5c963e3bda4f4a30a14ff 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",
",Performance regression,Add scipy stats module to jax py_library sources
50038c07c815b82d412af43996a122e33eecd385,Matthew Johnson: fix build file issues,setup.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from setuptools import setup
from glob import glob

setup(
    name='jax',
    version='0.0',
    description='Differentiate, compile, and transform Numpy code.',
    author='JAX team',
    author_email='jax-team@google.com',
    packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
              'jax.experimental'],
    install_requires=['numpy>=1.12', 'six', 'protobuf'],
    url='https://github.com/google/jax',
    license='Apache-2.0',
    package_data={'jax.lib': glob('jax/lib/*.so')},
)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from setuptools import setup
from glob import glob

setup(
    name='jax',
    version='0.0',
    description='Differentiate, compile, and transform Numpy code.',
    author='JAX team',
    author_email='jax-team@google.com',
    packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
              'jax.experimental'],
    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
    url='https://github.com/google/jax',
    license='Apache-2.0',
    package_data={'jax.lib': glob('jax/lib/*.so')},
)
","diff --git a/setup.py b/setup.py
index 2b2ebc68daac0973f22d4a4d69d4273f7afe90ca..b634177d3bc1891d516bfccf6b4c4ac41589d437 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},
",Documentation / comment mismatch,Add absl-py to install requirements in setup.py
50038c07c815b82d412af43996a122e33eecd385,Matthew Johnson: fix build file issues,tests/BUILD,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

licenses([""notice""])  # Apache 2

load("":build_defs.bzl"", ""jax_test"")

jax_test(
    name = ""core_test"",
    srcs = [""tests/core_test.py""],
    shard_count = {
        ""cpu"": 5,
    },
)

jax_test(
    name = ""lax_test"",
    srcs = [""tests/lax_test.py""],
    shard_count = {
        ""cpu"": 40,
        ""gpu"": 20,
    },
)

jax_test(
    name = ""lax_numpy_test"",
    srcs = [""tests/lax_numpy_test.py""],
    shard_count = {
        ""cpu"": 40,
        ""gpu"": 20,
    },
)

jax_test(
    name = ""lax_numpy_indexing_test"",
    srcs = [""tests/lax_numpy_indexing_test.py""],
    shard_count = {
        ""cpu"": 10,
        ""gpu"": 2,
    },
)

jax_test(
    name = ""lax_scipy_test"",
    srcs = [""tests/lax_scipy_test.py""],
    shard_count = {
        ""cpu"": 10,
        ""gpu"": 2,
    },
)

jax_test(
    name = ""random_test"",
    srcs = [""tests/random_test.py""],
)

jax_test(
    name = ""api_test"",
    srcs = [""tests/api_test.py""],
)

jax_test(
    name = ""batching_test"",
    srcs = [""tests/batching_test.py""],
)

jax_test(
    name = ""stax_test"",
    srcs = [""tests/stax_test.py""],
    deps = ["":stax""],
)

jax_test(
    name = ""minmax_test"",
    srcs = [""tests/minmax_test.py""],
    deps = ["":minmax""],
)

jax_test(
    name = ""lapax_test"",
    srcs = [""tests/lapax_test.py""],
    deps = ["":lapax""],
)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

licenses([""notice""])  # Apache 2

load("":build_defs.bzl"", ""jax_test"")

jax_test(
    name = ""core_test"",
    srcs = [""core_test.py""],
    shard_count = {
        ""cpu"": 5,
    },
)

jax_test(
    name = ""lax_test"",
    srcs = [""lax_test.py""],
    shard_count = {
        ""cpu"": 40,
        ""gpu"": 20,
    },
)

jax_test(
    name = ""lax_numpy_test"",
    srcs = [""lax_numpy_test.py""],
    shard_count = {
        ""cpu"": 40,
        ""gpu"": 20,
    },
)

jax_test(
    name = ""lax_numpy_indexing_test"",
    srcs = [""lax_numpy_indexing_test.py""],
    shard_count = {
        ""cpu"": 10,
        ""gpu"": 2,
    },
)

jax_test(
    name = ""lax_scipy_test"",
    srcs = [""lax_scipy_test.py""],
    shard_count = {
        ""cpu"": 10,
        ""gpu"": 2,
    },
)

jax_test(
    name = ""random_test"",
    srcs = [""random_test.py""],
)

jax_test(
    name = ""api_test"",
    srcs = [""api_test.py""],
)

jax_test(
    name = ""batching_test"",
    srcs = [""batching_test.py""],
)

jax_test(
    name = ""stax_test"",
    srcs = [""stax_test.py""],
    deps = [""//jax:stax""],
)

jax_test(
    name = ""minmax_test"",
    srcs = [""minmax_test.py""],
    deps = [""//jax:minmax""],
)

jax_test(
    name = ""lapax_test"",
    srcs = [""lapax_test.py""],
    deps = [""//jax:lapax""],
)
","diff --git a/tests/BUILD b/tests/BUILD
index 310d71c906ca3dd4c0ae9a29f13ae43bec1b0ec5..95d6ee29e121c29f4866826e98257437896cbad6 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )
",Exception handling / swallowed exceptions,Remove 'tests/' prefix from test file paths and update dependencies to use absolute labels.
51fc713089f8e456d95f79e4ed58687eb72edbfb,"Matthew Johnson: remove absl from examples, fix import statements",examples/interactive.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division

from absl import app

import IPython
import numpy as onp

import jax
import jax.numpy as np
from jax import lax
from jax import random
from jax import jit, grad, vmap, jacfwd, jacrev, hessian


def main(unused_argv):
  IPython.embed(user_ns=dict(globals(), **locals()))

if __name__ == ""__main__"":
  app.run(main)
",,,Index out of range,Minor update
51fc713089f8e456d95f79e4ed58687eb72edbfb,"Matthew Johnson: remove absl from examples, fix import statements",examples/mnist_classifier.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A basic MNIST example using JAX together with the mini-libraries stax, for
neural network building, and minmax, for first-order stochastic optimization.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time
import itertools

from absl import app
import numpy.random as npr

import jax.numpy as np
from jax import jit, grad
from jax.experimental import minmax
from jax.examples import datasets
from jax.experimental import stax
from jax.experimental.stax import Dense, Relu, LogSoftmax


def loss(params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return -np.mean(preds * targets)

def accuracy(params, batch):
  inputs, targets = batch
  target_class = np.argmax(targets, axis=1)
  predicted_class = np.argmax(predict(params, inputs), axis=1)
  return np.mean(predicted_class == target_class)

init_random_params, predict = stax.serial(
    Dense(1024), Relu,
    Dense(1024), Relu,
    Dense(10), LogSoftmax)

def main(unused_argv):
  step_size = 0.001
  num_epochs = 10
  batch_size = 32
  momentum_mass = 0.9

  train_images, train_labels, test_images, test_labels = datasets.mnist()
  num_train = train_images.shape[0]
  num_complete_batches, leftover = divmod(num_train, batch_size)
  num_batches = num_complete_batches + bool(leftover)

  def data_stream():
    rng = npr.RandomState(0)
    while True:
      perm = rng.permutation(num_train)
      for i in range(num_batches):
        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
        yield train_images[batch_idx], train_labels[batch_idx]
  batches = data_stream()

  opt_init, opt_update = minmax.momentum(step_size, mass=momentum_mass)

  @jit
  def update(i, opt_state, batch):
    params = minmax.get_params(opt_state)
    return opt_update(i, grad(loss)(params, batch), opt_state)

  _, init_params = init_random_params((-1, 28 * 28))
  opt_state = opt_init(init_params)
  itercount = itertools.count()

  for epoch in range(num_epochs):
    start_time = time.time()
    for _ in range(num_batches):
      opt_state = update(next(itercount), opt_state, next(batches))
    epoch_time = time.time() - start_time

    params = minmax.get_params(opt_state)
    train_acc = accuracy(params, (train_images, train_labels))
    test_acc = accuracy(params, (test_images, test_labels))
    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
    print(""Training set accuracy {}"".format(train_acc))
    print(""Test set accuracy {}"".format(test_acc))


if __name__ == ""__main__"":
  app.run(main)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A basic MNIST example using JAX together with the mini-libraries stax, for
neural network building, and minmax, for first-order stochastic optimization.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time
import itertools

import numpy.random as npr

import jax.numpy as np
from jax import jit, grad
from jax.experimental import minmax
from jax.experimental import stax
from jax.experimental.stax import Dense, Relu, LogSoftmax
import datasets


def loss(params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return -np.mean(preds * targets)

def accuracy(params, batch):
  inputs, targets = batch
  target_class = np.argmax(targets, axis=1)
  predicted_class = np.argmax(predict(params, inputs), axis=1)
  return np.mean(predicted_class == target_class)

init_random_params, predict = stax.serial(
    Dense(1024), Relu,
    Dense(1024), Relu,
    Dense(10), LogSoftmax)

if __name__ == ""__main__"":
  step_size = 0.001
  num_epochs = 10
  batch_size = 32
  momentum_mass = 0.9

  train_images, train_labels, test_images, test_labels = datasets.mnist()
  num_train = train_images.shape[0]
  num_complete_batches, leftover = divmod(num_train, batch_size)
  num_batches = num_complete_batches + bool(leftover)

  def data_stream():
    rng = npr.RandomState(0)
    while True:
      perm = rng.permutation(num_train)
      for i in range(num_batches):
        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
        yield train_images[batch_idx], train_labels[batch_idx]
  batches = data_stream()

  opt_init, opt_update = minmax.momentum(step_size, mass=momentum_mass)

  @jit
  def update(i, opt_state, batch):
    params = minmax.get_params(opt_state)
    return opt_update(i, grad(loss)(params, batch), opt_state)

  _, init_params = init_random_params((-1, 28 * 28))
  opt_state = opt_init(init_params)
  itercount = itertools.count()

  for epoch in range(num_epochs):
    start_time = time.time()
    for _ in range(num_batches):
      opt_state = update(next(itercount), opt_state, next(batches))
    epoch_time = time.time() - start_time

    params = minmax.get_params(opt_state)
    train_acc = accuracy(params, (train_images, train_labels))
    test_acc = accuracy(params, (test_images, test_labels))
    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
    print(""Training set accuracy {}"".format(train_acc))
    print(""Test set accuracy {}"".format(test_acc))
","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index f88624f9741e49a3222e8903fbe19ab1cd421184..7eba6b9602e1e061139c93007845815744f233f3 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,15 +23,14 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.examples import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):
@@ -50,7 +49,7 @@ init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(10), LogSoftmax)
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)
",Encoding / formatting bug,Refactor MNIST classifier example to remove absl dependency and simplify main function invocation
51fc713089f8e456d95f79e4ed58687eb72edbfb,"Matthew Johnson: remove absl from examples, fix import statements",examples/mnist_classifier_fromscratch.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A basic MNIST example using Numpy and JAX.

The primary aim here is simplicity and minimal dependencies.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time

from absl import app
import numpy.random as npr

from jax.api import jit, grad
from jax.examples import datasets
from jax.scipy.misc import logsumexp
import jax.numpy as np


def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
  return [(scale * rng.randn(m, n), scale * rng.randn(n))
          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]

def predict(params, inputs):
  for w, b in params:
    outputs = np.dot(inputs, w) + b
    inputs = np.tanh(outputs)
  return outputs - logsumexp(outputs, axis=1, keepdims=True)

def loss(params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return -np.mean(preds * targets)

def accuracy(params, batch):
  inputs, targets = batch
  target_class = np.argmax(targets, axis=1)
  predicted_class = np.argmax(predict(params, inputs), axis=1)
  return np.mean(predicted_class == target_class)


def main(unused_argv):
  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
  param_scale = 0.1
  step_size = 0.001
  num_epochs = 10
  batch_size = 32

  train_images, train_labels, test_images, test_labels = datasets.mnist()
  num_train = train_images.shape[0]
  num_complete_batches, leftover = divmod(num_train, batch_size)
  num_batches = num_complete_batches + bool(leftover)

  def data_stream():
    rng = npr.RandomState(0)
    while True:
      perm = rng.permutation(num_train)
      for i in range(num_batches):
        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
        yield train_images[batch_idx], train_labels[batch_idx]
  batches = data_stream()

  @jit
  def update(params, batch):
    grads = grad(loss)(params, batch)
    return [(w - step_size * dw, b - step_size * db)
            for (w, b), (dw, db) in zip(params, grads)]

  params = init_random_params(param_scale, layer_sizes)
  for epoch in range(num_epochs):
    start_time = time.time()
    for _ in range(num_batches):
      params = update(params, next(batches))
    epoch_time = time.time() - start_time

    train_acc = accuracy(params, (train_images, train_labels))
    test_acc = accuracy(params, (test_images, test_labels))
    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
    print(""Training set accuracy {}"".format(train_acc))
    print(""Test set accuracy {}"".format(test_acc))


if __name__ == ""__main__"":
  app.run(main)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A basic MNIST example using Numpy and JAX.

The primary aim here is simplicity and minimal dependencies.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time

import numpy.random as npr

from jax.api import jit, grad
from jax.scipy.misc import logsumexp
import jax.numpy as np
import datasets


def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
  return [(scale * rng.randn(m, n), scale * rng.randn(n))
          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]

def predict(params, inputs):
  for w, b in params:
    outputs = np.dot(inputs, w) + b
    inputs = np.tanh(outputs)
  return outputs - logsumexp(outputs, axis=1, keepdims=True)

def loss(params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return -np.mean(preds * targets)

def accuracy(params, batch):
  inputs, targets = batch
  target_class = np.argmax(targets, axis=1)
  predicted_class = np.argmax(predict(params, inputs), axis=1)
  return np.mean(predicted_class == target_class)


if __name__ == ""__main__"":
  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
  param_scale = 0.1
  step_size = 0.001
  num_epochs = 10
  batch_size = 32

  train_images, train_labels, test_images, test_labels = datasets.mnist()
  num_train = train_images.shape[0]
  num_complete_batches, leftover = divmod(num_train, batch_size)
  num_batches = num_complete_batches + bool(leftover)

  def data_stream():
    rng = npr.RandomState(0)
    while True:
      perm = rng.permutation(num_train)
      for i in range(num_batches):
        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
        yield train_images[batch_idx], train_labels[batch_idx]
  batches = data_stream()

  @jit
  def update(params, batch):
    grads = grad(loss)(params, batch)
    return [(w - step_size * dw, b - step_size * db)
            for (w, b), (dw, db) in zip(params, grads)]

  params = init_random_params(param_scale, layer_sizes)
  for epoch in range(num_epochs):
    start_time = time.time()
    for _ in range(num_batches):
      params = update(params, next(batches))
    epoch_time = time.time() - start_time

    train_acc = accuracy(params, (train_images, train_labels))
    test_acc = accuracy(params, (test_images, test_labels))
    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
    print(""Training set accuracy {}"".format(train_acc))
    print(""Test set accuracy {}"".format(test_acc))
","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 58b660024ad1ccb527866fc8c84a7feaa1495691..2910b1c3bf5bc78aac5fc26b5e0b5e6f5e7bd361 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,13 +23,12 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-from jax.examples import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -54,7 +53,7 @@ def accuracy(params, batch):
   return np.mean(predicted_class == target_class)
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
   param_scale = 0.1
   step_size = 0.001
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)
",Localization / i18n bug,Refactor MNIST classifier example to remove absl dependency and simplify main entry point
51fc713089f8e456d95f79e4ed58687eb72edbfb,"Matthew Johnson: remove absl from examples, fix import statements",examples/mnist_vae.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A basic variational autoencoder (VAE) on binarized MNIST using Numpy and JAX.

This file uses the stax network definition library and the minmax optimization
library.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import time

from absl import app
import matplotlib.pyplot as plt

import jax.numpy as np
from jax import jit, grad, lax, random
from jax.examples import datasets
from jax.experimental import minmax
from jax.experimental import stax
from jax.experimental.stax import Dense, FanOut, Relu, Softplus


def gaussian_kl(mu, sigmasq):
  """"""KL divergence from a diagonal Gaussian to the standard Gaussian.""""""
  return -0.5 * np.sum(1. + np.log(sigmasq) - mu**2. - sigmasq)

def gaussian_sample(rng, mu, sigmasq):
  """"""Sample a diagonal Gaussian.""""""
  return mu + np.sqrt(sigmasq) * random.normal(rng, mu.shape)

def bernoulli_logpdf(logits, x):
  """"""Bernoulli log pdf of data x given logits.""""""
  return -np.sum(np.logaddexp(0., np.where(x, -1., 1.) * logits))

def elbo(rng, params, images):
  """"""Monte Carlo estimate of the negative evidence lower bound.""""""
  enc_params, dec_params = params
  mu_z, sigmasq_z = encode(enc_params, images)
  logits_x = decode(dec_params, gaussian_sample(rng, mu_z, sigmasq_z))
  return bernoulli_logpdf(logits_x, images) - gaussian_kl(mu_z, sigmasq_z)

def image_sample(rng, params, nrow, ncol):
  """"""Sample images from the generative model.""""""
  _, dec_params = params
  code_rng, img_rng = random.split(rng)
  logits = decode(dec_params, random.normal(code_rng, (nrow * ncol, 10)))
  sampled_images = random.bernoulli(img_rng, np.logaddexp(0., logits))
  return image_grid(nrow, ncol, sampled_images, (28, 28))

def image_grid(nrow, ncol, imagevecs, imshape):
  """"""Reshape a stack of image vectors into an image grid for plotting.""""""
  images = iter(imagevecs.reshape((-1,) + imshape))
  return np.vstack([np.hstack([next(images).T for _ in range(ncol)][::-1])
                    for _ in range(nrow)]).T


encoder_init, encode = stax.serial(
    Dense(512), Relu,
    Dense(512), Relu,
    FanOut(2),
    stax.parallel(Dense(10), stax.serial(Dense(10), Softplus)),
)

decoder_init, decode = stax.serial(
    Dense(512), Relu,
    Dense(512), Relu,
    Dense(28 * 28),
)


def main(unused_argv):
  step_size = 0.001
  num_epochs = 100
  batch_size = 32
  nrow, ncol = 10, 10  # sampled image grid size
  rng = random.PRNGKey(0)

  test_rng = random.PRNGKey(1)  # fixed prng key for evaluation
  imfile = os.path.join(os.getenv(""TMPDIR"", ""/tmp/""), ""mnist_vae_{:03d}.png"")

  train_images, _, test_images, _ = datasets.mnist(permute_train=True)
  num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
  num_batches = num_complete_batches + bool(leftover)

  # TODO(mattjj): automatically keep large closed-over consts device-persistent
  train_images = jit(lambda x: x)(train_images)  # dataset on device

  _, init_encoder_params = encoder_init((batch_size, 28 * 28))
  _, init_decoder_params = decoder_init((batch_size, 10))
  init_params = init_encoder_params, init_decoder_params

  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)

  def binarize_batch(rng, i, images):
    i = i % num_batches
    batch = lax.dynamic_slice_in_dim(images, i * batch_size, batch_size)
    return random.bernoulli(rng, batch)

  @jit
  def run_epoch(rng, opt_state, images):
    def body_fun(i, (rng, opt_state, images)):
      rng, elbo_rng, data_rng = random.split(rng, 3)
      batch = binarize_batch(data_rng, i, images)
      loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
      g = grad(loss)(minmax.get_params(opt_state))
      return rng, opt_update(i, g, opt_state), images
    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))

  @jit
  def evaluate(opt_state, images):
    params = minmax.get_params(opt_state)
    elbo_rng, data_rng, image_rng = random.split(test_rng, 3)
    binarized_test = random.bernoulli(data_rng, images)
    test_elbo = elbo(elbo_rng, params, binarized_test) / images.shape[0]
    sampled_images = image_sample(image_rng, params, nrow, ncol)
    return test_elbo, sampled_images

  opt_state = opt_init(init_params)
  for epoch in range(num_epochs):
    tic = time.time()
    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
    test_elbo, images = evaluate(opt_state, test_images)
    print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)


if __name__ == ""__main__"":
  app.run(main)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A basic variational autoencoder (VAE) on binarized MNIST using Numpy and JAX.

This file uses the stax network definition library and the minmax optimization
library.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import time

import matplotlib.pyplot as plt

import jax.numpy as np
from jax import jit, grad, lax, random
from jax.experimental import minmax
from jax.experimental import stax
from jax.experimental.stax import Dense, FanOut, Relu, Softplus
import datasets


def gaussian_kl(mu, sigmasq):
  """"""KL divergence from a diagonal Gaussian to the standard Gaussian.""""""
  return -0.5 * np.sum(1. + np.log(sigmasq) - mu**2. - sigmasq)

def gaussian_sample(rng, mu, sigmasq):
  """"""Sample a diagonal Gaussian.""""""
  return mu + np.sqrt(sigmasq) * random.normal(rng, mu.shape)

def bernoulli_logpdf(logits, x):
  """"""Bernoulli log pdf of data x given logits.""""""
  return -np.sum(np.logaddexp(0., np.where(x, -1., 1.) * logits))

def elbo(rng, params, images):
  """"""Monte Carlo estimate of the negative evidence lower bound.""""""
  enc_params, dec_params = params
  mu_z, sigmasq_z = encode(enc_params, images)
  logits_x = decode(dec_params, gaussian_sample(rng, mu_z, sigmasq_z))
  return bernoulli_logpdf(logits_x, images) - gaussian_kl(mu_z, sigmasq_z)

def image_sample(rng, params, nrow, ncol):
  """"""Sample images from the generative model.""""""
  _, dec_params = params
  code_rng, img_rng = random.split(rng)
  logits = decode(dec_params, random.normal(code_rng, (nrow * ncol, 10)))
  sampled_images = random.bernoulli(img_rng, np.logaddexp(0., logits))
  return image_grid(nrow, ncol, sampled_images, (28, 28))

def image_grid(nrow, ncol, imagevecs, imshape):
  """"""Reshape a stack of image vectors into an image grid for plotting.""""""
  images = iter(imagevecs.reshape((-1,) + imshape))
  return np.vstack([np.hstack([next(images).T for _ in range(ncol)][::-1])
                    for _ in range(nrow)]).T


encoder_init, encode = stax.serial(
    Dense(512), Relu,
    Dense(512), Relu,
    FanOut(2),
    stax.parallel(Dense(10), stax.serial(Dense(10), Softplus)),
)

decoder_init, decode = stax.serial(
    Dense(512), Relu,
    Dense(512), Relu,
    Dense(28 * 28),
)


if __name__ == ""__main__"":
  step_size = 0.001
  num_epochs = 100
  batch_size = 32
  nrow, ncol = 10, 10  # sampled image grid size
  rng = random.PRNGKey(0)

  test_rng = random.PRNGKey(1)  # fixed prng key for evaluation
  imfile = os.path.join(os.getenv(""TMPDIR"", ""/tmp/""), ""mnist_vae_{:03d}.png"")

  train_images, _, test_images, _ = datasets.mnist(permute_train=True)
  num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
  num_batches = num_complete_batches + bool(leftover)

  # TODO(mattjj): automatically keep large closed-over consts device-persistent
  train_images = jit(lambda x: x)(train_images)  # dataset on device

  _, init_encoder_params = encoder_init((batch_size, 28 * 28))
  _, init_decoder_params = decoder_init((batch_size, 10))
  init_params = init_encoder_params, init_decoder_params

  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)

  def binarize_batch(rng, i, images):
    i = i % num_batches
    batch = lax.dynamic_slice_in_dim(images, i * batch_size, batch_size)
    return random.bernoulli(rng, batch)

  @jit
  def run_epoch(rng, opt_state, images):
    def body_fun(i, (rng, opt_state, images)):
      rng, elbo_rng, data_rng = random.split(rng, 3)
      batch = binarize_batch(data_rng, i, images)
      loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
      g = grad(loss)(minmax.get_params(opt_state))
      return rng, opt_update(i, g, opt_state), images
    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))

  @jit
  def evaluate(opt_state, images):
    params = minmax.get_params(opt_state)
    elbo_rng, data_rng, image_rng = random.split(test_rng, 3)
    binarized_test = random.bernoulli(data_rng, images)
    test_elbo = elbo(elbo_rng, params, binarized_test) / images.shape[0]
    sampled_images = image_sample(image_rng, params, nrow, ncol)
    return test_elbo, sampled_images

  opt_state = opt_init(init_params)
  for epoch in range(num_epochs):
    tic = time.time()
    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
    test_elbo, images = evaluate(opt_state, test_images)
    print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 7e9f719bbeaeae9776c177a90d5774cf0642db5c..e97c181e46ff17ada755fc7b877681f788f20f4f 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,15 +25,14 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -84,7 +83,7 @@ decoder_init, decode = stax.serial(
 )
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 100
   batch_size = 32
@@ -138,7 +137,3 @@ def main(unused_argv):
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)
",Incorrect default / config value,Refactor MNIST VAE example to remove absl dependency and simplify main function invocation
51fc713089f8e456d95f79e4ed58687eb72edbfb,"Matthew Johnson: remove absl from examples, fix import statements",examples/resnet50.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A mock-up showing a ResNet50 network with training on synthetic data.

This file uses the stax neural network definition library and the minmax
optimization library.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from absl import app

import numpy.random as npr

import jax.numpy as np
from jax import jit, grad
from jax.experimental import minmax
from jax.experimental import stax
from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                   FanOut, Flatten, GeneralConv, Identity,
                                   MaxPool, Relu, LogSoftmax)


# ResNet blocks compose other layers

def ConvBlock(kernel_size, filters, strides=(2, 2)):
  ks = kernel_size
  filters1, filters2, filters3 = filters
  Main = stax.serial(
      Conv(filters1, (1, 1), strides), BatchNorm(), Relu,
      Conv(filters2, (ks, ks), padding='SAME'), BatchNorm(), Relu,
      Conv(filters3, (1, 1)), BatchNorm())
  Shortcut = stax.serial(Conv(filters3, (1, 1), strides), BatchNorm())
  return stax.serial(FanOut(2), stax.parallel(Main, Shortcut), FanInSum, Relu)


def IdentityBlock(kernel_size, filters):
  ks = kernel_size
  filters1, filters2 = filters
  def make_main(input_shape):
    # the number of output channels depends on the number of input channels
    return stax.serial(
        Conv(filters1, (1, 1)), BatchNorm(), Relu,
        Conv(filters2, (ks, ks), padding='SAME'), BatchNorm(), Relu,
        Conv(input_shape[3], (1, 1)), BatchNorm())
  Main = stax.shape_dependent(make_main)
  return stax.serial(FanOut(2), stax.parallel(Main, Identity), FanInSum, Relu)


# ResNet architectures compose layers and ResNet blocks

def ResNet50(num_classes):
  return stax.serial(
      GeneralConv(('HWCN', 'OIHW', 'NHWC'), 64, (7, 7), (2, 2), 'SAME'),
      BatchNorm(), Relu, MaxPool((3, 3), strides=(2, 2)),
      ConvBlock(3, [64, 64, 256], strides=(1, 1)),
      IdentityBlock(3, [64, 64]),
      IdentityBlock(3, [64, 64]),
      ConvBlock(3, [128, 128, 512]),
      IdentityBlock(3, [128, 128]),
      IdentityBlock(3, [128, 128]),
      IdentityBlock(3, [128, 128]),
      ConvBlock(3, [256, 256, 1024]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      ConvBlock(3, [512, 512, 2048]),
      IdentityBlock(3, [512, 512]),
      IdentityBlock(3, [512, 512]),
      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)


def main(argv):
  del argv  # Unused.

  batch_size = 8
  num_classes = 1001
  input_shape = (224, 224, 3, batch_size)
  step_size = 0.1
  num_steps = 10

  init_fun, predict_fun = ResNet50(num_classes)
  _, init_params = init_fun(input_shape)

  def loss(params, batch):
    inputs, targets = batch
    logits = predict_fun(params, inputs)
    return np.sum(logits * targets)

  def accuracy(params, batch):
    inputs, targets = batch
    target_class = np.argmax(targets, axis=-1)
    predicted_class = np.argmax(predict_fun(params, inputs), axis=-1)
    return np.mean(predicted_class == target_class)

  def synth_batches():
    rng = npr.RandomState(0)
    while True:
      images = rng.rand(*input_shape).astype('float32')
      labels = rng.randint(num_classes, size=(batch_size, 1))
      onehot_labels = labels == np.arange(num_classes)
      yield images, onehot_labels

  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)
  batches = synth_batches()

  @jit
  def update(i, opt_state, batch):
    params = minmax.get_params(opt_state)
    return opt_update(i, grad(loss)(params, batch), opt_state)

  opt_state = opt_init(init_params)
  for i in xrange(num_steps):
    opt_state = update(i, opt_state, next(batches))
  trained_params = minmax.get_params(opt_state)


if __name__ == '__main__':
  app.run(main)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""A mock-up showing a ResNet50 network with training on synthetic data.

This file uses the stax neural network definition library and the minmax
optimization library.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy.random as npr

import jax.numpy as np
from jax import jit, grad
from jax.experimental import minmax
from jax.experimental import stax
from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                   FanOut, Flatten, GeneralConv, Identity,
                                   MaxPool, Relu, LogSoftmax)


# ResNet blocks compose other layers

def ConvBlock(kernel_size, filters, strides=(2, 2)):
  ks = kernel_size
  filters1, filters2, filters3 = filters
  Main = stax.serial(
      Conv(filters1, (1, 1), strides), BatchNorm(), Relu,
      Conv(filters2, (ks, ks), padding='SAME'), BatchNorm(), Relu,
      Conv(filters3, (1, 1)), BatchNorm())
  Shortcut = stax.serial(Conv(filters3, (1, 1), strides), BatchNorm())
  return stax.serial(FanOut(2), stax.parallel(Main, Shortcut), FanInSum, Relu)


def IdentityBlock(kernel_size, filters):
  ks = kernel_size
  filters1, filters2 = filters
  def make_main(input_shape):
    # the number of output channels depends on the number of input channels
    return stax.serial(
        Conv(filters1, (1, 1)), BatchNorm(), Relu,
        Conv(filters2, (ks, ks), padding='SAME'), BatchNorm(), Relu,
        Conv(input_shape[3], (1, 1)), BatchNorm())
  Main = stax.shape_dependent(make_main)
  return stax.serial(FanOut(2), stax.parallel(Main, Identity), FanInSum, Relu)


# ResNet architectures compose layers and ResNet blocks

def ResNet50(num_classes):
  return stax.serial(
      GeneralConv(('HWCN', 'OIHW', 'NHWC'), 64, (7, 7), (2, 2), 'SAME'),
      BatchNorm(), Relu, MaxPool((3, 3), strides=(2, 2)),
      ConvBlock(3, [64, 64, 256], strides=(1, 1)),
      IdentityBlock(3, [64, 64]),
      IdentityBlock(3, [64, 64]),
      ConvBlock(3, [128, 128, 512]),
      IdentityBlock(3, [128, 128]),
      IdentityBlock(3, [128, 128]),
      IdentityBlock(3, [128, 128]),
      ConvBlock(3, [256, 256, 1024]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      IdentityBlock(3, [256, 256]),
      ConvBlock(3, [512, 512, 2048]),
      IdentityBlock(3, [512, 512]),
      IdentityBlock(3, [512, 512]),
      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)


if __name__ == ""__main__"":
  batch_size = 8
  num_classes = 1001
  input_shape = (224, 224, 3, batch_size)
  step_size = 0.1
  num_steps = 10

  init_fun, predict_fun = ResNet50(num_classes)
  _, init_params = init_fun(input_shape)

  def loss(params, batch):
    inputs, targets = batch
    logits = predict_fun(params, inputs)
    return np.sum(logits * targets)

  def accuracy(params, batch):
    inputs, targets = batch
    target_class = np.argmax(targets, axis=-1)
    predicted_class = np.argmax(predict_fun(params, inputs), axis=-1)
    return np.mean(predicted_class == target_class)

  def synth_batches():
    rng = npr.RandomState(0)
    while True:
      images = rng.rand(*input_shape).astype('float32')
      labels = rng.randint(num_classes, size=(batch_size, 1))
      onehot_labels = labels == np.arange(num_classes)
      yield images, onehot_labels

  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)
  batches = synth_batches()

  @jit
  def update(i, opt_state, batch):
    params = minmax.get_params(opt_state)
    return opt_update(i, grad(loss)(params, batch), opt_state)

  opt_state = opt_init(init_params)
  for i in xrange(num_steps):
    opt_state = update(i, opt_state, next(batches))
  trained_params = minmax.get_params(opt_state)
","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24aa87c1932891a4f81dbee55ce63967c5..d44f40e5620bf078ee0f84a9931f6dda53f7c035 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)
",Boundary condition / off-by-one,Remove absl dependency and simplify main function in resnet50.py
6001fd219d04608d832d286887318e1ff296c1b5,"Peter Hawkins: [JAX] Python 3 fix: xla_client.initialize_platform_name() accepts a string, not bytes, so we shouldn't pass it bytes. (The previous Python 3 change was a bit overzealous and made both changes; apparently this path isn't tested by the tests because they pass an explicit --jax_xla_backend flag..)

PiperOrigin-RevId: 222810811",jax/lib/xla_bridge.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Interface and utility functions to XLA.

This module wraps the XLA client(s) and builders to standardize their interfaces
and provide some automatic type mapping logic for converting between Numpy and
XLA. There are also a handful of related casting utilities.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import warnings

from absl import flags
import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy

from . import xla_data_pb2
from . import xla_client


FLAGS = flags.FLAGS
flags.DEFINE_bool('jax_enable_x64', False, 'Enable 64-bit types to be used.')
flags.DEFINE_string('jax_dump_hlo_graph', None, 'Regexp of HLO graphs to dump.')
flags.DEFINE_bool('jax_hlo_profile', False, 'Enables HLO profiling mode.')
flags.DEFINE_string('jax_dump_hlo_unoptimized', None,
                    'Dirpath for unoptimized HLO dump.')
flags.DEFINE_string('jax_dump_hlo_optimized', None,
                    'Dirpath for optimized HLO dump.')
flags.DEFINE_string('jax_dump_hlo_per_pass', None,
                    'Dirpath for per-pass HLO dump.')
flags.DEFINE_integer('jax_replica_count', 1, 'Replica count for computations.')
flags.DEFINE_enum(
    'jax_xla_backend', 'xla', ['xla', 'xrt'],
    'Either ""xla"" for the XLA service directly, or ""xrt"" for an XRT backend.')
flags.DEFINE_string(
    'jax_backend_target', 'local',
    'Either ""local"" or ""rpc:address"" to connect to a remote service target.')
flags.DEFINE_string(
    'jax_platform_name', '',
    'Platform name for XLA. The default is to attempt to use a '
    'GPU if available, but fall back to CPU otherwise. To set '
    'the platform manually, pass ""Host"" for CPU or ""CUDA"" for '
    'GPU.')

# Prefix for HLO-dump flags indicating output should go into Sponge's
# visible output files.
_SPONGE_PREFIX = '/SPONGE/'


def _hlo_path(path, name):
  path = path.replace(_SPONGE_PREFIX,
                      os.getenv('TEST_UNDECLARED_OUTPUTS_DIR', ''))
  path = os.path.join(path, name)
  if not os.path.exists(path):
    os.mkdir(path)
  return path


def get_compile_options():
  """"""Returns the compile options to use, as derived from flag values.""""""
  compile_options = None
  if FLAGS.jax_dump_hlo_graph is not None:
    compile_options = get_xla_client().CompileOptions()
    compile_options.generate_hlo_graph = FLAGS.jax_dump_hlo_graph
  if FLAGS.jax_hlo_profile:
    compile_options = compile_options or get_xla_client().CompileOptions()
    compile_options.hlo_profile = True
  if FLAGS.jax_dump_hlo_unoptimized:
    compile_options = compile_options or get_xla_client().CompileOptions()
    path = _hlo_path(FLAGS.jax_dump_hlo_unoptimized, 'hlo_unoptimized')
    compile_options.dump_unoptimized_hlo_proto_to = path
  if FLAGS.jax_dump_hlo_optimized:
    compile_options = compile_options or get_xla_client().CompileOptions()
    path = _hlo_path(FLAGS.jax_dump_hlo_optimized, 'hlo_optimized')
    compile_options.dump_optimized_hlo_proto_to = path
  if FLAGS.jax_dump_hlo_per_pass:
    compile_options = compile_options or get_xla_client().CompileOptions()
    path = _hlo_path(FLAGS.jax_dump_hlo_per_pass, 'hlo_per_pass')
    compile_options.dump_per_pass_hlo_proto_to = path
  return compile_options


def memoize(func):
  class memodict(dict):
    def __missing__(self, key):
      val = self[key] = func(key)
      return val
  return memodict().__getitem__


def memoize_thunk(func):
  cached = []
  return lambda: cached[0] if cached else (cached.append(func()) or cached[0])


@memoize_thunk
def get_xla_client():
  return _get_xla_client(FLAGS.jax_xla_backend,
                         FLAGS.jax_platform_name,
                         FLAGS.jax_replica_count)


def _get_xla_client(backend_name, platform_name, replica_count):
  """"""Configures and returns a handle to the XLA client.

  Args:
    backend_name: backend name, 'xla' or 'xrt'
    platform_name: platform name for XLA backend
    replica_count: number of computation replicas with which to configure the
      backend library.

  Returns:
    A client library module, or an object that behaves identically to one.
  """"""
  xla_client.initialize_replica_count(replica_count)
  if backend_name == 'xla':
    if platform_name:
      xla_client.initialize_platform_name(platform_name)
    else:
      try:
        xla_client.initialize_platform_name(b'CUDA')
      except RuntimeError:
        warnings.warn('No GPU found, falling back to CPU.')
        xla_client.initialize_platform_name(b'Host')
  return xla_client


def get_replica_count():
  return get_xla_client().get_replica_count()


_backend_flag_to_type = {
    'xla': xla_client.BackendType.XLA_LOCAL,
    'xrt': xla_client.BackendType.XRT,
}


@memoize_thunk
def _get_backend():
  return xla_client.BackendSpec(_backend_flag_to_type[FLAGS.jax_xla_backend],
                                FLAGS.jax_backend_target)


def device_put(pyval):
  # TODO(frostig): Accept a replica id for placement. For now, this places on
  # the first replica only.
  return get_xla_client().LocalBuffer.from_pyval(pyval, backend=_get_backend())


Shape = xla_client.Shape        # pylint: disable=invalid-name


### utility functions

# Similar or identical dtype-to-etype conversion tables exist in the XLA
# clients, but because their organization hasn't been made consistent across
# clients yet, we repeat the information here.
_etype_to_dtype = {
    xla_data_pb2.PRED: onp.dtype('bool'),
    xla_data_pb2.S8: onp.dtype('int8'),
    xla_data_pb2.S16: onp.dtype('int16'),
    xla_data_pb2.S32: onp.dtype('int32'),
    xla_data_pb2.S64: onp.dtype('int64'),
    xla_data_pb2.U8: onp.dtype('uint8'),
    xla_data_pb2.U16: onp.dtype('uint16'),
    xla_data_pb2.U32: onp.dtype('uint32'),
    xla_data_pb2.U64: onp.dtype('uint64'),
    xla_data_pb2.F16: onp.dtype('float16'),
    xla_data_pb2.F32: onp.dtype('float32'),
    xla_data_pb2.F64: onp.dtype('float64'),
    xla_data_pb2.C64: onp.dtype('complex64'),
}

# Note the conversion on the key. Numpy has a known issue wherein dtype hashing
# doesn't work as expected (https://github.com/numpy/numpy/issues/7242). Thus,
# when keying by dtype in this dict, we use the string form of dtypes.
_dtype_to_etype = {str(dt): et for et, dt in _etype_to_dtype.items()}


@memoize
def dtype_to_etype(dtype):
  """"""Convert from dtype to canonical etype (reading FLAGS.jax_enable_x64).""""""
  return _dtype_to_etype[canonicalize_dtype(dtype)]


_dtype_to_32bit_dtype = {
    str(onp.dtype('int64')): onp.dtype('int32'),
    str(onp.dtype('uint64')): onp.dtype('uint32'),
    str(onp.dtype('float64')): onp.dtype('float32'),
    str(onp.dtype('complex128')): onp.dtype('complex64'),
}


def canonicalize_dtype(dtype):
  """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
  # This function is a thin wrapper around the memoized _canonicalize_dtype to
  # handle the case where FLAGS haven't been parsed yet, for example because
  # this function is called at module loading time. This situation can't obtain
  # during tracing and instead can arise when there are module-level constants
  # computed using lax or lax_numpy.
  if FLAGS.is_parsed():
    return _canonicalize_dtype(dtype)
  else:
    return dtype


@memoize
def _canonicalize_dtype(dtype):
  """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
  dtype = onp.dtype(dtype)
  if FLAGS.jax_enable_x64:
    return str(dtype)
  else:
    return str(_dtype_to_32bit_dtype.get(str(dtype), dtype))


@memoize_thunk
def supported_numpy_dtypes():
  return {canonicalize_dtype(dtype) for dtype in _etype_to_dtype.values()}


def canonicalize_shape(shape):
  """"""Given an xla_client.Shape, return a new instance with canonical dtypes.""""""
  if shape.is_tuple():
    return Shape.tuple_shape(tuple(
        canonicalize_shape(s) for s in shape.tuple_shapes()))
  else:
    return Shape.array_shape(
        canonicalize_dtype(shape.element_type()),
        shape.dimensions(), shape.minor_to_major())


# TODO(mattjj,frostig): try to remove this function
def normalize_to_xla_dtypes(val):
  """"""Normalize dtypes in a value.""""""
  if hasattr(val, '__array__') or onp.isscalar(val):
    return onp.asarray(val, dtype=canonicalize_dtype(onp.result_type(val)))
  elif isinstance(val, (tuple, list)):
    return tuple(normalize_to_xla_dtypes(x) for x in val)
  raise TypeError('Can\'t convert to XLA: {}'.format(val))


# TODO(mattjj,frostig): try to remove this function
def shape_of(value):
  """"""Given a Python or XLA value, return its canonicalized XLA Shape.""""""
  if hasattr(value, 'shape') and hasattr(value, 'dtype'):
    return Shape.array_shape(canonicalize_dtype(value.dtype), value.shape)
  elif onp.isscalar(value):
    return shape_of(onp.asarray(value))
  elif isinstance(value, (tuple, list)):
    return Shape.tuple_shape(tuple(shape_of(elt) for elt in value))
  else:
    raise TypeError('Unexpected type: {}'.format(type(value)))


def infeed_put(replica_id, pyval):
  pyval = normalize_to_xla_dtypes(pyval)
  return get_xla_client().transfer_to_infeed(
      pyval, replica_number=replica_id)


class _JaxComputationBuilderBase(object):
  """"""Base class implementing all of JaxComputationBuilder.

  This class is intended to override and augment the interface of an XLA
  ComputationBuilder to form JaxComputationBuilder, as made clear by
  `get_jax_computation_builder_class`, which relies on Python's
  method-resolution order to set up inheritance-like behavior. The class
  inheritance setup is deferred because the choice of the XLA ComputationBuilder
  class is based on the result of `get_xla_client()`. That is, the choice is
  based at least on the setting of flags, which are available only after module
  initialization time.
  """"""
  # The JAXComputationBuilder is implemented using subclassing and inheritance
  # (via this base class), rather than a wrap-and-delegate style, simply to
  # avoid having to spell out all the methods to be forwarded to a wrapped
  # ComputationBuilder, especially since the underlying ComputationBuilders are
  # likely to be revised in the future. An alternative is to generate these
  # forwarding methods programmatically.

  # Method name case follows that of the XLA ComputationBuilder
  # pylint: disable=invalid-name

  def Build(self, *args, **kwargs):
    return super(_JaxComputationBuilderBase, self).Build(
        *args, backend=_get_backend(), **kwargs)

  def Parameter(self, value, name=None, parameter_num=None):
    return super(_JaxComputationBuilderBase, self).ParameterWithShape(
        shape_of(value), name=name, parameter_num=parameter_num)

  def NumpyArrayConstant(self, value):
    normalized_value = normalize_to_xla_dtypes(value)
    return super(_JaxComputationBuilderBase, self).Constant(normalized_value)

  def ConstantLike(self, example_value, value):
    example_value = onp.asarray(example_value)
    return self.Constant(onp.array(value).astype(example_value.dtype))

  def Constant(self, py_val):
    """"""Translate constant `py_val` to a constant for this ComputationBuilder.

    Args:
      py_val: a Python value to be translated to a constant.

    Returns:
      A representation of the constant, either a ComputationDataHandle or None
    """"""
    py_type = type(py_val)
    if py_type in _constant_handlers:
      return _constant_handlers[py_type](self, py_val)
    else:
      raise TypeError(""No constant handler for type: {}"".format(py_type))


@memoize_thunk
def get_jax_computation_builder_class():
  xla_base = get_xla_client().ComputationBuilder
  jax_base = _JaxComputationBuilderBase
  return type('JaxComputationBuilder', (jax_base, xla_base), {})


def make_computation_builder(name):
  return get_jax_computation_builder_class()(name)


def register_constant_handler(type_, handler_fun):
  _constant_handlers[type_] = handler_fun
_constant_handlers = {}


def _ndarray_constant_handler(c, val):
  """"""Constant handler for ndarray literals, handling zero-size strides.

  This function essentially calls c.NumpyArrayConstant(val) except it has
  special handling of arrays with any strides of size zero: for those, it
  generates appropriate calls to NumpyArrayConstant, Broadcast, and Transpose
  to avoid staging in large literals that might arise from np.zeros or np.ones
  or the output of lax.broadcast (which uses onp.broadcast_to which in turn
  uses size-zero strides).

  Args:
    c: XLA client ComputationBuilder.
    val: an ndarray.

  Returns:
    An XLA ComputationDataHandle / XlaOp representing the constant ndarray
    staged into the XLA Computation.
  """"""
  if onp.any(onp.equal(0, val.strides)):
    zero_stride_axes, = onp.where(onp.equal(0, val.strides))
    other_axes, = onp.where(onp.not_equal(0, val.strides))
    collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None)
                              for ax in range(val.ndim))]
    xla_val = c.Broadcast(c.NumpyArrayConstant(collapsed_val),
                          onp.take(val.shape, zero_stride_axes))
    permutation = onp.argsort(tuple(zero_stride_axes) + tuple(other_axes))
    return c.Transpose(xla_val, permutation)
  else:
    return c.NumpyArrayConstant(val)
register_constant_handler(onp.ndarray, _ndarray_constant_handler)


for scalar_type in [onp.int8, onp.int16, onp.int32, onp.int64,
                    onp.uint8, onp.uint16, onp.uint32, onp.uint64,
                    onp.float16, onp.float32, onp.float64, onp.float128,
                    float, int, bool, onp.bool_]:
  register_constant_handler(scalar_type,
                            lambda c, val: c.NumpyArrayConstant(val))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Interface and utility functions to XLA.

This module wraps the XLA client(s) and builders to standardize their interfaces
and provide some automatic type mapping logic for converting between Numpy and
XLA. There are also a handful of related casting utilities.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import warnings

from absl import flags
import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy

from . import xla_data_pb2
from . import xla_client


FLAGS = flags.FLAGS
flags.DEFINE_bool('jax_enable_x64', False, 'Enable 64-bit types to be used.')
flags.DEFINE_string('jax_dump_hlo_graph', None, 'Regexp of HLO graphs to dump.')
flags.DEFINE_bool('jax_hlo_profile', False, 'Enables HLO profiling mode.')
flags.DEFINE_string('jax_dump_hlo_unoptimized', None,
                    'Dirpath for unoptimized HLO dump.')
flags.DEFINE_string('jax_dump_hlo_optimized', None,
                    'Dirpath for optimized HLO dump.')
flags.DEFINE_string('jax_dump_hlo_per_pass', None,
                    'Dirpath for per-pass HLO dump.')
flags.DEFINE_integer('jax_replica_count', 1, 'Replica count for computations.')
flags.DEFINE_enum(
    'jax_xla_backend', 'xla', ['xla', 'xrt'],
    'Either ""xla"" for the XLA service directly, or ""xrt"" for an XRT backend.')
flags.DEFINE_string(
    'jax_backend_target', 'local',
    'Either ""local"" or ""rpc:address"" to connect to a remote service target.')
flags.DEFINE_string(
    'jax_platform_name', '',
    'Platform name for XLA. The default is to attempt to use a '
    'GPU if available, but fall back to CPU otherwise. To set '
    'the platform manually, pass ""Host"" for CPU or ""CUDA"" for '
    'GPU.')

# Prefix for HLO-dump flags indicating output should go into Sponge's
# visible output files.
_SPONGE_PREFIX = '/SPONGE/'


def _hlo_path(path, name):
  path = path.replace(_SPONGE_PREFIX,
                      os.getenv('TEST_UNDECLARED_OUTPUTS_DIR', ''))
  path = os.path.join(path, name)
  if not os.path.exists(path):
    os.mkdir(path)
  return path


def get_compile_options():
  """"""Returns the compile options to use, as derived from flag values.""""""
  compile_options = None
  if FLAGS.jax_dump_hlo_graph is not None:
    compile_options = get_xla_client().CompileOptions()
    compile_options.generate_hlo_graph = FLAGS.jax_dump_hlo_graph
  if FLAGS.jax_hlo_profile:
    compile_options = compile_options or get_xla_client().CompileOptions()
    compile_options.hlo_profile = True
  if FLAGS.jax_dump_hlo_unoptimized:
    compile_options = compile_options or get_xla_client().CompileOptions()
    path = _hlo_path(FLAGS.jax_dump_hlo_unoptimized, 'hlo_unoptimized')
    compile_options.dump_unoptimized_hlo_proto_to = path
  if FLAGS.jax_dump_hlo_optimized:
    compile_options = compile_options or get_xla_client().CompileOptions()
    path = _hlo_path(FLAGS.jax_dump_hlo_optimized, 'hlo_optimized')
    compile_options.dump_optimized_hlo_proto_to = path
  if FLAGS.jax_dump_hlo_per_pass:
    compile_options = compile_options or get_xla_client().CompileOptions()
    path = _hlo_path(FLAGS.jax_dump_hlo_per_pass, 'hlo_per_pass')
    compile_options.dump_per_pass_hlo_proto_to = path
  return compile_options


def memoize(func):
  class memodict(dict):
    def __missing__(self, key):
      val = self[key] = func(key)
      return val
  return memodict().__getitem__


def memoize_thunk(func):
  cached = []
  return lambda: cached[0] if cached else (cached.append(func()) or cached[0])


@memoize_thunk
def get_xla_client():
  return _get_xla_client(FLAGS.jax_xla_backend,
                         FLAGS.jax_platform_name,
                         FLAGS.jax_replica_count)


def _get_xla_client(backend_name, platform_name, replica_count):
  """"""Configures and returns a handle to the XLA client.

  Args:
    backend_name: backend name, 'xla' or 'xrt'
    platform_name: platform name for XLA backend
    replica_count: number of computation replicas with which to configure the
      backend library.

  Returns:
    A client library module, or an object that behaves identically to one.
  """"""
  xla_client.initialize_replica_count(replica_count)
  if backend_name == 'xla':
    if platform_name:
      xla_client.initialize_platform_name(platform_name)
    else:
      try:
        xla_client.initialize_platform_name('CUDA')
      except RuntimeError:
        warnings.warn('No GPU found, falling back to CPU.')
        xla_client.initialize_platform_name('Host')
  return xla_client


def get_replica_count():
  return get_xla_client().get_replica_count()


_backend_flag_to_type = {
    'xla': xla_client.BackendType.XLA_LOCAL,
    'xrt': xla_client.BackendType.XRT,
}


@memoize_thunk
def _get_backend():
  return xla_client.BackendSpec(_backend_flag_to_type[FLAGS.jax_xla_backend],
                                FLAGS.jax_backend_target)


def device_put(pyval):
  # TODO(frostig): Accept a replica id for placement. For now, this places on
  # the first replica only.
  return get_xla_client().LocalBuffer.from_pyval(pyval, backend=_get_backend())


Shape = xla_client.Shape        # pylint: disable=invalid-name


### utility functions

# Similar or identical dtype-to-etype conversion tables exist in the XLA
# clients, but because their organization hasn't been made consistent across
# clients yet, we repeat the information here.
_etype_to_dtype = {
    xla_data_pb2.PRED: onp.dtype('bool'),
    xla_data_pb2.S8: onp.dtype('int8'),
    xla_data_pb2.S16: onp.dtype('int16'),
    xla_data_pb2.S32: onp.dtype('int32'),
    xla_data_pb2.S64: onp.dtype('int64'),
    xla_data_pb2.U8: onp.dtype('uint8'),
    xla_data_pb2.U16: onp.dtype('uint16'),
    xla_data_pb2.U32: onp.dtype('uint32'),
    xla_data_pb2.U64: onp.dtype('uint64'),
    xla_data_pb2.F16: onp.dtype('float16'),
    xla_data_pb2.F32: onp.dtype('float32'),
    xla_data_pb2.F64: onp.dtype('float64'),
    xla_data_pb2.C64: onp.dtype('complex64'),
}

# Note the conversion on the key. Numpy has a known issue wherein dtype hashing
# doesn't work as expected (https://github.com/numpy/numpy/issues/7242). Thus,
# when keying by dtype in this dict, we use the string form of dtypes.
_dtype_to_etype = {str(dt): et for et, dt in _etype_to_dtype.items()}


@memoize
def dtype_to_etype(dtype):
  """"""Convert from dtype to canonical etype (reading FLAGS.jax_enable_x64).""""""
  return _dtype_to_etype[canonicalize_dtype(dtype)]


_dtype_to_32bit_dtype = {
    str(onp.dtype('int64')): onp.dtype('int32'),
    str(onp.dtype('uint64')): onp.dtype('uint32'),
    str(onp.dtype('float64')): onp.dtype('float32'),
    str(onp.dtype('complex128')): onp.dtype('complex64'),
}


def canonicalize_dtype(dtype):
  """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
  # This function is a thin wrapper around the memoized _canonicalize_dtype to
  # handle the case where FLAGS haven't been parsed yet, for example because
  # this function is called at module loading time. This situation can't obtain
  # during tracing and instead can arise when there are module-level constants
  # computed using lax or lax_numpy.
  if FLAGS.is_parsed():
    return _canonicalize_dtype(dtype)
  else:
    return dtype


@memoize
def _canonicalize_dtype(dtype):
  """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
  dtype = onp.dtype(dtype)
  if FLAGS.jax_enable_x64:
    return str(dtype)
  else:
    return str(_dtype_to_32bit_dtype.get(str(dtype), dtype))


@memoize_thunk
def supported_numpy_dtypes():
  return {canonicalize_dtype(dtype) for dtype in _etype_to_dtype.values()}


def canonicalize_shape(shape):
  """"""Given an xla_client.Shape, return a new instance with canonical dtypes.""""""
  if shape.is_tuple():
    return Shape.tuple_shape(tuple(
        canonicalize_shape(s) for s in shape.tuple_shapes()))
  else:
    return Shape.array_shape(
        canonicalize_dtype(shape.element_type()),
        shape.dimensions(), shape.minor_to_major())


# TODO(mattjj,frostig): try to remove this function
def normalize_to_xla_dtypes(val):
  """"""Normalize dtypes in a value.""""""
  if hasattr(val, '__array__') or onp.isscalar(val):
    return onp.asarray(val, dtype=canonicalize_dtype(onp.result_type(val)))
  elif isinstance(val, (tuple, list)):
    return tuple(normalize_to_xla_dtypes(x) for x in val)
  raise TypeError('Can\'t convert to XLA: {}'.format(val))


# TODO(mattjj,frostig): try to remove this function
def shape_of(value):
  """"""Given a Python or XLA value, return its canonicalized XLA Shape.""""""
  if hasattr(value, 'shape') and hasattr(value, 'dtype'):
    return Shape.array_shape(canonicalize_dtype(value.dtype), value.shape)
  elif onp.isscalar(value):
    return shape_of(onp.asarray(value))
  elif isinstance(value, (tuple, list)):
    return Shape.tuple_shape(tuple(shape_of(elt) for elt in value))
  else:
    raise TypeError('Unexpected type: {}'.format(type(value)))


def infeed_put(replica_id, pyval):
  pyval = normalize_to_xla_dtypes(pyval)
  return get_xla_client().transfer_to_infeed(
      pyval, replica_number=replica_id)


class _JaxComputationBuilderBase(object):
  """"""Base class implementing all of JaxComputationBuilder.

  This class is intended to override and augment the interface of an XLA
  ComputationBuilder to form JaxComputationBuilder, as made clear by
  `get_jax_computation_builder_class`, which relies on Python's
  method-resolution order to set up inheritance-like behavior. The class
  inheritance setup is deferred because the choice of the XLA ComputationBuilder
  class is based on the result of `get_xla_client()`. That is, the choice is
  based at least on the setting of flags, which are available only after module
  initialization time.
  """"""
  # The JAXComputationBuilder is implemented using subclassing and inheritance
  # (via this base class), rather than a wrap-and-delegate style, simply to
  # avoid having to spell out all the methods to be forwarded to a wrapped
  # ComputationBuilder, especially since the underlying ComputationBuilders are
  # likely to be revised in the future. An alternative is to generate these
  # forwarding methods programmatically.

  # Method name case follows that of the XLA ComputationBuilder
  # pylint: disable=invalid-name

  def Build(self, *args, **kwargs):
    return super(_JaxComputationBuilderBase, self).Build(
        *args, backend=_get_backend(), **kwargs)

  def Parameter(self, value, name=None, parameter_num=None):
    return super(_JaxComputationBuilderBase, self).ParameterWithShape(
        shape_of(value), name=name, parameter_num=parameter_num)

  def NumpyArrayConstant(self, value):
    normalized_value = normalize_to_xla_dtypes(value)
    return super(_JaxComputationBuilderBase, self).Constant(normalized_value)

  def ConstantLike(self, example_value, value):
    example_value = onp.asarray(example_value)
    return self.Constant(onp.array(value).astype(example_value.dtype))

  def Constant(self, py_val):
    """"""Translate constant `py_val` to a constant for this ComputationBuilder.

    Args:
      py_val: a Python value to be translated to a constant.

    Returns:
      A representation of the constant, either a ComputationDataHandle or None
    """"""
    py_type = type(py_val)
    if py_type in _constant_handlers:
      return _constant_handlers[py_type](self, py_val)
    else:
      raise TypeError(""No constant handler for type: {}"".format(py_type))


@memoize_thunk
def get_jax_computation_builder_class():
  xla_base = get_xla_client().ComputationBuilder
  jax_base = _JaxComputationBuilderBase
  return type('JaxComputationBuilder', (jax_base, xla_base), {})


def make_computation_builder(name):
  return get_jax_computation_builder_class()(name)


def register_constant_handler(type_, handler_fun):
  _constant_handlers[type_] = handler_fun
_constant_handlers = {}


def _ndarray_constant_handler(c, val):
  """"""Constant handler for ndarray literals, handling zero-size strides.

  This function essentially calls c.NumpyArrayConstant(val) except it has
  special handling of arrays with any strides of size zero: for those, it
  generates appropriate calls to NumpyArrayConstant, Broadcast, and Transpose
  to avoid staging in large literals that might arise from np.zeros or np.ones
  or the output of lax.broadcast (which uses onp.broadcast_to which in turn
  uses size-zero strides).

  Args:
    c: XLA client ComputationBuilder.
    val: an ndarray.

  Returns:
    An XLA ComputationDataHandle / XlaOp representing the constant ndarray
    staged into the XLA Computation.
  """"""
  if onp.any(onp.equal(0, val.strides)):
    zero_stride_axes, = onp.where(onp.equal(0, val.strides))
    other_axes, = onp.where(onp.not_equal(0, val.strides))
    collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None)
                              for ax in range(val.ndim))]
    xla_val = c.Broadcast(c.NumpyArrayConstant(collapsed_val),
                          onp.take(val.shape, zero_stride_axes))
    permutation = onp.argsort(tuple(zero_stride_axes) + tuple(other_axes))
    return c.Transpose(xla_val, permutation)
  else:
    return c.NumpyArrayConstant(val)
register_constant_handler(onp.ndarray, _ndarray_constant_handler)


for scalar_type in [onp.int8, onp.int16, onp.int32, onp.int64,
                    onp.uint8, onp.uint16, onp.uint32, onp.uint64,
                    onp.float16, onp.float32, onp.float64, onp.float128,
                    float, int, bool, onp.bool_]:
  register_constant_handler(scalar_type,
                            lambda c, val: c.NumpyArrayConstant(val))
","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index f5b9cf1536bc61925c03ff479b11ab55fa3e414e..442a1f935bc66ee61abcb30ae83d9754e4273a6c 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -133,10 +133,10 @@ def _get_xla_client(backend_name, platform_name, replica_count):
       xla_client.initialize_platform_name(platform_name)
     else:
       try:
-        xla_client.initialize_platform_name(b'CUDA')
+        xla_client.initialize_platform_name('CUDA')
       except RuntimeError:
         warnings.warn('No GPU found, falling back to CPU.')
-        xla_client.initialize_platform_name(b'Host')
+        xla_client.initialize_platform_name('Host')
   return xla_client
 
 
",Feature flag / config toggle bug,Fix XLA client initialization by passing platform names as strings instead of bytes.
1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Matthew Johnson: fix handling of symbolic zeros for a few special primitives

PiperOrigin-RevId: 223264329",jax/lax.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
#   assert False  # TODO

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')
  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])

  starts = onp.zeros((len(operands), t.ndim), dtype=int)
  starts[1:, dimension] = limit_points[:-1]
  limits = onp.tile(t.shape, (len(operands), 1))
  limits[:, dimension] = limit_points

  return [slice(t, start, limit) if o is None else None
          for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices)]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g =tangents[0]
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
  values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
  tangents_out = keys_tangents_out, values_tangents_out

  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(fun_name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(fun_name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(fun_name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(fun_name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(fun_name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
#   assert False  # TODO

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')
  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])

  starts = onp.zeros((len(operands), t.ndim), dtype=int)
  starts[1:, dimension] = limit_points[:-1]
  limits = onp.tile(t.shape, (len(operands), 1))
  limits[:, dimension] = limit_points

  return [slice(t, start, limit) if o is None else None
          for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices)]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(fun_name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(fun_name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(fun_name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(fun_name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(fun_name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","diff --git a/jax/lax.py b/jax/lax.py
index 122fd6cbfcbe768d9142863db99e85862c8c05a9..5666cf2121e0392d7fe3c1be72e5e8fd1d416f08 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1678,7 +1678,12 @@ def dynamic_update_slice_jvp(primals, tangents, update_shape):
   g_operand, g_update, g_start_indices = tangents
   assert g_start_indices is ad_util.zero
   val_out = dynamic_update_slice(operand, update, start_indices)
-  tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
+  if g_operand is ad_util.zero and g_update is ad_util.zero:
+    tangent_out = ad_util.zero
+  else:
+    g_operand = ad.instantiate_zeros(operand, g_operand)
+    g_update = ad.instantiate_zeros(update, g_update)
+    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
   return val_out, tangent_out
 
 def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
@@ -1717,7 +1722,7 @@ def index_take_translation_rule(c, src, *idxs, **kwargs):
 def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
   src = primals[0]
   idxs = tuple(primals[1:])
-  g =tangents[0]
+  g = ad.instantiate_zeros(src, tangents[0])
   return index_take(src, idxs, axes), index_take(g, idxs, axes)
 
 def index_take_transpose_rule(t, src, *idxs, **kwargs):
@@ -1747,6 +1752,8 @@ def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
   src, dst = primals[0], primals[1]
   idxs = tuple(primals[2:])
   g_src, g_dst = tangents[0], tangents[1]
+  g_src = ad.instantiate_zeros(src, g_src)
+  g_dst = ad.instantiate_zeros(dst, g_dst)
   val_out = index_untake(src, dst, idxs, axes)
   tangent_out = index_untake(g_src, g_dst, idxs, axes)
   return val_out, tangent_out
@@ -2064,10 +2071,17 @@ def sort_key_val_jvp(primals, tangents, dimension):
 
   val_out = sort_key_val(keys, values, dimension)
 
-  keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
-  values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
-  tangents_out = keys_tangents_out, values_tangents_out
+  if keys_tangents is ad_util.zero:
+    keys_tangents_out = ad_util.zero
+  else:
+    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
 
+  if values_tangents is ad_util.zero:
+    values_tangents_out = ad_util.zero
+  else:
+    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
+
+  tangents_out = keys_tangents_out, values_tangents_out
   return core.pack(val_out), core.pack(tangents_out)
 
 def sort_key_val_transpose_rule(t, keys, values, dimension):
",Dependency / version mismatch,Improve JVP rules for lax operations by instantiating zeros and handling zero tangents.
1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Matthew Johnson: fix handling of symbolic zeros for a few special primitives

PiperOrigin-RevId: 223264329",tests/lax_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
from functools import partial
import itertools

from absl import flags
from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from jax import api
from jax import core
from jax import lax
from jax import test_util as jtu
from jax import lax_reference
from jax.interpreters import xla
from jax.lib import xla_bridge

FLAGS = flags.FLAGS


def num_float_bits(dtype):
  return onp.finfo(xla_bridge.canonicalize_dtype(dtype)).bits


### lax tests

# For standard unops and binops, we can generate a large number of tests on
# arguments of appropriate shapes and dtypes using the following table.

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
all_dtypes = float_dtypes + complex_dtypes + int_dtypes + bool_dtypes

compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]

OpRecord = collections.namedtuple(""OpRecord"",
                                  [""op"", ""nargs"", ""dtypes"", ""rng"", ""tol""])


def op_record(op, nargs, dtypes, rng, tol=1e-5):
  return OpRecord(op, nargs, dtypes, rng, tol)

LAX_OPS = [
    op_record(lax.neg, 1, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.sign, 1, default_dtypes, jtu.rand_small()),
    op_record(lax.floor, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.ceil, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.round, 1, float_dtypes, jtu.rand_default()),

    op_record(lax.is_finite, 1, float_dtypes, jtu.rand_small()),

    op_record(lax.exp, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.expm1, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.log, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.log1p, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.tanh, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.sin, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.cos, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.atan2, 2, float_dtypes, jtu.rand_default()),

    op_record(lax.sqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.rsqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.square, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.reciprocal, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.tan, 1, float_dtypes, jtu.rand_default()),
    op_record(lax.asin, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.acos, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.atan, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.sinh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.cosh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.asinh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.acosh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),

    op_record(lax.lgamma, 1, float_dtypes, jtu.rand_positive()),
    op_record(lax.digamma, 1, float_dtypes, jtu.rand_positive()),
    op_record(lax.erf, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.erfc, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.erf_inv, 1, float_dtypes, jtu.rand_small(), tol=1e-2),

    op_record(lax.real, 1, complex_dtypes, jtu.rand_default()),
    op_record(lax.imag, 1, complex_dtypes, jtu.rand_default()),
    op_record(lax.complex, 2, [onp.float32], jtu.rand_default()),
    op_record(lax.conj, 1, [onp.float32] + complex_dtypes, jtu.rand_default()),
    op_record(lax.abs, 1, default_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.pow, 2, float_dtypes + complex_dtypes, jtu.rand_positive()),

    op_record(lax.bitwise_and, 2, bool_dtypes, jtu.rand_small()),
    op_record(lax.bitwise_not, 1, bool_dtypes, jtu.rand_small()),
    op_record(lax.bitwise_or, 2, bool_dtypes, jtu.rand_small()),
    op_record(lax.bitwise_xor, 2, bool_dtypes, jtu.rand_small()),

    op_record(lax.add, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.sub, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.mul, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.div, 2, default_dtypes + complex_dtypes, jtu.rand_nonzero()),
    op_record(lax.rem, 2, default_dtypes, jtu.rand_nonzero()),

    op_record(lax.max, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.min, 2, default_dtypes, jtu.rand_small()),

    op_record(lax.eq, 2, all_dtypes, jtu.rand_some_equal()),
    op_record(lax.ne, 2, all_dtypes, jtu.rand_small()),
    op_record(lax.ge, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.gt, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.le, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.lt, 2, default_dtypes, jtu.rand_small()),
]

CombosWithReplacement = itertools.combinations_with_replacement


class LaxTest(jtu.JaxTestCase):
  """"""Numerical tests for LAX operations.""""""

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(
          rec.op.__name__, shapes, itertools.repeat(dtype)),
       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
      for rec in LAX_OPS
      for shape_group in compatible_shapes
      for shapes in CombosWithReplacement(shape_group, rec.nargs)
      for dtype in rec.dtypes)
  def testOp(self, op, rng, shapes, dtype):
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(
          rec.op.__name__, shapes, itertools.repeat(dtype)),
       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
       ""tol"": rec.tol}
      for rec in LAX_OPS
      for shape_group in compatible_shapes
      for shapes in CombosWithReplacement(shape_group, rec.nargs)
      for dtype in rec.dtypes)
  def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    numpy_op = getattr(lax_reference, op.__name__)
    self._CheckAgainstNumpy(op, numpy_op, args_maker, tol=tol)

  # TODO test shift_left, shift_right_arithmetic, shift_right_logical

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
          from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testConvertElementType(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.convert_element_type(x, to_dtype)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
       .format(from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.convert_element_type(x, to_dtype)
    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
       .format(from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testBitcastConvertType(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
       .format(from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
          jtu.format_shape_dtype_string(min_shape, dtype),
          jtu.format_shape_dtype_string(operand_shape, dtype),
          jtu.format_shape_dtype_string(max_shape, dtype)),
       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
      for min_shape, operand_shape, max_shape in [
          [(), (2, 3), ()],
          [(2, 3), (2, 3), ()],
          [(), (2, 3), (2, 3)],
          [(2, 3), (2, 3), (2, 3)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
    shapes = [min_shape, operand_shape, max_shape]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
          jtu.format_shape_dtype_string(min_shape, dtype),
          jtu.format_shape_dtype_string(operand_shape, dtype),
          jtu.format_shape_dtype_string(max_shape, dtype)),
       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
      for min_shape, operand_shape, max_shape in [
          [(), (2, 3), ()],
          [(2, 3), (2, 3), ()],
          [(), (2, 3), (2, 3)],
          [(2, 3), (2, 3), (2, 3)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
                            rng):
    shapes = [min_shape, operand_shape, max_shape]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
          num_arrs),
       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
       ""num_arrs"": num_arrs, ""rng"": rng}
      for num_arrs in [3]
      for dtype in default_dtypes
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for dim in range(len(base_shape))
      for rng in [jtu.rand_default()])
  def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    op = lambda *args: lax.concatenate(args, dim)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
          num_arrs),
       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
       ""num_arrs"": num_arrs, ""rng"": rng}
      for num_arrs in [3]
      for dtype in default_dtypes
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for dim in range(len(base_shape))
      for rng in [jtu.rand_default()])
  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    op = lambda *args: lax.concatenate(args, dim)
    numpy_op = lambda *args: lax_reference.concatenate(args, dim)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
          ""strides"": strides, ""padding"": padding, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([2, 3], repeat=3)]
      for dtype in [onp.float32]
      for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_small()])
  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.conv(lhs, rhs, strides, padding)

    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
          ""strides"": strides, ""padding"": padding, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([2, 3], repeat=3)]
      for dtype in [onp.float32]
      for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_small()])
  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
                           rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)
    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       ""_lhs_dilation={}_rhs_dilation={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype),
           strides, padding, lhs_dilation, rhs_dilation),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
      for lhs_dilation, rhs_dilation in itertools.product(
          [(1, 1), (1, 2), (2, 2)], repeat=2)
      for rng in [jtu.rand_small()])
  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
                                 padding, lhs_dilation, rhs_dilation, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.conv_with_general_padding(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)

    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       ""_lhs_dilation={}_rhs_dilation={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype),
           strides, padding, lhs_dilation, rhs_dilation),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
      for lhs_dilation, rhs_dilation in itertools.product(
          [(1, 1), (1, 2), (2, 2)], repeat=2)
      for rng in [jtu.rand_small()])
  def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
      rhs_dilation, rng):
    # TODO(mattjj): make this test pass
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.conv_with_general_padding(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)

    def numpy_fun(lhs, rhs):
      return lax_reference.conv_with_general_padding(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)

    self._CheckAgainstNumpy(fun, numpy_fun, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       ""_lhs_dilation={}_rhs_dilation={}""
       ""_dims={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype),
           strides, padding, lhs_dilation, rhs_dilation,
           "","".join(dim_nums)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
       ""rhs_dilation"": rhs_dilation, ""dimension_numbers"": dim_nums,
       ""perms"": perms, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([2, 3], repeat=3)]
      for dtype in [onp.float32] for strides in [(1, 1), (2, 1)]
      for padding in [((1, 2), (2, 0))]
      for lhs_dilation, rhs_dilation in itertools.product(
          [(1, 1), (1, 2)], repeat=2)
      for rng in [jtu.rand_small()]
      for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
                              ([0, 1, 2, 3], [0, 1, 2, 3])),
                             ((""NHWC"", ""HWIO"", ""NHWC""),
                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
                             padding, lhs_dilation, rhs_dilation,
                             dimension_numbers, perms, rng):
    lhs_perm, rhs_perm = perms  # permute to compatible shapes

    def args_maker():
      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),
              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]

    def fun(lhs, rhs):
      return lax.conv_general_dilated(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,
          dimension_numbers)

    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  # TODO(mattjj): test conv_general_dilated against numpy

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
          jtu.format_shape_dtype_string(lhs_shape, dtype),
          jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": rng}
      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDot(self, lhs_shape, rhs_shape, dtype, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
          jtu.format_shape_dtype_string(lhs_shape, dtype),
          jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": rng}
      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               lhs_contracting, rhs_contracting),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""lhs_contracting"": lhs_contracting, ""rhs_contracting"": rhs_contracting,
       ""rng"": rng}
      for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [
          # these all fail with ""RuntimeError: Unimplemented: Dot with
          # non-standard contracting dimensions not implemented.""
          # [(3, 5), (2, 5), [1], [1]],
          # [(5, 3), (5, 2), [0], [0]],
          # [(5, 3, 2), (5, 2, 4), [0], [0]],
          # [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],
          # [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],
          [(3, 2), (2, 4), [1], [0]],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
                                 lhs_contracting, rhs_contracting, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))

    def fun(lhs, rhs):
      return lax.dot_general(lhs, rhs, dimension_numbers)

    self._CompileAndCheck(fun, args_maker, check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               dimension_numbers),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
      for lhs_shape, rhs_shape, dimension_numbers in [
          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
                                     dimension_numbers, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.dot_general(lhs, rhs, dimension_numbers)

    self._CompileAndCheck(fun, args_maker, check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               dimension_numbers),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
      for lhs_shape, rhs_shape, dimension_numbers in [
          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
                                 dimension_numbers, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)
    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
          shape, onp.dtype(dtype).name, broadcast_sizes),
       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
       ""rng"": rng}
      for shape in [(), (2, 3)]
      for dtype in default_dtypes
      for broadcast_sizes in [(), (2,), (1, 2)]
      for rng in [jtu.rand_default()])
  def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.broadcast(x, broadcast_sizes)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
       ""rng"": rng}
      for shape in [(), (2, 3)]
      for dtype in default_dtypes
      for broadcast_sizes in [(), (2,), (1, 2)]
      for rng in [jtu.rand_default()])
  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.broadcast(x, broadcast_sizes)
    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
          jtu.format_shape_dtype_string(inshape, dtype),
          outshape, broadcast_dimensions),
       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
       ""dimensions"": broadcast_dimensions, ""rng"": rng}
      for inshape, outshape, broadcast_dimensions in [
          ([2], [2, 2], [0]),
          ([2], [2, 2], [1]),
          ([2], [2, 3], [0]),
          ([], [2, 3], []),
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
    args_maker = lambda: [rng(inshape, dtype)]
    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
          jtu.format_shape_dtype_string(inshape, dtype),
          outshape, broadcast_dimensions),
       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
       ""dimensions"": broadcast_dimensions, ""rng"": rng}
      for inshape, outshape, broadcast_dimensions in [
          ([2], [2, 2], [0]),
          ([2], [2, 2], [1]),
          ([2], [2, 3], [0]),
          ([], [2, 3], []),
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
                                     dimensions, rng):
    args_maker = lambda: [rng(inshape, dtype)]
    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": rng}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
      ]
      for rng in [jtu.rand_default()])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    args_maker = lambda: [rng(arg_shape, dtype)]
    op = lambda x: lax.reshape(x, out_shape)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": rng}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
      ]
      for rng in [jtu.rand_default()])
  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
    args_maker = lambda: [rng(arg_shape, dtype)]
    op = lambda x: lax.reshape(x, out_shape)
    numpy_op = lambda x: lax_reference.reshape(x, out_shape)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_pads={}""
       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
      for shape in [(2, 3)]
      for dtype in default_dtypes
      for pads in [[(1, 2, 1), (0, 1, 0)]])
  def testPad(self, shape, dtype, pads, rng):
    args_maker = lambda: [rng(shape, dtype)]
    fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_pads={}""
       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
      for shape in [(2, 3)]
      for dtype in default_dtypes
      for pads in [[(1, 2, 1), (0, 1, 0)]])
  def testPadAgainstNumpy(self, shape, dtype, pads, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
    numpy_op = lambda x: lax_reference.pad(x, onp.array(0, dtype), pads)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  def testReverse(self):
    rev = api.jit(lambda operand: lax.rev(operand, dimensions))

    dimensions = [0]
    self.assertAllClose(onp.array([3, 2, 1]), rev(onp.array([1, 2, 3])),
                        check_dtypes=False)

    dimensions = [0, 1]
    self.assertAllClose(onp.array([[6, 5, 4], [3, 2, 1]]),
                        rev(onp.array([[1, 2, 3], [4, 5, 6]])),
                        check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
       ""rng"": rng}
      for arg_shape in [(), (3,), (2, 3)]
      for pred_shape in ([(), arg_shape] if arg_shape else [()])
      for arg_dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):

    def args_maker():
      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
              rng(arg_shape, arg_dtype)]

    return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
       ""rng"": rng}
      for arg_shape in [(), (3,), (2, 3)]
      for pred_shape in ([(), arg_shape] if arg_shape else [()])
      for arg_dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):

    def args_maker():
      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
              rng(arg_shape, arg_dtype)]

    return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, limit_indices, strides),
       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
      for shape, start_indices, limit_indices, strides in [
        [(3,), (1,), (2,), None],
        [(7,), (4,), (7,), None],
        [(5,), (1,), (5,), (2,)],
        [(8,), (1,), (6,), (2,)],
        [(5, 3), (1, 1), (3, 2), None],
        [(5, 3), (1, 1), (3, 1), None],
        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
        [(5, 3), (1, 1), (2, 1), (1, 1)],
        [(5, 3), (1, 1), (5, 3), (2, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSlice(self, shape, dtype, starts, limits, strides, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.slice(x, starts, limits, strides)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, limit_indices, strides),
       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
      for shape, start_indices, limit_indices, strides in [
        [(3,), (1,), (2,), None],
        [(7,), (4,), (7,), None],
        [(5,), (1,), (5,), (2,)],
        [(8,), (1,), (6,), (2,)],
        [(5, 3), (1, 1), (3, 2), None],
        [(5, 3), (1, 1), (3, 1), None],
        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
        [(5, 3), (1, 1), (2, 1), (1, 1)],
        [(5, 3), (1, 1), (5, 3), (2, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
                            strides, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.slice(x, starts, limits, strides)
    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, size_indices),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""size_indices"": size_indices, ""rng"": rng}
      for shape, start_indices, size_indices in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, size_indices),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""size_indices"": size_indices, ""rng"": rng}
      for shape, start_indices, size_indices in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
                                   size_indices, rng):
    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)
    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, update_shape),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""update_shape"": update_shape, ""rng"": rng}
      for shape, start_indices, update_shape in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
                             rng):

    def args_maker():
      return [rng(shape, dtype), rng(update_shape, dtype),
              onp.array(start_indices)]

    self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
                          check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, update_shape),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""update_shape"": update_shape, ""rng"": rng}
      for shape, start_indices, update_shape in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
                             rng):

    def args_maker():
      return [rng(shape, dtype), rng(update_shape, dtype),
              onp.array(start_indices)]

    self._CheckAgainstNumpy(lax.dynamic_update_slice,
                            lax_reference.dynamic_update_slice, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_perm={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), perm),
       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
      for shape, perm in [
        [(3, 4), (1, 0)],
        [(3, 4), (0, 1)],
        [(3, 4, 5), (2, 1, 0)],
        [(3, 4, 5), (1, 0, 2)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testTranspose(self, shape, dtype, perm, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.transpose(x, perm)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_perm={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), perm),
       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
      for shape, perm in [
        [(3, 4), (1, 0)],
        [(3, 4), (0, 1)],
        [(3, 4, 5), (2, 1, 0)],
        [(3, 4, 5), (1, 0, 2)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.transpose(x, perm)
    numpy_op = lambda x: lax_reference.transpose(x, perm)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
       ""dims"": dims, ""rng"": rng}
      for init_val, op, dtypes in [
          (0, lax.add, default_dtypes),
          (-onp.inf, lax.max, float_dtypes),
          (onp.iinfo(onp.int32).min, lax.max, [onp.int32]),
          (onp.iinfo(onp.int64).min, lax.max, [onp.int64]),
          (onp.iinfo(onp.uint32).min, lax.max, [onp.uint32]),
          (onp.iinfo(onp.uint64).min, lax.max, [onp.uint64]),
          (onp.inf, lax.min, float_dtypes),
          (onp.iinfo(onp.int32).max, lax.min, [onp.int32]),
          (onp.iinfo(onp.int64).max, lax.min, [onp.int64]),
          (onp.iinfo(onp.uint32).max, lax.min, [onp.uint32]),
          (onp.iinfo(onp.uint64).max, lax.min, [onp.uint64]),
      ]
      for dtype in dtypes
      for shape, dims in [
          [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
          [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
      ]
      for rng in [jtu.rand_small()])
  def testReduce(self, op, init_val, shape, dtype, dims, rng):
    init_val = onp.asarray(init_val, dtype=dtype)
    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
    args_maker = lambda: [rng(shape, dtype), init_val]
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

    # we separately test the version that uses a concrete init_val because it
    # can hit different code paths
    fun = lambda operand: lax.reduce(operand, init_val, op, dims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_dtype={}_padding={}""
       .format(op.__name__, onp.dtype(dtype).name, padding),
       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
       ""rng"": rng}
      for init_val, op, dtypes in [
          (0, lax.add, [onp.float32]),
          (-onp.inf, lax.max, [onp.float32]),
          (onp.inf, lax.min, [onp.float32]),
      ]
      for dtype in dtypes
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_small()])
  def testReduceWindow(self, op, init_val, dtype, padding, rng):
    init_val = onp.asarray(init_val, dtype=dtype)

    # We need this conditional and the corresponding loop logic to be in the
    # test method, rather than at the parameterized test level, because it
    # depends on FLAGS for the device under test.
    if FLAGS.jax_test_dut == ""tpu"":
      all_configs = [((4, 6), (2, 1), (1, 2))]
    else:
      all_configs = itertools.chain(
          itertools.product(
              [(4, 6)],
              [(2, 1), (1, 2)],
              [(1, 1), (2, 1), (1, 2)]),
          itertools.product(
              [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],
              [(1, 2, 2, 1), (1, 1, 1, 1)]))

    def fun(operand, init_val):
      return lax.reduce_window(operand, init_val, op, dims, strides, padding)

    # pylint: disable=cell-var-from-loop
    for shape, dims, strides in all_configs:
      args_maker = lambda: [rng(shape, dtype), init_val]
      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
    # pylint: enable=cell-var-from-loop

    # we separately test the version that uses a concrete init_val because it
    # can hit different code paths
    def fun(operand):
      return lax.reduce_window(operand, init_val, op, dims, strides, padding)

    # pylint: disable=cell-var-from-loop
    for shape, dims, strides in all_configs:
      args_maker = lambda: [rng(shape, dtype)]
      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
    # pylint: enable=cell-var-from-loop

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
      for dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(5,), (5, 7)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSort(self, shape, dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort only implemented for R1 on non-TPU backends""
      return absltest.unittest.skip(msg)

    args_maker = lambda: [rng(shape, dtype)]
    fun = lambda x: lax.sort(x, axis)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
      for dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(5,), (5, 7)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortAgainstNumpy(self, shape, dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort only implemented for R1 on non-TPU backends""
      return absltest.unittest.skip(msg)

    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.sort(x, axis)
    numpy_op = lambda x: lax_reference.sort(x, axis)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, key_dtype),
          jtu.format_shape_dtype_string(shape, val_dtype),
          axis),
       ""rng"": rng, ""shape"": shape,
       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
      for key_dtype in [onp.float32, onp.int32, onp.uint32]
      for val_dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(3,), (5, 3)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort_key_val only implemented for R1 non-TPU backends""
      return absltest.unittest.skip(msg)

    # This test relies on the property that wherever keys are tied, values are
    # too, since we don't guarantee the same ordering of values with equal keys.
    # To avoid that case, we generate unique keys (globally in the key array).
    perm_rng = onp.random.RandomState(0)
    def args_maker():
      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
      keys = perm_rng.permutation(flat_keys).reshape(shape)
      values = rng(shape, val_dtype)
      return keys, values

    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, key_dtype),
          jtu.format_shape_dtype_string(shape, val_dtype),
          axis),
       ""rng"": rng, ""shape"": shape,
       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
      for key_dtype in [onp.float32, onp.int32, onp.uint32]
      for val_dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(3,), (5, 3)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort_key_val only implemented for R1 non-TPU backends""
      return absltest.unittest.skip(msg)

    # This test relies on the property that wherever keys are tied, values are
    # too, since we don't guarantee the same ordering of values with equal keys.
    # To avoid that case, we generate unique keys (globally in the key array).
    perm_rng = onp.random.RandomState(0)
    def args_maker():
      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
      keys = perm_rng.permutation(flat_keys).reshape(shape)
      values = rng(shape, val_dtype)
      return keys, values

    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)
    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  def testWhileWithTuple(self):
    limit = 10

    def loop_cond(state):
      pos, _ = state
      return lax.lt(pos, limit)

    def loop_body(state):
      pos, count = state
      return (lax.add(pos, 1), lax.add(count, 1))

    def loop(init):
      result = lax._while_loop(loop_cond, loop_body, (init, 0))
      _, count = result
      return count

    cloop = api.jit(loop)

    self.assertEqual(loop(2), limit - 2)
    self.assertEqual(cloop(2), limit - 2)
    self.assertEqual(cloop(2), limit - 2)
    self.assertEqual(cloop(3), limit - 3)

  def testNestedWhile(self):

    def outer_loop(num):  # pylint: disable=missing-docstring
      def cond_fun(state):
        num, i, _ = state
        return lax.lt(i, num)

      def body_fun(state):
        num, i, count = state
        return (num, lax.add(i, 1), inner_loop(i, count))

      init_val = (num, 0, 0)
      _, i, count = lax._while_loop(cond_fun, body_fun, init_val)
      return (i, count)

    def inner_loop(i, count):  # pylint: disable=missing-docstring
      def cond_fun(state):
        i, j, _ = state
        return lax.le(j, i)

      def body_fun(state):
        i, j, count = state
        return (i, lax.add(j, 1), lax.add(count, 1))

      init_val = (i, 0, count)
      _, _, count = lax._while_loop(cond_fun, body_fun, init_val)
      return count

    cloop = api.jit(outer_loop)

    self.assertEqual(outer_loop(3), (3, 6))
    self.assertEqual(cloop(3), (3, 6))
    self.assertEqual(cloop(3), (3, 6))
    self.assertEqual(cloop(2), (2, 3))
    self.assertEqual(cloop(4), (4, 10))

  def testNestedWhileWithDynamicUpdateSlice(self):
    num = 5

    def update_entry(arr, val, i, j):
      val = lax.reshape(val, [1, 1])
      return lax.dynamic_update_slice(arr, val, (i, j))

    def outer_loop(arr):  # pylint: disable=missing-docstring

      def cond_fun(state):
        i, num, _, _ = state
        return lax.lt(i, num)

      def body_fun(state):
        i, num, arr, out = state
        return (lax.add(i, 1), num, arr, inner_loop(i, arr, out))

      out = onp.zeros(arr.shape, dtype=arr.dtype)
      init_val = (0, num, arr, out)
      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
      return out

    def inner_loop(i, arr, out):  # pylint: disable=missing-docstring

      def cond_fun(state):
        i, j, _, _ = state
        return lax.le(j, i)

      def body_fun(state):
        i, j, arr, out = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        arr_i_j = lax.dynamic_index_in_dim(arr_i, j, 0, False)
        out = update_entry(out, arr_i_j, i, j)
        return (i, lax.add(j, 1), arr, out)

      init_val = (i, 0, arr, out)
      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
      return out

    cloop = api.jit(outer_loop)
    arr = npr.RandomState(0).randn(5, 5)
    self.assertAllClose(outer_loop(arr), onp.tril(arr), check_dtypes=False)
    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)

  def testLoopWithConjunctionCondition(self):
    def sum_first_n(arr, num):  # pylint: disable=missing-docstring
      def cond_fun(state):
        arr, num, i, _ = state
        return lax.bitwise_and(lax.lt(i, num), lax.lt(i, arr.shape[0]))

      def body_fun(state):
        arr, num, i, total = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return (arr, num, lax.add(i, 1), lax.add(total, arr_i))

      init_val = (arr, num, 0, 0.)
      _, _, _, total = lax._while_loop(cond_fun, body_fun, init_val)
      return total

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  def testForiLoopBasic(self):
    def count(num):
      def body_fun(i, tot):
        return lax.add(tot, i)
      return lax.fori_loop(0, num, body_fun, 0)

    cfun = api.jit(count)

    self.assertEqual(count(2), 1)
    self.assertEqual(count(2), cfun(2))
    self.assertEqual(count(3), 3)
    self.assertEqual(count(3), cfun(3))
    self.assertEqual(count(4), 6)
    self.assertEqual(count(4), cfun(4))

  def testForiLoopTupleState(self):
    def sum_first_n(arr, num):
      def body_fun(i, state):
        arr, total = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return (arr, lax.add(total, arr_i))

      init_val = (arr, 0.)
      _, total = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun,
                               init_val)
      return total

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  def testForiLoopDictState(self):
    def sum_first_n(arr, num):
      def body_fun(i, state):
        arr, total = state['arr'], state['total']
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return {'arr': arr, 'total': lax.add(total, arr_i)}

      init_val = {'arr': arr, 'total': 0.}
      out_val = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
      return out_val['total']

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  def testForiLoopEmptyTupleInState(self):
    def sum_first_n(arr, num):
      def body_fun(i, state):
        arr, total, _ = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return (arr, lax.add(total, arr_i), ())

      init_val = (arr, 0., ())
      _, tot, _ = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
      return tot

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": rng}
      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),
                                   ((5, 3, 2), (5, 2, 4)),
                                   ((1, 2, 2, 3), (1, 2, 3, 1))]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)

  def testCollapse(self):

    @api.jit
    def collapse_first_two(x):
      return lax.collapse(x, 0, 2)

    self.assertEqual((6,), collapse_first_two(onp.zeros((2, 3))).shape)
    self.assertEqual((6, 4), collapse_first_two(onp.zeros((2, 3, 4))).shape)
    self.assertEqual((2, 3, 4),
                     collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
      for dtype in all_dtypes
      for shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexTake(self, shape, dtype, idxs, axes, rng):
    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
    fun = lambda src, idxs: lax.index_take(src, idxs, axes)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
       ""rng"": rng}
      for dtype in default_dtypes
      for dst_shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
    # We call lax.index_take to get the shapes right
    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
    ridxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
    args_maker = lambda: [rng(src_shape, dtype), rng(dst_shape, dtype), ridxs()]
    fun = lambda src, dst, idxs: lax.index_untake(src, dst, idxs, axes)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)


GradTestSpec = collections.namedtuple(
    ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])

LAX_GRAD_OPS = [
    GradTestSpec(lax.neg, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.floor, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.ceil, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.round, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    # GradTestSpec(lax.rem, nargs=2, order=2, rng=jtu.rand_default(),
    #              dtypes=[onp.float64]),  # TODO(mattjj): enable

    GradTestSpec(lax.exp, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.expm1, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.log, nargs=1, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.log1p, nargs=1, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.tanh, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.sin, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.cos, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),

    GradTestSpec(lax.erf, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.erfc, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.erf_inv, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64]),
    # GradTestSpec(lax.lgamma, nargs=1, order=2, rng=jtu.rand_small(),
    #              dtypes=[onp.float64]),  # TODO(mattjj): enable

    GradTestSpec(lax.real, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.complex64]),
    GradTestSpec(lax.imag, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.complex64]),
    # GradTestSpec(lax.complex, nargs=2, order=2, rng=jtu.rand_default(),
    #              dtypes=[onp.float32]),  # TODO(mattjj): enable
    GradTestSpec(lax.conj, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float32, onp.complex64]),
    GradTestSpec(lax.abs, nargs=1, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.pow, nargs=2, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),

    GradTestSpec(lax.add, nargs=2, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.sub, nargs=2, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.mul, nargs=2, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.div, nargs=2, order=1, rng=jtu.rand_not_small(),
                 dtypes=[onp.float64, onp.complex64]),

    GradTestSpec(lax.max, nargs=2, order=2, rng=jtu.rand_some_equal(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.min, nargs=2, order=2, rng=jtu.rand_some_equal(),
                 dtypes=[onp.float64]),
]


def check_grads(f, args, order, atol=None, rtol=None, eps=None):
  # TODO(mattjj,dougalm): add higher-order check
  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
  atol = atol or default_tol
  rtol = rtol or default_tol
  eps = eps or default_tol
  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)


def check_grads_bilinear(f, args, order, atol=None, rtol=None):
  # Can use large eps to make up for numerical inaccuracies since the op is
  # bilinear (relying on the fact that we only check one arg at a time)
  lhs, rhs = args
  check_grads(lambda lhs: f(lhs, rhs), (lhs,), order, atol, rtol, eps=1.)
  check_grads(lambda rhs: f(lhs, rhs), (rhs,), order, atol, rtol, eps=1.)


class LaxAutodiffTest(jtu.JaxTestCase):

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(
          rec.op.__name__, shapes, itertools.repeat(dtype)),
       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
       ""order"": rec.order}
      for rec in LAX_GRAD_OPS
      for shape_group in compatible_shapes
      for shapes in CombosWithReplacement(shape_group, rec.nargs)
      for dtype in rec.dtypes
  )
  def testOpGrad(self, op, rng, shapes, dtype, order):
    if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
      if dtype is onp.complex64:
        return absltest.unittest.skip(""complex grads unimplemented on tpu"")
      if op is lax.pow:
        return absltest.unittest.skip(""pow grad imprecise on tpu"")
    tol = 1e-1 if num_float_bits(dtype) == 32 else None
    args = tuple(rng(shape, dtype) for shape in shapes)
    check_grads(op, args, order, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
          from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.float64], repeat=2)
      for rng in [jtu.rand_default()])
  def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
    args = (rng((2, 3), from_dtype),)
    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
    check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)

  @parameterized.named_parameters(
      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
          jtu.format_shape_dtype_string(min_shape, dtype),
          jtu.format_shape_dtype_string(operand_shape, dtype),
          jtu.format_shape_dtype_string(max_shape, dtype)),
       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
      for min_shape, operand_shape, max_shape in [
          [(), (), ()],
          [(), (2, 3), ()],
          [(2, 3), (2, 3), (2, 3)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    shapes = [min_shape, operand_shape, max_shape]
    min, operand, max = (rng(shape, dtype) for shape in shapes)
    min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
    check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
          num_arrs),
       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
       ""num_arrs"": num_arrs, ""rng"": rng}
      for num_arrs in [3]
      for dtype in float_dtypes
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for dim in range(len(base_shape))
      for rng in [jtu.rand_default()])
  def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
    operands = tuple(rng(shape, dtype) for shape in shapes)
    concatenate = lambda *args: lax.concatenate(args, dim)
    check_grads(concatenate, operands, 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               strides, padding),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""rng"": rng,}
       for lhs_shape, rhs_shape, all_strides in itertools.chain(
           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)])
            for b, i, j in itertools.product([2, 3], repeat=3)],
           [((4, 2, 1), (3, 2, 1), [(1,)])])
       for strides in all_strides
       for dtype in [onp.float32]
       for padding in [""VALID"", ""SAME""]
       for rng in [jtu.rand_small()])
  @jtu.skip_on_devices(""tpu"")
  def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    conv = partial(lax.conv, window_strides=strides, padding=padding)
    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
       ""rhs_dilation={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               strides, padding, lhs_dil, rhs_dil),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
       ""rhs_dil"": rhs_dil, ""rng"": rng}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in
       itertools.chain(
           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
             [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
             [(1, 1), (2, 1)], [(1, 1)])
            for b, i, j in itertools.product([2, 3], repeat=3)],
           [((4, 2, 1), (3, 2, 1), [(1,)], [((1, 1),), ((0, 0),)],
             [(1,), (2,)], [(1,), (2,)])])
       for strides in all_strides
       for rhs_dil in rhs_dils
       for lhs_dil in lhs_dils
       for dtype in [onp.float32]
       for padding in all_pads
       for rng in [jtu.rand_small()])
  @jtu.skip_on_devices(""tpu"")
  def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                     padding, lhs_dil, rhs_dil, rng):
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    conv = partial(lax.conv_with_general_padding, window_strides=strides,
                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
       ""rhs_dilation={}_dims={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               strides, padding, lhs_dil, rhs_dil, "","".join(dim_nums)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
       ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
       ""perms"": perms}
      for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
          [(1, 1), (2, 1)], [(1, 1)])
          for b, i, j in itertools.product([1, 2], repeat=3)]
      for strides in all_strides
      for rhs_dil in rhs_dils
      for lhs_dil in lhs_dils
      for dtype in [onp.float32]
      for padding in all_pads
      for rng in [jtu.rand_default()]
      for dim_nums, perms in [
          ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
  @jtu.skip_on_devices(""tpu"")
  def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                 padding, lhs_dil, rhs_dil, dimension_numbers,
                                 perms, rng):
    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-3
    lhs_perm, rhs_perm = perms  # permute to compatible shapes
    lhs = onp.transpose(rng(lhs_shape, dtype), lhs_perm)
    rhs = onp.transpose(rng(rhs_shape, dtype), rhs_perm)
    conv = partial(lax.conv_general_dilated, window_strides=strides,
                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil,
                   dimension_numbers=dimension_numbers)
    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
          jtu.format_shape_dtype_string(lhs_shape, dtype),
          jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
      for dtype in float_dtypes)
  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
  def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
    tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               dimension_numbers),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""dimension_numbers"": dimension_numbers, ""rng"": jtu.rand_small()}
      for lhs_shape, rhs_shape, dimension_numbers in [
          ((3, 2), (2, 4), (([1], [0]), ([], []))),
          ((3, 5), (2, 5), (([1], [1]), ([], []))),
          ((5, 3), (5, 2), (([0], [0]), ([], []))),
          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
      ]
      for dtype in float_dtypes)
  @jtu.skip_on_devices(""tpu"")
  def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
                                          dimension_numbers, rng):
    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-2
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
    check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
          shape, onp.dtype(dtype).name, broadcast_sizes),
       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
       ""rng"": rng}
      for shape in [(), (2, 3)]
      for dtype in float_dtypes
      for broadcast_sizes in [(), (2,), (1, 2)]
      for rng in [jtu.rand_default()])
  def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    args = (rng(shape, dtype),)
    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
    check_grads(broadcast, args, 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
          jtu.format_shape_dtype_string(inshape, dtype),
          outshape, broadcast_dimensions),
       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
       ""dimensions"": broadcast_dimensions, ""rng"": rng}
      for inshape, outshape, broadcast_dimensions in [
          ([2], [2, 2], [0]),
          ([2], [2, 2], [1]),
          ([2], [2, 3], [0]),
          ([], [2, 3], []),
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(inshape, dtype)
    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
    check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": rng}
      for dtype in float_dtypes
      for arg_shape, out_shape in [
          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
      ]
      for rng in [jtu.rand_default()])
  def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(arg_shape, dtype)
    reshape = lambda x: lax.reshape(x, out_shape)
    check_grads(reshape, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_pads={}""
       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
      for shape in [(2, 3)]
      for dtype in float_dtypes
      for pads in [[(1, 2, 1), (0, 1, 0)]])
  def testPadGrad(self, shape, dtype, pads, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None

    operand = rng(shape, dtype)
    pad = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
    check_grads(pad, (operand,), 2, tol, tol, tol)

    operand = rng(shape, dtype)
    padding_value = onp.array(0., dtype)
    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)
    check_grads(pad, (operand, padding_value), 2, tol, tol, tol)

  def testReverseGrad(self):
    rev = lambda operand: lax.rev(operand, dimensions)

    dimensions = [0]
    check_grads(rev, (onp.array([3., 2., 1.]),), 2)

    dimensions = [0, 1]
    check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)

  @parameterized.named_parameters(
      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
          jtu.format_shape_dtype_string(arg_shape, dtype)),
       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""dtype"": dtype,
       ""rng"": rng}
      for arg_shape in [(), (3,), (2, 3)]
      for pred_shape in ([(), arg_shape] if arg_shape else [()])
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    pred = rng(pred_shape, onp.bool_)
    on_true = rng(arg_shape, dtype)
    on_false = rng(arg_shape, dtype)
    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
    check_grads(select, (on_true, on_false), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, limit_indices, strides),
       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
      for shape, start_indices, limit_indices, strides in [
        [(3,), (1,), (2,), None],
        [(7,), (4,), (7,), None],
        [(5,), (1,), (5,), (2,)],
        [(8,), (1,), (6,), (2,)],
        [(5, 3), (1, 1), (3, 2), None],
        [(5, 3), (1, 1), (3, 1), None],
        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
        [(5, 3), (1, 1), (2, 1), (1, 1)],
        [(5, 3), (1, 1), (5, 3), (2, 1)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    slice = lambda x: lax.slice(x, starts, limits, strides)
    check_grads(slice, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, size_indices),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""size_indices"": size_indices, ""rng"": rng}
      for shape, start_indices, size_indices in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
                           rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
    check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, update_shape),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""update_shape"": update_shape, ""rng"": rng}
      for shape, start_indices, update_shape in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
                                 update_shape, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    update = rng(update_shape, dtype)
    start_indices = onp.array(start_indices)
    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
    check_grads(dus, (operand, update), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_perm={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), perm),
       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
      for shape, perm in [
        [(3, 4), (1, 0)],
        [(3, 4), (0, 1)],
        [(3, 4, 5), (2, 1, 0)],
        [(3, 4, 5), (1, 0, 2)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testTransposeGrad(self, shape, dtype, perm, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    transpose = lambda x: lax.transpose(x, perm)
    check_grads(transpose, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
       ""dims"": dims, ""rng"": rng}
      for init_val, op, dtypes in [
          (0, lax.add, float_dtypes),
          (-onp.inf, lax.max, float_dtypes),
          (onp.inf, lax.min, float_dtypes),
      ]
      for dtype in dtypes
      for shape, dims in [
          [(3, 4, 5), (0,)],
          [(3, 4, 5), (1, 2)],
          [(3, 4, 5), (0, 2)],
          [(3, 4, 5), (0, 1, 2)]
      ]
      for rng in [jtu.rand_small()])
  def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    init_val = onp.asarray(init_val, dtype=dtype)
    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
    check_grads(reduce, (operand,), 1, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_dtype={}_padding={}""
       .format(op.__name__, onp.dtype(dtype).name, padding),
       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
       ""rng"": rng}
      for init_val, op, dtypes, rng in [
          (0, lax.add, [onp.float32], jtu.rand_small()),
          (-onp.inf, lax.max, [onp.float32], jtu.rand_default()),
          (onp.inf, lax.min, [onp.float32], jtu.rand_default()),
      ]
      for dtype in dtypes
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_default()])
  def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
    init_val = onp.asarray(init_val, dtype=dtype)

    # We need this conditional and the corresponding loop logic to be in the
    # test method, rather than at the parameterized test level, because it
    # depends on FLAGS for the device under test.
    if FLAGS.jax_test_dut == ""tpu"":
      all_configs = [((6, 5, 4, 3), (2, 2, 1, 1), (1, 2, 1, 1))]
    else:
      all_configs = itertools.chain(
          itertools.product(
              [(4, 6)],  # shapes
              [(2, 1), (1, 2)],  # window_dimensions
              [(1, 1), (2, 1), (1, 2)]  # strides
          ),
          itertools.product(
              [(3, 2, 4, 6)],  # shapes
              [(1, 1, 2, 1), (2, 1, 2, 1)],  # window_dimensions
              [(1, 2, 2, 1), (1, 1, 1, 1)]),  # strides
      )

    def fun(operand):
      return lax.reduce_window(operand, init_val, op, dims, strides, padding)

    # pylint: disable=cell-var-from-loop
    for shape, dims, strides in all_configs:
      operand = rng(shape, dtype)
      if op is lax.add:
        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
      else:
        # this test can fail if there are duplicates in operand
        self.assertEqual(onp.unique(operand).size, operand.size,
                         msg=""test requires operand elements to be unique."")
        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
                            1e-2, 1e-2, 1e-2)
    # pylint: enable=cell-var-from-loop

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
      for dtype in [onp.float32]
      for shape in [(5,), (5, 7)]
      for axis in [len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortGrad(self, shape, dtype, axis, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    sort = lambda x: lax.sort(x, axis)
    check_grads(sort, (operand,), 2, tol, tol, tol)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, key_dtype),
          jtu.format_shape_dtype_string(shape, val_dtype),
          axis),
       ""rng"": rng, ""shape"": shape,
       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
      for key_dtype in [onp.float32]
      for val_dtype in [onp.float32]
      for shape in [(3,), (5, 3)]
      for axis in [len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
    # This test relies on the property that wherever keys are tied, values are
    # too, since we don't guarantee the same ordering of values with equal keys.
    # To avoid that case, we generate unique keys (globally in the key array).
    perm_rng = onp.random.RandomState(0)
    def args_maker():
      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
      keys = perm_rng.permutation(flat_keys).reshape(shape)
      values = rng(shape, val_dtype)
      return keys, values
    keys, values = args_maker()

    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
    check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
      for dtype in float_dtypes
      for shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
    src = rng(shape, dtype)
    index_take = lambda src: lax.index_take(src, idxs, axes)
    check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
       ""rng"": rng}
      for dtype in float_dtypes
      for dst_shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
    # We call lax.index_take to get the shapes right
    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape

    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
    src = rng(src_shape, dtype)
    dst = rng(dst_shape, dtype)
    index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
    check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)


if __name__ == '__main__':
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
from functools import partial
import itertools

from absl import flags
from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from jax import api
from jax import core
from jax import lax
from jax import test_util as jtu
from jax import lax_reference
from jax.interpreters import xla
from jax.lib import xla_bridge

FLAGS = flags.FLAGS


def num_float_bits(dtype):
  return onp.finfo(xla_bridge.canonicalize_dtype(dtype)).bits


### lax tests

# For standard unops and binops, we can generate a large number of tests on
# arguments of appropriate shapes and dtypes using the following table.

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
all_dtypes = float_dtypes + complex_dtypes + int_dtypes + bool_dtypes

compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]

OpRecord = collections.namedtuple(""OpRecord"",
                                  [""op"", ""nargs"", ""dtypes"", ""rng"", ""tol""])


def op_record(op, nargs, dtypes, rng, tol=1e-5):
  return OpRecord(op, nargs, dtypes, rng, tol)

LAX_OPS = [
    op_record(lax.neg, 1, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.sign, 1, default_dtypes, jtu.rand_small()),
    op_record(lax.floor, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.ceil, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.round, 1, float_dtypes, jtu.rand_default()),

    op_record(lax.is_finite, 1, float_dtypes, jtu.rand_small()),

    op_record(lax.exp, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.expm1, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.log, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.log1p, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.tanh, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.sin, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.cos, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.atan2, 2, float_dtypes, jtu.rand_default()),

    op_record(lax.sqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.rsqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.square, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.reciprocal, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.tan, 1, float_dtypes, jtu.rand_default()),
    op_record(lax.asin, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.acos, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.atan, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.sinh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.cosh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.asinh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
    op_record(lax.acosh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),

    op_record(lax.lgamma, 1, float_dtypes, jtu.rand_positive()),
    op_record(lax.digamma, 1, float_dtypes, jtu.rand_positive()),
    op_record(lax.erf, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.erfc, 1, float_dtypes, jtu.rand_small()),
    op_record(lax.erf_inv, 1, float_dtypes, jtu.rand_small(), tol=1e-2),

    op_record(lax.real, 1, complex_dtypes, jtu.rand_default()),
    op_record(lax.imag, 1, complex_dtypes, jtu.rand_default()),
    op_record(lax.complex, 2, [onp.float32], jtu.rand_default()),
    op_record(lax.conj, 1, [onp.float32] + complex_dtypes, jtu.rand_default()),
    op_record(lax.abs, 1, default_dtypes + complex_dtypes, jtu.rand_default()),
    op_record(lax.pow, 2, float_dtypes + complex_dtypes, jtu.rand_positive()),

    op_record(lax.bitwise_and, 2, bool_dtypes, jtu.rand_small()),
    op_record(lax.bitwise_not, 1, bool_dtypes, jtu.rand_small()),
    op_record(lax.bitwise_or, 2, bool_dtypes, jtu.rand_small()),
    op_record(lax.bitwise_xor, 2, bool_dtypes, jtu.rand_small()),

    op_record(lax.add, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.sub, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.mul, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
    op_record(lax.div, 2, default_dtypes + complex_dtypes, jtu.rand_nonzero()),
    op_record(lax.rem, 2, default_dtypes, jtu.rand_nonzero()),

    op_record(lax.max, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.min, 2, default_dtypes, jtu.rand_small()),

    op_record(lax.eq, 2, all_dtypes, jtu.rand_some_equal()),
    op_record(lax.ne, 2, all_dtypes, jtu.rand_small()),
    op_record(lax.ge, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.gt, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.le, 2, default_dtypes, jtu.rand_small()),
    op_record(lax.lt, 2, default_dtypes, jtu.rand_small()),
]

CombosWithReplacement = itertools.combinations_with_replacement


class LaxTest(jtu.JaxTestCase):
  """"""Numerical tests for LAX operations.""""""

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(
          rec.op.__name__, shapes, itertools.repeat(dtype)),
       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
      for rec in LAX_OPS
      for shape_group in compatible_shapes
      for shapes in CombosWithReplacement(shape_group, rec.nargs)
      for dtype in rec.dtypes)
  def testOp(self, op, rng, shapes, dtype):
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(
          rec.op.__name__, shapes, itertools.repeat(dtype)),
       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
       ""tol"": rec.tol}
      for rec in LAX_OPS
      for shape_group in compatible_shapes
      for shapes in CombosWithReplacement(shape_group, rec.nargs)
      for dtype in rec.dtypes)
  def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    numpy_op = getattr(lax_reference, op.__name__)
    self._CheckAgainstNumpy(op, numpy_op, args_maker, tol=tol)

  # TODO test shift_left, shift_right_arithmetic, shift_right_logical

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
          from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testConvertElementType(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.convert_element_type(x, to_dtype)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
       .format(from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.convert_element_type(x, to_dtype)
    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
       .format(from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testBitcastConvertType(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
       .format(from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
      for rng in [jtu.rand_default()])
  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
    args_maker = lambda: [rng((2, 3), from_dtype)]
    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
          jtu.format_shape_dtype_string(min_shape, dtype),
          jtu.format_shape_dtype_string(operand_shape, dtype),
          jtu.format_shape_dtype_string(max_shape, dtype)),
       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
      for min_shape, operand_shape, max_shape in [
          [(), (2, 3), ()],
          [(2, 3), (2, 3), ()],
          [(), (2, 3), (2, 3)],
          [(2, 3), (2, 3), (2, 3)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
    shapes = [min_shape, operand_shape, max_shape]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
          jtu.format_shape_dtype_string(min_shape, dtype),
          jtu.format_shape_dtype_string(operand_shape, dtype),
          jtu.format_shape_dtype_string(max_shape, dtype)),
       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
      for min_shape, operand_shape, max_shape in [
          [(), (2, 3), ()],
          [(2, 3), (2, 3), ()],
          [(), (2, 3), (2, 3)],
          [(2, 3), (2, 3), (2, 3)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
                            rng):
    shapes = [min_shape, operand_shape, max_shape]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
          num_arrs),
       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
       ""num_arrs"": num_arrs, ""rng"": rng}
      for num_arrs in [3]
      for dtype in default_dtypes
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for dim in range(len(base_shape))
      for rng in [jtu.rand_default()])
  def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    op = lambda *args: lax.concatenate(args, dim)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
          num_arrs),
       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
       ""num_arrs"": num_arrs, ""rng"": rng}
      for num_arrs in [3]
      for dtype in default_dtypes
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for dim in range(len(base_shape))
      for rng in [jtu.rand_default()])
  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
    op = lambda *args: lax.concatenate(args, dim)
    numpy_op = lambda *args: lax_reference.concatenate(args, dim)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
          ""strides"": strides, ""padding"": padding, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([2, 3], repeat=3)]
      for dtype in [onp.float32]
      for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_small()])
  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.conv(lhs, rhs, strides, padding)

    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
          ""strides"": strides, ""padding"": padding, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([2, 3], repeat=3)]
      for dtype in [onp.float32]
      for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_small()])
  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
                           rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)
    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       ""_lhs_dilation={}_rhs_dilation={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype),
           strides, padding, lhs_dilation, rhs_dilation),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
      for lhs_dilation, rhs_dilation in itertools.product(
          [(1, 1), (1, 2), (2, 2)], repeat=2)
      for rng in [jtu.rand_small()])
  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
                                 padding, lhs_dilation, rhs_dilation, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.conv_with_general_padding(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)

    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       ""_lhs_dilation={}_rhs_dilation={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype),
           strides, padding, lhs_dilation, rhs_dilation),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
      for lhs_dilation, rhs_dilation in itertools.product(
          [(1, 1), (1, 2), (2, 2)], repeat=2)
      for rng in [jtu.rand_small()])
  def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
      rhs_dilation, rng):
    # TODO(mattjj): make this test pass
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.conv_with_general_padding(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)

    def numpy_fun(lhs, rhs):
      return lax_reference.conv_with_general_padding(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)

    self._CheckAgainstNumpy(fun, numpy_fun, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       ""_lhs_dilation={}_rhs_dilation={}""
       ""_dims={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype),
           strides, padding, lhs_dilation, rhs_dilation,
           "","".join(dim_nums)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
       ""rhs_dilation"": rhs_dilation, ""dimension_numbers"": dim_nums,
       ""perms"": perms, ""rng"": rng}
      for lhs_shape, rhs_shape in [
          ((b, i, 9, 10), (j, i, 4, 5))
          for b, i, j in itertools.product([2, 3], repeat=3)]
      for dtype in [onp.float32] for strides in [(1, 1), (2, 1)]
      for padding in [((1, 2), (2, 0))]
      for lhs_dilation, rhs_dilation in itertools.product(
          [(1, 1), (1, 2)], repeat=2)
      for rng in [jtu.rand_small()]
      for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
                              ([0, 1, 2, 3], [0, 1, 2, 3])),
                             ((""NHWC"", ""HWIO"", ""NHWC""),
                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
                             padding, lhs_dilation, rhs_dilation,
                             dimension_numbers, perms, rng):
    lhs_perm, rhs_perm = perms  # permute to compatible shapes

    def args_maker():
      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),
              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]

    def fun(lhs, rhs):
      return lax.conv_general_dilated(
          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,
          dimension_numbers)

    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  # TODO(mattjj): test conv_general_dilated against numpy

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
          jtu.format_shape_dtype_string(lhs_shape, dtype),
          jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": rng}
      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDot(self, lhs_shape, rhs_shape, dtype, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
          jtu.format_shape_dtype_string(lhs_shape, dtype),
          jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": rng}
      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               lhs_contracting, rhs_contracting),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""lhs_contracting"": lhs_contracting, ""rhs_contracting"": rhs_contracting,
       ""rng"": rng}
      for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [
          # these all fail with ""RuntimeError: Unimplemented: Dot with
          # non-standard contracting dimensions not implemented.""
          # [(3, 5), (2, 5), [1], [1]],
          # [(5, 3), (5, 2), [0], [0]],
          # [(5, 3, 2), (5, 2, 4), [0], [0]],
          # [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],
          # [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],
          [(3, 2), (2, 4), [1], [0]],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
                                 lhs_contracting, rhs_contracting, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))

    def fun(lhs, rhs):
      return lax.dot_general(lhs, rhs, dimension_numbers)

    self._CompileAndCheck(fun, args_maker, check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               dimension_numbers),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
      for lhs_shape, rhs_shape, dimension_numbers in [
          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
                                     dimension_numbers, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]

    def fun(lhs, rhs):
      return lax.dot_general(lhs, rhs, dimension_numbers)

    self._CompileAndCheck(fun, args_maker, check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               dimension_numbers),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
      for lhs_shape, rhs_shape, dimension_numbers in [
          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
                                 dimension_numbers, rng):
    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)
    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
          shape, onp.dtype(dtype).name, broadcast_sizes),
       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
       ""rng"": rng}
      for shape in [(), (2, 3)]
      for dtype in default_dtypes
      for broadcast_sizes in [(), (2,), (1, 2)]
      for rng in [jtu.rand_default()])
  def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.broadcast(x, broadcast_sizes)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
       ""rng"": rng}
      for shape in [(), (2, 3)]
      for dtype in default_dtypes
      for broadcast_sizes in [(), (2,), (1, 2)]
      for rng in [jtu.rand_default()])
  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.broadcast(x, broadcast_sizes)
    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
          jtu.format_shape_dtype_string(inshape, dtype),
          outshape, broadcast_dimensions),
       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
       ""dimensions"": broadcast_dimensions, ""rng"": rng}
      for inshape, outshape, broadcast_dimensions in [
          ([2], [2, 2], [0]),
          ([2], [2, 2], [1]),
          ([2], [2, 3], [0]),
          ([], [2, 3], []),
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
    args_maker = lambda: [rng(inshape, dtype)]
    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
          jtu.format_shape_dtype_string(inshape, dtype),
          outshape, broadcast_dimensions),
       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
       ""dimensions"": broadcast_dimensions, ""rng"": rng}
      for inshape, outshape, broadcast_dimensions in [
          ([2], [2, 2], [0]),
          ([2], [2, 2], [1]),
          ([2], [2, 3], [0]),
          ([], [2, 3], []),
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
                                     dimensions, rng):
    args_maker = lambda: [rng(inshape, dtype)]
    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": rng}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
      ]
      for rng in [jtu.rand_default()])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    args_maker = lambda: [rng(arg_shape, dtype)]
    op = lambda x: lax.reshape(x, out_shape)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": rng}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
      ]
      for rng in [jtu.rand_default()])
  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
    args_maker = lambda: [rng(arg_shape, dtype)]
    op = lambda x: lax.reshape(x, out_shape)
    numpy_op = lambda x: lax_reference.reshape(x, out_shape)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_pads={}""
       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
      for shape in [(2, 3)]
      for dtype in default_dtypes
      for pads in [[(1, 2, 1), (0, 1, 0)]])
  def testPad(self, shape, dtype, pads, rng):
    args_maker = lambda: [rng(shape, dtype)]
    fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_pads={}""
       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
      for shape in [(2, 3)]
      for dtype in default_dtypes
      for pads in [[(1, 2, 1), (0, 1, 0)]])
  def testPadAgainstNumpy(self, shape, dtype, pads, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
    numpy_op = lambda x: lax_reference.pad(x, onp.array(0, dtype), pads)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  def testReverse(self):
    rev = api.jit(lambda operand: lax.rev(operand, dimensions))

    dimensions = [0]
    self.assertAllClose(onp.array([3, 2, 1]), rev(onp.array([1, 2, 3])),
                        check_dtypes=False)

    dimensions = [0, 1]
    self.assertAllClose(onp.array([[6, 5, 4], [3, 2, 1]]),
                        rev(onp.array([[1, 2, 3], [4, 5, 6]])),
                        check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
       ""rng"": rng}
      for arg_shape in [(), (3,), (2, 3)]
      for pred_shape in ([(), arg_shape] if arg_shape else [()])
      for arg_dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):

    def args_maker():
      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
              rng(arg_shape, arg_dtype)]

    return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
       ""rng"": rng}
      for arg_shape in [(), (3,), (2, 3)]
      for pred_shape in ([(), arg_shape] if arg_shape else [()])
      for arg_dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):

    def args_maker():
      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
              rng(arg_shape, arg_dtype)]

    return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, limit_indices, strides),
       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
      for shape, start_indices, limit_indices, strides in [
        [(3,), (1,), (2,), None],
        [(7,), (4,), (7,), None],
        [(5,), (1,), (5,), (2,)],
        [(8,), (1,), (6,), (2,)],
        [(5, 3), (1, 1), (3, 2), None],
        [(5, 3), (1, 1), (3, 1), None],
        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
        [(5, 3), (1, 1), (2, 1), (1, 1)],
        [(5, 3), (1, 1), (5, 3), (2, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSlice(self, shape, dtype, starts, limits, strides, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.slice(x, starts, limits, strides)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, limit_indices, strides),
       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
      for shape, start_indices, limit_indices, strides in [
        [(3,), (1,), (2,), None],
        [(7,), (4,), (7,), None],
        [(5,), (1,), (5,), (2,)],
        [(8,), (1,), (6,), (2,)],
        [(5, 3), (1, 1), (3, 2), None],
        [(5, 3), (1, 1), (3, 1), None],
        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
        [(5, 3), (1, 1), (2, 1), (1, 1)],
        [(5, 3), (1, 1), (5, 3), (2, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
                            strides, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.slice(x, starts, limits, strides)
    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, size_indices),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""size_indices"": size_indices, ""rng"": rng}
      for shape, start_indices, size_indices in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, size_indices),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""size_indices"": size_indices, ""rng"": rng}
      for shape, start_indices, size_indices in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
                                   size_indices, rng):
    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)
    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, update_shape),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""update_shape"": update_shape, ""rng"": rng}
      for shape, start_indices, update_shape in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
                             rng):

    def args_maker():
      return [rng(shape, dtype), rng(update_shape, dtype),
              onp.array(start_indices)]

    self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
                          check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, update_shape),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""update_shape"": update_shape, ""rng"": rng}
      for shape, start_indices, update_shape in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, start_indices,
                                         update_shape, rng):

    def args_maker():
      return [rng(shape, dtype), rng(update_shape, dtype),
              onp.array(start_indices)]

    self._CheckAgainstNumpy(lax.dynamic_update_slice,
                            lax_reference.dynamic_update_slice, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_perm={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), perm),
       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
      for shape, perm in [
        [(3, 4), (1, 0)],
        [(3, 4), (0, 1)],
        [(3, 4, 5), (2, 1, 0)],
        [(3, 4, 5), (1, 0, 2)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testTranspose(self, shape, dtype, perm, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.transpose(x, perm)
    self._CompileAndCheck(op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_perm={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), perm),
       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
      for shape, perm in [
        [(3, 4), (1, 0)],
        [(3, 4), (0, 1)],
        [(3, 4, 5), (2, 1, 0)],
        [(3, 4, 5), (1, 0, 2)],
      ]
      for dtype in default_dtypes
      for rng in [jtu.rand_default()])
  def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.transpose(x, perm)
    numpy_op = lambda x: lax_reference.transpose(x, perm)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
       ""dims"": dims, ""rng"": rng}
      for init_val, op, dtypes in [
          (0, lax.add, default_dtypes),
          (-onp.inf, lax.max, float_dtypes),
          (onp.iinfo(onp.int32).min, lax.max, [onp.int32]),
          (onp.iinfo(onp.int64).min, lax.max, [onp.int64]),
          (onp.iinfo(onp.uint32).min, lax.max, [onp.uint32]),
          (onp.iinfo(onp.uint64).min, lax.max, [onp.uint64]),
          (onp.inf, lax.min, float_dtypes),
          (onp.iinfo(onp.int32).max, lax.min, [onp.int32]),
          (onp.iinfo(onp.int64).max, lax.min, [onp.int64]),
          (onp.iinfo(onp.uint32).max, lax.min, [onp.uint32]),
          (onp.iinfo(onp.uint64).max, lax.min, [onp.uint64]),
      ]
      for dtype in dtypes
      for shape, dims in [
          [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
          [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
      ]
      for rng in [jtu.rand_small()])
  def testReduce(self, op, init_val, shape, dtype, dims, rng):
    init_val = onp.asarray(init_val, dtype=dtype)
    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
    args_maker = lambda: [rng(shape, dtype), init_val]
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

    # we separately test the version that uses a concrete init_val because it
    # can hit different code paths
    fun = lambda operand: lax.reduce(operand, init_val, op, dims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_dtype={}_padding={}""
       .format(op.__name__, onp.dtype(dtype).name, padding),
       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
       ""rng"": rng}
      for init_val, op, dtypes in [
          (0, lax.add, [onp.float32]),
          (-onp.inf, lax.max, [onp.float32]),
          (onp.inf, lax.min, [onp.float32]),
      ]
      for dtype in dtypes
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_small()])
  def testReduceWindow(self, op, init_val, dtype, padding, rng):
    init_val = onp.asarray(init_val, dtype=dtype)

    # We need this conditional and the corresponding loop logic to be in the
    # test method, rather than at the parameterized test level, because it
    # depends on FLAGS for the device under test.
    if FLAGS.jax_test_dut == ""tpu"":
      all_configs = [((4, 6), (2, 1), (1, 2))]
    else:
      all_configs = itertools.chain(
          itertools.product(
              [(4, 6)],
              [(2, 1), (1, 2)],
              [(1, 1), (2, 1), (1, 2)]),
          itertools.product(
              [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],
              [(1, 2, 2, 1), (1, 1, 1, 1)]))

    def fun(operand, init_val):
      return lax.reduce_window(operand, init_val, op, dims, strides, padding)

    # pylint: disable=cell-var-from-loop
    for shape, dims, strides in all_configs:
      args_maker = lambda: [rng(shape, dtype), init_val]
      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
    # pylint: enable=cell-var-from-loop

    # we separately test the version that uses a concrete init_val because it
    # can hit different code paths
    def fun(operand):
      return lax.reduce_window(operand, init_val, op, dims, strides, padding)

    # pylint: disable=cell-var-from-loop
    for shape, dims, strides in all_configs:
      args_maker = lambda: [rng(shape, dtype)]
      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
    # pylint: enable=cell-var-from-loop

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
      for dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(5,), (5, 7)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSort(self, shape, dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort only implemented for R1 on non-TPU backends""
      return absltest.unittest.skip(msg)

    args_maker = lambda: [rng(shape, dtype)]
    fun = lambda x: lax.sort(x, axis)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
      for dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(5,), (5, 7)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortAgainstNumpy(self, shape, dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort only implemented for R1 on non-TPU backends""
      return absltest.unittest.skip(msg)

    args_maker = lambda: [rng(shape, dtype)]
    op = lambda x: lax.sort(x, axis)
    numpy_op = lambda x: lax_reference.sort(x, axis)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, key_dtype),
          jtu.format_shape_dtype_string(shape, val_dtype),
          axis),
       ""rng"": rng, ""shape"": shape,
       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
      for key_dtype in [onp.float32, onp.int32, onp.uint32]
      for val_dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(3,), (5, 3)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort_key_val only implemented for R1 non-TPU backends""
      return absltest.unittest.skip(msg)

    # This test relies on the property that wherever keys are tied, values are
    # too, since we don't guarantee the same ordering of values with equal keys.
    # To avoid that case, we generate unique keys (globally in the key array).
    perm_rng = onp.random.RandomState(0)
    def args_maker():
      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
      keys = perm_rng.permutation(flat_keys).reshape(shape)
      values = rng(shape, val_dtype)
      return keys, values

    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, key_dtype),
          jtu.format_shape_dtype_string(shape, val_dtype),
          axis),
       ""rng"": rng, ""shape"": shape,
       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
      for key_dtype in [onp.float32, onp.int32, onp.uint32]
      for val_dtype in [onp.float32, onp.int32, onp.uint32]
      for shape in [(3,), (5, 3)]
      for axis in [-1, len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
      msg = ""sort_key_val only implemented for R1 non-TPU backends""
      return absltest.unittest.skip(msg)

    # This test relies on the property that wherever keys are tied, values are
    # too, since we don't guarantee the same ordering of values with equal keys.
    # To avoid that case, we generate unique keys (globally in the key array).
    perm_rng = onp.random.RandomState(0)
    def args_maker():
      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
      keys = perm_rng.permutation(flat_keys).reshape(shape)
      values = rng(shape, val_dtype)
      return keys, values

    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)
    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)
    self._CheckAgainstNumpy(op, numpy_op, args_maker)

  def testWhileWithTuple(self):
    limit = 10

    def loop_cond(state):
      pos, _ = state
      return lax.lt(pos, limit)

    def loop_body(state):
      pos, count = state
      return (lax.add(pos, 1), lax.add(count, 1))

    def loop(init):
      result = lax._while_loop(loop_cond, loop_body, (init, 0))
      _, count = result
      return count

    cloop = api.jit(loop)

    self.assertEqual(loop(2), limit - 2)
    self.assertEqual(cloop(2), limit - 2)
    self.assertEqual(cloop(2), limit - 2)
    self.assertEqual(cloop(3), limit - 3)

  def testNestedWhile(self):

    def outer_loop(num):  # pylint: disable=missing-docstring
      def cond_fun(state):
        num, i, _ = state
        return lax.lt(i, num)

      def body_fun(state):
        num, i, count = state
        return (num, lax.add(i, 1), inner_loop(i, count))

      init_val = (num, 0, 0)
      _, i, count = lax._while_loop(cond_fun, body_fun, init_val)
      return (i, count)

    def inner_loop(i, count):  # pylint: disable=missing-docstring
      def cond_fun(state):
        i, j, _ = state
        return lax.le(j, i)

      def body_fun(state):
        i, j, count = state
        return (i, lax.add(j, 1), lax.add(count, 1))

      init_val = (i, 0, count)
      _, _, count = lax._while_loop(cond_fun, body_fun, init_val)
      return count

    cloop = api.jit(outer_loop)

    self.assertEqual(outer_loop(3), (3, 6))
    self.assertEqual(cloop(3), (3, 6))
    self.assertEqual(cloop(3), (3, 6))
    self.assertEqual(cloop(2), (2, 3))
    self.assertEqual(cloop(4), (4, 10))

  def testNestedWhileWithDynamicUpdateSlice(self):
    num = 5

    def update_entry(arr, val, i, j):
      val = lax.reshape(val, [1, 1])
      return lax.dynamic_update_slice(arr, val, (i, j))

    def outer_loop(arr):  # pylint: disable=missing-docstring

      def cond_fun(state):
        i, num, _, _ = state
        return lax.lt(i, num)

      def body_fun(state):
        i, num, arr, out = state
        return (lax.add(i, 1), num, arr, inner_loop(i, arr, out))

      out = onp.zeros(arr.shape, dtype=arr.dtype)
      init_val = (0, num, arr, out)
      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
      return out

    def inner_loop(i, arr, out):  # pylint: disable=missing-docstring

      def cond_fun(state):
        i, j, _, _ = state
        return lax.le(j, i)

      def body_fun(state):
        i, j, arr, out = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        arr_i_j = lax.dynamic_index_in_dim(arr_i, j, 0, False)
        out = update_entry(out, arr_i_j, i, j)
        return (i, lax.add(j, 1), arr, out)

      init_val = (i, 0, arr, out)
      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
      return out

    cloop = api.jit(outer_loop)
    arr = npr.RandomState(0).randn(5, 5)
    self.assertAllClose(outer_loop(arr), onp.tril(arr), check_dtypes=False)
    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)

  def testLoopWithConjunctionCondition(self):
    def sum_first_n(arr, num):  # pylint: disable=missing-docstring
      def cond_fun(state):
        arr, num, i, _ = state
        return lax.bitwise_and(lax.lt(i, num), lax.lt(i, arr.shape[0]))

      def body_fun(state):
        arr, num, i, total = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return (arr, num, lax.add(i, 1), lax.add(total, arr_i))

      init_val = (arr, num, 0, 0.)
      _, _, _, total = lax._while_loop(cond_fun, body_fun, init_val)
      return total

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  def testForiLoopBasic(self):
    def count(num):
      def body_fun(i, tot):
        return lax.add(tot, i)
      return lax.fori_loop(0, num, body_fun, 0)

    cfun = api.jit(count)

    self.assertEqual(count(2), 1)
    self.assertEqual(count(2), cfun(2))
    self.assertEqual(count(3), 3)
    self.assertEqual(count(3), cfun(3))
    self.assertEqual(count(4), 6)
    self.assertEqual(count(4), cfun(4))

  def testForiLoopTupleState(self):
    def sum_first_n(arr, num):
      def body_fun(i, state):
        arr, total = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return (arr, lax.add(total, arr_i))

      init_val = (arr, 0.)
      _, total = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun,
                               init_val)
      return total

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  def testForiLoopDictState(self):
    def sum_first_n(arr, num):
      def body_fun(i, state):
        arr, total = state['arr'], state['total']
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return {'arr': arr, 'total': lax.add(total, arr_i)}

      init_val = {'arr': arr, 'total': 0.}
      out_val = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
      return out_val['total']

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  def testForiLoopEmptyTupleInState(self):
    def sum_first_n(arr, num):
      def body_fun(i, state):
        arr, total, _ = state
        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
        return (arr, lax.add(total, arr_i), ())

      init_val = (arr, 0., ())
      _, tot, _ = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
      return tot

    cfun = api.jit(sum_first_n)
    x = npr.RandomState(0).randn(10)

    for num in [0, 5, 10, 15]:
      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
                          check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": rng}
      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),
                                   ((5, 3, 2), (5, 2, 4)),
                                   ((1, 2, 2, 3), (1, 2, 3, 1))]
      for dtype in float_dtypes
      for rng in [jtu.rand_small()])
  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
    self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)

  def testCollapse(self):

    @api.jit
    def collapse_first_two(x):
      return lax.collapse(x, 0, 2)

    self.assertEqual((6,), collapse_first_two(onp.zeros((2, 3))).shape)
    self.assertEqual((6, 4), collapse_first_two(onp.zeros((2, 3, 4))).shape)
    self.assertEqual((2, 3, 4),
                     collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
      for dtype in all_dtypes
      for shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexTake(self, shape, dtype, idxs, axes, rng):
    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
    fun = lambda src, idxs: lax.index_take(src, idxs, axes)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
       ""rng"": rng}
      for dtype in default_dtypes
      for dst_shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
    # We call lax.index_take to get the shapes right
    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
    ridxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
    args_maker = lambda: [rng(src_shape, dtype), rng(dst_shape, dtype), ridxs()]
    fun = lambda src, dst, idxs: lax.index_untake(src, dst, idxs, axes)
    self._CompileAndCheck(fun, args_maker, check_dtypes=True)


GradTestSpec = collections.namedtuple(
    ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])

LAX_GRAD_OPS = [
    GradTestSpec(lax.neg, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.floor, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.ceil, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.round, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    # GradTestSpec(lax.rem, nargs=2, order=2, rng=jtu.rand_default(),
    #              dtypes=[onp.float64]),  # TODO(mattjj): enable

    GradTestSpec(lax.exp, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.expm1, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.log, nargs=1, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.log1p, nargs=1, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.tanh, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.sin, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.cos, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64]),

    GradTestSpec(lax.erf, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.erfc, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.erf_inv, nargs=1, order=2, rng=jtu.rand_small(),
                 dtypes=[onp.float64]),
    # GradTestSpec(lax.lgamma, nargs=1, order=2, rng=jtu.rand_small(),
    #              dtypes=[onp.float64]),  # TODO(mattjj): enable

    GradTestSpec(lax.real, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.complex64]),
    GradTestSpec(lax.imag, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.complex64]),
    # GradTestSpec(lax.complex, nargs=2, order=2, rng=jtu.rand_default(),
    #              dtypes=[onp.float32]),  # TODO(mattjj): enable
    GradTestSpec(lax.conj, nargs=1, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float32, onp.complex64]),
    GradTestSpec(lax.abs, nargs=1, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.pow, nargs=2, order=2, rng=jtu.rand_positive(),
                 dtypes=[onp.float64, onp.complex64]),

    GradTestSpec(lax.add, nargs=2, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.sub, nargs=2, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.mul, nargs=2, order=2, rng=jtu.rand_default(),
                 dtypes=[onp.float64, onp.complex64]),
    GradTestSpec(lax.div, nargs=2, order=1, rng=jtu.rand_not_small(),
                 dtypes=[onp.float64, onp.complex64]),

    GradTestSpec(lax.max, nargs=2, order=2, rng=jtu.rand_some_equal(),
                 dtypes=[onp.float64]),
    GradTestSpec(lax.min, nargs=2, order=2, rng=jtu.rand_some_equal(),
                 dtypes=[onp.float64]),
]


def check_grads(f, args, order, atol=None, rtol=None, eps=None):
  # TODO(mattjj,dougalm): add higher-order check
  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
  atol = atol or default_tol
  rtol = rtol or default_tol
  eps = eps or default_tol
  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)


def check_grads_bilinear(f, args, order, atol=None, rtol=None):
  # Can use large eps to make up for numerical inaccuracies since the op is
  # bilinear (relying on the fact that we only check one arg at a time)
  lhs, rhs = args
  check_grads(lambda lhs: f(lhs, rhs), (lhs,), order, atol, rtol, eps=1.)
  check_grads(lambda rhs: f(lhs, rhs), (rhs,), order, atol, rtol, eps=1.)


class LaxAutodiffTest(jtu.JaxTestCase):

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(
          rec.op.__name__, shapes, itertools.repeat(dtype)),
       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
       ""order"": rec.order}
      for rec in LAX_GRAD_OPS
      for shape_group in compatible_shapes
      for shapes in CombosWithReplacement(shape_group, rec.nargs)
      for dtype in rec.dtypes
  )
  def testOpGrad(self, op, rng, shapes, dtype, order):
    if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
      if dtype is onp.complex64:
        return absltest.unittest.skip(""complex grads unimplemented on tpu"")
      if op is lax.pow:
        return absltest.unittest.skip(""pow grad imprecise on tpu"")
    tol = 1e-1 if num_float_bits(dtype) == 32 else None
    args = tuple(rng(shape, dtype) for shape in shapes)
    check_grads(op, args, order, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
          from_dtype, to_dtype),
       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
      for from_dtype, to_dtype in itertools.product(
          [onp.float32, onp.float64], repeat=2)
      for rng in [jtu.rand_default()])
  def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
    args = (rng((2, 3), from_dtype),)
    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
    check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)

  @parameterized.named_parameters(
      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
          jtu.format_shape_dtype_string(min_shape, dtype),
          jtu.format_shape_dtype_string(operand_shape, dtype),
          jtu.format_shape_dtype_string(max_shape, dtype)),
       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
      for min_shape, operand_shape, max_shape in [
          [(), (), ()],
          [(), (2, 3), ()],
          [(2, 3), (2, 3), (2, 3)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    shapes = [min_shape, operand_shape, max_shape]
    min, operand, max = (rng(shape, dtype) for shape in shapes)
    min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
    check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
          num_arrs),
       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
       ""num_arrs"": num_arrs, ""rng"": rng}
      for num_arrs in [3]
      for dtype in float_dtypes
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for dim in range(len(base_shape))
      for rng in [jtu.rand_default()])
  def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
    operands = tuple(rng(shape, dtype) for shape in shapes)
    concatenate = lambda *args: lax.concatenate(args, dim)
    check_grads(concatenate, operands, 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               strides, padding),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""rng"": rng,}
       for lhs_shape, rhs_shape, all_strides in itertools.chain(
           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)])
            for b, i, j in itertools.product([2, 3], repeat=3)],
           [((4, 2, 1), (3, 2, 1), [(1,)])])
       for strides in all_strides
       for dtype in [onp.float32]
       for padding in [""VALID"", ""SAME""]
       for rng in [jtu.rand_small()])
  @jtu.skip_on_devices(""tpu"")
  def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    conv = partial(lax.conv, window_strides=strides, padding=padding)
    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
       ""rhs_dilation={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               strides, padding, lhs_dil, rhs_dil),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
       ""rhs_dil"": rhs_dil, ""rng"": rng}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in
       itertools.chain(
           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
             [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
             [(1, 1), (2, 1)], [(1, 1)])
            for b, i, j in itertools.product([2, 3], repeat=3)],
           [((4, 2, 1), (3, 2, 1), [(1,)], [((1, 1),), ((0, 0),)],
             [(1,), (2,)], [(1,), (2,)])])
       for strides in all_strides
       for rhs_dil in rhs_dils
       for lhs_dil in lhs_dils
       for dtype in [onp.float32]
       for padding in all_pads
       for rng in [jtu.rand_small()])
  @jtu.skip_on_devices(""tpu"")
  def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                     padding, lhs_dil, rhs_dil, rng):
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    conv = partial(lax.conv_with_general_padding, window_strides=strides,
                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
       ""rhs_dilation={}_dims={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               strides, padding, lhs_dil, rhs_dil, "","".join(dim_nums)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
       ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
       ""perms"": perms}
      for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
          [(1, 1), (2, 1)], [(1, 1)])
          for b, i, j in itertools.product([1, 2], repeat=3)]
      for strides in all_strides
      for rhs_dil in rhs_dils
      for lhs_dil in lhs_dils
      for dtype in [onp.float32]
      for padding in all_pads
      for rng in [jtu.rand_default()]
      for dim_nums, perms in [
          ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
  @jtu.skip_on_devices(""tpu"")
  def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                 padding, lhs_dil, rhs_dil, dimension_numbers,
                                 perms, rng):
    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-3
    lhs_perm, rhs_perm = perms  # permute to compatible shapes
    lhs = onp.transpose(rng(lhs_shape, dtype), lhs_perm)
    rhs = onp.transpose(rng(rhs_shape, dtype), rhs_perm)
    conv = partial(lax.conv_general_dilated, window_strides=strides,
                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil,
                   dimension_numbers=dimension_numbers)
    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
          jtu.format_shape_dtype_string(lhs_shape, dtype),
          jtu.format_shape_dtype_string(rhs_shape, dtype)),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
      for dtype in float_dtypes)
  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
  def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
    tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
               jtu.format_shape_dtype_string(rhs_shape, dtype),
               dimension_numbers),
       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
       ""dimension_numbers"": dimension_numbers, ""rng"": jtu.rand_small()}
      for lhs_shape, rhs_shape, dimension_numbers in [
          ((3, 2), (2, 4), (([1], [0]), ([], []))),
          ((3, 5), (2, 5), (([1], [1]), ([], []))),
          ((5, 3), (5, 2), (([0], [0]), ([], []))),
          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
      ]
      for dtype in float_dtypes)
  @jtu.skip_on_devices(""tpu"")
  def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
                                          dimension_numbers, rng):
    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-2
    lhs = rng(lhs_shape, dtype)
    rhs = rng(rhs_shape, dtype)
    dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
    check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
          shape, onp.dtype(dtype).name, broadcast_sizes),
       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
       ""rng"": rng}
      for shape in [(), (2, 3)]
      for dtype in float_dtypes
      for broadcast_sizes in [(), (2,), (1, 2)]
      for rng in [jtu.rand_default()])
  def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    args = (rng(shape, dtype),)
    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
    check_grads(broadcast, args, 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
          jtu.format_shape_dtype_string(inshape, dtype),
          outshape, broadcast_dimensions),
       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
       ""dimensions"": broadcast_dimensions, ""rng"": rng}
      for inshape, outshape, broadcast_dimensions in [
          ([2], [2, 2], [0]),
          ([2], [2, 2], [1]),
          ([2], [2, 3], [0]),
          ([], [2, 3], []),
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(inshape, dtype)
    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
    check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": rng}
      for dtype in float_dtypes
      for arg_shape, out_shape in [
          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
      ]
      for rng in [jtu.rand_default()])
  def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(arg_shape, dtype)
    reshape = lambda x: lax.reshape(x, out_shape)
    check_grads(reshape, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_pads={}""
       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
      for shape in [(2, 3)]
      for dtype in float_dtypes
      for pads in [[(1, 2, 1), (0, 1, 0)]])
  def testPadGrad(self, shape, dtype, pads, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None

    operand = rng(shape, dtype)
    pad = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
    check_grads(pad, (operand,), 2, tol, tol, tol)

    operand = rng(shape, dtype)
    padding_value = onp.array(0., dtype)
    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)
    check_grads(pad, (operand, padding_value), 2, tol, tol, tol)

  def testReverseGrad(self):
    rev = lambda operand: lax.rev(operand, dimensions)

    dimensions = [0]
    check_grads(rev, (onp.array([3., 2., 1.]),), 2)

    dimensions = [0, 1]
    check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)

  @parameterized.named_parameters(
      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
          jtu.format_shape_dtype_string(arg_shape, dtype)),
       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""dtype"": dtype,
       ""rng"": rng}
      for arg_shape in [(), (3,), (2, 3)]
      for pred_shape in ([(), arg_shape] if arg_shape else [()])
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    pred = rng(pred_shape, onp.bool_)
    on_true = rng(arg_shape, dtype)
    on_false = rng(arg_shape, dtype)
    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
    check_grads(select, (on_true, on_false), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"":
       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, limit_indices, strides),
       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
      for shape, start_indices, limit_indices, strides in [
        [(3,), (1,), (2,), None],
        [(7,), (4,), (7,), None],
        [(5,), (1,), (5,), (2,)],
        [(8,), (1,), (6,), (2,)],
        [(5, 3), (1, 1), (3, 2), None],
        [(5, 3), (1, 1), (3, 1), None],
        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
        [(5, 3), (1, 1), (2, 1), (1, 1)],
        [(5, 3), (1, 1), (5, 3), (2, 1)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    slice = lambda x: lax.slice(x, starts, limits, strides)
    check_grads(slice, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, size_indices),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""size_indices"": size_indices, ""rng"": rng}
      for shape, start_indices, size_indices in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
                           rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
    check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
          jtu.format_shape_dtype_string(shape, dtype),
          start_indices, update_shape),
       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
       ""update_shape"": update_shape, ""rng"": rng}
      for shape, start_indices, update_shape in [
        [(3,), (1,), (1,)],
        [(5, 3), (1, 1), (3, 1)],
        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
                                 update_shape, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    update = rng(update_shape, dtype)
    start_indices = onp.array(start_indices)

    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
    check_grads(dus, (operand, update), 2, tol, tol, tol)

    dus = lambda x: lax.dynamic_update_slice(x, update, start_indices)
    check_grads(dus, (operand,), 2, tol, tol, tol)

    dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)
    check_grads(dus, (update,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_perm={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), perm),
       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
      for shape, perm in [
        [(3, 4), (1, 0)],
        [(3, 4), (0, 1)],
        [(3, 4, 5), (2, 1, 0)],
        [(3, 4, 5), (1, 0, 2)],
      ]
      for dtype in float_dtypes
      for rng in [jtu.rand_default()])
  def testTransposeGrad(self, shape, dtype, perm, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    transpose = lambda x: lax.transpose(x, perm)
    check_grads(transpose, (operand,), 2, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
       ""dims"": dims, ""rng"": rng}
      for init_val, op, dtypes in [
          (0, lax.add, float_dtypes),
          (-onp.inf, lax.max, float_dtypes),
          (onp.inf, lax.min, float_dtypes),
      ]
      for dtype in dtypes
      for shape, dims in [
          [(3, 4, 5), (0,)],
          [(3, 4, 5), (1, 2)],
          [(3, 4, 5), (0, 2)],
          [(3, 4, 5), (0, 1, 2)]
      ]
      for rng in [jtu.rand_small()])
  def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    init_val = onp.asarray(init_val, dtype=dtype)
    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
    check_grads(reduce, (operand,), 1, tol, tol, tol)

  @parameterized.named_parameters(
      {""testcase_name"": ""_op={}_dtype={}_padding={}""
       .format(op.__name__, onp.dtype(dtype).name, padding),
       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
       ""rng"": rng}
      for init_val, op, dtypes, rng in [
          (0, lax.add, [onp.float32], jtu.rand_small()),
          (-onp.inf, lax.max, [onp.float32], jtu.rand_default()),
          (onp.inf, lax.min, [onp.float32], jtu.rand_default()),
      ]
      for dtype in dtypes
      for padding in [""VALID"", ""SAME""]
      for rng in [jtu.rand_default()])
  def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
    init_val = onp.asarray(init_val, dtype=dtype)

    # We need this conditional and the corresponding loop logic to be in the
    # test method, rather than at the parameterized test level, because it
    # depends on FLAGS for the device under test.
    if FLAGS.jax_test_dut == ""tpu"":
      all_configs = [((6, 5, 4, 3), (2, 2, 1, 1), (1, 2, 1, 1))]
    else:
      all_configs = itertools.chain(
          itertools.product(
              [(4, 6)],  # shapes
              [(2, 1), (1, 2)],  # window_dimensions
              [(1, 1), (2, 1), (1, 2)]  # strides
          ),
          itertools.product(
              [(3, 2, 4, 6)],  # shapes
              [(1, 1, 2, 1), (2, 1, 2, 1)],  # window_dimensions
              [(1, 2, 2, 1), (1, 1, 1, 1)]),  # strides
      )

    def fun(operand):
      return lax.reduce_window(operand, init_val, op, dims, strides, padding)

    # pylint: disable=cell-var-from-loop
    for shape, dims, strides in all_configs:
      operand = rng(shape, dtype)
      if op is lax.add:
        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
      else:
        # this test can fail if there are duplicates in operand
        self.assertEqual(onp.unique(operand).size, operand.size,
                         msg=""test requires operand elements to be unique."")
        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
                            1e-2, 1e-2, 1e-2)
    # pylint: enable=cell-var-from-loop

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
      for dtype in [onp.float32]
      for shape in [(5,), (5, 7)]
      for axis in [len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortGrad(self, shape, dtype, axis, rng):
    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
    operand = rng(shape, dtype)
    sort = lambda x: lax.sort(x, axis)
    check_grads(sort, (operand,), 2, tol, tol, tol)

  # TODO(b/205052657): enable more tests when supported
  @parameterized.named_parameters(
      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(shape, key_dtype),
          jtu.format_shape_dtype_string(shape, val_dtype),
          axis),
       ""rng"": rng, ""shape"": shape,
       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
      for key_dtype in [onp.float32]
      for val_dtype in [onp.float32]
      for shape in [(3,), (5, 3)]
      for axis in [len(shape) - 1]
      for rng in [jtu.rand_default()])
  def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
    # This test relies on the property that wherever keys are tied, values are
    # too, since we don't guarantee the same ordering of values with equal keys.
    # To avoid that case, we generate unique keys (globally in the key array).
    perm_rng = onp.random.RandomState(0)
    def args_maker():
      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
      keys = perm_rng.permutation(flat_keys).reshape(shape)
      values = rng(shape, val_dtype)
      return keys, values
    keys, values = args_maker()

    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
    check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)

  @parameterized.named_parameters(
      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
      for dtype in float_dtypes
      for shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
    src = rng(shape, dtype)
    index_take = lambda src: lax.index_take(src, idxs, axes)
    check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)

  @parameterized.named_parameters(
      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
       ""rng"": rng}
      for dtype in float_dtypes
      for dst_shape, idxs, axes in [
          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
      ]
      for rng in [jtu.rand_default()])
  def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
    # We call lax.index_take to get the shapes right
    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape

    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
    src = rng(src_shape, dtype)
    dst = rng(dst_shape, dtype)
    index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
    check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)


if __name__ == '__main__':
  absltest.main()
","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 136d641d99d30121e6673774e3f9dda9ec0ff42c..800709fef0eb1110514c2d70b11f6668f6a841ba 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -839,8 +839,8 @@ class LaxTest(jtu.JaxTestCase):
       ]
       for dtype in default_dtypes
       for rng in [jtu.rand_default()])
-  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
-                             rng):
+  def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                         update_shape, rng):
 
     def args_maker():
       return [rng(shape, dtype), rng(update_shape, dtype),
@@ -1797,9 +1797,16 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     operand = rng(shape, dtype)
     update = rng(update_shape, dtype)
     start_indices = onp.array(start_indices)
+
     dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
     check_grads(dus, (operand, update), 2, tol, tol, tol)
 
+    dus = lambda x: lax.dynamic_update_slice(x, update, start_indices)
+    check_grads(dus, (operand,), 2, tol, tol, tol)
+
+    dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)
+    check_grads(dus, (update,), 2, tol, tol, tol)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
",Security - injection / XSS / SQLi,`Update LaxTest to include gradient checks for dynamic_update_slice with respect to both operand and update`
f5232aaeea52c794569d83a5578ff54d82902c52,Dougal Maclaurin: Fixed bug in vjp with constant-zero tangent outputs,jax/api.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import itertools

import numpy as onp

from . import core
from . import linear_util as lu
from .core import pack, eval_jaxpr
from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                        tree_map)
from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
from .abstract_arrays import ShapedArray
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching

map = safe_map

def jit(fun, static_argnums=()):
  def f_jitted(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
    f, dyn_args = argnums_partial(f, dyn_argnums, args)
    args_flat, in_trees = unzip2(map(tree_to_jaxtuples, dyn_args))
    check_args(args_flat)
    flat_fun, out_tree = flatten_fun(f, in_trees)
    out_flat = xla.xla_call(flat_fun, *args_flat)
    return build_tree(out_tree(), out_flat)

  return f_jitted

def grad(fun, argnums=0):
  def grad_f(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    f_partial, dyn_args = argnums_partial(f, argnums, args)
    ans, vjp_py = vjp(f_partial, *dyn_args)
    check_scalar(ans)
    g = vjp_py(onp.ones((), onp.result_type(ans)))
    return g[0] if isinstance(argnums, int) else g


  return grad_f

@curry
def jacfwd(fun, x):
  fun = lu.wrap_init(fun)
  pushfwd = partial(jvp, fun, (x,))
  std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
  y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

@curry
def jacrev(fun, x):
  fun = lu.wrap_init(fun)
  y, pullback = vjp(fun, x)
  std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
  jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

def hessian(fun):
  return jacfwd(jacrev(fun))

def vmap(fun, *args, **kwargs):
  in_bdims = kwargs.pop(""in_bdims"", 0)
  out_bdim = kwargs.pop(""out_bdim"", 0)
  if kwargs:
    msg = ""vmap keyword args must be 'in_bdims' and/or 'out_bdim', got {}.""
    raise TypeError(msg.format(', '.join(kwargs)))

  if type(in_bdims) is int:
    in_bdims = (in_bdims,) * len(args)
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_flat = batching.batch(flat_fun, in_flat, in_bdims, out_bdim)
  return build_tree(out_tree(), out_flat)

def jvp(fun, primals, tangents):
  def flatten_arg(primal, tangent):
    primal_jtuple, tree_def = tree_to_jaxtuples(primal)
    tangent_jtuple, tree_def_2 = tree_to_jaxtuples(tangent)
    assert tree_def == tree_def_2, (tree_def, tree_def_2)
    return primal_jtuple, tangent_jtuple, tree_def

  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  ps_flat, ts_flat, in_trees = unzip3(map(flatten_arg, primals, tangents))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_tangent = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
  return (build_tree(out_tree(), out_primal), build_tree(out_tree(), out_tangent))

def linearize(traceable, *primals):
  fun = lu.wrap_init(traceable)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_pval, jaxpr, consts = ad.linearize(flat_fun, *primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  lifted_jvp = partial(lift_linearized, jaxpr, consts, (in_trees, out_tree), out_pval)
  return out_primal_py, lifted_jvp

def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
  def fun(*args):
    primals = pack(args) # doesn't matter what these are-they'll be ignored
    tangents = pack(args)
    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
    return list(pe.merge_pvals(ans, out_pval))[1]

  return unflatten_fun(fun, io_tree, *py_args)

def vjp(fun, *primals):
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  check_args(primals_flat)
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  ct_in_trees = [out_tree]
  ct_out_tree = PyTreeDef(node_types[tuple], None, in_trees)
  def out_vjp_packed(cotangent_in):
    return out_vjp(cotangent_in)
  vjp_py = partial(unflatten_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
  return out_primal_py, vjp_py


def trace_to_jaxpr(traceable, py_pvals, **kwargs):
  fun = lu.wrap_init(traceable)
  pvals, in_trees = unzip2(map(tree_to_pval_tuples, py_pvals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
  return jaxpr, consts, out_pval, (in_trees, out_tree())

def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
  def fun(*args):
    ans = eval_jaxpr(jaxpr, consts, (), *args)
    return pe.merge_pvals(ans, pvals)
  return unflatten_fun(fun, io_tree, *py_args)


device_put = jit(lambda x: x)
device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
device_get = partial(tree_map, device_get_array)


@lu.transformation_with_aux
def flatten_fun(in_trees, *args, **kwargs):
  py_args = map(build_tree, in_trees, args)
  ans = yield py_args
  yield process_pytree(pack, ans)


def unflatten_fun(fun, io_tree, *py_args):
  in_trees_expected, out_tree = io_tree
  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
  for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
    if in_tree != expected:
      raise TypeError(""Expected {}, got {}"".format(expected, in_tree))

  ans = fun(*args)
  return build_tree(out_tree, ans)


tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
tree_to_jaxtuples = partial(process_pytree, pack)


def argnums_partial(f, dyn_argnums, args):
  if isinstance(dyn_argnums, int):
    dyn_argnums = (dyn_argnums,)
  else:
    dyn_argnums = tuple(dyn_argnums)
  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
                      for i, arg in enumerate(args)])
  dyn_args = [args[i] for i in dyn_argnums]
  return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args

@lu.transformation
def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
  args = [None if arg is None else arg.val for arg in fixed_args]
  for i, arg in zip(dyn_argnums, dyn_args):
    args[i] = arg
  ans = yield args
  yield ans

def check_args(args):
  for arg in args:
    if not (isinstance(arg, core.Tracer) or core.valid_jaxtype(arg)):
      raise TypeError(""Argument '{}' of type {} is not a valid JAX type""
                      .format(arg, type(arg)))

def check_scalar(x):
  msg = ""Gradient only defined for scalar-output functions. Output was: {}"".format
  try:
    aval = core.get_aval(x)
    if not (isinstance(aval, ShapedArray) and aval.shape == ()):
      raise TypeError(msg(x))
  except TypeError:
    raise TypeError(msg(x))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import itertools

import numpy as onp

from . import core
from . import linear_util as lu
from .core import pack, eval_jaxpr
from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                        tree_map)
from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
from .abstract_arrays import ShapedArray
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching

map = safe_map

def jit(fun, static_argnums=()):
  def f_jitted(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
    f, dyn_args = argnums_partial(f, dyn_argnums, args)
    args_flat, in_trees = unzip2(map(tree_to_jaxtuples, dyn_args))
    check_args(args_flat)
    flat_fun, out_tree = flatten_fun(f, in_trees)
    out_flat = xla.xla_call(flat_fun, *args_flat)
    return build_tree(out_tree(), out_flat)

  return f_jitted

def grad(fun, argnums=0):
  def grad_f(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    f_partial, dyn_args = argnums_partial(f, argnums, args)
    ans, vjp_py = vjp(f_partial, *dyn_args)
    check_scalar(ans)
    g = vjp_py(onp.ones((), onp.result_type(ans)))
    return g[0] if isinstance(argnums, int) else g


  return grad_f

@curry
def jacfwd(fun, x):
  fun = lu.wrap_init(fun)
  pushfwd = partial(jvp, fun, (x,))
  std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
  y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

@curry
def jacrev(fun, x):
  fun = lu.wrap_init(fun)
  y, pullback = vjp(fun, x)
  std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
  jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

def hessian(fun):
  return jacfwd(jacrev(fun))

def vmap(fun, *args, **kwargs):
  in_bdims = kwargs.pop(""in_bdims"", 0)
  out_bdim = kwargs.pop(""out_bdim"", 0)
  if kwargs:
    msg = ""vmap keyword args must be 'in_bdims' and/or 'out_bdim', got {}.""
    raise TypeError(msg.format(', '.join(kwargs)))

  if type(in_bdims) is int:
    in_bdims = (in_bdims,) * len(args)
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_flat = batching.batch(flat_fun, in_flat, in_bdims, out_bdim)
  return build_tree(out_tree(), out_flat)

def jvp(fun, primals, tangents):
  def flatten_arg(primal, tangent):
    primal_jtuple, tree_def = tree_to_jaxtuples(primal)
    tangent_jtuple, tree_def_2 = tree_to_jaxtuples(tangent)
    assert tree_def == tree_def_2, (tree_def, tree_def_2)
    return primal_jtuple, tangent_jtuple, tree_def

  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  ps_flat, ts_flat, in_trees = unzip3(map(flatten_arg, primals, tangents))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_tangent = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
  return (build_tree(out_tree(), out_primal), build_tree(out_tree(), out_tangent))

def linearize(traceable, *primals):
  fun = lu.wrap_init(traceable)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_pval, jaxpr, consts = ad.linearize(flat_fun, *primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  lifted_jvp = partial(lift_linearized, jaxpr, consts, (in_trees, out_tree), out_pval)
  return out_primal_py, lifted_jvp

def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
  def fun(*args):
    primals = pack(args) # doesn't matter what these are-they'll be ignored
    tangents = pack(args)
    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
    return pe.merge_pvals(ans, out_pval)

  return unflatten_fun(fun, io_tree, *py_args)

def vjp(fun, *primals):
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  check_args(primals_flat)
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  ct_in_trees = [out_tree]
  ct_out_tree = PyTreeDef(node_types[tuple], None, in_trees)
  def out_vjp_packed(cotangent_in):
    return out_vjp(cotangent_in)
  vjp_py = partial(unflatten_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
  return out_primal_py, vjp_py


def trace_to_jaxpr(traceable, py_pvals, **kwargs):
  fun = lu.wrap_init(traceable)
  pvals, in_trees = unzip2(map(tree_to_pval_tuples, py_pvals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
  return jaxpr, consts, out_pval, (in_trees, out_tree())

def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
  def fun(*args):
    ans = eval_jaxpr(jaxpr, consts, (), *args)
    return pe.merge_pvals(ans, pvals)
  return unflatten_fun(fun, io_tree, *py_args)


device_put = jit(lambda x: x)
device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
device_get = partial(tree_map, device_get_array)


@lu.transformation_with_aux
def flatten_fun(in_trees, *args, **kwargs):
  py_args = map(build_tree, in_trees, args)
  ans = yield py_args
  yield process_pytree(pack, ans)


def unflatten_fun(fun, io_tree, *py_args):
  in_trees_expected, out_tree = io_tree
  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
  for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
    if in_tree != expected:
      raise TypeError(""Expected {}, got {}"".format(expected, in_tree))

  ans = fun(*args)
  return build_tree(out_tree, ans)


tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
tree_to_jaxtuples = partial(process_pytree, pack)


def argnums_partial(f, dyn_argnums, args):
  if isinstance(dyn_argnums, int):
    dyn_argnums = (dyn_argnums,)
  else:
    dyn_argnums = tuple(dyn_argnums)
  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
                      for i, arg in enumerate(args)])
  dyn_args = [args[i] for i in dyn_argnums]
  return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args

@lu.transformation
def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
  args = [None if arg is None else arg.val for arg in fixed_args]
  for i, arg in zip(dyn_argnums, dyn_args):
    args[i] = arg
  ans = yield args
  yield ans

def check_args(args):
  for arg in args:
    if not (isinstance(arg, core.Tracer) or core.valid_jaxtype(arg)):
      raise TypeError(""Argument '{}' of type {} is not a valid JAX type""
                      .format(arg, type(arg)))

def check_scalar(x):
  msg = ""Gradient only defined for scalar-output functions. Output was: {}"".format
  try:
    aval = core.get_aval(x)
    if not (isinstance(aval, ShapedArray) and aval.shape == ()):
      raise TypeError(msg(x))
  except TypeError:
    raise TypeError(msg(x))
","diff --git a/jax/api.py b/jax/api.py
index 4bd4005ea8e80a9f399dba959eed0951524c855b..9a2240d58492f448dcc6fd42b910c4a720876feb 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -123,8 +123,8 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
   def fun(*args):
     primals = pack(args) # doesn't matter what these are-they'll be ignored
     tangents = pack(args)
-    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
-    return list(pe.merge_pvals(ans, out_pval))[1]
+    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
+    return pe.merge_pvals(ans, out_pval)
 
   return unflatten_fun(fun, io_tree, *py_args)
 
",Incorrect default / config value,Update `lift_linearized` to correctly handle output partial values by changing the return type of `eval_jaxpr` and
f5232aaeea52c794569d83a5578ff54d82902c52,Dougal Maclaurin: Fixed bug in vjp with constant-zero tangent outputs,jax/interpreters/ad.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from . import partial_eval as pe
from . import xla
from .. import core as core
from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                       zeros_like_p, zero, Zero)
from ..util import unzip2, unzip3, safe_map, safe_zip, partial
from ..tree_util import process_pytree, build_tree, register_pytree_node
from ..linear_util import thunk, staged, transformation, transformation_with_aux, wrap_init

from six.moves import builtins, reduce

zip = safe_zip
map = safe_map

def jvp(fun):
  return jvpfun(jvp_subtrace(fun))

@transformation
def jvpfun(primals, tangents):
  with new_master(JVPTrace) as master:
    out_primal, out_tangent = yield master, primals, tangents
    del master
  out_tangent = instantiate_zeros(out_primal, out_tangent)
  yield (out_primal, out_tangent)

@transformation
def jvp_subtrace(master, primals, tangents):
  trace = JVPTrace(master, core.cur_sublevel())
  for x in list(primals) + list(tangents):
    if isinstance(x, Tracer):
      assert x.trace.level < trace.level
  ans = yield map(partial(JVPTracer, trace), primals, tangents)
  out_tracer = trace.full_raise(ans)
  out_primal, out_tangent = out_tracer.primal, out_tracer.tangent
  yield (out_primal, out_tangent)

@transformation
def pack_output(*args):
  ans = yield args
  yield pack(ans)

def linearize(traceable, *primals):
  jvpfun = pack_output(jvp(traceable))
  tangent_avals = [get_aval(p).at_least_vspace() for p in primals]
  in_pvals = (pe.PartialVal((None, pack(primals))),
              pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
  out_pv, out_const = out_pval
  assert out_pv is None or out_pv[0] is None
  primal_out = tuple(out_const)[0]
  return primal_out, out_pval, jaxpr, consts


def vjp(traceable, primals):
  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
  def vjp_(ct):
    dummy_primal_and_ct = pack((core.unit, ct))
    _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
    return instantiate_zeros(pack(primals), arg_cts[1])

  return out_primal, vjp_


def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
  def write_cotangent(v, ct):
    # assert v not in primal_env
    if ct is not None:
      ct_env[v] = add_tangents(ct_env[v], ct) if v in ct_env else ct

  def read_cotangent(v):
    return ct_env.get(v, zero)

  primal_env = {v: val
                for v, val in zip(jaxpr.freevars, freevar_vals)
                if val is not None}
  primal_env.update(zip(jaxpr.constvars, consts))
  ct_env = {jaxpr.outvar: cotangent_in}

  for eqn in jaxpr.eqns[::-1]:
    cts_in = map(read_cotangent, eqn.outvars)
    ct_in = TangentTuple(cts_in) if eqn.destructure else cts_in[0]
    invals = map(primal_env.get, eqn.invars)
    if eqn.bound_subjaxprs:
      subjaxprs, sub_consts, sub_freevar_vals = unzip3([
          (subjaxpr,
           map(primal_env.get, const_vars),
           map(primal_env.get, bound_vars))
          for subjaxpr, const_vars, bound_vars in eqn.bound_subjaxprs])
      cts_out, ct_free_vars_out = get_primitive_transpose(eqn.primitive)(
          eqn.params, subjaxprs, sub_consts, sub_freevar_vals, invals, ct_in)
      # TODO(dougalm): support cases != 1
      assert(len(eqn.bound_subjaxprs) == 1)
      _, _, bound_vars = eqn.bound_subjaxprs[0]
      map(write_cotangent, bound_vars, ct_free_vars_out)
    else:
      cts_out = get_primitive_transpose(eqn.primitive)(ct_in, *invals, **eqn.params)

    if cts_out is zero:
      cts_out = [zero for _ in eqn.invars]
    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
    for var, ct in builtins.zip(eqn.invars, cts_out):
      write_cotangent(var, ct)

  cotangents_out = map(read_cotangent, jaxpr.invars)
  freevar_cts = map(read_cotangent, jaxpr.freevars)
  return freevar_cts, cotangents_out

def get_primitive_transpose(p):
  try:
    return primitive_transposes[p]
  except KeyError:
    raise NotImplementedError(
      ""Reverse-mode differentiation rule for '{}' not implemented"".format(p))

class TangentTuple(tuple):
  pass

register_pytree_node(
    TangentTuple, lambda xs: (xs, None), lambda _, xs: TangentTuple(xs))

class JVPTrace(Trace):

  def pure(self, val):
    return JVPTracer(self, val, zero)

  def lift(self, val):
    return JVPTracer(self, val, zero)

  def sublift(self,val):
    return JVPTracer(self, val.primal, val.tangent)

  def process_primitive(self, primitive, tracers, params):
    primals_in = [t.primal for t in tracers]
    tangents_in = [t.tangent for t in tracers]
    try:
      jvp = primitive_jvps[primitive]
    except KeyError:
      raise NotImplementedError(
          ""Forward-mode differentiation rule for '{}' not implemented""
          .format(primitive))
    primal_out, tangent_out = jvp(primals_in, tangents_in, **params)
    return JVPTracer(self, primal_out, tangent_out)

  def process_call(self, call_primitive, f, tracers, params):
    primals = [t.primal for t in tracers]
    tangents = [t.tangent for t in tracers]
    nonzero_tangents, in_tree_def = tree_to_jaxtuples(tangents)
    f, out_tree_def = traceable(jvp_subtrace(f, self.master), in_tree_def)
    result = call_primitive.bind(f, pack(primals), nonzero_tangents)
    primal_out, tangent_out = build_tree(out_tree_def(), result)
    return JVPTracer(self, primal_out, tangent_out)

  def post_process_call(self, _, out_tracer):
    out_jtuple, tree_def = tree_to_jaxtuples((out_tracer.primal, out_tracer.tangent))
    master = self.master
    def todo(x):
      trace = JVPTrace(master, core.cur_sublevel())
      return JVPTracer(trace, *build_tree(tree_def, x))

    return out_jtuple, todo

  def join(self, xt, yt):
    isfull = lambda t: t is not zero and not isinstance(t, TangentTuple)
    if isfull(xt) and isfull(yt):
      return xt, yt
    elif isfull(xt):
      if yt is zero:
        return xt, zeros_like_jaxval(xt)
      elif isinstance(xt, TangentTuple):
        return xt, JaxTuple(map(zeros_like_jaxval, xt))
      else:
        raise TypeError
    elif isfull(yt):
      if xt is zero:
        return zeros_like_jaxval(yt), yt
      elif isinstance(xt, TangentTuple):
        return JaxTuple(map(zeros_like_jaxval, yt)), yt
      else:
        raise TypeError
    elif isinstance(xt, TangentTuple) or isinstance(yt, TangentTuple):
      if xt is zero:
        xt = TangentTuple((zero,) * len(yt))
      elif yt is zero:
        yt = TangentTuple((zero,) * len(xt))
      return TangentTuple(map(self.join), xt, yt)
    elif xt is zero and yt is zero:
      return xt, yt
    else:
      raise TypeError((xt, yt))

  def pack(self, tracers):
    primals = pack(t.primal for t in tracers)
    tangents = TangentTuple([t.tangent for t in tracers])
    return JVPTracer(self, primals, tangents)


class JVPTracer(Tracer):
  def __init__(self, trace, primal, tangent):
    self.trace = trace
    self.primal = primal
    self.tangent = tangent

  @property
  def aval(self):
    # TODO(dougalm): add epsilon ball
    return get_aval(self.primal)

  def unpack(self):
    if self.tangent is zero:
      tangent = [zero] * len(self.primal)
    else:
      tangent = self.tangent
    return map(partial(JVPTracer, self.trace), self.primal, tangent)

  def full_lower(self):
    if self.tangent is zero:
      return core.full_lower(self.primal)
    else:
      return self

# -------------------- Primitives --------------------


primitive_jvps = {}
composite_jvps = {}

primitive_transposes = {}


def deflinear(primitive, transpose_rule):
  primitive_jvps[primitive] = partial(linear_jvp, primitive)
  primitive_transposes[primitive] = partial(linear_transpose, transpose_rule)


def linear_jvp(primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  if all(tangent is zero for tangent in tangents):
    return val_out, zero
  else:
    tangents = map(instantiate_zeros, primals, tangents)
    return val_out, primitive.bind(*tangents, **params)


def linear_transpose(transpose_rule, cotangent, *args, **kwargs):
  return zero if cotangent is zero else transpose_rule(cotangent, **kwargs)


def defjvp(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp, jvprules, primitive)


def standard_jvp(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def defjvp2(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp2, jvprules, primitive)


def standard_jvp2(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, val_out, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def add_tangents(x, y):
  if x is zero:
    return y
  elif y is zero:
    return x
  else:
    return add_jaxvals(x, y)


def defbilinear_broadcasting(bcast, prim, lhs_rule, rhs_rule):
  assert isinstance(prim, Primitive)
  lhs_jvp = lambda g, x, y, **kwargs: prim.bind(bcast(g, y), y, **kwargs)
  rhs_jvp = lambda g, x, y, **kwargs: prim.bind(x, bcast(g, x), **kwargs)
  defjvp(prim, lhs_jvp, rhs_jvp)
  primitive_transposes[prim] = partial(bilinear_transpose, lhs_rule, rhs_rule)
defbilinear = partial(defbilinear_broadcasting, lambda g, x: g)


def bilinear_transpose(lhs_rule, rhs_rule, cotangent, x, y, **kwargs):
  assert x is None or y is None
  if x is None:
    out = zero if cotangent is zero else lhs_rule(cotangent, y, **kwargs)
    return out, None
  else:
    out = zero if cotangent is zero else rhs_rule(cotangent, x, **kwargs)
    return None, out


def defjvp_zero(primitive):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(zero_jvp, primitive)


def zero_jvp(primitive, primals, tangents, **params):
  return primitive.bind(*primals, **params), zero


deflinear(zeros_like_p, lambda t: (zeros_like_jaxval(t),))
deflinear(core.identity_p, lambda t: (t,))
deflinear(core.pack_p, lambda t: list(t) if t is not zero else zero)
deflinear(add_jaxvals_p, lambda t: (t, t))


def instantiate_zeros(example, tangent):
  if tangent is zero:
    return zeros_like_jaxval(example)
  elif isinstance(tangent, TangentTuple):
    return pack(map(instantiate_zeros, example, tangent))
  else:
    return tangent

@transformation_with_aux
def traceable(in_tree_def, new_primals, new_tangents):
  new_tangents = build_tree(in_tree_def, new_tangents)
  primal_out, tangent_out = yield new_primals, new_tangents
  out_jtuple, tree_def = tree_to_jaxtuples((primal_out, tangent_out))
  yield out_jtuple, tree_def

@transformation_with_aux
def transposed_fun(jaxpr, in_tree_def, args):
  consts, freevar_vals, ct = args
  ct, freevar_vals = build_tree(in_tree_def, (ct, freevar_vals))
  freevar_cts, cotangents_out = yield jaxpr, consts, freevar_vals, ct
  out_jtuple, tree_def = tree_to_jaxtuples((cotangents_out, freevar_cts))
  yield out_jtuple, tree_def

def call_transpose(primitive, params, jaxpr, consts, freevar_vals, args, ct):
  jaxpr, = jaxpr
  consts, = consts
  freevar_vals, = freevar_vals
  assert isinstance(jaxpr, core.Jaxpr)
  assert all(a is None for a in args), ""TODO(dougalm): handle non-tangent primal args""
  (ct, freevar_vals), in_tree_def = tree_to_jaxtuples((ct, freevar_vals))
  fun = wrap_init(backward_pass)
  fun, out_tree_def = transposed_fun(fun, jaxpr, in_tree_def)
  all_args = pack((pack(consts), pack(freevar_vals), ct))
  # TODO(dougalm): consider signalling to bind that there are no traces in the closure
  ans = primitive.bind(fun, all_args, **params)
  return build_tree(out_tree_def(), ans)

primitive_transposes[core.call_p] = partial(call_transpose, call_p)
primitive_transposes[pe.compiled_call_p] = partial(call_transpose, pe.compiled_call_p)
primitive_transposes[xla.xla_call_p] = partial(call_transpose, xla.xla_call_p)


tree_to_jaxtuples = partial(process_pytree, pack)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from . import partial_eval as pe
from . import xla
from .. import core as core
from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                       zeros_like_p, zero, Zero)
from ..util import unzip2, unzip3, safe_map, safe_zip, partial
from ..tree_util import process_pytree, build_tree, register_pytree_node
from ..linear_util import thunk, staged, transformation, transformation_with_aux, wrap_init

from six.moves import builtins, reduce

zip = safe_zip
map = safe_map

def jvp(fun):
  return jvpfun(jvp_subtrace(fun))

@transformation
def jvpfun(primals, tangents):
  with new_master(JVPTrace) as master:
    out_primal, out_tangent = yield master, primals, tangents
    del master
  out_tangent = instantiate_zeros(out_primal, out_tangent)
  yield (out_primal, out_tangent)

@transformation
def jvp_subtrace(master, primals, tangents):
  trace = JVPTrace(master, core.cur_sublevel())
  for x in list(primals) + list(tangents):
    if isinstance(x, Tracer):
      assert x.trace.level < trace.level
  ans = yield map(partial(JVPTracer, trace), primals, tangents)
  out_tracer = trace.full_raise(ans)
  out_primal, out_tangent = out_tracer.primal, out_tracer.tangent
  yield (out_primal, out_tangent)

@transformation
def pack_output(*args):
  ans = yield args
  yield pack(ans)

def linearize(traceable, *primals):
  jvpfun = pack_output(jvp(traceable))
  tangent_avals = [get_aval(p).at_least_vspace() for p in primals]
  in_pvals = (pe.PartialVal((None, pack(primals))),
              pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
  pval_primal, pval_tangent = unpair_pval(out_pval)
  aval_primal, const_primal = pval_primal
  assert aval_primal is None
  return const_primal, pval_tangent, jaxpr, consts

def vjp(traceable, primals):
  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
  def vjp_(ct):
    ct = ignore_consts(ct, pval)
    dummy_primal_and_ct = pack((core.unit, ct))
    _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
    return instantiate_zeros(pack(primals), arg_cts[1])

  return out_primal, vjp_

def ignore_consts(ct, pval):
  aval, const = pval
  if isinstance(aval, core.AbstractValue):
    return ct
  elif isinstance(aval, pe.JaxprTracerTuple):
    return pack(map(ignore_consts, ct, zip(aval, const)))
  elif aval is None:
    return core.unit
  else:
    raise TypeError(aval)

def unpair_pval(pval):
  aval, const = pval
  const_1, const_2 = const
  if aval is None:
    return (None, const_1), (None, const_2)
  else:
    aval_1, aval_2 = aval
    return (aval_1, const_1), (aval_2, const_2)

def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
  def write_cotangent(v, ct):
    # assert v not in primal_env
    if ct is not None:
      ct_env[v] = add_tangents(ct_env[v], ct) if v in ct_env else ct

  def read_cotangent(v):
    return ct_env.get(v, zero)

  primal_env = {v: val
                for v, val in zip(jaxpr.freevars, freevar_vals)
                if val is not None}
  primal_env.update(zip(jaxpr.constvars, consts))
  ct_env = {jaxpr.outvar: cotangent_in}

  for eqn in jaxpr.eqns[::-1]:
    cts_in = map(read_cotangent, eqn.outvars)
    ct_in = TangentTuple(cts_in) if eqn.destructure else cts_in[0]
    invals = map(primal_env.get, eqn.invars)
    if eqn.bound_subjaxprs:
      subjaxprs, sub_consts, sub_freevar_vals = unzip3([
          (subjaxpr,
           map(primal_env.get, const_vars),
           map(primal_env.get, bound_vars))
          for subjaxpr, const_vars, bound_vars in eqn.bound_subjaxprs])
      cts_out, ct_free_vars_out = get_primitive_transpose(eqn.primitive)(
          eqn.params, subjaxprs, sub_consts, sub_freevar_vals, invals, ct_in)
      # TODO(dougalm): support cases != 1
      assert(len(eqn.bound_subjaxprs) == 1)
      _, _, bound_vars = eqn.bound_subjaxprs[0]
      map(write_cotangent, bound_vars, ct_free_vars_out)
    else:
      cts_out = get_primitive_transpose(eqn.primitive)(ct_in, *invals, **eqn.params)

    if cts_out is zero:
      cts_out = [zero for _ in eqn.invars]

    for var, ct in zip(eqn.invars, cts_out):
      write_cotangent(var, ct)

  cotangents_out = map(read_cotangent, jaxpr.invars)
  freevar_cts = map(read_cotangent, jaxpr.freevars)
  return freevar_cts, cotangents_out

def get_primitive_transpose(p):
  try:
    return primitive_transposes[p]
  except KeyError:
    raise NotImplementedError(
      ""Reverse-mode differentiation rule for '{}' not implemented"".format(p))

class TangentTuple(tuple):
  pass

register_pytree_node(
    TangentTuple, lambda xs: (xs, None), lambda _, xs: TangentTuple(xs))

class JVPTrace(Trace):

  def pure(self, val):
    return JVPTracer(self, val, zero)

  def lift(self, val):
    return JVPTracer(self, val, zero)

  def sublift(self,val):
    return JVPTracer(self, val.primal, val.tangent)

  def process_primitive(self, primitive, tracers, params):
    primals_in = [t.primal for t in tracers]
    tangents_in = [t.tangent for t in tracers]
    try:
      jvp = primitive_jvps[primitive]
    except KeyError:
      raise NotImplementedError(
          ""Forward-mode differentiation rule for '{}' not implemented""
          .format(primitive))
    primal_out, tangent_out = jvp(primals_in, tangents_in, **params)
    return JVPTracer(self, primal_out, tangent_out)

  def process_call(self, call_primitive, f, tracers, params):
    primals = [t.primal for t in tracers]
    tangents = [t.tangent for t in tracers]
    nonzero_tangents, in_tree_def = tree_to_jaxtuples(tangents)
    f, out_tree_def = traceable(jvp_subtrace(f, self.master), in_tree_def)
    result = call_primitive.bind(f, pack(primals), nonzero_tangents)
    primal_out, tangent_out = build_tree(out_tree_def(), result)
    return JVPTracer(self, primal_out, tangent_out)

  def post_process_call(self, _, out_tracer):
    out_jtuple, tree_def = tree_to_jaxtuples((out_tracer.primal, out_tracer.tangent))
    master = self.master
    def todo(x):
      trace = JVPTrace(master, core.cur_sublevel())
      return JVPTracer(trace, *build_tree(tree_def, x))

    return out_jtuple, todo

  def join(self, xt, yt):
    isfull = lambda t: t is not zero and not isinstance(t, TangentTuple)
    if isfull(xt) and isfull(yt):
      return xt, yt
    elif isfull(xt):
      if yt is zero:
        return xt, zeros_like_jaxval(xt)
      elif isinstance(xt, TangentTuple):
        return xt, JaxTuple(map(zeros_like_jaxval, xt))
      else:
        raise TypeError
    elif isfull(yt):
      if xt is zero:
        return zeros_like_jaxval(yt), yt
      elif isinstance(xt, TangentTuple):
        return JaxTuple(map(zeros_like_jaxval, yt)), yt
      else:
        raise TypeError
    elif isinstance(xt, TangentTuple) or isinstance(yt, TangentTuple):
      if xt is zero:
        xt = TangentTuple((zero,) * len(yt))
      elif yt is zero:
        yt = TangentTuple((zero,) * len(xt))
      return TangentTuple(map(self.join), xt, yt)
    elif xt is zero and yt is zero:
      return xt, yt
    else:
      raise TypeError((xt, yt))

  def pack(self, tracers):
    primals = pack(t.primal for t in tracers)
    tangents = TangentTuple([t.tangent for t in tracers])
    return JVPTracer(self, primals, tangents)


class JVPTracer(Tracer):
  def __init__(self, trace, primal, tangent):
    self.trace = trace
    self.primal = primal
    self.tangent = tangent

  @property
  def aval(self):
    # TODO(dougalm): add epsilon ball
    return get_aval(self.primal)

  def unpack(self):
    if self.tangent is zero:
      tangent = [zero] * len(self.primal)
    else:
      tangent = self.tangent
    return map(partial(JVPTracer, self.trace), self.primal, tangent)

  def full_lower(self):
    if self.tangent is zero:
      return core.full_lower(self.primal)
    else:
      return self

# -------------------- Primitives --------------------


primitive_jvps = {}
composite_jvps = {}

primitive_transposes = {}


def deflinear(primitive, transpose_rule):
  primitive_jvps[primitive] = partial(linear_jvp, primitive)
  primitive_transposes[primitive] = partial(linear_transpose, transpose_rule)


def linear_jvp(primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  if all(tangent is zero for tangent in tangents):
    return val_out, zero
  else:
    tangents = map(instantiate_zeros, primals, tangents)
    return val_out, primitive.bind(*tangents, **params)


def linear_transpose(transpose_rule, cotangent, *args, **kwargs):
  return zero if cotangent is zero else transpose_rule(cotangent, **kwargs)


def defjvp(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp, jvprules, primitive)


def standard_jvp(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def defjvp2(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp2, jvprules, primitive)


def standard_jvp2(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, val_out, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def add_tangents(x, y):
  if x is zero:
    return y
  elif y is zero:
    return x
  else:
    return add_jaxvals(x, y)


def defbilinear_broadcasting(bcast, prim, lhs_rule, rhs_rule):
  assert isinstance(prim, Primitive)
  lhs_jvp = lambda g, x, y, **kwargs: prim.bind(bcast(g, y), y, **kwargs)
  rhs_jvp = lambda g, x, y, **kwargs: prim.bind(x, bcast(g, x), **kwargs)
  defjvp(prim, lhs_jvp, rhs_jvp)
  primitive_transposes[prim] = partial(bilinear_transpose, lhs_rule, rhs_rule)
defbilinear = partial(defbilinear_broadcasting, lambda g, x: g)


def bilinear_transpose(lhs_rule, rhs_rule, cotangent, x, y, **kwargs):
  assert x is None or y is None
  if x is None:
    out = zero if cotangent is zero else lhs_rule(cotangent, y, **kwargs)
    return out, None
  else:
    out = zero if cotangent is zero else rhs_rule(cotangent, x, **kwargs)
    return None, out


def defjvp_zero(primitive):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(zero_jvp, primitive)


def zero_jvp(primitive, primals, tangents, **params):
  return primitive.bind(*primals, **params), zero


deflinear(zeros_like_p, lambda t: (zeros_like_jaxval(t),))
deflinear(core.identity_p, lambda t: (t,))
deflinear(core.pack_p, lambda t: list(t) if t is not zero else zero)
deflinear(add_jaxvals_p, lambda t: (t, t))


def instantiate_zeros(example, tangent):
  if tangent is zero:
    return zeros_like_jaxval(example)
  elif isinstance(tangent, TangentTuple):
    return pack(map(instantiate_zeros, example, tangent))
  else:
    return tangent

@transformation_with_aux
def traceable(in_tree_def, new_primals, new_tangents):
  new_tangents = build_tree(in_tree_def, new_tangents)
  primal_out, tangent_out = yield new_primals, new_tangents
  out_jtuple, tree_def = tree_to_jaxtuples((primal_out, tangent_out))
  yield out_jtuple, tree_def

@transformation_with_aux
def transposed_fun(jaxpr, in_tree_def, args):
  consts, freevar_vals, ct = args
  ct, freevar_vals = build_tree(in_tree_def, (ct, freevar_vals))
  freevar_cts, cotangents_out = yield jaxpr, consts, freevar_vals, ct
  out_jtuple, tree_def = tree_to_jaxtuples((cotangents_out, freevar_cts))
  yield out_jtuple, tree_def

def call_transpose(primitive, params, jaxpr, consts, freevar_vals, args, ct):
  jaxpr, = jaxpr
  consts, = consts
  freevar_vals, = freevar_vals
  assert isinstance(jaxpr, core.Jaxpr)
  assert all(a is None for a in args), ""TODO(dougalm): handle non-tangent primal args""
  (ct, freevar_vals), in_tree_def = tree_to_jaxtuples((ct, freevar_vals))
  fun = wrap_init(backward_pass)
  fun, out_tree_def = transposed_fun(fun, jaxpr, in_tree_def)
  all_args = pack((pack(consts), pack(freevar_vals), ct))
  # TODO(dougalm): consider signalling to bind that there are no traces in the closure
  ans = primitive.bind(fun, all_args, **params)
  return build_tree(out_tree_def(), ans)

primitive_transposes[core.call_p] = partial(call_transpose, call_p)
primitive_transposes[pe.compiled_call_p] = partial(call_transpose, pe.compiled_call_p)
primitive_transposes[xla.xla_call_p] = partial(call_transpose, xla.xla_call_p)


tree_to_jaxtuples = partial(process_pytree, pack)
","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f84bef02673ae37dfbde39b1e0d6cb61..3f205b1dbe1d4af4e6e55db3ef83e59575956261 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -64,21 +64,40 @@ def linearize(traceable, *primals):
   in_pvals = (pe.PartialVal((None, pack(primals))),
               pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
   jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
-  out_pv, out_const = out_pval
-  assert out_pv is None or out_pv[0] is None
-  primal_out = tuple(out_const)[0]
-  return primal_out, out_pval, jaxpr, consts
-
+  pval_primal, pval_tangent = unpair_pval(out_pval)
+  aval_primal, const_primal = pval_primal
+  assert aval_primal is None
+  return const_primal, pval_tangent, jaxpr, consts
 
 def vjp(traceable, primals):
-  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
+  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
   def vjp_(ct):
+    ct = ignore_consts(ct, pval)
     dummy_primal_and_ct = pack((core.unit, ct))
     _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
     return instantiate_zeros(pack(primals), arg_cts[1])
 
   return out_primal, vjp_
 
+def ignore_consts(ct, pval):
+  aval, const = pval
+  if isinstance(aval, core.AbstractValue):
+    return ct
+  elif isinstance(aval, pe.JaxprTracerTuple):
+    return pack(map(ignore_consts, ct, zip(aval, const)))
+  elif aval is None:
+    return core.unit
+  else:
+    raise TypeError(aval)
+
+def unpair_pval(pval):
+  aval, const = pval
+  const_1, const_2 = const
+  if aval is None:
+    return (None, const_1), (None, const_2)
+  else:
+    aval_1, aval_2 = aval
+    return (aval_1, const_1), (aval_2, const_2)
 
 def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
   def write_cotangent(v, ct):
@@ -116,8 +135,8 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
-    for var, ct in builtins.zip(eqn.invars, cts_out):
+
+    for var, ct in zip(eqn.invars, cts_out):
       write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)
",Boundary condition / off-by-one,`Refactor linearize and vjp to handle paired partial values and ignore constants in cotangents`
f5232aaeea52c794569d83a5578ff54d82902c52,Dougal Maclaurin: Fixed bug in vjp with constant-zero tangent outputs,tests/quickercheck.py,"from collections import namedtuple
from functools import partial
import numpy.random as npr
import jax.numpy as np
from jax import jit, jvp, vjp
import itertools as it
import sys

npr.seed(0)

from jax.util import unzip2, safe_zip, safe_map

map = safe_map
zip = safe_zip

subfun_prob = 0.5
thin_prob = 0.1
size_reduction_factor = 3

Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
Prim = namedtuple('Prim', ['fun'])
ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
Var = namedtuple('Var', ['name', 'vartype'])
Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])

def gen_fun_and_types(size):
  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
  fun, _ = gen_function(size, in_types)
  return fun

def gen_function(size, in_types):
  eqns = []
  in_vars = map(fresh_var, in_types)
  cur_vars = in_vars[:]
  for _ in range(gen_nonneg_int(size)):
    if not cur_vars:
      break
    if npr.rand() < subfun_prob:
      arg_vars = gen_subset(cur_vars)
      arg_types = [v.vartype for v in arg_vars]
      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
      fun = partial(eval_fun, fun)
    else:
      arity = choice(primitive_generators.keys())
      arg_vars = gen_sized_subset(cur_vars, arity)
      arg_types = [v.vartype for v in arg_vars]
      prim_gen = weighted_choice(primitive_generators[arity])
      fun, out_type = prim_gen(size, *arg_types)
      fun = wrap_singleton(fun)
      out_types = [out_type]

    out_vars = map(fresh_var, out_types)
    eqns.append(Eqn(arg_vars, out_vars, fun))
    cur_vars.extend(out_vars)
    cur_vars = thin(cur_vars, thin_prob)

  out_vars = gen_subset(cur_vars)
  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]

def eval_fun(fun, *args):
  def read(v):
    return env[v]
  def write(v, x):
    env[v] = x

  env = {}
  map(write, fun.in_vars, args)
  for in_vars, out_vars, f in fun.eqns:
    out_vals = f(*map(read, in_vars))
    map(write, out_vars, out_vals)

  return map(read, fun.out_vars)

counter = it.count()
def fresh_var(ty):
  return Var(counter.next(), ty)

def gen_array_type(size):
  # TODO(dougalm): randomize this
  return ArrayType((2,2), np.float32)

def gen_array_val(array_type):
  # TODO(dougalm): different sizes and dtypes
  return npr.randn(*array_type.shape)

def gen_neg(size, t):
  return (lambda x: -x), t

def gen_trig(size, t):
  op = choice([np.sin, np.cos])
  return op, t

def gen_binop(size, t1, t2):
  unifier, t_out = gen_broadcasting_unifier(t1, t2)
  binop = choice([lambda x, y: x + y,
                  lambda x, y: x * y])
  def unify_and_binop(x, y):
    x_, y_ = unifier(x, y)
    return binop(x_, y_)

  return unify_and_binop, t_out

def thin(xs, p):
  return [x for x in xs if npr.rand() > p]

def gen_broadcasting_unifier(t1, t2):
  assert t1.shape == t2.shape
  return lambda x, y: (x,y), t1
  # TODO: generate slices and paddings to match shapes

def wrap_singleton(f):
  return lambda *xs: (f(*xs),)

unary_primitive_generators = [
  (3, gen_trig),
  (1, gen_neg) ]

binary_primitive_generators = [
  (1, gen_binop)]

primitive_generators = { 1: unary_primitive_generators,
                         2: binary_primitive_generators }

def gen_nonneg_int(size):
  return npr.randint(size)

choice = npr.choice

def weighted_choice(weighted_choices):
  weights, choices = unzip2(weighted_choices)
  return npr_choice(choices, weights)

def npr_choice(xs, weights=None):
  # npr.choice isn't actually RS -> [a] -> a
  # because it inspects the components to see if they're array-like
  assert xs
  n = len(xs)
  if weights is None:
    i = npr.randint(n)
  else:
    normalizer = float(sum(weights))
    weights = [w / normalizer for w in weights]
    i = npr.choice(range(n), p=weights)
  return xs[i]

def gen_sized_subset(xs, size):
  return [npr_choice(xs) for _ in range(size)]

def gen_subset(xs):
  if not xs:
    return []

  return gen_sized_subset(xs, npr.randint(len(xs) + 1))

def gen_vals(vs):
  return [gen_array_val(v.vartype) for v in vs]

def inner_prod(xs, ys):
  xys = zip(xs, ys)
  assert all(x.shape == y.shape for x, y in xys)
  return sum(np.sum(x * y) for x, y in xys)

def jvp_fd(fun, args, tangents):
  EPS = 1e-4
  def eval_eps(eps):
    return fun(*[x if t is None else x + eps * t
                 for x, t in zip(args, tangents)])

  ys_neg = eval_eps(-EPS)
  ys_pos = eval_eps(EPS)
  ys = eval_eps(0.0)
  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
  return ys, deriv

def check_all_close(xs, ys, tol=1e-3):
  for x, y in zip(xs, ys):
    check_close(x, y, tol)

def check_close(x, y, tol=1e-3):
  assert np.shape(x) == np.shape(y)
  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
  # assert x.dtype == y.dtype
  assert np.allclose(x, y, rtol=tol, atol=tol), \
     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)

def partial_argnums(f, args, dyn_argnums):
  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
  def f_(*dyn_args):
    args = fixed_args[:]
    for i, arg in zip(dyn_argnums, dyn_args):
      args[i] = arg
    return f(*args)

  dyn_args = [args[i] for i in dyn_argnums]
  return f_, dyn_args

def jit_is_identity(fun):
  vals = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  ans = fun(*vals)
  static_argnums = thin(range(len(vals)), 0.5)
  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
  check_all_close(ans, ans_jitted)

def jvp_matches_fd(fun):
  vals = gen_vals(fun.in_vars)
  tangents = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  tangents = [tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, deriv1 = jvp_fd(fun, vals, tangents)
  ans2, deriv2 = jvp(fun, vals, tangents)
  check_all_close(ans1, ans2)
  check_all_close(deriv1, deriv2)


def vjp_matches_fd(fun):
  # print fun
  vals = gen_vals(fun.in_vars)
  in_tangents = gen_vals(fun.in_vars)
  in_cotangents = gen_vals(fun.out_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  in_tangents = [in_tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
  ans2, vjpfun = vjp(fun, *vals)
  out_cotangents = vjpfun(in_cotangents)
  check_all_close(ans1, ans2)
  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
  check_close(inner_prod_fd, inner_prod_ad)


properties = [
  jit_is_identity,
  jvp_matches_fd,
  vjp_matches_fd,
]
# vmap_matches_map ]

def run_tests():
  sizes = [3, 10]
  num_examples = 50
  cases = it.product(sizes, range(num_examples), properties)
  for i, (size, _, check_prop) in enumerate(cases):
    sys.stderr.write('\rTested: {}'.format(i))
    check_prop(gen_fun_and_types(size))
  print ""\nok""

if __name__ == ""__main__"":
  run_tests()
","from collections import namedtuple
from functools import partial
import numpy.random as npr
import jax.numpy as np
from jax import jit, jvp, vjp
import itertools as it
import sys

npr.seed(0)

from jax.util import unzip2, safe_zip, safe_map

map = safe_map
zip = safe_zip

subfun_prob = 0.5
thin_prob = 0.1
size_reduction_factor = 3

Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
Prim = namedtuple('Prim', ['fun'])
ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
Var = namedtuple('Var', ['name', 'vartype'])
Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])

def gen_fun_and_types(size):
  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
  fun, _ = gen_function(size, in_types)
  return fun

def gen_function(size, in_types):
  eqns = []
  in_vars = map(fresh_var, in_types)
  cur_vars = in_vars[:]
  for _ in range(gen_nonneg_int(size)):
    if not cur_vars:
      break
    if npr.rand() < subfun_prob:
      arg_vars = gen_subset(cur_vars)
      arg_types = [v.vartype for v in arg_vars]
      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
      fun = partial(eval_fun, fun)
    else:
      arity = choice(primitive_generators.keys())
      arg_vars = gen_sized_subset(cur_vars, arity)
      arg_types = [v.vartype for v in arg_vars]
      prim_gen = weighted_choice(primitive_generators[arity])
      fun, out_type = prim_gen(size, *arg_types)
      fun = wrap_singleton(fun)
      out_types = [out_type]

    out_vars = map(fresh_var, out_types)
    eqns.append(Eqn(arg_vars, out_vars, fun))
    cur_vars.extend(out_vars)
    cur_vars = thin(cur_vars, thin_prob)

  out_vars = gen_subset(cur_vars)
  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]

def eval_fun(fun, *args):
  def read(v):
    return env[v]
  def write(v, x):
    env[v] = x

  env = {}
  map(write, fun.in_vars, args)
  for in_vars, out_vars, f in fun.eqns:
    out_vals = f(*map(read, in_vars))
    map(write, out_vars, out_vals)

  return map(read, fun.out_vars)

counter = it.count()
def fresh_var(ty):
  return Var(counter.next(), ty)

def gen_array_type(size):
  # TODO(dougalm): randomize this
  return ArrayType((2,2), np.float32)

def gen_array_val(array_type):
  # TODO(dougalm): different sizes and dtypes
  return npr.randn(*array_type.shape)

def gen_neg(size, t):
  return (lambda x: -x), t

def gen_trig(size, t):
  op = choice([np.sin, np.cos])
  return op, t

def gen_binop(size, t1, t2):
  unifier, t_out = gen_broadcasting_unifier(t1, t2)
  binop = choice([lambda x, y: x + y,
                  lambda x, y: x * y])
  def unify_and_binop(x, y):
    x_, y_ = unifier(x, y)
    return binop(x_, y_)

  return unify_and_binop, t_out

def thin(xs, p):
  return [x for x in xs if npr.rand() > p]

def gen_broadcasting_unifier(t1, t2):
  assert t1.shape == t2.shape
  return lambda x, y: (x,y), t1
  # TODO: generate slices and paddings to match shapes

def wrap_singleton(f):
  return lambda *xs: (f(*xs),)

unary_primitive_generators = [
  (3, gen_trig),
  (1, gen_neg) ]

binary_primitive_generators = [
  (1, gen_binop)]

primitive_generators = { 1: unary_primitive_generators,
                         2: binary_primitive_generators }

def gen_nonneg_int(size):
  return npr.randint(size)

choice = npr.choice

def weighted_choice(weighted_choices):
  weights, choices = unzip2(weighted_choices)
  return npr_choice(choices, weights)

def npr_choice(xs, weights=None):
  # npr.choice isn't actually RS -> [a] -> a
  # because it inspects the components to see if they're array-like
  assert xs
  n = len(xs)
  if weights is None:
    i = npr.randint(n)
  else:
    normalizer = float(sum(weights))
    weights = [w / normalizer for w in weights]
    i = npr.choice(range(n), p=weights)
  return xs[i]

def gen_sized_subset(xs, size):
  return [npr_choice(xs) for _ in range(size)]

def gen_subset(xs):
  if not xs:
    return []

  return gen_sized_subset(xs, npr.randint(len(xs) + 1))

def gen_vals(vs):
  return [gen_array_val(v.vartype) for v in vs]

def inner_prod(xs, ys):
  xys = zip(xs, ys)
  assert all(x.shape == y.shape for x, y in xys)
  return sum(np.sum(x * y) for x, y in xys)

def jvp_fd(fun, args, tangents):
  EPS = 1e-4
  def eval_eps(eps):
    return fun(*[x if t is None else x + eps * t
                 for x, t in zip(args, tangents)])

  ys_neg = eval_eps(-EPS)
  ys_pos = eval_eps(EPS)
  ys = eval_eps(0.0)
  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
  return ys, deriv

def check_all_close(xs, ys, tol=1e-3):
  for x, y in zip(xs, ys):
    check_close(x, y, tol)

def check_close(x, y, tol=1e-3):
  assert np.shape(x) == np.shape(y)
  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
  # assert x.dtype == y.dtype
  assert np.allclose(x, y, rtol=tol, atol=tol), \
     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)

def partial_argnums(f, args, dyn_argnums):
  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
  def f_(*dyn_args):
    args = fixed_args[:]
    for i, arg in zip(dyn_argnums, dyn_args):
      args[i] = arg
    return f(*args)

  dyn_args = [args[i] for i in dyn_argnums]
  return f_, dyn_args

def jit_is_identity(fun):
  vals = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  ans = fun(*vals)
  static_argnums = thin(range(len(vals)), 0.5)
  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
  check_all_close(ans, ans_jitted)

def jvp_matches_fd(fun):
  vals = gen_vals(fun.in_vars)
  tangents = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  tangents = [tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, deriv1 = jvp_fd(fun, vals, tangents)
  ans2, deriv2 = jvp(fun, vals, tangents)
  check_all_close(ans1, ans2)
  check_all_close(deriv1, deriv2)


def vjp_matches_fd(fun):
  vals = gen_vals(fun.in_vars)
  in_tangents = gen_vals(fun.in_vars)
  in_cotangents = gen_vals(fun.out_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  in_tangents = [in_tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
  ans2, vjpfun = vjp(fun, *vals)
  out_cotangents = vjpfun(in_cotangents)
  check_all_close(ans1, ans2)
  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
  check_close(inner_prod_fd, inner_prod_ad)

properties = [
  jit_is_identity,
  jvp_matches_fd,
  vjp_matches_fd,
]
# vmap_matches_map ]

def run_tests():
  sizes = [3, 10]
  num_examples = 50
  cases = it.product(sizes, range(num_examples), properties)
  for i, (size, _, check_prop) in enumerate(cases):
    sys.stderr.write('\rTested: {}'.format(i))
    try:
      fun = gen_fun_and_types(size)
      check_prop(fun)
    except:
      print fun
      raise

  print ""\nok""

if __name__ == ""__main__"":
  run_tests()
","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 34162c5a22a36459582adb129152e67298c55d85..828e12c136ff0b3ee7e59c1b5ef31957714e54ef 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -216,7 +216,6 @@ def jvp_matches_fd(fun):
 
 
 def vjp_matches_fd(fun):
-  # print fun
   vals = gen_vals(fun.in_vars)
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
@@ -232,7 +231,6 @@ def vjp_matches_fd(fun):
   inner_prod_ad = inner_prod(in_tangents, out_cotangents)
   check_close(inner_prod_fd, inner_prod_ad)
 
-
 properties = [
   jit_is_identity,
   jvp_matches_fd,
@@ -246,7 +244,13 @@ def run_tests():
   cases = it.product(sizes, range(num_examples), properties)
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
-    check_prop(gen_fun_and_types(size))
+    try:
+      fun = gen_fun_and_types(size)
+      check_prop(fun)
+    except:
+      print fun
+      raise
+
   print ""\nok""
 
 if __name__ == ""__main__"":
",Null/None handling,"Refactor quickercheck tests to improve error handling and debugging 

* Removed unused print statement in `vjp_matches_fd` function"
585f011d63e77bfafc200fff50d13f0af73e17f7,Dougal: Error message for unimplemented numpy functions,jax/numpy/lax_numpy.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


# TODO plan how to handle unsupported ops
def _not_implemented(fun):
  return None

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


def _not_implemented(fun):
  def wrapped(*args, **kwargs):
    raise Exception(""Numpy function {} not yet implemented"".format(fun))
  return wrapped

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d0a43546abd181d602d7e9bdb48dd89e031d8c16..82b078bd9c53753948b9ec4ed500b84d46047167 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -820,9 +820,10 @@ def _argminmax(op, a, axis):
   return min(mask_idxs, axis)
 
 
-# TODO plan how to handle unsupported ops
 def _not_implemented(fun):
-  return None
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
 
 argpartition = _not_implemented(onp.argpartition)
 argsort = _not_implemented(onp.argsort)
",Incorrect caching / stale cache,Raise informative exceptions for unimplemented NumPy functions in lax_numpy.
2e4ff400faa0bb8c7137810639cf769a683730c5,Dougal Maclaurin: Fixed bug in vjp with constant-zero tangent outputs,jax/api.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import itertools

import numpy as onp

from . import core
from . import linear_util as lu
from .core import pack, eval_jaxpr
from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                        tree_map)
from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
from .abstract_arrays import ShapedArray
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching

map = safe_map

def jit(fun, static_argnums=()):
  def f_jitted(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
    f, dyn_args = argnums_partial(f, dyn_argnums, args)
    args_flat, in_trees = unzip2(map(tree_to_jaxtuples, dyn_args))
    check_args(args_flat)
    flat_fun, out_tree = flatten_fun(f, in_trees)
    out_flat = xla.xla_call(flat_fun, *args_flat)
    return build_tree(out_tree(), out_flat)

  return f_jitted

def grad(fun, argnums=0):
  def grad_f(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    f_partial, dyn_args = argnums_partial(f, argnums, args)
    ans, vjp_py = vjp(f_partial, *dyn_args)
    check_scalar(ans)
    g = vjp_py(onp.ones((), onp.result_type(ans)))
    return g[0] if isinstance(argnums, int) else g


  return grad_f

@curry
def jacfwd(fun, x):
  fun = lu.wrap_init(fun)
  pushfwd = partial(jvp, fun, (x,))
  std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
  y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

@curry
def jacrev(fun, x):
  fun = lu.wrap_init(fun)
  y, pullback = vjp(fun, x)
  std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
  jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

def hessian(fun):
  return jacfwd(jacrev(fun))

def vmap(fun, *args, **kwargs):
  in_bdims = kwargs.pop(""in_bdims"", 0)
  out_bdim = kwargs.pop(""out_bdim"", 0)
  if kwargs:
    msg = ""vmap keyword args must be 'in_bdims' and/or 'out_bdim', got {}.""
    raise TypeError(msg.format(', '.join(kwargs)))

  if type(in_bdims) is int:
    in_bdims = (in_bdims,) * len(args)
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_flat = batching.batch(flat_fun, in_flat, in_bdims, out_bdim)
  return build_tree(out_tree(), out_flat)

def jvp(fun, primals, tangents):
  def flatten_arg(primal, tangent):
    primal_jtuple, tree_def = tree_to_jaxtuples(primal)
    tangent_jtuple, tree_def_2 = tree_to_jaxtuples(tangent)
    assert tree_def == tree_def_2, (tree_def, tree_def_2)
    return primal_jtuple, tangent_jtuple, tree_def

  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  ps_flat, ts_flat, in_trees = unzip3(map(flatten_arg, primals, tangents))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_tangent = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
  return (build_tree(out_tree(), out_primal), build_tree(out_tree(), out_tangent))

def linearize(traceable, *primals):
  fun = lu.wrap_init(traceable)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_pval, jaxpr, consts = ad.linearize(flat_fun, *primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  lifted_jvp = partial(lift_linearized, jaxpr, consts, (in_trees, out_tree), out_pval)
  return out_primal_py, lifted_jvp

def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
  def fun(*args):
    primals = pack(args) # doesn't matter what these are-they'll be ignored
    tangents = pack(args)
    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
    return list(pe.merge_pvals(ans, out_pval))[1]

  return unflatten_fun(fun, io_tree, *py_args)

def vjp(fun, *primals):
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  check_args(primals_flat)
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  ct_in_trees = [out_tree]
  ct_out_tree = PyTreeDef(node_types[tuple], None, in_trees)
  def out_vjp_packed(cotangent_in):
    return out_vjp(cotangent_in)
  vjp_py = partial(unflatten_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
  return out_primal_py, vjp_py


def trace_to_jaxpr(traceable, py_pvals, **kwargs):
  fun = lu.wrap_init(traceable)
  pvals, in_trees = unzip2(map(tree_to_pval_tuples, py_pvals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
  return jaxpr, consts, out_pval, (in_trees, out_tree())

def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
  def fun(*args):
    ans = eval_jaxpr(jaxpr, consts, (), *args)
    return pe.merge_pvals(ans, pvals)
  return unflatten_fun(fun, io_tree, *py_args)


device_put = jit(lambda x: x)
device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
device_get = partial(tree_map, device_get_array)


@lu.transformation_with_aux
def flatten_fun(in_trees, *args, **kwargs):
  py_args = map(build_tree, in_trees, args)
  ans = yield py_args
  yield process_pytree(pack, ans)


def unflatten_fun(fun, io_tree, *py_args):
  in_trees_expected, out_tree = io_tree
  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
  for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
    if in_tree != expected:
      raise TypeError(""Expected {}, got {}"".format(expected, in_tree))

  ans = fun(*args)
  return build_tree(out_tree, ans)


tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
tree_to_jaxtuples = partial(process_pytree, pack)


def argnums_partial(f, dyn_argnums, args):
  if isinstance(dyn_argnums, int):
    dyn_argnums = (dyn_argnums,)
  else:
    dyn_argnums = tuple(dyn_argnums)
  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
                      for i, arg in enumerate(args)])
  dyn_args = [args[i] for i in dyn_argnums]
  return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args

@lu.transformation
def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
  args = [None if arg is None else arg.val for arg in fixed_args]
  for i, arg in zip(dyn_argnums, dyn_args):
    args[i] = arg
  ans = yield args
  yield ans

def check_args(args):
  for arg in args:
    if not (isinstance(arg, core.Tracer) or core.valid_jaxtype(arg)):
      raise TypeError(""Argument '{}' of type {} is not a valid JAX type""
                      .format(arg, type(arg)))

def check_scalar(x):
  msg = ""Gradient only defined for scalar-output functions. Output was: {}"".format
  try:
    aval = core.get_aval(x)
    if not (isinstance(aval, ShapedArray) and aval.shape == ()):
      raise TypeError(msg(x))
  except TypeError:
    raise TypeError(msg(x))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import itertools

import numpy as onp

from . import core
from . import linear_util as lu
from .core import pack, eval_jaxpr
from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                        tree_map)
from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
from .abstract_arrays import ShapedArray
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching

map = safe_map

def jit(fun, static_argnums=()):
  def f_jitted(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
    f, dyn_args = argnums_partial(f, dyn_argnums, args)
    args_flat, in_trees = unzip2(map(tree_to_jaxtuples, dyn_args))
    check_args(args_flat)
    flat_fun, out_tree = flatten_fun(f, in_trees)
    out_flat = xla.xla_call(flat_fun, *args_flat)
    return build_tree(out_tree(), out_flat)

  return f_jitted

def grad(fun, argnums=0):
  def grad_f(*args, **kwargs):
    f = lu.wrap_init(fun, kwargs)
    f_partial, dyn_args = argnums_partial(f, argnums, args)
    ans, vjp_py = vjp(f_partial, *dyn_args)
    check_scalar(ans)
    g = vjp_py(onp.ones((), onp.result_type(ans)))
    return g[0] if isinstance(argnums, int) else g


  return grad_f

@curry
def jacfwd(fun, x):
  fun = lu.wrap_init(fun)
  pushfwd = partial(jvp, fun, (x,))
  std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
  y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

@curry
def jacrev(fun, x):
  fun = lu.wrap_init(fun)
  y, pullback = vjp(fun, x)
  std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
  jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
  return jac_flat.reshape(onp.shape(y) + onp.shape(x))

def hessian(fun):
  return jacfwd(jacrev(fun))

def vmap(fun, *args, **kwargs):
  in_bdims = kwargs.pop(""in_bdims"", 0)
  out_bdim = kwargs.pop(""out_bdim"", 0)
  if kwargs:
    msg = ""vmap keyword args must be 'in_bdims' and/or 'out_bdim', got {}.""
    raise TypeError(msg.format(', '.join(kwargs)))

  if type(in_bdims) is int:
    in_bdims = (in_bdims,) * len(args)
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_flat = batching.batch(flat_fun, in_flat, in_bdims, out_bdim)
  return build_tree(out_tree(), out_flat)

def jvp(fun, primals, tangents):
  def flatten_arg(primal, tangent):
    primal_jtuple, tree_def = tree_to_jaxtuples(primal)
    tangent_jtuple, tree_def_2 = tree_to_jaxtuples(tangent)
    assert tree_def == tree_def_2, (tree_def, tree_def_2)
    return primal_jtuple, tangent_jtuple, tree_def

  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  ps_flat, ts_flat, in_trees = unzip3(map(flatten_arg, primals, tangents))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_tangent = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
  return (build_tree(out_tree(), out_primal), build_tree(out_tree(), out_tangent))

def linearize(traceable, *primals):
  fun = lu.wrap_init(traceable)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_pval, jaxpr, consts = ad.linearize(flat_fun, *primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  lifted_jvp = partial(lift_linearized, jaxpr, consts, (in_trees, out_tree), out_pval)
  return out_primal_py, lifted_jvp

def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
  def fun(*args):
    primals = pack(args) # doesn't matter what these are-they'll be ignored
    tangents = pack(args)
    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
    return pe.merge_pvals(ans, out_pval)

  return unflatten_fun(fun, io_tree, *py_args)

def vjp(fun, *primals):
  if not isinstance(fun, lu.WrappedFun):
    fun = lu.wrap_init(fun)
  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
  check_args(primals_flat)
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)
  out_tree = out_tree()
  out_primal_py = build_tree(out_tree, out_primal)
  ct_in_trees = [out_tree]
  ct_out_tree = PyTreeDef(node_types[tuple], None, in_trees)
  def out_vjp_packed(cotangent_in):
    return out_vjp(cotangent_in)
  vjp_py = partial(unflatten_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
  return out_primal_py, vjp_py


def trace_to_jaxpr(traceable, py_pvals, **kwargs):
  fun = lu.wrap_init(traceable)
  pvals, in_trees = unzip2(map(tree_to_pval_tuples, py_pvals))
  flat_fun, out_tree = flatten_fun(fun, in_trees)
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
  return jaxpr, consts, out_pval, (in_trees, out_tree())

def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
  def fun(*args):
    ans = eval_jaxpr(jaxpr, consts, (), *args)
    return pe.merge_pvals(ans, pvals)
  return unflatten_fun(fun, io_tree, *py_args)


device_put = jit(lambda x: x)
device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
device_get = partial(tree_map, device_get_array)


@lu.transformation_with_aux
def flatten_fun(in_trees, *args, **kwargs):
  py_args = map(build_tree, in_trees, args)
  ans = yield py_args
  yield process_pytree(pack, ans)


def unflatten_fun(fun, io_tree, *py_args):
  in_trees_expected, out_tree = io_tree
  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
  for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
    if in_tree != expected:
      raise TypeError(""Expected {}, got {}"".format(expected, in_tree))

  ans = fun(*args)
  return build_tree(out_tree, ans)


tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
tree_to_jaxtuples = partial(process_pytree, pack)


def argnums_partial(f, dyn_argnums, args):
  if isinstance(dyn_argnums, int):
    dyn_argnums = (dyn_argnums,)
  else:
    dyn_argnums = tuple(dyn_argnums)
  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
                      for i, arg in enumerate(args)])
  dyn_args = [args[i] for i in dyn_argnums]
  return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args

@lu.transformation
def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
  args = [None if arg is None else arg.val for arg in fixed_args]
  for i, arg in zip(dyn_argnums, dyn_args):
    args[i] = arg
  ans = yield args
  yield ans

def check_args(args):
  for arg in args:
    if not (isinstance(arg, core.Tracer) or core.valid_jaxtype(arg)):
      raise TypeError(""Argument '{}' of type {} is not a valid JAX type""
                      .format(arg, type(arg)))

def check_scalar(x):
  msg = ""Gradient only defined for scalar-output functions. Output was: {}"".format
  try:
    aval = core.get_aval(x)
    if not (isinstance(aval, ShapedArray) and aval.shape == ()):
      raise TypeError(msg(x))
  except TypeError:
    raise TypeError(msg(x))
","diff --git a/jax/api.py b/jax/api.py
index 4bd4005ea8e80a9f399dba959eed0951524c855b..9a2240d58492f448dcc6fd42b910c4a720876feb 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -123,8 +123,8 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
   def fun(*args):
     primals = pack(args) # doesn't matter what these are-they'll be ignored
     tangents = pack(args)
-    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
-    return list(pe.merge_pvals(ans, out_pval))[1]
+    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
+    return pe.merge_pvals(ans, out_pval)
 
   return unflatten_fun(fun, io_tree, *py_args)
 
",Incorrect default / config value,Update `lift_linearized` to correctly handle output values by changing the return type of `eval_jaxpr` and removing
2e4ff400faa0bb8c7137810639cf769a683730c5,Dougal Maclaurin: Fixed bug in vjp with constant-zero tangent outputs,jax/interpreters/ad.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from . import partial_eval as pe
from . import xla
from .. import core as core
from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                       zeros_like_p, zero, Zero)
from ..util import unzip2, unzip3, safe_map, safe_zip, partial
from ..tree_util import process_pytree, build_tree, register_pytree_node
from ..linear_util import thunk, staged, transformation, transformation_with_aux, wrap_init

from six.moves import builtins, reduce

zip = safe_zip
map = safe_map

def jvp(fun):
  return jvpfun(jvp_subtrace(fun))

@transformation
def jvpfun(primals, tangents):
  with new_master(JVPTrace) as master:
    out_primal, out_tangent = yield master, primals, tangents
    del master
  out_tangent = instantiate_zeros(out_primal, out_tangent)
  yield (out_primal, out_tangent)

@transformation
def jvp_subtrace(master, primals, tangents):
  trace = JVPTrace(master, core.cur_sublevel())
  for x in list(primals) + list(tangents):
    if isinstance(x, Tracer):
      assert x.trace.level < trace.level
  ans = yield map(partial(JVPTracer, trace), primals, tangents)
  out_tracer = trace.full_raise(ans)
  out_primal, out_tangent = out_tracer.primal, out_tracer.tangent
  yield (out_primal, out_tangent)

@transformation
def pack_output(*args):
  ans = yield args
  yield pack(ans)

def linearize(traceable, *primals):
  jvpfun = pack_output(jvp(traceable))
  tangent_avals = [get_aval(p).at_least_vspace() for p in primals]
  in_pvals = (pe.PartialVal((None, pack(primals))),
              pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
  out_pv, out_const = out_pval
  assert out_pv is None or out_pv[0] is None
  primal_out = tuple(out_const)[0]
  return primal_out, out_pval, jaxpr, consts


def vjp(traceable, primals):
  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
  def vjp_(ct):
    dummy_primal_and_ct = pack((core.unit, ct))
    _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
    return instantiate_zeros(pack(primals), arg_cts[1])

  return out_primal, vjp_


def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
  def write_cotangent(v, ct):
    # assert v not in primal_env
    if ct is not None:
      ct_env[v] = add_tangents(ct_env[v], ct) if v in ct_env else ct

  def read_cotangent(v):
    return ct_env.get(v, zero)

  primal_env = {v: val
                for v, val in zip(jaxpr.freevars, freevar_vals)
                if val is not None}
  primal_env.update(zip(jaxpr.constvars, consts))
  ct_env = {jaxpr.outvar: cotangent_in}

  for eqn in jaxpr.eqns[::-1]:
    cts_in = map(read_cotangent, eqn.outvars)
    ct_in = TangentTuple(cts_in) if eqn.destructure else cts_in[0]
    invals = map(primal_env.get, eqn.invars)
    if eqn.bound_subjaxprs:
      subjaxprs, sub_consts, sub_freevar_vals = unzip3([
          (subjaxpr,
           map(primal_env.get, const_vars),
           map(primal_env.get, bound_vars))
          for subjaxpr, const_vars, bound_vars in eqn.bound_subjaxprs])
      cts_out, ct_free_vars_out = get_primitive_transpose(eqn.primitive)(
          eqn.params, subjaxprs, sub_consts, sub_freevar_vals, invals, ct_in)
      # TODO(dougalm): support cases != 1
      assert(len(eqn.bound_subjaxprs) == 1)
      _, _, bound_vars = eqn.bound_subjaxprs[0]
      map(write_cotangent, bound_vars, ct_free_vars_out)
    else:
      cts_out = get_primitive_transpose(eqn.primitive)(ct_in, *invals, **eqn.params)

    if cts_out is zero:
      cts_out = [zero for _ in eqn.invars]
    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
    for var, ct in builtins.zip(eqn.invars, cts_out):
      write_cotangent(var, ct)

  cotangents_out = map(read_cotangent, jaxpr.invars)
  freevar_cts = map(read_cotangent, jaxpr.freevars)
  return freevar_cts, cotangents_out

def get_primitive_transpose(p):
  try:
    return primitive_transposes[p]
  except KeyError:
    raise NotImplementedError(
      ""Reverse-mode differentiation rule for '{}' not implemented"".format(p))

class TangentTuple(tuple):
  pass

register_pytree_node(
    TangentTuple, lambda xs: (xs, None), lambda _, xs: TangentTuple(xs))

class JVPTrace(Trace):

  def pure(self, val):
    return JVPTracer(self, val, zero)

  def lift(self, val):
    return JVPTracer(self, val, zero)

  def sublift(self,val):
    return JVPTracer(self, val.primal, val.tangent)

  def process_primitive(self, primitive, tracers, params):
    primals_in = [t.primal for t in tracers]
    tangents_in = [t.tangent for t in tracers]
    try:
      jvp = primitive_jvps[primitive]
    except KeyError:
      raise NotImplementedError(
          ""Forward-mode differentiation rule for '{}' not implemented""
          .format(primitive))
    primal_out, tangent_out = jvp(primals_in, tangents_in, **params)
    return JVPTracer(self, primal_out, tangent_out)

  def process_call(self, call_primitive, f, tracers, params):
    primals = [t.primal for t in tracers]
    tangents = [t.tangent for t in tracers]
    nonzero_tangents, in_tree_def = tree_to_jaxtuples(tangents)
    f, out_tree_def = traceable(jvp_subtrace(f, self.master), in_tree_def)
    result = call_primitive.bind(f, pack(primals), nonzero_tangents)
    primal_out, tangent_out = build_tree(out_tree_def(), result)
    return JVPTracer(self, primal_out, tangent_out)

  def post_process_call(self, _, out_tracer):
    out_jtuple, tree_def = tree_to_jaxtuples((out_tracer.primal, out_tracer.tangent))
    master = self.master
    def todo(x):
      trace = JVPTrace(master, core.cur_sublevel())
      return JVPTracer(trace, *build_tree(tree_def, x))

    return out_jtuple, todo

  def join(self, xt, yt):
    isfull = lambda t: t is not zero and not isinstance(t, TangentTuple)
    if isfull(xt) and isfull(yt):
      return xt, yt
    elif isfull(xt):
      if yt is zero:
        return xt, zeros_like_jaxval(xt)
      elif isinstance(xt, TangentTuple):
        return xt, JaxTuple(map(zeros_like_jaxval, xt))
      else:
        raise TypeError
    elif isfull(yt):
      if xt is zero:
        return zeros_like_jaxval(yt), yt
      elif isinstance(xt, TangentTuple):
        return JaxTuple(map(zeros_like_jaxval, yt)), yt
      else:
        raise TypeError
    elif isinstance(xt, TangentTuple) or isinstance(yt, TangentTuple):
      if xt is zero:
        xt = TangentTuple((zero,) * len(yt))
      elif yt is zero:
        yt = TangentTuple((zero,) * len(xt))
      return TangentTuple(map(self.join), xt, yt)
    elif xt is zero and yt is zero:
      return xt, yt
    else:
      raise TypeError((xt, yt))

  def pack(self, tracers):
    primals = pack(t.primal for t in tracers)
    tangents = TangentTuple([t.tangent for t in tracers])
    return JVPTracer(self, primals, tangents)


class JVPTracer(Tracer):
  def __init__(self, trace, primal, tangent):
    self.trace = trace
    self.primal = primal
    self.tangent = tangent

  @property
  def aval(self):
    # TODO(dougalm): add epsilon ball
    return get_aval(self.primal)

  def unpack(self):
    if self.tangent is zero:
      tangent = [zero] * len(self.primal)
    else:
      tangent = self.tangent
    return map(partial(JVPTracer, self.trace), self.primal, tangent)

  def full_lower(self):
    if self.tangent is zero:
      return core.full_lower(self.primal)
    else:
      return self

# -------------------- Primitives --------------------


primitive_jvps = {}
composite_jvps = {}

primitive_transposes = {}


def deflinear(primitive, transpose_rule):
  primitive_jvps[primitive] = partial(linear_jvp, primitive)
  primitive_transposes[primitive] = partial(linear_transpose, transpose_rule)


def linear_jvp(primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  if all(tangent is zero for tangent in tangents):
    return val_out, zero
  else:
    tangents = map(instantiate_zeros, primals, tangents)
    return val_out, primitive.bind(*tangents, **params)


def linear_transpose(transpose_rule, cotangent, *args, **kwargs):
  return zero if cotangent is zero else transpose_rule(cotangent, **kwargs)


def defjvp(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp, jvprules, primitive)


def standard_jvp(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def defjvp2(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp2, jvprules, primitive)


def standard_jvp2(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, val_out, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def add_tangents(x, y):
  if x is zero:
    return y
  elif y is zero:
    return x
  else:
    return add_jaxvals(x, y)


def defbilinear_broadcasting(bcast, prim, lhs_rule, rhs_rule):
  assert isinstance(prim, Primitive)
  lhs_jvp = lambda g, x, y, **kwargs: prim.bind(bcast(g, y), y, **kwargs)
  rhs_jvp = lambda g, x, y, **kwargs: prim.bind(x, bcast(g, x), **kwargs)
  defjvp(prim, lhs_jvp, rhs_jvp)
  primitive_transposes[prim] = partial(bilinear_transpose, lhs_rule, rhs_rule)
defbilinear = partial(defbilinear_broadcasting, lambda g, x: g)


def bilinear_transpose(lhs_rule, rhs_rule, cotangent, x, y, **kwargs):
  assert x is None or y is None
  if x is None:
    out = zero if cotangent is zero else lhs_rule(cotangent, y, **kwargs)
    return out, None
  else:
    out = zero if cotangent is zero else rhs_rule(cotangent, x, **kwargs)
    return None, out


def defjvp_zero(primitive):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(zero_jvp, primitive)


def zero_jvp(primitive, primals, tangents, **params):
  return primitive.bind(*primals, **params), zero


deflinear(zeros_like_p, lambda t: (zeros_like_jaxval(t),))
deflinear(core.identity_p, lambda t: (t,))
deflinear(core.pack_p, lambda t: list(t) if t is not zero else zero)
deflinear(add_jaxvals_p, lambda t: (t, t))


def instantiate_zeros(example, tangent):
  if tangent is zero:
    return zeros_like_jaxval(example)
  elif isinstance(tangent, TangentTuple):
    return pack(map(instantiate_zeros, example, tangent))
  else:
    return tangent

@transformation_with_aux
def traceable(in_tree_def, new_primals, new_tangents):
  new_tangents = build_tree(in_tree_def, new_tangents)
  primal_out, tangent_out = yield new_primals, new_tangents
  out_jtuple, tree_def = tree_to_jaxtuples((primal_out, tangent_out))
  yield out_jtuple, tree_def

@transformation_with_aux
def transposed_fun(jaxpr, in_tree_def, args):
  consts, freevar_vals, ct = args
  ct, freevar_vals = build_tree(in_tree_def, (ct, freevar_vals))
  freevar_cts, cotangents_out = yield jaxpr, consts, freevar_vals, ct
  out_jtuple, tree_def = tree_to_jaxtuples((cotangents_out, freevar_cts))
  yield out_jtuple, tree_def

def call_transpose(primitive, params, jaxpr, consts, freevar_vals, args, ct):
  jaxpr, = jaxpr
  consts, = consts
  freevar_vals, = freevar_vals
  assert isinstance(jaxpr, core.Jaxpr)
  assert all(a is None for a in args), ""TODO(dougalm): handle non-tangent primal args""
  (ct, freevar_vals), in_tree_def = tree_to_jaxtuples((ct, freevar_vals))
  fun = wrap_init(backward_pass)
  fun, out_tree_def = transposed_fun(fun, jaxpr, in_tree_def)
  all_args = pack((pack(consts), pack(freevar_vals), ct))
  # TODO(dougalm): consider signalling to bind that there are no traces in the closure
  ans = primitive.bind(fun, all_args, **params)
  return build_tree(out_tree_def(), ans)

primitive_transposes[core.call_p] = partial(call_transpose, call_p)
primitive_transposes[pe.compiled_call_p] = partial(call_transpose, pe.compiled_call_p)
primitive_transposes[xla.xla_call_p] = partial(call_transpose, xla.xla_call_p)


tree_to_jaxtuples = partial(process_pytree, pack)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from . import partial_eval as pe
from . import xla
from .. import core as core
from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                       zeros_like_p, zero, Zero)
from ..util import unzip2, unzip3, safe_map, safe_zip, partial
from ..tree_util import process_pytree, build_tree, register_pytree_node
from ..linear_util import thunk, staged, transformation, transformation_with_aux, wrap_init

from six.moves import builtins, reduce

zip = safe_zip
map = safe_map

def jvp(fun):
  return jvpfun(jvp_subtrace(fun))

@transformation
def jvpfun(primals, tangents):
  with new_master(JVPTrace) as master:
    out_primal, out_tangent = yield master, primals, tangents
    del master
  out_tangent = instantiate_zeros(out_primal, out_tangent)
  yield (out_primal, out_tangent)

@transformation
def jvp_subtrace(master, primals, tangents):
  trace = JVPTrace(master, core.cur_sublevel())
  for x in list(primals) + list(tangents):
    if isinstance(x, Tracer):
      assert x.trace.level < trace.level
  ans = yield map(partial(JVPTracer, trace), primals, tangents)
  out_tracer = trace.full_raise(ans)
  out_primal, out_tangent = out_tracer.primal, out_tracer.tangent
  yield (out_primal, out_tangent)

@transformation
def pack_output(*args):
  ans = yield args
  yield pack(ans)

def linearize(traceable, *primals):
  jvpfun = pack_output(jvp(traceable))
  tangent_avals = [get_aval(p).at_least_vspace() for p in primals]
  in_pvals = (pe.PartialVal((None, pack(primals))),
              pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
  jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
  pval_primal, pval_tangent = unpair_pval(out_pval)
  aval_primal, const_primal = pval_primal
  assert aval_primal is None
  return const_primal, pval_tangent, jaxpr, consts

def vjp(traceable, primals):
  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
  def vjp_(ct):
    ct = ignore_consts(ct, pval)
    dummy_primal_and_ct = pack((core.unit, ct))
    _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
    return instantiate_zeros(pack(primals), arg_cts[1])

  return out_primal, vjp_

def ignore_consts(ct, pval):
  aval, const = pval
  if isinstance(aval, core.AbstractValue):
    return ct
  elif isinstance(aval, pe.JaxprTracerTuple):
    return pack(map(ignore_consts, ct, zip(aval, const)))
  elif aval is None:
    return core.unit
  else:
    raise TypeError(aval)

def unpair_pval(pval):
  aval, const = pval
  const_1, const_2 = const
  if aval is None:
    return (None, const_1), (None, const_2)
  else:
    aval_1, aval_2 = aval
    return (aval_1, const_1), (aval_2, const_2)

def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
  def write_cotangent(v, ct):
    # assert v not in primal_env
    if ct is not None:
      ct_env[v] = add_tangents(ct_env[v], ct) if v in ct_env else ct

  def read_cotangent(v):
    return ct_env.get(v, zero)

  primal_env = {v: val
                for v, val in zip(jaxpr.freevars, freevar_vals)
                if val is not None}
  primal_env.update(zip(jaxpr.constvars, consts))
  ct_env = {jaxpr.outvar: cotangent_in}

  for eqn in jaxpr.eqns[::-1]:
    cts_in = map(read_cotangent, eqn.outvars)
    ct_in = TangentTuple(cts_in) if eqn.destructure else cts_in[0]
    invals = map(primal_env.get, eqn.invars)
    if eqn.bound_subjaxprs:
      subjaxprs, sub_consts, sub_freevar_vals = unzip3([
          (subjaxpr,
           map(primal_env.get, const_vars),
           map(primal_env.get, bound_vars))
          for subjaxpr, const_vars, bound_vars in eqn.bound_subjaxprs])
      cts_out, ct_free_vars_out = get_primitive_transpose(eqn.primitive)(
          eqn.params, subjaxprs, sub_consts, sub_freevar_vals, invals, ct_in)
      # TODO(dougalm): support cases != 1
      assert(len(eqn.bound_subjaxprs) == 1)
      _, _, bound_vars = eqn.bound_subjaxprs[0]
      map(write_cotangent, bound_vars, ct_free_vars_out)
    else:
      cts_out = get_primitive_transpose(eqn.primitive)(ct_in, *invals, **eqn.params)

    if cts_out is zero:
      cts_out = [zero for _ in eqn.invars]

    for var, ct in zip(eqn.invars, cts_out):
      write_cotangent(var, ct)

  cotangents_out = map(read_cotangent, jaxpr.invars)
  freevar_cts = map(read_cotangent, jaxpr.freevars)
  return freevar_cts, cotangents_out

def get_primitive_transpose(p):
  try:
    return primitive_transposes[p]
  except KeyError:
    raise NotImplementedError(
      ""Reverse-mode differentiation rule for '{}' not implemented"".format(p))

class TangentTuple(tuple):
  pass

register_pytree_node(
    TangentTuple, lambda xs: (xs, None), lambda _, xs: TangentTuple(xs))

class JVPTrace(Trace):

  def pure(self, val):
    return JVPTracer(self, val, zero)

  def lift(self, val):
    return JVPTracer(self, val, zero)

  def sublift(self,val):
    return JVPTracer(self, val.primal, val.tangent)

  def process_primitive(self, primitive, tracers, params):
    primals_in = [t.primal for t in tracers]
    tangents_in = [t.tangent for t in tracers]
    try:
      jvp = primitive_jvps[primitive]
    except KeyError:
      raise NotImplementedError(
          ""Forward-mode differentiation rule for '{}' not implemented""
          .format(primitive))
    primal_out, tangent_out = jvp(primals_in, tangents_in, **params)
    return JVPTracer(self, primal_out, tangent_out)

  def process_call(self, call_primitive, f, tracers, params):
    primals = [t.primal for t in tracers]
    tangents = [t.tangent for t in tracers]
    nonzero_tangents, in_tree_def = tree_to_jaxtuples(tangents)
    f, out_tree_def = traceable(jvp_subtrace(f, self.master), in_tree_def)
    result = call_primitive.bind(f, pack(primals), nonzero_tangents)
    primal_out, tangent_out = build_tree(out_tree_def(), result)
    return JVPTracer(self, primal_out, tangent_out)

  def post_process_call(self, _, out_tracer):
    out_jtuple, tree_def = tree_to_jaxtuples((out_tracer.primal, out_tracer.tangent))
    master = self.master
    def todo(x):
      trace = JVPTrace(master, core.cur_sublevel())
      return JVPTracer(trace, *build_tree(tree_def, x))

    return out_jtuple, todo

  def join(self, xt, yt):
    isfull = lambda t: t is not zero and not isinstance(t, TangentTuple)
    if isfull(xt) and isfull(yt):
      return xt, yt
    elif isfull(xt):
      if yt is zero:
        return xt, zeros_like_jaxval(xt)
      elif isinstance(xt, TangentTuple):
        return xt, JaxTuple(map(zeros_like_jaxval, xt))
      else:
        raise TypeError
    elif isfull(yt):
      if xt is zero:
        return zeros_like_jaxval(yt), yt
      elif isinstance(xt, TangentTuple):
        return JaxTuple(map(zeros_like_jaxval, yt)), yt
      else:
        raise TypeError
    elif isinstance(xt, TangentTuple) or isinstance(yt, TangentTuple):
      if xt is zero:
        xt = TangentTuple((zero,) * len(yt))
      elif yt is zero:
        yt = TangentTuple((zero,) * len(xt))
      return TangentTuple(map(self.join), xt, yt)
    elif xt is zero and yt is zero:
      return xt, yt
    else:
      raise TypeError((xt, yt))

  def pack(self, tracers):
    primals = pack(t.primal for t in tracers)
    tangents = TangentTuple([t.tangent for t in tracers])
    return JVPTracer(self, primals, tangents)


class JVPTracer(Tracer):
  def __init__(self, trace, primal, tangent):
    self.trace = trace
    self.primal = primal
    self.tangent = tangent

  @property
  def aval(self):
    # TODO(dougalm): add epsilon ball
    return get_aval(self.primal)

  def unpack(self):
    if self.tangent is zero:
      tangent = [zero] * len(self.primal)
    else:
      tangent = self.tangent
    return map(partial(JVPTracer, self.trace), self.primal, tangent)

  def full_lower(self):
    if self.tangent is zero:
      return core.full_lower(self.primal)
    else:
      return self

# -------------------- Primitives --------------------


primitive_jvps = {}
composite_jvps = {}

primitive_transposes = {}


def deflinear(primitive, transpose_rule):
  primitive_jvps[primitive] = partial(linear_jvp, primitive)
  primitive_transposes[primitive] = partial(linear_transpose, transpose_rule)


def linear_jvp(primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  if all(tangent is zero for tangent in tangents):
    return val_out, zero
  else:
    tangents = map(instantiate_zeros, primals, tangents)
    return val_out, primitive.bind(*tangents, **params)


def linear_transpose(transpose_rule, cotangent, *args, **kwargs):
  return zero if cotangent is zero else transpose_rule(cotangent, **kwargs)


def defjvp(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp, jvprules, primitive)


def standard_jvp(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def defjvp2(primitive, *jvprules):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(standard_jvp2, jvprules, primitive)


def standard_jvp2(jvprules, primitive, primals, tangents, **params):
  val_out = primitive.bind(*primals, **params)
  tangents_out = (rule(t, val_out, *primals, **params) for rule, t in zip(jvprules, tangents)
                  if rule is not None and t is not zero)
  return val_out, reduce(add_tangents, tangents_out, zero)


def add_tangents(x, y):
  if x is zero:
    return y
  elif y is zero:
    return x
  else:
    return add_jaxvals(x, y)


def defbilinear_broadcasting(bcast, prim, lhs_rule, rhs_rule):
  assert isinstance(prim, Primitive)
  lhs_jvp = lambda g, x, y, **kwargs: prim.bind(bcast(g, y), y, **kwargs)
  rhs_jvp = lambda g, x, y, **kwargs: prim.bind(x, bcast(g, x), **kwargs)
  defjvp(prim, lhs_jvp, rhs_jvp)
  primitive_transposes[prim] = partial(bilinear_transpose, lhs_rule, rhs_rule)
defbilinear = partial(defbilinear_broadcasting, lambda g, x: g)


def bilinear_transpose(lhs_rule, rhs_rule, cotangent, x, y, **kwargs):
  assert x is None or y is None
  if x is None:
    out = zero if cotangent is zero else lhs_rule(cotangent, y, **kwargs)
    return out, None
  else:
    out = zero if cotangent is zero else rhs_rule(cotangent, x, **kwargs)
    return None, out


def defjvp_zero(primitive):
  assert isinstance(primitive, Primitive)
  primitive_jvps[primitive] = partial(zero_jvp, primitive)


def zero_jvp(primitive, primals, tangents, **params):
  return primitive.bind(*primals, **params), zero


deflinear(zeros_like_p, lambda t: (zeros_like_jaxval(t),))
deflinear(core.identity_p, lambda t: (t,))
deflinear(core.pack_p, lambda t: list(t) if t is not zero else zero)
deflinear(add_jaxvals_p, lambda t: (t, t))


def instantiate_zeros(example, tangent):
  if tangent is zero:
    return zeros_like_jaxval(example)
  elif isinstance(tangent, TangentTuple):
    return pack(map(instantiate_zeros, example, tangent))
  else:
    return tangent

@transformation_with_aux
def traceable(in_tree_def, new_primals, new_tangents):
  new_tangents = build_tree(in_tree_def, new_tangents)
  primal_out, tangent_out = yield new_primals, new_tangents
  out_jtuple, tree_def = tree_to_jaxtuples((primal_out, tangent_out))
  yield out_jtuple, tree_def

@transformation_with_aux
def transposed_fun(jaxpr, in_tree_def, args):
  consts, freevar_vals, ct = args
  ct, freevar_vals = build_tree(in_tree_def, (ct, freevar_vals))
  freevar_cts, cotangents_out = yield jaxpr, consts, freevar_vals, ct
  out_jtuple, tree_def = tree_to_jaxtuples((cotangents_out, freevar_cts))
  yield out_jtuple, tree_def

def call_transpose(primitive, params, jaxpr, consts, freevar_vals, args, ct):
  jaxpr, = jaxpr
  consts, = consts
  freevar_vals, = freevar_vals
  assert isinstance(jaxpr, core.Jaxpr)
  assert all(a is None for a in args), ""TODO(dougalm): handle non-tangent primal args""
  (ct, freevar_vals), in_tree_def = tree_to_jaxtuples((ct, freevar_vals))
  fun = wrap_init(backward_pass)
  fun, out_tree_def = transposed_fun(fun, jaxpr, in_tree_def)
  all_args = pack((pack(consts), pack(freevar_vals), ct))
  # TODO(dougalm): consider signalling to bind that there are no traces in the closure
  ans = primitive.bind(fun, all_args, **params)
  return build_tree(out_tree_def(), ans)

primitive_transposes[core.call_p] = partial(call_transpose, call_p)
primitive_transposes[pe.compiled_call_p] = partial(call_transpose, pe.compiled_call_p)
primitive_transposes[xla.xla_call_p] = partial(call_transpose, xla.xla_call_p)


tree_to_jaxtuples = partial(process_pytree, pack)
","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f84bef02673ae37dfbde39b1e0d6cb61..3f205b1dbe1d4af4e6e55db3ef83e59575956261 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -64,21 +64,40 @@ def linearize(traceable, *primals):
   in_pvals = (pe.PartialVal((None, pack(primals))),
               pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
   jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
-  out_pv, out_const = out_pval
-  assert out_pv is None or out_pv[0] is None
-  primal_out = tuple(out_const)[0]
-  return primal_out, out_pval, jaxpr, consts
-
+  pval_primal, pval_tangent = unpair_pval(out_pval)
+  aval_primal, const_primal = pval_primal
+  assert aval_primal is None
+  return const_primal, pval_tangent, jaxpr, consts
 
 def vjp(traceable, primals):
-  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
+  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
   def vjp_(ct):
+    ct = ignore_consts(ct, pval)
     dummy_primal_and_ct = pack((core.unit, ct))
     _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
     return instantiate_zeros(pack(primals), arg_cts[1])
 
   return out_primal, vjp_
 
+def ignore_consts(ct, pval):
+  aval, const = pval
+  if isinstance(aval, core.AbstractValue):
+    return ct
+  elif isinstance(aval, pe.JaxprTracerTuple):
+    return pack(map(ignore_consts, ct, zip(aval, const)))
+  elif aval is None:
+    return core.unit
+  else:
+    raise TypeError(aval)
+
+def unpair_pval(pval):
+  aval, const = pval
+  const_1, const_2 = const
+  if aval is None:
+    return (None, const_1), (None, const_2)
+  else:
+    aval_1, aval_2 = aval
+    return (aval_1, const_1), (aval_2, const_2)
 
 def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
   def write_cotangent(v, ct):
@@ -116,8 +135,8 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
-    for var, ct in builtins.zip(eqn.invars, cts_out):
+
+    for var, ct in zip(eqn.invars, cts_out):
       write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)
",Boundary condition / off-by-one,"Refactor `linearize` to return primal and tangent values separately, and add `ignore_consts` function to handle constants in"
2e4ff400faa0bb8c7137810639cf769a683730c5,Dougal Maclaurin: Fixed bug in vjp with constant-zero tangent outputs,tests/quickercheck.py,"from collections import namedtuple
from functools import partial
import numpy.random as npr
import jax.numpy as np
from jax import jit, jvp, vjp
import itertools as it
import sys

npr.seed(0)

from jax.util import unzip2, safe_zip, safe_map

map = safe_map
zip = safe_zip

subfun_prob = 0.5
thin_prob = 0.1
size_reduction_factor = 3

Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
Prim = namedtuple('Prim', ['fun'])
ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
Var = namedtuple('Var', ['name', 'vartype'])
Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])

def gen_fun_and_types(size):
  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
  fun, _ = gen_function(size, in_types)
  return fun

def gen_function(size, in_types):
  eqns = []
  in_vars = map(fresh_var, in_types)
  cur_vars = in_vars[:]
  for _ in range(gen_nonneg_int(size)):
    if not cur_vars:
      break
    if npr.rand() < subfun_prob:
      arg_vars = gen_subset(cur_vars)
      arg_types = [v.vartype for v in arg_vars]
      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
      fun = partial(eval_fun, fun)
    else:
      arity = choice(primitive_generators.keys())
      arg_vars = gen_sized_subset(cur_vars, arity)
      arg_types = [v.vartype for v in arg_vars]
      prim_gen = weighted_choice(primitive_generators[arity])
      fun, out_type = prim_gen(size, *arg_types)
      fun = wrap_singleton(fun)
      out_types = [out_type]

    out_vars = map(fresh_var, out_types)
    eqns.append(Eqn(arg_vars, out_vars, fun))
    cur_vars.extend(out_vars)
    cur_vars = thin(cur_vars, thin_prob)

  out_vars = gen_subset(cur_vars)
  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]

def eval_fun(fun, *args):
  def read(v):
    return env[v]
  def write(v, x):
    env[v] = x

  env = {}
  map(write, fun.in_vars, args)
  for in_vars, out_vars, f in fun.eqns:
    out_vals = f(*map(read, in_vars))
    map(write, out_vars, out_vals)

  return map(read, fun.out_vars)

counter = it.count()
def fresh_var(ty):
  return Var(counter.next(), ty)

def gen_array_type(size):
  # TODO(dougalm): randomize this
  return ArrayType((2,2), np.float32)

def gen_array_val(array_type):
  # TODO(dougalm): different sizes and dtypes
  return npr.randn(*array_type.shape)

def gen_neg(size, t):
  return (lambda x: -x), t

def gen_trig(size, t):
  op = choice([np.sin, np.cos])
  return op, t

def gen_binop(size, t1, t2):
  unifier, t_out = gen_broadcasting_unifier(t1, t2)
  binop = choice([lambda x, y: x + y,
                  lambda x, y: x * y])
  def unify_and_binop(x, y):
    x_, y_ = unifier(x, y)
    return binop(x_, y_)

  return unify_and_binop, t_out

def thin(xs, p):
  return [x for x in xs if npr.rand() > p]

def gen_broadcasting_unifier(t1, t2):
  assert t1.shape == t2.shape
  return lambda x, y: (x,y), t1
  # TODO: generate slices and paddings to match shapes

def wrap_singleton(f):
  return lambda *xs: (f(*xs),)

unary_primitive_generators = [
  (3, gen_trig),
  (1, gen_neg) ]

binary_primitive_generators = [
  (1, gen_binop)]

primitive_generators = { 1: unary_primitive_generators,
                         2: binary_primitive_generators }

def gen_nonneg_int(size):
  return npr.randint(size)

choice = npr.choice

def weighted_choice(weighted_choices):
  weights, choices = unzip2(weighted_choices)
  return npr_choice(choices, weights)

def npr_choice(xs, weights=None):
  # npr.choice isn't actually RS -> [a] -> a
  # because it inspects the components to see if they're array-like
  assert xs
  n = len(xs)
  if weights is None:
    i = npr.randint(n)
  else:
    normalizer = float(sum(weights))
    weights = [w / normalizer for w in weights]
    i = npr.choice(range(n), p=weights)
  return xs[i]

def gen_sized_subset(xs, size):
  return [npr_choice(xs) for _ in range(size)]

def gen_subset(xs):
  if not xs:
    return []

  return gen_sized_subset(xs, npr.randint(len(xs) + 1))

def gen_vals(vs):
  return [gen_array_val(v.vartype) for v in vs]

def inner_prod(xs, ys):
  xys = zip(xs, ys)
  assert all(x.shape == y.shape for x, y in xys)
  return sum(np.sum(x * y) for x, y in xys)

def jvp_fd(fun, args, tangents):
  EPS = 1e-4
  def eval_eps(eps):
    return fun(*[x if t is None else x + eps * t
                 for x, t in zip(args, tangents)])

  ys_neg = eval_eps(-EPS)
  ys_pos = eval_eps(EPS)
  ys = eval_eps(0.0)
  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
  return ys, deriv

def check_all_close(xs, ys, tol=1e-3):
  for x, y in zip(xs, ys):
    check_close(x, y, tol)

def check_close(x, y, tol=1e-3):
  assert np.shape(x) == np.shape(y)
  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
  # assert x.dtype == y.dtype
  assert np.allclose(x, y, rtol=tol, atol=tol), \
     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)

def partial_argnums(f, args, dyn_argnums):
  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
  def f_(*dyn_args):
    args = fixed_args[:]
    for i, arg in zip(dyn_argnums, dyn_args):
      args[i] = arg
    return f(*args)

  dyn_args = [args[i] for i in dyn_argnums]
  return f_, dyn_args

def jit_is_identity(fun):
  vals = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  ans = fun(*vals)
  static_argnums = thin(range(len(vals)), 0.5)
  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
  check_all_close(ans, ans_jitted)

def jvp_matches_fd(fun):
  vals = gen_vals(fun.in_vars)
  tangents = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  tangents = [tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, deriv1 = jvp_fd(fun, vals, tangents)
  ans2, deriv2 = jvp(fun, vals, tangents)
  check_all_close(ans1, ans2)
  check_all_close(deriv1, deriv2)


def vjp_matches_fd(fun):
  # print fun
  vals = gen_vals(fun.in_vars)
  in_tangents = gen_vals(fun.in_vars)
  in_cotangents = gen_vals(fun.out_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  in_tangents = [in_tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
  ans2, vjpfun = vjp(fun, *vals)
  out_cotangents = vjpfun(in_cotangents)
  check_all_close(ans1, ans2)
  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
  check_close(inner_prod_fd, inner_prod_ad)


properties = [
  jit_is_identity,
  jvp_matches_fd,
  vjp_matches_fd,
]
# vmap_matches_map ]

def run_tests():
  sizes = [3, 10]
  num_examples = 50
  cases = it.product(sizes, range(num_examples), properties)
  for i, (size, _, check_prop) in enumerate(cases):
    sys.stderr.write('\rTested: {}'.format(i))
    check_prop(gen_fun_and_types(size))
  print ""\nok""

if __name__ == ""__main__"":
  run_tests()
","from collections import namedtuple
from functools import partial
import numpy.random as npr
import jax.numpy as np
from jax import jit, jvp, vjp
import itertools as it
import sys

npr.seed(0)

from jax.util import unzip2, safe_zip, safe_map

map = safe_map
zip = safe_zip

subfun_prob = 0.5
thin_prob = 0.1
size_reduction_factor = 3

Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
Prim = namedtuple('Prim', ['fun'])
ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
Var = namedtuple('Var', ['name', 'vartype'])
Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])

def gen_fun_and_types(size):
  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
  fun, _ = gen_function(size, in_types)
  return fun

def gen_function(size, in_types):
  eqns = []
  in_vars = map(fresh_var, in_types)
  cur_vars = in_vars[:]
  for _ in range(gen_nonneg_int(size)):
    if not cur_vars:
      break
    if npr.rand() < subfun_prob:
      arg_vars = gen_subset(cur_vars)
      arg_types = [v.vartype for v in arg_vars]
      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
      fun = partial(eval_fun, fun)
    else:
      arity = choice(primitive_generators.keys())
      arg_vars = gen_sized_subset(cur_vars, arity)
      arg_types = [v.vartype for v in arg_vars]
      prim_gen = weighted_choice(primitive_generators[arity])
      fun, out_type = prim_gen(size, *arg_types)
      fun = wrap_singleton(fun)
      out_types = [out_type]

    out_vars = map(fresh_var, out_types)
    eqns.append(Eqn(arg_vars, out_vars, fun))
    cur_vars.extend(out_vars)
    cur_vars = thin(cur_vars, thin_prob)

  out_vars = gen_subset(cur_vars)
  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]

def eval_fun(fun, *args):
  def read(v):
    return env[v]
  def write(v, x):
    env[v] = x

  env = {}
  map(write, fun.in_vars, args)
  for in_vars, out_vars, f in fun.eqns:
    out_vals = f(*map(read, in_vars))
    map(write, out_vars, out_vals)

  return map(read, fun.out_vars)

counter = it.count()
def fresh_var(ty):
  return Var(counter.next(), ty)

def gen_array_type(size):
  # TODO(dougalm): randomize this
  return ArrayType((2,2), np.float32)

def gen_array_val(array_type):
  # TODO(dougalm): different sizes and dtypes
  return npr.randn(*array_type.shape)

def gen_neg(size, t):
  return (lambda x: -x), t

def gen_trig(size, t):
  op = choice([np.sin, np.cos])
  return op, t

def gen_binop(size, t1, t2):
  unifier, t_out = gen_broadcasting_unifier(t1, t2)
  binop = choice([lambda x, y: x + y,
                  lambda x, y: x * y])
  def unify_and_binop(x, y):
    x_, y_ = unifier(x, y)
    return binop(x_, y_)

  return unify_and_binop, t_out

def thin(xs, p):
  return [x for x in xs if npr.rand() > p]

def gen_broadcasting_unifier(t1, t2):
  assert t1.shape == t2.shape
  return lambda x, y: (x,y), t1
  # TODO: generate slices and paddings to match shapes

def wrap_singleton(f):
  return lambda *xs: (f(*xs),)

unary_primitive_generators = [
  (3, gen_trig),
  (1, gen_neg) ]

binary_primitive_generators = [
  (1, gen_binop)]

primitive_generators = { 1: unary_primitive_generators,
                         2: binary_primitive_generators }

def gen_nonneg_int(size):
  return npr.randint(size)

choice = npr.choice

def weighted_choice(weighted_choices):
  weights, choices = unzip2(weighted_choices)
  return npr_choice(choices, weights)

def npr_choice(xs, weights=None):
  # npr.choice isn't actually RS -> [a] -> a
  # because it inspects the components to see if they're array-like
  assert xs
  n = len(xs)
  if weights is None:
    i = npr.randint(n)
  else:
    normalizer = float(sum(weights))
    weights = [w / normalizer for w in weights]
    i = npr.choice(range(n), p=weights)
  return xs[i]

def gen_sized_subset(xs, size):
  return [npr_choice(xs) for _ in range(size)]

def gen_subset(xs):
  if not xs:
    return []

  return gen_sized_subset(xs, npr.randint(len(xs) + 1))

def gen_vals(vs):
  return [gen_array_val(v.vartype) for v in vs]

def inner_prod(xs, ys):
  xys = zip(xs, ys)
  assert all(x.shape == y.shape for x, y in xys)
  return sum(np.sum(x * y) for x, y in xys)

def jvp_fd(fun, args, tangents):
  EPS = 1e-4
  def eval_eps(eps):
    return fun(*[x if t is None else x + eps * t
                 for x, t in zip(args, tangents)])

  ys_neg = eval_eps(-EPS)
  ys_pos = eval_eps(EPS)
  ys = eval_eps(0.0)
  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
  return ys, deriv

def check_all_close(xs, ys, tol=1e-3):
  for x, y in zip(xs, ys):
    check_close(x, y, tol)

def check_close(x, y, tol=1e-3):
  assert np.shape(x) == np.shape(y)
  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
  # assert x.dtype == y.dtype
  assert np.allclose(x, y, rtol=tol, atol=tol), \
     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)

def partial_argnums(f, args, dyn_argnums):
  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
  def f_(*dyn_args):
    args = fixed_args[:]
    for i, arg in zip(dyn_argnums, dyn_args):
      args[i] = arg
    return f(*args)

  dyn_args = [args[i] for i in dyn_argnums]
  return f_, dyn_args

def jit_is_identity(fun):
  vals = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  ans = fun(*vals)
  static_argnums = thin(range(len(vals)), 0.5)
  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
  check_all_close(ans, ans_jitted)

def jvp_matches_fd(fun):
  vals = gen_vals(fun.in_vars)
  tangents = gen_vals(fun.in_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  tangents = [tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, deriv1 = jvp_fd(fun, vals, tangents)
  ans2, deriv2 = jvp(fun, vals, tangents)
  check_all_close(ans1, ans2)
  check_all_close(deriv1, deriv2)


def vjp_matches_fd(fun):
  vals = gen_vals(fun.in_vars)
  in_tangents = gen_vals(fun.in_vars)
  in_cotangents = gen_vals(fun.out_vars)
  fun = partial(eval_fun, fun)
  dyn_argnums = thin(range(len(vals)), 0.5)
  in_tangents = [in_tangents[i] for i in dyn_argnums]
  fun, vals = partial_argnums(fun, vals, dyn_argnums)
  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
  ans2, vjpfun = vjp(fun, *vals)
  out_cotangents = vjpfun(in_cotangents)
  check_all_close(ans1, ans2)
  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
  check_close(inner_prod_fd, inner_prod_ad)

properties = [
  jit_is_identity,
  jvp_matches_fd,
  vjp_matches_fd,
]
# vmap_matches_map ]

def run_tests():
  sizes = [3, 10]
  num_examples = 50
  cases = it.product(sizes, range(num_examples), properties)
  for i, (size, _, check_prop) in enumerate(cases):
    sys.stderr.write('\rTested: {}'.format(i))
    try:
      fun = gen_fun_and_types(size)
      check_prop(fun)
    except:
      print fun
      raise

  print ""\nok""

if __name__ == ""__main__"":
  run_tests()
","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 34162c5a22a36459582adb129152e67298c55d85..828e12c136ff0b3ee7e59c1b5ef31957714e54ef 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -216,7 +216,6 @@ def jvp_matches_fd(fun):
 
 
 def vjp_matches_fd(fun):
-  # print fun
   vals = gen_vals(fun.in_vars)
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
@@ -232,7 +231,6 @@ def vjp_matches_fd(fun):
   inner_prod_ad = inner_prod(in_tangents, out_cotangents)
   check_close(inner_prod_fd, inner_prod_ad)
 
-
 properties = [
   jit_is_identity,
   jvp_matches_fd,
@@ -246,7 +244,13 @@ def run_tests():
   cases = it.product(sizes, range(num_examples), properties)
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
-    check_prop(gen_fun_and_types(size))
+    try:
+      fun = gen_fun_and_types(size)
+      check_prop(fun)
+    except:
+      print fun
+      raise
+
   print ""\nok""
 
 if __name__ == ""__main__"":
",Null/None handling,"Refactor quickercheck tests to improve error handling and debugging 

* Removed unused print statement in `vjp_matches_fd` function"
d7b7200884d5a766ffe6398bbcdd105716d915b8,Dougal: Error message for unimplemented numpy functions,jax/numpy/lax_numpy.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


# TODO plan how to handle unsupported ops
def _not_implemented(fun):
  return None

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


def _not_implemented(fun):
  def wrapped(*args, **kwargs):
    raise Exception(""Numpy function {} not yet implemented"".format(fun))
  return wrapped

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 99bd78dcaae26f97596d0435a7e934f3cabeda69..de7a52d4d718669a09a7e2bf7bed7c7781381fb5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -822,9 +822,10 @@ def _argminmax(op, a, axis):
   return min(mask_idxs, axis)
 
 
-# TODO plan how to handle unsupported ops
 def _not_implemented(fun):
-  return None
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
 
 argpartition = _not_implemented(onp.argpartition)
 argsort = _not_implemented(onp.argsort)
",Exception handling / swallowed exceptions,"```
Improve _not_implemented decorator to raise informative exceptions
```"
f2795cbdeadf5a27a8294d56c9888297a7bcf441,"Peter Hawkins: [JAX] Add a NUMPY_SCALAR_SHAPE constant shape to test_utils.py to allow tests for both numpy scalars as distinct from 0D ndarrays.

Fix mishandling of scalars when passed to np.reshape().

PiperOrigin-RevId: 224326107",jax/numpy/lax_numpy.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


# TODO plan how to handle unsupported ops
def _not_implemented(fun):
  return None

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


# TODO plan how to handle unsupported ops
def _not_implemented(fun):
  return None

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 99bd78dcaae26f97596d0435a7e934f3cabeda69..5a28af9f2d3d3e0f78d6f6ae0f69cd9ccc9a620c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -383,7 +383,7 @@ def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
   else:
     raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))
 
-  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
+  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
   computed_newshape = onp.reshape(dummy_val, newshape).shape
   return lax.reshape(a, computed_newshape, dims)
 
",Exception handling / swallowed exceptions,Replace `a.shape` with `shape(a)` in `reshape` function for consistency.
f2795cbdeadf5a27a8294d56c9888297a7bcf441,"Peter Hawkins: [JAX] Add a NUMPY_SCALAR_SHAPE constant shape to test_utils.py to allow tests for both numpy scalars as distinct from 0D ndarrays.

Fix mishandling of scalars when passed to np.reshape().

PiperOrigin-RevId: 224326107",jax/test_util.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import re

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from . import api
from .config import flags
from .util import partial
from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce

FLAGS = flags.FLAGS
flags.DEFINE_enum(
    'jax_test_dut',
    None,
    enum_values=['cpu', 'gpu', 'tpu'],
    help=
    'Describes the device under test in case special consideration is required.'
)


EPS = 1e-4
ATOL = 1e-4
RTOL = 1e-4

_dtype = lambda x: getattr(x, 'dtype', None) or onp.asarray(x).dtype


def numpy_eq(x, y):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    return onp.allclose(x, y, 1e-3, 1e-3)
  else:
    return onp.allclose(x, y)


def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    atol = max(atol, 1e-1)
    rtol = max(rtol, 1e-1)
  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)


def check_eq(xs, ys):
  assert tree_all(tree_multimap(numpy_eq, xs, ys)), \
      '\n{} != \n{}'.format(xs, ys)


def check_close(xs, ys, atol=ATOL, rtol=RTOL):
  close = partial(numpy_close, atol=atol, rtol=rtol)
  assert tree_all(tree_multimap(close, xs, ys)), '\n{} != \n{}'.format(xs, ys)


def inner_prod(xs, ys):
  contract = lambda x, y: onp.real(onp.vdot(x, y))
  return tree_reduce(onp.add, tree_multimap(contract, xs, ys))


add = partial(tree_multimap, onp.add)
sub = partial(tree_multimap, onp.subtract)
conj = partial(tree_map, onp.conj)


def scalar_mul(xs, a):
  return tree_map(lambda x: onp.multiply(x, a, dtype=_dtype(x)), xs)


def rand_like(rng, x):
  shape = onp.shape(x)
  dtype = _dtype(x)
  randn = lambda: onp.asarray(rng.randn(*shape), dtype=dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    return randn() + 1.0j * randn()
  else:
    return randn()


def numerical_jvp(f, primals, tangents, eps=EPS):
  delta = scalar_mul(tangents, EPS)
  f_pos = f(*add(primals, delta))
  f_neg = f(*sub(primals, delta))
  return scalar_mul(sub(f_pos, f_neg), 0.5 / EPS)


def check_jvp(f, f_jvp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  rng = onp.random.RandomState(0)
  tangent = tree_map(partial(rand_like, rng), args)
  v_out, t_out = f_jvp(args, tangent)
  v_out_expected = f(*args)
  t_out_expected = numerical_jvp(f, args, tangent, eps=eps)
  check_eq(v_out, v_out_expected)
  check_close(t_out, t_out_expected, atol=atol, rtol=rtol)


def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  _rand_like = partial(rand_like, onp.random.RandomState(0))
  v_out, vjpfun = f_vjp(*args)
  v_out_expected = f(*args)
  check_eq(v_out, v_out_expected)
  tangent = tree_map(_rand_like, args)
  tangent_out = numerical_jvp(f, args, tangent, eps=EPS)
  cotangent = tree_map(_rand_like, v_out)
  cotangent_out = conj(vjpfun(conj(cotangent)))
  ip = inner_prod(tangent, cotangent_out)
  ip_expected = inner_prod(tangent_out, cotangent)
  check_close(ip, ip_expected, atol=atol, rtol=rtol)


def skip_on_devices(*disabled_devices):
  """"""A decorator for test methods to skip the test on certain devices.""""""
  def skip(test_method):
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      device = FLAGS.jax_test_dut
      if device in disabled_devices:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported on {}.'.format(test_name, device.upper()))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def skip_on_flag(flag_name, skip_value):
  """"""A decorator for test methods to skip the test when flags are set.""""""
  def skip(test_method):        # pylint: disable=missing-docstring
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      flag_value = getattr(FLAGS, flag_name)
      if flag_value == skip_value:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported when FLAGS.{} is {}'.format(
                test_name, flag_name, flag_value))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def format_test_name_suffix(opname, shapes, dtypes):
  arg_descriptions = (format_shape_dtype_string(shape, dtype)
                      for shape, dtype in zip(shapes, dtypes))
  return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))


def format_shape_dtype_string(shape, dtype):
  if onp.isscalar(shape):
    shapestr = str(shape) + ','
  else:
    shapestr = ','.join(str(dim) for dim in shape)
  return '{}[{}]'.format(onp.dtype(dtype).name, shapestr)


def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
  """"""Produce random values given shape, dtype, scale, and post-processor.

  Args:
    rand: a function for producing random values of a given shape, e.g. a
      bound version of either onp.RandomState.randn or onp.RandomState.rand.
    shape: a shape value as a tuple of positive integers.
    dtype: a numpy dtype.
    scale: optional, a multiplicative scale for the random values (default 1).
    post: optional, a callable for post-processing the random values (default
      identity).

  Returns:
    An ndarray of the given shape and dtype using random values based on a call
    to rand but scaled, converted to the appropriate dtype, and post-processed.
  """"""
  r = lambda: onp.asarray(scale * rand(*shape), dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    vals = r() + 1.0j * r()
  else:
    vals = r()
  return onp.asarray(post(vals), dtype)


def rand_default():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3)


def rand_nonzero():
  post = lambda x: onp.where(x == 0, 1, x)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3, post=post)


def rand_positive():
  post = lambda x: x + 1
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2, post=post)


def rand_small():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=1e-3)


def rand_not_small():
  post = lambda x: x + onp.where(x > 0, 10., -10.)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3., post=post)


def rand_small_positive():
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2e-5)


def rand_some_equal():
  randn = npr.RandomState(0).randn
  rng = npr.RandomState(0)

  def post(x):
    flips = rng.rand(*onp.shape(x)) < 0.5
    return onp.where(flips, x.ravel()[0], x)

  return partial(_rand_dtype, randn, scale=100., post=post)


# TODO(mattjj): doesn't handle complex types
def rand_some_inf():
  """"""Return a random sampler that produces infinities in floating types.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    if not onp.issubdtype(dtype, onp.float):
      # only float types have inf
      return base_rand(shape, dtype)

    posinf_flips = rng.rand(*shape) < 0.1
    neginf_flips = rng.rand(*shape) < 0.1

    vals = base_rand(shape, dtype)
    vals = onp.where(posinf_flips, onp.inf, vals)
    vals = onp.where(neginf_flips, -onp.inf, vals)

    return onp.asarray(vals, dtype=dtype)

  return rand


# TODO(mattjj): doesn't handle complex types
def rand_some_zero():
  """"""Return a random sampler that produces some zeros.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    zeros = rng.rand(*shape) < 0.5

    vals = base_rand(shape, dtype)
    vals = onp.where(zeros, 0, vals)

    return onp.asarray(vals, dtype=dtype)

  return rand


def rand_bool():
  rng = npr.RandomState(0)
  return lambda shape, dtype: rng.rand(*shape) < 0.5

def check_raises(thunk, err_type, msg):
  try:
    thunk()
    assert False
  except err_type as e:
    assert str(e) == msg, ""{}\n\n{}\n"".format(e, msg)

def check_raises_regexp(thunk, err_type, pattern):
  try:
    thunk()
    assert False
  except err_type as e:
    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)


class JaxTestCase(parameterized.TestCase):
  """"""Base class for JAX tests including numerical checks and boilerplate.""""""

  def assertArraysAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y are close (up to numerical tolerances).""""""
    dtype = lambda x: str(onp.asarray(x).dtype)
    tol = 1e-2 if str(onp.dtype(onp.float32)) in {dtype(x), dtype(y)} else 1e-5
    atol = atol or tol
    rtol = rtol or tol

    if FLAGS.jax_test_dut == 'tpu':
      atol = max(atol, 0.5)
      rtol = max(rtol, 1e-1)

    if not onp.allclose(x, y, atol=atol, rtol=rtol, equal_nan=True):
      msg = ('Arguments x and y not equal to tolerance atol={}, rtol={}:\n'
             'x:\n{}\n'
             'y:\n{}\n').format(atol, rtol, x, y)
      raise self.failureException(msg)

    if check_dtypes:
      self.assertDtypesMatch(x, y)

  def assertDtypesMatch(self, x, y):
    if FLAGS.jax_enable_x64:
      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)

  def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""
    if isinstance(x, (tuple, list)):
      self.assertIsInstance(y, (tuple, list))
      self.assertEqual(len(x), len(y))
      for x_elt, y_elt in zip(x, y):
        self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
    else:
      is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
      self.assertTrue(is_array(x))
      self.assertTrue(is_array(y))
      x = onp.asarray(x)
      y = onp.asarray(y)
      self.assertArraysAllClose(x, y, check_dtypes, atol=atol, rtol=rtol)

  def _CompileAndCheck(self, fun, args_maker, check_dtypes,
                       rtol=None, atol=None):
    """"""Helper method for running JAX compilation and allclose assertions.""""""
    args = args_maker()

    def wrapped_fun(*args):
      self.assertTrue(python_should_be_executing)
      return fun(*args)

    python_should_be_executing = True
    python_ans = fun(*args)

    cfun = api.jit(wrapped_fun)
    python_should_be_executing = True
    monitored_ans = cfun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, monitored_ans, check_dtypes, rtol, atol)
    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

    args = args_maker()

    python_should_be_executing = True
    python_ans = fun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

  def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                         check_dtypes=False, tol=1e-5):
    args = args_maker()
    lax_ans = lax_op(*args)
    numpy_ans = numpy_reference_op(*args)
    self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                        atol=tol, rtol=tol)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import re

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from . import api
from .config import flags
from .util import partial
from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce

FLAGS = flags.FLAGS
flags.DEFINE_enum(
    'jax_test_dut',
    None,
    enum_values=['cpu', 'gpu', 'tpu'],
    help=
    'Describes the device under test in case special consideration is required.'
)


EPS = 1e-4
ATOL = 1e-4
RTOL = 1e-4

_dtype = lambda x: getattr(x, 'dtype', None) or onp.asarray(x).dtype


def numpy_eq(x, y):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    return onp.allclose(x, y, 1e-3, 1e-3)
  else:
    return onp.allclose(x, y)


def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    atol = max(atol, 1e-1)
    rtol = max(rtol, 1e-1)
  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)


def check_eq(xs, ys):
  assert tree_all(tree_multimap(numpy_eq, xs, ys)), \
      '\n{} != \n{}'.format(xs, ys)


def check_close(xs, ys, atol=ATOL, rtol=RTOL):
  close = partial(numpy_close, atol=atol, rtol=rtol)
  assert tree_all(tree_multimap(close, xs, ys)), '\n{} != \n{}'.format(xs, ys)


def inner_prod(xs, ys):
  contract = lambda x, y: onp.real(onp.vdot(x, y))
  return tree_reduce(onp.add, tree_multimap(contract, xs, ys))


add = partial(tree_multimap, onp.add)
sub = partial(tree_multimap, onp.subtract)
conj = partial(tree_map, onp.conj)


def scalar_mul(xs, a):
  return tree_map(lambda x: onp.multiply(x, a, dtype=_dtype(x)), xs)


def rand_like(rng, x):
  shape = onp.shape(x)
  dtype = _dtype(x)
  randn = lambda: onp.asarray(rng.randn(*shape), dtype=dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    return randn() + 1.0j * randn()
  else:
    return randn()


def numerical_jvp(f, primals, tangents, eps=EPS):
  delta = scalar_mul(tangents, EPS)
  f_pos = f(*add(primals, delta))
  f_neg = f(*sub(primals, delta))
  return scalar_mul(sub(f_pos, f_neg), 0.5 / EPS)


def check_jvp(f, f_jvp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  rng = onp.random.RandomState(0)
  tangent = tree_map(partial(rand_like, rng), args)
  v_out, t_out = f_jvp(args, tangent)
  v_out_expected = f(*args)
  t_out_expected = numerical_jvp(f, args, tangent, eps=eps)
  check_eq(v_out, v_out_expected)
  check_close(t_out, t_out_expected, atol=atol, rtol=rtol)


def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  _rand_like = partial(rand_like, onp.random.RandomState(0))
  v_out, vjpfun = f_vjp(*args)
  v_out_expected = f(*args)
  check_eq(v_out, v_out_expected)
  tangent = tree_map(_rand_like, args)
  tangent_out = numerical_jvp(f, args, tangent, eps=EPS)
  cotangent = tree_map(_rand_like, v_out)
  cotangent_out = conj(vjpfun(conj(cotangent)))
  ip = inner_prod(tangent, cotangent_out)
  ip_expected = inner_prod(tangent_out, cotangent)
  check_close(ip, ip_expected, atol=atol, rtol=rtol)


def skip_on_devices(*disabled_devices):
  """"""A decorator for test methods to skip the test on certain devices.""""""
  def skip(test_method):
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      device = FLAGS.jax_test_dut
      if device in disabled_devices:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported on {}.'.format(test_name, device.upper()))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def skip_on_flag(flag_name, skip_value):
  """"""A decorator for test methods to skip the test when flags are set.""""""
  def skip(test_method):        # pylint: disable=missing-docstring
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      flag_value = getattr(FLAGS, flag_name)
      if flag_value == skip_value:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported when FLAGS.{} is {}'.format(
                test_name, flag_name, flag_value))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def format_test_name_suffix(opname, shapes, dtypes):
  arg_descriptions = (format_shape_dtype_string(shape, dtype)
                      for shape, dtype in zip(shapes, dtypes))
  return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))


class _NumpyScalar(object):

  def __len__(self):
    return 0

# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
# identical to 0-D arrays, and we want to write tests that exercise both paths.
NUMPY_SCALAR_SHAPE = _NumpyScalar()


def _dims_of_shape(shape):
  """"""Converts `shape` to a tuple of dimensions.""""""
  return shape if shape != NUMPY_SCALAR_SHAPE else ()


def _cast_to_shape(value, shape, dtype):
  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
  if shape != NUMPY_SCALAR_SHAPE:
    return value
  else:
    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
    # scalar.
    return dtype(value)


def format_shape_dtype_string(shape, dtype):
  typestr = onp.dtype(dtype).name
  if shape == NUMPY_SCALAR_SHAPE:
    return typestr

  if onp.isscalar(shape):
    shapestr = str(shape) + ','
  else:
    shapestr = ','.join(str(dim) for dim in shape)
  return '{}[{}]'.format(typestr, shapestr)


def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
  """"""Produce random values given shape, dtype, scale, and post-processor.

  Args:
    rand: a function for producing random values of a given shape, e.g. a
      bound version of either onp.RandomState.randn or onp.RandomState.rand.
    shape: a shape value as a tuple of positive integers.
    dtype: a numpy dtype.
    scale: optional, a multiplicative scale for the random values (default 1).
    post: optional, a callable for post-processing the random values (default
      identity).

  Returns:
    An ndarray of the given shape and dtype using random values based on a call
    to rand but scaled, converted to the appropriate dtype, and post-processed.
  """"""
  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    vals = r() + 1.0j * r()
  else:
    vals = r()
  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)


def rand_default():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3)


def rand_nonzero():
  post = lambda x: onp.where(x == 0, 1, x)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3, post=post)


def rand_positive():
  post = lambda x: x + 1
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2, post=post)


def rand_small():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=1e-3)


def rand_not_small():
  post = lambda x: x + onp.where(x > 0, 10., -10.)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3., post=post)


def rand_small_positive():
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2e-5)


def rand_some_equal():
  randn = npr.RandomState(0).randn
  rng = npr.RandomState(0)

  def post(x):
    flips = rng.rand(*onp.shape(x)) < 0.5
    return onp.where(flips, x.ravel()[0], x)

  return partial(_rand_dtype, randn, scale=100., post=post)


# TODO(mattjj): doesn't handle complex types
def rand_some_inf():
  """"""Return a random sampler that produces infinities in floating types.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    if not onp.issubdtype(dtype, onp.float):
      # only float types have inf
      return base_rand(shape, dtype)

    dims = _dims_of_shape(shape)
    posinf_flips = rng.rand(*dims) < 0.1
    neginf_flips = rng.rand(*dims) < 0.1

    vals = base_rand(shape, dtype)
    vals = onp.where(posinf_flips, onp.inf, vals)
    vals = onp.where(neginf_flips, -onp.inf, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


# TODO(mattjj): doesn't handle complex types
def rand_some_zero():
  """"""Return a random sampler that produces some zeros.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    dims = _dims_of_shape(shape)
    zeros = rng.rand(*dims) < 0.5

    vals = base_rand(shape, dtype)
    vals = onp.where(zeros, 0, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


def rand_bool():
  rng = npr.RandomState(0)
  def generator(shape, dtype):
    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
  return generator

def check_raises(thunk, err_type, msg):
  try:
    thunk()
    assert False
  except err_type as e:
    assert str(e) == msg, ""{}\n\n{}\n"".format(e, msg)

def check_raises_regexp(thunk, err_type, pattern):
  try:
    thunk()
    assert False
  except err_type as e:
    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)


class JaxTestCase(parameterized.TestCase):
  """"""Base class for JAX tests including numerical checks and boilerplate.""""""

  def assertArraysAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y are close (up to numerical tolerances).""""""
    dtype = lambda x: str(onp.asarray(x).dtype)
    tol = 1e-2 if str(onp.dtype(onp.float32)) in {dtype(x), dtype(y)} else 1e-5
    atol = atol or tol
    rtol = rtol or tol

    if FLAGS.jax_test_dut == 'tpu':
      atol = max(atol, 0.5)
      rtol = max(rtol, 1e-1)

    if not onp.allclose(x, y, atol=atol, rtol=rtol, equal_nan=True):
      msg = ('Arguments x and y not equal to tolerance atol={}, rtol={}:\n'
             'x:\n{}\n'
             'y:\n{}\n').format(atol, rtol, x, y)
      raise self.failureException(msg)

    if check_dtypes:
      self.assertDtypesMatch(x, y)

  def assertDtypesMatch(self, x, y):
    if FLAGS.jax_enable_x64:
      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)

  def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""
    if isinstance(x, (tuple, list)):
      self.assertIsInstance(y, (tuple, list))
      self.assertEqual(len(x), len(y))
      for x_elt, y_elt in zip(x, y):
        self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
    else:
      is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
      self.assertTrue(is_array(x))
      self.assertTrue(is_array(y))
      x = onp.asarray(x)
      y = onp.asarray(y)
      self.assertArraysAllClose(x, y, check_dtypes, atol=atol, rtol=rtol)

  def _CompileAndCheck(self, fun, args_maker, check_dtypes,
                       rtol=None, atol=None):
    """"""Helper method for running JAX compilation and allclose assertions.""""""
    args = args_maker()

    def wrapped_fun(*args):
      self.assertTrue(python_should_be_executing)
      return fun(*args)

    python_should_be_executing = True
    python_ans = fun(*args)

    cfun = api.jit(wrapped_fun)
    python_should_be_executing = True
    monitored_ans = cfun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, monitored_ans, check_dtypes, rtol, atol)
    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

    args = args_maker()

    python_should_be_executing = True
    python_ans = fun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

  def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                         check_dtypes=False, tol=1e-5):
    args = args_maker()
    lax_ans = lax_op(*args)
    numpy_ans = numpy_reference_op(*args)
    self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                        atol=tol, rtol=tol)
","diff --git a/jax/test_util.py b/jax/test_util.py
index 2c05a0b65bd9dbff5766102910b461f4c06154e8..649b47be28503fe6df49466d27445fe1e62648a3 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -167,12 +167,41 @@ def format_test_name_suffix(opname, shapes, dtypes):
   return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))
 
 
+class _NumpyScalar(object):
+
+  def __len__(self):
+    return 0
+
+# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
+# identical to 0-D arrays, and we want to write tests that exercise both paths.
+NUMPY_SCALAR_SHAPE = _NumpyScalar()
+
+
+def _dims_of_shape(shape):
+  """"""Converts `shape` to a tuple of dimensions.""""""
+  return shape if shape != NUMPY_SCALAR_SHAPE else ()
+
+
+def _cast_to_shape(value, shape, dtype):
+  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
+  if shape != NUMPY_SCALAR_SHAPE:
+    return value
+  else:
+    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
+    # scalar.
+    return dtype(value)
+
+
 def format_shape_dtype_string(shape, dtype):
+  typestr = onp.dtype(dtype).name
+  if shape == NUMPY_SCALAR_SHAPE:
+    return typestr
+
   if onp.isscalar(shape):
     shapestr = str(shape) + ','
   else:
     shapestr = ','.join(str(dim) for dim in shape)
-  return '{}[{}]'.format(onp.dtype(dtype).name, shapestr)
+  return '{}[{}]'.format(typestr, shapestr)
 
 
 def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
@@ -191,12 +220,12 @@ def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
     An ndarray of the given shape and dtype using random values based on a call
     to rand but scaled, converted to the appropriate dtype, and post-processed.
   """"""
-  r = lambda: onp.asarray(scale * rand(*shape), dtype)
+  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
   if onp.issubdtype(dtype, onp.complexfloating):
     vals = r() + 1.0j * r()
   else:
     vals = r()
-  return onp.asarray(post(vals), dtype)
+  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)
 
 
 def rand_default():
@@ -255,14 +284,15 @@ def rand_some_inf():
       # only float types have inf
       return base_rand(shape, dtype)
 
-    posinf_flips = rng.rand(*shape) < 0.1
-    neginf_flips = rng.rand(*shape) < 0.1
+    dims = _dims_of_shape(shape)
+    posinf_flips = rng.rand(*dims) < 0.1
+    neginf_flips = rng.rand(*dims) < 0.1
 
     vals = base_rand(shape, dtype)
     vals = onp.where(posinf_flips, onp.inf, vals)
     vals = onp.where(neginf_flips, -onp.inf, vals)
 
-    return onp.asarray(vals, dtype=dtype)
+    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)
 
   return rand
 
@@ -275,19 +305,22 @@ def rand_some_zero():
 
   def rand(shape, dtype):
     """"""The random sampler function.""""""
-    zeros = rng.rand(*shape) < 0.5
+    dims = _dims_of_shape(shape)
+    zeros = rng.rand(*dims) < 0.5
 
     vals = base_rand(shape, dtype)
     vals = onp.where(zeros, 0, vals)
 
-    return onp.asarray(vals, dtype=dtype)
+    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)
 
   return rand
 
 
 def rand_bool():
   rng = npr.RandomState(0)
-  return lambda shape, dtype: rng.rand(*shape) < 0.5
+  def generator(shape, dtype):
+    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
+  return generator
 
 def check_raises(thunk, err_type, msg):
   try:
",Missing input validation,"Add support for numpy scalars in test utilities

* Introduce a special singleton shape `NUMPY_SCALAR_SHAPE` to"
f2795cbdeadf5a27a8294d56c9888297a7bcf441,"Peter Hawkins: [JAX] Add a NUMPY_SCALAR_SHAPE constant shape to test_utils.py to allow tests for both numpy scalars as distinct from 0D ndarrays.

Fix mishandling of scalars when passed to np.reshape().

PiperOrigin-RevId: 224326107",tests/lax_numpy_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

FLAGS = config.FLAGS

all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True])
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape)))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2])
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape)))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()])
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
          ""_"".join(str(d) for d in shape),
          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in all_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes)
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes)
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape)))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes)
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes)
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ]))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  config.config_with_absl()
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

FLAGS = config.FLAGS

array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
# fixed so the tests pass.
all_shapes = array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True])
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape)))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2])
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape)))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()])
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes)
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes)
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape)))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes)
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes)
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ]))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  config.config_with_absl()
  absltest.main()
","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 39163081207bbecfed8d1f3d9db58d8f57408b97..5eae4fdee2181e2c277e835d8b863d49b67e7f7d 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -32,7 +32,11 @@ from jax.config import config
 
 FLAGS = config.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
+# fixed so the tests pass.
+all_shapes = array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -53,10 +57,10 @@ def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
 JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
     op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
     op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
@@ -306,12 +310,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(
-      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
-          ""_"".join(str(d) for d in shape),
-          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
+          jtu.format_shape_dtype_string(shape, fill_value_dtype),
+          onp.dtype(out_dtype).name),
        ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
        ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
-      for shape in all_shapes
+      for shape in array_shapes
       for fill_value_dtype in default_dtypes
       for out_dtype in default_dtypes)
   def testFull(self, shape, fill_value_dtype, out_dtype, rng):
@@ -345,6 +349,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
       for arg_shape, out_shape in [
+          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
+          ((), (1, 1, 1)),
+          ((7, 0), (0, 42, 101)),
           ((3, 4), 12),
           ((3, 4), (12,)),
           ((3, 4), -1),
",Dependency / version mismatch,Refactor LaxBackedNumpyTests to use separate shape lists and update bitwise ops to use int_dtypes
b4344a07bc44f4548c557fbfc3dc08b9978ce001,Matthew Johnson: fix typo in WORKSPACE,WORKSPACE,"http_archive(
    name = ""io_bazel_rules_closure"",
    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
    urls = [
        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
    ],
)


# To update TensorFlow to a new revision,
# a) update URL and strip_prefix to the new git commit hash
# b) get the sha256 hash of the commit by running:
#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
#    and update the sha256 with the result.
http_archive(
    name = ""org_tensorflow"",
    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
    strip_prefix=""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd""
    urls = [
      ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
    ],
)

# For development, one can use a local TF repository instead.
# local_repository(
#    name = ""org_tensorflow"",
#    path = ""tensorflow"",
# )


load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")

tf_workspace(
    path_prefix = """",
    tf_repo_name = ""org_tensorflow"",
)
","http_archive(
    name = ""io_bazel_rules_closure"",
    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
    urls = [
        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
    ],
)

# To update TensorFlow to a new revision,
# a) update URL and strip_prefix to the new git commit hash
# b) get the sha256 hash of the commit by running:
#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
#    and update the sha256 with the result.
http_archive(
    name = ""org_tensorflow"",
    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
    urls = [
        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
    ],
)

# For development, one can use a local TF repository instead.
# local_repository(
#    name = ""org_tensorflow"",
#    path = ""tensorflow"",
# )

load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")

tf_workspace(
    path_prefix = """",
    tf_repo_name = ""org_tensorflow"",
)
","diff --git a/WORKSPACE b/WORKSPACE
index bbb200c79222b08e00c4c70448c9e1b8e1270723..0df9205617cca0f40f564ae6f274eb9b288e45a0 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -8,7 +8,6 @@ http_archive(
     ],
 )
 
-
 # To update TensorFlow to a new revision,
 # a) update URL and strip_prefix to the new git commit hash
 # b) get the sha256 hash of the commit by running:
@@ -17,9 +16,9 @@ http_archive(
 http_archive(
     name = ""org_tensorflow"",
     sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
-    strip_prefix=""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd""
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
     ],
 )
 
@@ -29,7 +28,6 @@ http_archive(
 #    path = ""tensorflow"",
 # )
 
-
 load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
 
 tf_workspace(
",Index out of range,Update TensorFlow dependency to use consistent formatting and remove unnecessary whitespace.
c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,"Peter Hawkins: [JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.",jax/abstract_arrays.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as onp
import six

from . import core
from . import ad_util
from . util import prod
from .lib import xla_bridge


def concretization_err_msg(fun):
  fname = getattr(fun, ""__name__"", fun)
  return (""Abstract value passed to function {} that requires a concrete value. ""
          ""Possibly tracing Python control flow using abstract values. ""
          ""If so, try using lax.cond or lax.while instead."").format(fname)

def concretization_function_error(fun):
  def error(self, *args):
    raise TypeError(concretization_err_msg(fun))
  return error


class UnshapedArray(core.AbstractValue):
  __slots__ = ['dtype']
  array_abstraction_level = 3

  def __init__(self, dtype):
    self.dtype = dtype

  def __eq__(self, other):
    return type(self) is type(other) and self.dtype == other.dtype

  def __hash__(self):
    return hash(str(self.dtype))

  def __repr__(self):
    return '{}({})'.format(self.__class__.__name__, self.str_short())

  _bool = _nonzero = concretization_function_error(bool)
  _float   = concretization_function_error(float)
  _int     = concretization_function_error(int)
  if six.PY2:
    _long    = concretization_function_error(long)
  _complex = concretization_function_error(complex)
  _hex     = concretization_function_error(hex)
  _oct     = concretization_function_error(oct)

  def at_least_vspace(self):
    return self

  def join(self, other):
    return self

  def str_short(self):
    return onp.dtype(self.dtype).name


class ShapedArray(UnshapedArray):
  __slots__ = ['shape']
  array_abstraction_level = 2

  def __init__(self, shape, dtype):
    self.dtype = onp.dtype(xla_bridge.canonicalize_dtype(dtype))
    self.shape = shape

  ndim = property(lambda self: len(self.shape))
  size = property(lambda self: prod(self.shape))

  def __eq__(self, other):
    return (type(self) is type(other)
            and self.dtype == other.dtype and self.shape == other.shape)

  def __hash__(self):
    return hash((self.shape, str(self.dtype)))

  def at_least_vspace(self):
    return self

  def join(self, other):
    if self.shape == other.shape and self.dtype == other.dtype:
      return self
    elif self.dtype == other.dtype:
      return UnshapedArray(self.dtype)
    else:
      raise TypeError(other)

  def str_short(self):
    dtypestr = onp.dtype(self.dtype).name
    shapestr = ','.join(map(str, self.shape))
    return '{}[{}]'.format(dtypestr, shapestr)

  def __len__(self):
    try:
      return self.shape[0]
    except IndexError:
      raise TypeError(""len() of unsized object"")  # same as numpy error


class ConcreteArray(ShapedArray):
  __slots__ = ['val']
  array_abstraction_level = 0

  def __init__(self, val):
    self.val = val
    self.shape = onp.shape(val)
    # canonicalized self.dtype doesn't necessarily match self.val
    self.dtype = onp.dtype(xla_bridge.canonicalize_dtype(onp.result_type(val)))
    assert self.dtype != onp.dtype('O')

  def __eq__(self, other):
    return (type(self) is type(other) and self.dtype == other.dtype
            and self.shape == other.shape and onp.all(self.val == other.val))

  def __hash__(self):
    return id(self.val)

  def at_least_vspace(self):
    return ShapedArray(self.shape, self.dtype)

  def join(self, other):
    if self == other:
      return self
    elif self.shape == other.shape and self.dtype == other.dtype:
      return ShapedArray(self.shape, self.dtype)
    elif self.dtype == other.dtype:
      return UnshapedArray(self.dtype)
    else:
      raise TypeError(other)

  def str_short(self):
    return str(self.val)


def make_shaped_array(x):
  dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
  return ShapedArray(onp.shape(x), dtype)

array_types = [onp.ndarray, onp.float64, onp.float32, onp.int64, onp.int32,
               onp.bool_, onp.uint64, onp.uint32, float, int, bool]

for t in array_types:
  core.pytype_aval_mappings[t] = ConcreteArray
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as onp
import six

from . import core
from . import ad_util
from . util import prod
from .lib import xla_bridge


def concretization_err_msg(fun):
  fname = getattr(fun, ""__name__"", fun)
  return (""Abstract value passed to function {} that requires a concrete value. ""
          ""Possibly tracing Python control flow using abstract values. ""
          ""If so, try using lax.cond or lax.while instead."").format(fname)

def concretization_function_error(fun):
  def error(self, *args):
    raise TypeError(concretization_err_msg(fun))
  return error


class UnshapedArray(core.AbstractValue):
  __slots__ = ['dtype']
  array_abstraction_level = 3

  def __init__(self, dtype):
    self.dtype = dtype

  def __eq__(self, other):
    return type(self) is type(other) and self.dtype == other.dtype

  def __hash__(self):
    return hash(str(self.dtype))

  def __repr__(self):
    return '{}({})'.format(self.__class__.__name__, self.str_short())

  _bool = _nonzero = concretization_function_error(bool)
  _float   = concretization_function_error(float)
  _int     = concretization_function_error(int)
  if six.PY2:
    _long    = concretization_function_error(long)
  _complex = concretization_function_error(complex)
  _hex     = concretization_function_error(hex)
  _oct     = concretization_function_error(oct)

  def at_least_vspace(self):
    return self

  def join(self, other):
    return self

  def str_short(self):
    return onp.dtype(self.dtype).name


class ShapedArray(UnshapedArray):
  __slots__ = ['shape']
  array_abstraction_level = 2

  def __init__(self, shape, dtype):
    self.dtype = onp.dtype(xla_bridge.canonicalize_dtype(dtype))
    self.shape = shape

  ndim = property(lambda self: len(self.shape))
  size = property(lambda self: prod(self.shape))

  def __eq__(self, other):
    return (type(self) is type(other)
            and self.dtype == other.dtype and self.shape == other.shape)

  def __hash__(self):
    return hash((self.shape, str(self.dtype)))

  def at_least_vspace(self):
    return self

  def join(self, other):
    if self.shape == other.shape and self.dtype == other.dtype:
      return self
    elif self.dtype == other.dtype:
      return UnshapedArray(self.dtype)
    else:
      raise TypeError(other)

  def str_short(self):
    dtypestr = onp.dtype(self.dtype).name
    shapestr = ','.join(map(str, self.shape))
    return '{}[{}]'.format(dtypestr, shapestr)

  def __len__(self):
    try:
      return self.shape[0]
    except IndexError:
      raise TypeError(""len() of unsized object"")  # same as numpy error


class ConcreteArray(ShapedArray):
  __slots__ = ['val']
  array_abstraction_level = 0

  def __init__(self, val):
    self.val = val
    self.shape = onp.shape(val)
    # canonicalized self.dtype doesn't necessarily match self.val
    self.dtype = onp.dtype(xla_bridge.canonicalize_dtype(onp.result_type(val)))
    assert self.dtype != onp.dtype('O')

  def __eq__(self, other):
    return (type(self) is type(other) and self.dtype == other.dtype
            and self.shape == other.shape and onp.all(self.val == other.val))

  def __hash__(self):
    return id(self.val)

  def at_least_vspace(self):
    return ShapedArray(self.shape, self.dtype)

  def join(self, other):
    if self == other:
      return self
    elif self.shape == other.shape and self.dtype == other.dtype:
      return ShapedArray(self.shape, self.dtype)
    elif self.dtype == other.dtype:
      return UnshapedArray(self.dtype)
    else:
      raise TypeError(other)

  def str_short(self):
    return str(self.val)


def make_shaped_array(x):
  dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
  return ShapedArray(onp.shape(x), dtype)

array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32, float,
               int, bool]

for t in array_types:
  core.pytype_aval_mappings[t] = ConcreteArray
","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 61fd8f7cb1bfef1ae842a7176821c29bd0a1397a..66d913c9ec5d9dcc9aee01f4dd69d482310a97de 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -152,8 +152,9 @@ def make_shaped_array(x):
   dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
   return ShapedArray(onp.shape(x), dtype)
 
-array_types = [onp.ndarray, onp.float64, onp.float32, onp.int64, onp.int32,
-               onp.bool_, onp.uint64, onp.uint32, float, int, bool]
+array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
+               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32, float,
+               int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray
",Numerical / precision bug,Add support for complex64 type in array_types mapping
c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,"Peter Hawkins: [JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.",jax/numpy/lax_numpy.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    all_scalar = _all(isscalar(x) for x in args)
    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
    keep_all = all_scalar or some_bools
    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


# TODO plan how to handle unsupported ops
def _not_implemented(fun):
  return None

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
from ..lib import xla_bridge
import jax.lax as lax
from ..util import memoize

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    from_dtypes = (_dtype(x) for x in args)
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


# TODO plan how to handle unsupported ops
def _not_implemented(fun):
  return None

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
repeat = _not_implemented(onp.repeat)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5a28af9f2d3d3e0f78d6f6ae0f69cd9ccc9a620c..180b8b6f8a585e4fc1790f80cce6a727c39e0bbd 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -126,10 +126,7 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    all_scalar = _all(isscalar(x) for x in args)
-    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
-    keep_all = all_scalar or some_bools
-    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
+    from_dtypes = (_dtype(x) for x in args)
     to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]
",Memory leak,Simplify `_promote_dtypes` by removing unnecessary scalar and bool checks.
c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,"Peter Hawkins: [JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.",jax/random.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""LAX-based pseudo-random number generators (PRNGs).""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from functools import partial

import numpy as onp

from . import lax
from . import numpy as np
from . import tree_util
from .lib import xla_bridge
from .api import jit


class PRNGKey(object):
  """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
  __slots__ = [""keypair""]

  def __init__(self, seed):
    """"""Create a new PRNG key.

    Args:
      seed: a scalar integer value used to initialize the PRNG key.

    Returns:
      A new PRNGKey object.
    """"""
    convert = lambda key: lax.convert_element_type(key, onp.uint32)
    if onp.shape(seed):
      raise TypeError(""PRNGKey seed must be a scalar."")
    if isinstance(seed, (int, onp.ndarray)):
      # Special handling of raw integer values, which may have be 64bit even
      # when jax_enable_x64=False and we don't want to drop the top 32 bits
      k1 = convert(onp.bitwise_and(onp.right_shift(seed, 32), 0xFFFFFFFF))
    else:
      k1 = convert(lax.shift_right_logical(seed, 32))
    k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
    self.keypair = (k1, k2)

  @classmethod
  def from_keypair(cls, keypair):
    """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
    new = cls.__new__(cls)
    new.keypair = tuple(keypair)
    return new


tree_util.register_pytree_node(PRNGKey, lambda k: (k.keypair, None),
                               lambda _, xs: PRNGKey.from_keypair(xs))


### utilities


def _make_rotate_left(dtype):
  if not onp.issubdtype(dtype, onp.integer):
    raise TypeError(""_rotate_left only accepts integer dtypes."")
  nbits = onp.array(onp.iinfo(dtype).bits, dtype)

  def _rotate_left(x, d):
    if lax._dtype(d) != lax._dtype(x):
      d = lax.convert_element_type(d, x.dtype)
    return (x << d) | lax.shift_right_logical(x, nbits - d)
  return _rotate_left


def _bit_stats(bits):
  """"""This is a debugging function to compute the statistics of bit fields.""""""
  return onp.array([list(map(int, onp.binary_repr(x, 64))) for x in bits]).mean(0)


### hash function and split


@jit
def threefry_2x32(keypair, count):
  """"""Apply the Threefry 2x32 hash.

  Args:
    keypair: a pair of 32bit unsigned integers used for the key.
    count: an array of dtype uint32 used for the counts.

  Returns:
    An array of dtype uint32 with the same shape as `count`.
  """"""
  # Based on ThreeFry2x32 by phawkins@ in //.../xla/client/lib/prng.cc
  key1, key2 = keypair[0], keypair[1]
  if not lax._dtype(key1) == lax._dtype(key2) == lax._dtype(count) == onp.uint32:
    msg = ""threefry_2x32 requires uint32 arguments, got {}""
    raise TypeError(msg.format([lax._dtype(x) for x in [key1, key2, count]]))

  rotate_left = _make_rotate_left(lax._dtype(count))

  def apply_round(v, rot):
    v = v[:]
    v[0] = v[0] + v[1]
    v[1] = rotate_left(v[1], rot)
    v[1] = v[0] ^ v[1]
    return v

  odd_size = count.size % 2
  if odd_size:
    x = list(np.split(np.concatenate([count.ravel(), onp.uint32([0])]), 2))
  else:
    x = list(np.split(count.ravel(), 2))

  rotations = [13, 15, 26, 6, 17, 29, 16, 24]
  ks = [key1, key2, key1 ^ key2 ^ onp.uint32(0x1BD11BDA)]

  x[0] = x[0] + ks[0]
  x[1] = x[1] + ks[1]

  for r in rotations[:4]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[1]
  x[1] = x[1] + ks[2] + onp.uint32(1)

  for r in rotations[4:]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[2]
  x[1] = x[1] + ks[0] + onp.uint32(2)

  for r in rotations[:4]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[0]
  x[1] = x[1] + ks[1] + onp.uint32(3)

  for r in rotations[4:]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[1]
  x[1] = x[1] + ks[2] + onp.uint32(4)

  for r in rotations[:4]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[2]
  x[1] = x[1] + ks[0] + onp.uint32(5)

  out = np.concatenate(x)
  assert out.dtype == onp.uint32
  return lax.reshape(out[:-1] if odd_size else out, count.shape)


@partial(jit, static_argnums=(1,))
def split(key, num=2):
  """"""Splits a PRNG key pair of 32bit unsigned integers into `num` new key pairs.

  Args:
    key: a PRNGKey used as the random key.
    num: optional, a positive integer indicating the number of keys to produce
      (default 2).

  Returns:
    A tuple of length `num` of new PRNGKey instances.
  """"""
  counts = onp.arange(num * 2, dtype=onp.uint32)
  bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
  keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
  return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)


def _random_bits(key, bit_width, shape):
  """"""Sample uniform random bits of given width and shape using PRNG key.""""""
  if bit_width not in (32, 64):
    raise TypeError(""requires 32- or 64-bit field width."")
  max_count = (bit_width // 32) * onp.prod(shape)
  if max_count >= onp.iinfo(onp.uint32).max:
    # TODO(mattjj): just split the key here
    raise TypeError(""requesting more random bits than a single call provides."")

  bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
  if bit_width == 64:
    bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
    bits = (bits[0] << 32) | bits[1]
  return lax.reshape(bits, shape)


### random samplers


@partial(jit, static_argnums=(1, 2))
def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
  """"""Sample uniform random values in [minval, maxval) with given shape/dtype.

  Args:
    key: a PRNGKey used as the random key.
    shape: a tuple of nonnegative integers representing the shape.
    dtype: optional, a float dtype for the returned values (default float32).
    minval: optional, a minimum (inclusive) value for the range (default 0).
    maxval: optional, a maximum (exclusive) value for the range (default 1).

  Returns:
    A random array with the specified shape and dtype.
  """"""
  if not onp.issubdtype(dtype, onp.floating):
    raise TypeError(""uniform only accepts floating point dtypes."")

  dtype = xla_bridge.canonicalize_dtype(dtype)
  minval = lax.convert_element_type(minval, dtype)
  maxval = lax.convert_element_type(maxval, dtype)
  finfo = onp.finfo(dtype)
  nbits, nmant = finfo.bits, finfo.nmant

  if nbits not in (32, 64):
    raise TypeError(""uniform only accepts 32- or 64-bit dtypes."")

  bits = _random_bits(key, nbits, shape)

  # The strategy here is to randomize only the mantissa bits with an exponent of
  # 1 (after applying the bias), then shift and scale to the desired range. The
  # bit-level transformation we use relies on Numpy and XLA having bit-for-bit
  # equivalent float representations, which might not be true on all platforms.
  float_bits = lax.bitwise_or(
      lax.shift_right_logical(bits, onp.array(nbits - nmant, lax._dtype(bits))),
      onp.array(1., dtype).view(onp.uint32 if nbits == 32 else onp.uint64))
  floats = lax.bitcast_convert_type(float_bits, dtype) - onp.array(1., dtype)
  return lax.max(
      minval,
      lax.reshape(floats * (maxval - minval) + minval, shape))


@partial(jit, static_argnums=(1, 4))
def randint(key, shape, minval, maxval, dtype=onp.int32):
  """"""Sample uniform random values in [minval, maxval) with given shape/dtype.

  Args:
    key: a PRNGKey used as the random key.
    shape: a tuple of nonnegative integers representing the shape.
    minval: optional, a minimum (inclusive) value for the range (default 0).
    maxval: optional, a maximum (exclusive) value for the range (default 1).
    dtype: optional, an int dtype for the returned values (default int32).

  Returns:
    A random array with the specified shape and dtype.
  """"""
  if not onp.issubdtype(dtype, onp.integer):
    raise TypeError(""randint only accepts integer dtypes."")

  dtype = xla_bridge.canonicalize_dtype(dtype)
  minval = lax.convert_element_type(minval, dtype)
  maxval = lax.convert_element_type(maxval, dtype)
  nbits = onp.iinfo(dtype).bits

  if nbits not in (32, 64):
    raise TypeError(""randint only accepts 32- or 64-bit dtypes."")

  # This algorithm is biased whenever (maxval - minval) is not a power of 2.
  # We generate double the number of random bits required by the dtype so as to
  # reduce that bias.
  k1, k2 = split(key)
  rbits = lambda key: _random_bits(key, nbits, shape)
  higher_bits, lower_bits = rbits(k1), rbits(k2)

  unsigned_dtype = onp.uint32 if nbits == 32 else onp.uint64
  span = lax.convert_element_type(maxval - minval, unsigned_dtype)

  # To compute a remainder operation on an integer that might have twice as many
  # bits as we can represent in the native unsigned dtype, we compute a
  # multiplier equal to 2**nbits % span (using that nbits is 32 or 64).
  multiplier = lax.rem(onp.array(2**16, unsigned_dtype), span)
  multiplier = lax.rem(lax.mul(multiplier, multiplier), span)
  if nbits == 64:
    multiplier = lax.rem(lax.mul(multiplier, multiplier), span)

  random_offset = lax.add(lax.mul(lax.rem(higher_bits, span), multiplier),
                          lax.rem(lower_bits, span))
  random_offset = lax.rem(random_offset, span)
  return lax.add(minval, lax.convert_element_type(random_offset, dtype))


@partial(jit, static_argnums=(2,))
def shuffle(key, x, axis=0):
  """"""Shuffle the elements of an array uniformly at random along an axis.

  Args:
    key: a PRNGKey used as the random key.
    x: the array to be shuffled.
    axis: optional, an int axis along which to shuffle (default 0).

  Returns:
    A shuffled version of x.
  """"""
  # On parallel architectures, Fisher-Yates is more expensive than doing
  # multiple sorts. This algorithm is based on one developed and analyzed by
  # tjablin@. We sort according to randomly-generated 32bit keys, but those keys
  # may have collisions. If we repeat the process, using fresh 32bit keys for
  # each sort, then whenever all pairs of elements have been assigned distinct
  # keys at some iteration (or equivalently when the strings formed by
  # concatenating the successive keys for each element are all distinct) then we
  # are guaranteed to have a perfect sample (assuming that either the sort is
  # stable or that any bias is not value-dependent). Since checking uniqueness
  # at runtime may be expensive, we use a heuristic static stop criterion
  # developed by tjablin@. See tensorflow/compiler/tf2xla/random_ops.cc for more
  # info, and for the original implementation of this algorithm. See also
  # Section 2 of http://people.csail.mit.edu/costis/6896sp11/lec5s.pdf for
  # another analysis (where the keys are generated one bit at a time).
  exponent = 3  # see tjablin@'s analysis for explanation of this parameter
  num_rounds = int(onp.ceil(exponent * onp.log(len(x)) / 32))

  for _ in range(num_rounds):
    key, subkey = split(key)
    sort_keys = _random_bits(subkey, 32, x.shape)
    _, x = lax.sort_key_val(sort_keys, x, axis)

  return x


@partial(jit, static_argnums=(1, 2))
def normal(key, shape, dtype=onp.float32):
  """"""Sample standard normal random values with given shape and float dtype.

  Args:
    key: a PRNGKey used as the random key.
    shape: a tuple of nonnegative integers representing the shape.
    dtype: optional, a float dtype for the returned values (default float32).

  Returns:
    A random array with the specified shape and dtype.
  """"""
  lo = onp.nextafter(onp.array(-1., dtype), 0., dtype=dtype)
  hi = onp.array(1., dtype)
  u = uniform(key, shape, dtype, lo, hi)
  return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)


@partial(jit, static_argnums=(2,))
def bernoulli(key, mean=onp.float32(0.5), shape=()):
  """"""Sample Bernoulli random values with given shape and mean.

  Args:
    key: a PRNGKey used as the random key.
    mean: optional, an array-like broadcastable to `shape` for the mean of the
      random variables (default 0.5).
    shape: optional, a tuple of nonnegative integers representing the shape
      (default scalar).

  Returns:
    A random array with the specified shape and boolean dtype.
  """"""
  shape = shape or onp.shape(mean)
  if not onp.issubdtype(lax._dtype(mean), onp.float32):
    mean = lax.convert_element_type(mean, onp.float32)
  if onp.shape(mean) != shape:
    mean = lax.broadcast(mean, shape)
  return lax.lt(uniform(key, shape), mean)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""LAX-based pseudo-random number generators (PRNGs).""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from functools import partial

import numpy as onp

from . import lax
from . import numpy as np
from . import tree_util
from .lib import xla_bridge
from .api import jit


class PRNGKey(object):
  """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
  __slots__ = [""keypair""]

  def __init__(self, seed):
    """"""Create a new PRNG key.

    Args:
      seed: a scalar integer value used to initialize the PRNG key.

    Returns:
      A new PRNGKey object.
    """"""
    convert = lambda key: lax.convert_element_type(key, onp.uint32)
    if onp.shape(seed):
      raise TypeError(""PRNGKey seed must be a scalar."")
    if isinstance(seed, (int, onp.ndarray)):
      # Special handling of raw integer values, which may have be 64bit even
      # when jax_enable_x64=False and we don't want to drop the top 32 bits
      k1 = convert(onp.bitwise_and(onp.right_shift(seed, 32), 0xFFFFFFFF))
    else:
      k1 = convert(lax.shift_right_logical(seed, 32))
    k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
    self.keypair = (k1, k2)

  @classmethod
  def from_keypair(cls, keypair):
    """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
    new = cls.__new__(cls)
    new.keypair = tuple(keypair)
    return new


tree_util.register_pytree_node(PRNGKey, lambda k: (k.keypair, None),
                               lambda _, xs: PRNGKey.from_keypair(xs))


### utilities


def _make_rotate_left(dtype):
  if not onp.issubdtype(dtype, onp.integer):
    raise TypeError(""_rotate_left only accepts integer dtypes."")
  nbits = onp.array(onp.iinfo(dtype).bits, dtype)

  def _rotate_left(x, d):
    if lax._dtype(d) != lax._dtype(x):
      d = lax.convert_element_type(d, x.dtype)
    return (x << d) | lax.shift_right_logical(x, nbits - d)
  return _rotate_left


def _bit_stats(bits):
  """"""This is a debugging function to compute the statistics of bit fields.""""""
  return onp.array([list(map(int, onp.binary_repr(x, 64))) for x in bits]).mean(0)


### hash function and split


@jit
def threefry_2x32(keypair, count):
  """"""Apply the Threefry 2x32 hash.

  Args:
    keypair: a pair of 32bit unsigned integers used for the key.
    count: an array of dtype uint32 used for the counts.

  Returns:
    An array of dtype uint32 with the same shape as `count`.
  """"""
  # Based on ThreeFry2x32 by phawkins@ in //.../xla/client/lib/prng.cc
  key1, key2 = keypair[0], keypair[1]
  if not lax._dtype(key1) == lax._dtype(key2) == lax._dtype(count) == onp.uint32:
    msg = ""threefry_2x32 requires uint32 arguments, got {}""
    raise TypeError(msg.format([lax._dtype(x) for x in [key1, key2, count]]))

  rotate_left = _make_rotate_left(lax._dtype(count))

  def apply_round(v, rot):
    v = v[:]
    v[0] = v[0] + v[1]
    v[1] = rotate_left(v[1], rot)
    v[1] = v[0] ^ v[1]
    return v

  odd_size = count.size % 2
  if odd_size:
    x = list(np.split(np.concatenate([count.ravel(), onp.uint32([0])]), 2))
  else:
    x = list(np.split(count.ravel(), 2))

  rotations = [13, 15, 26, 6, 17, 29, 16, 24]
  ks = [key1, key2, key1 ^ key2 ^ onp.uint32(0x1BD11BDA)]

  x[0] = x[0] + ks[0]
  x[1] = x[1] + ks[1]

  for r in rotations[:4]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[1]
  x[1] = x[1] + ks[2] + onp.uint32(1)

  for r in rotations[4:]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[2]
  x[1] = x[1] + ks[0] + onp.uint32(2)

  for r in rotations[:4]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[0]
  x[1] = x[1] + ks[1] + onp.uint32(3)

  for r in rotations[4:]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[1]
  x[1] = x[1] + ks[2] + onp.uint32(4)

  for r in rotations[:4]:
    x = apply_round(x, r)
  x[0] = x[0] + ks[2]
  x[1] = x[1] + ks[0] + onp.uint32(5)

  out = np.concatenate(x)
  assert out.dtype == onp.uint32
  return lax.reshape(out[:-1] if odd_size else out, count.shape)


@partial(jit, static_argnums=(1,))
def split(key, num=2):
  """"""Splits a PRNG key pair of 32bit unsigned integers into `num` new key pairs.

  Args:
    key: a PRNGKey used as the random key.
    num: optional, a positive integer indicating the number of keys to produce
      (default 2).

  Returns:
    A tuple of length `num` of new PRNGKey instances.
  """"""
  counts = onp.arange(num * 2, dtype=onp.uint32)
  bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
  keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
  return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)


def _random_bits(key, bit_width, shape):
  """"""Sample uniform random bits of given width and shape using PRNG key.""""""
  if bit_width not in (32, 64):
    raise TypeError(""requires 32- or 64-bit field width."")
  max_count = (bit_width // 32) * onp.prod(shape)
  if max_count >= onp.iinfo(onp.uint32).max:
    # TODO(mattjj): just split the key here
    raise TypeError(""requesting more random bits than a single call provides."")

  bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
  if bit_width == 64:
    bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
    bits = (bits[0] << onp.uint64(32)) | bits[1]
  return lax.reshape(bits, shape)


### random samplers


@partial(jit, static_argnums=(1, 2))
def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
  """"""Sample uniform random values in [minval, maxval) with given shape/dtype.

  Args:
    key: a PRNGKey used as the random key.
    shape: a tuple of nonnegative integers representing the shape.
    dtype: optional, a float dtype for the returned values (default float32).
    minval: optional, a minimum (inclusive) value for the range (default 0).
    maxval: optional, a maximum (exclusive) value for the range (default 1).

  Returns:
    A random array with the specified shape and dtype.
  """"""
  if not onp.issubdtype(dtype, onp.floating):
    raise TypeError(""uniform only accepts floating point dtypes."")

  dtype = xla_bridge.canonicalize_dtype(dtype)
  minval = lax.convert_element_type(minval, dtype)
  maxval = lax.convert_element_type(maxval, dtype)
  finfo = onp.finfo(dtype)
  nbits, nmant = finfo.bits, finfo.nmant

  if nbits not in (32, 64):
    raise TypeError(""uniform only accepts 32- or 64-bit dtypes."")

  bits = _random_bits(key, nbits, shape)

  # The strategy here is to randomize only the mantissa bits with an exponent of
  # 1 (after applying the bias), then shift and scale to the desired range. The
  # bit-level transformation we use relies on Numpy and XLA having bit-for-bit
  # equivalent float representations, which might not be true on all platforms.
  float_bits = lax.bitwise_or(
      lax.shift_right_logical(bits, onp.array(nbits - nmant, lax._dtype(bits))),
      onp.array(1., dtype).view(onp.uint32 if nbits == 32 else onp.uint64))
  floats = lax.bitcast_convert_type(float_bits, dtype) - onp.array(1., dtype)
  return lax.max(
      minval,
      lax.reshape(floats * (maxval - minval) + minval, shape))


@partial(jit, static_argnums=(1, 4))
def randint(key, shape, minval, maxval, dtype=onp.int32):
  """"""Sample uniform random values in [minval, maxval) with given shape/dtype.

  Args:
    key: a PRNGKey used as the random key.
    shape: a tuple of nonnegative integers representing the shape.
    minval: optional, a minimum (inclusive) value for the range (default 0).
    maxval: optional, a maximum (exclusive) value for the range (default 1).
    dtype: optional, an int dtype for the returned values (default int32).

  Returns:
    A random array with the specified shape and dtype.
  """"""
  if not onp.issubdtype(dtype, onp.integer):
    raise TypeError(""randint only accepts integer dtypes."")

  dtype = xla_bridge.canonicalize_dtype(dtype)
  minval = lax.convert_element_type(minval, dtype)
  maxval = lax.convert_element_type(maxval, dtype)
  nbits = onp.iinfo(dtype).bits

  if nbits not in (32, 64):
    raise TypeError(""randint only accepts 32- or 64-bit dtypes."")

  # This algorithm is biased whenever (maxval - minval) is not a power of 2.
  # We generate double the number of random bits required by the dtype so as to
  # reduce that bias.
  k1, k2 = split(key)
  rbits = lambda key: _random_bits(key, nbits, shape)
  higher_bits, lower_bits = rbits(k1), rbits(k2)

  unsigned_dtype = onp.uint32 if nbits == 32 else onp.uint64
  span = lax.convert_element_type(maxval - minval, unsigned_dtype)

  # To compute a remainder operation on an integer that might have twice as many
  # bits as we can represent in the native unsigned dtype, we compute a
  # multiplier equal to 2**nbits % span (using that nbits is 32 or 64).
  multiplier = lax.rem(onp.array(2**16, unsigned_dtype), span)
  multiplier = lax.rem(lax.mul(multiplier, multiplier), span)
  if nbits == 64:
    multiplier = lax.rem(lax.mul(multiplier, multiplier), span)

  random_offset = lax.add(lax.mul(lax.rem(higher_bits, span), multiplier),
                          lax.rem(lower_bits, span))
  random_offset = lax.rem(random_offset, span)
  return lax.add(minval, lax.convert_element_type(random_offset, dtype))


@partial(jit, static_argnums=(2,))
def shuffle(key, x, axis=0):
  """"""Shuffle the elements of an array uniformly at random along an axis.

  Args:
    key: a PRNGKey used as the random key.
    x: the array to be shuffled.
    axis: optional, an int axis along which to shuffle (default 0).

  Returns:
    A shuffled version of x.
  """"""
  # On parallel architectures, Fisher-Yates is more expensive than doing
  # multiple sorts. This algorithm is based on one developed and analyzed by
  # tjablin@. We sort according to randomly-generated 32bit keys, but those keys
  # may have collisions. If we repeat the process, using fresh 32bit keys for
  # each sort, then whenever all pairs of elements have been assigned distinct
  # keys at some iteration (or equivalently when the strings formed by
  # concatenating the successive keys for each element are all distinct) then we
  # are guaranteed to have a perfect sample (assuming that either the sort is
  # stable or that any bias is not value-dependent). Since checking uniqueness
  # at runtime may be expensive, we use a heuristic static stop criterion
  # developed by tjablin@. See tensorflow/compiler/tf2xla/random_ops.cc for more
  # info, and for the original implementation of this algorithm. See also
  # Section 2 of http://people.csail.mit.edu/costis/6896sp11/lec5s.pdf for
  # another analysis (where the keys are generated one bit at a time).
  exponent = 3  # see tjablin@'s analysis for explanation of this parameter
  num_rounds = int(onp.ceil(exponent * onp.log(len(x)) / 32))

  for _ in range(num_rounds):
    key, subkey = split(key)
    sort_keys = _random_bits(subkey, 32, x.shape)
    _, x = lax.sort_key_val(sort_keys, x, axis)

  return x


@partial(jit, static_argnums=(1, 2))
def normal(key, shape, dtype=onp.float32):
  """"""Sample standard normal random values with given shape and float dtype.

  Args:
    key: a PRNGKey used as the random key.
    shape: a tuple of nonnegative integers representing the shape.
    dtype: optional, a float dtype for the returned values (default float32).

  Returns:
    A random array with the specified shape and dtype.
  """"""
  lo = onp.nextafter(onp.array(-1., dtype), 0., dtype=dtype)
  hi = onp.array(1., dtype)
  u = uniform(key, shape, dtype, lo, hi)
  return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)


@partial(jit, static_argnums=(2,))
def bernoulli(key, mean=onp.float32(0.5), shape=()):
  """"""Sample Bernoulli random values with given shape and mean.

  Args:
    key: a PRNGKey used as the random key.
    mean: optional, an array-like broadcastable to `shape` for the mean of the
      random variables (default 0.5).
    shape: optional, a tuple of nonnegative integers representing the shape
      (default scalar).

  Returns:
    A random array with the specified shape and boolean dtype.
  """"""
  shape = shape or onp.shape(mean)
  if not onp.issubdtype(lax._dtype(mean), onp.float32):
    mean = lax.convert_element_type(mean, onp.float32)
  if onp.shape(mean) != shape:
    mean = lax.broadcast(mean, shape)
  return lax.lt(uniform(key, shape), mean)
","diff --git a/jax/random.py b/jax/random.py
index 0edd3f1fbfa40fba6264a6caf4d18160742004e3..2b7e4f34b7b58fe8d40e60193f750d33fe5b18ce 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -187,7 +187,7 @@ def _random_bits(key, bit_width, shape):
   bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
-    bits = (bits[0] << 32) | bits[1]
+    bits = (bits[0] << onp.uint64(32)) | bits[1]
   return lax.reshape(bits, shape)
 
 
",Deadlock / livelock,Fix threefry_2x32 bit shifting to use uint64 literal for consistency.
c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,"Peter Hawkins: [JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.",tests/lax_numpy_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

FLAGS = config.FLAGS

array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
# fixed so the tests pass.
all_shapes = array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True])
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape)))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2])
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape)))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()])
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes)
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes)
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape)))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes)
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes)
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ]))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  config.config_with_absl()
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

FLAGS = config.FLAGS

array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
unsigned_dtypes = (
    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_BITWISE_OP_RECORDS = [
    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


def _dtypes_are_compatible_for_bitwise_ops(args):
  if len(args) <= 1:
    return True
  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
  width = lambda dtype: onp.iinfo(dtype).bits
  x, y = args
  if width(x) > width(y):
    x, y = y, x
  # The following condition seems a little ad hoc, but seems to capture what
  # numpy actually implements.
  return (
      is_signed(x) == is_signed(y)
      or (width(x) == 32 and width(y) == 32)
      or (width(x) == 32 and width(y) == 64 and is_signed(y)))


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in JAX_BITWISE_OP_RECORDS
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in filter(
          _dtypes_are_compatible_for_bitwise_ops,
          CombosWithReplacement(rec.dtypes, rec.nargs)))
  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True])
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape)))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2])
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape)))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()])
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes)
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes)
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape)))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes)
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes)
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ]))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  config.config_with_absl()
  absltest.main()
","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5eae4fdee2181e2c277e835d8b863d49b67e7f7d..d275fc2a4198bb86cf8847d07645d10863add1fd 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -34,13 +34,13 @@ FLAGS = config.FLAGS
 
 array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
 
-# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
-# fixed so the tests pass.
-all_shapes = array_shapes
+all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
 int_dtypes = [onp.int32, onp.int64]
+unsigned_dtypes = (
+    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
@@ -57,10 +57,6 @@ def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
 JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
     op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
     op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
@@ -109,6 +105,17 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
 ]
 
+JAX_BITWISE_OP_RECORDS = [
+    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+]
+
 JAX_REDUCER_RECORDS = [
     op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
@@ -128,6 +135,22 @@ JAX_ARGMINMAX_RECORDS = [
 CombosWithReplacement = itertools.combinations_with_replacement
 
 
+def _dtypes_are_compatible_for_bitwise_ops(args):
+  if len(args) <= 1:
+    return True
+  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
+  width = lambda dtype: onp.iinfo(dtype).bits
+  x, y = args
+  if width(x) > width(y):
+    x, y = y, x
+  # The following condition seems a little ad hoc, but seems to capture what
+  # numpy actually implements.
+  return (
+      is_signed(x) == is_signed(y)
+      or (width(x) == 32 and width(y) == 32)
+      or (width(x) == 32 and width(y) == 64 and is_signed(y)))
+
+
 class LaxBackedNumpyTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed Numpy implementation.""""""
 
@@ -148,6 +171,21 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in JAX_BITWISE_OP_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in filter(
+          _dtypes_are_compatible_for_bitwise_ops,
+          CombosWithReplacement(rec.dtypes, rec.nargs)))
+  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),
",Missing or broken tests,Add support for bitwise operations and unsigned integer types in LAX-backed Numpy tests.
bfd58854e915439dc9aad057a30dddfee343e681,Peter Hawkins: Fix test case to test for x64 mode during test rather than init time.,tests/lax_numpy_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

FLAGS = config.FLAGS

array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
unsigned_dtypes = (
    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_BITWISE_OP_RECORDS = [
    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


def _dtypes_are_compatible_for_bitwise_ops(args):
  if len(args) <= 1:
    return True
  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
  width = lambda dtype: onp.iinfo(dtype).bits
  x, y = args
  if width(x) > width(y):
    x, y = y, x
  # The following condition seems a little ad hoc, but seems to capture what
  # numpy actually implements.
  return (
      is_signed(x) == is_signed(y)
      or (width(x) == 32 and width(y) == 32)
      or (width(x) == 32 and width(y) == 64 and is_signed(y)))


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in JAX_BITWISE_OP_RECORDS
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in filter(
          _dtypes_are_compatible_for_bitwise_ops,
          CombosWithReplacement(rec.dtypes, rec.nargs)))
  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True])
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape)))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2])
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape)))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()])
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes)
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes)
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape)))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes)
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes)
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ]))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  config.config_with_absl()
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

FLAGS = config.FLAGS

array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
unsigned_dtypes = [onp.uint32, onp.uint64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_BITWISE_OP_RECORDS = [
    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


def _dtypes_are_compatible_for_bitwise_ops(args):
  if len(args) <= 1:
    return True
  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
  width = lambda dtype: onp.iinfo(dtype).bits
  x, y = args
  if width(x) > width(y):
    x, y = y, x
  # The following condition seems a little ad hoc, but seems to capture what
  # numpy actually implements.
  return (
      is_signed(x) == is_signed(y)
      or (width(x) == 32 and width(y) == 32)
      or (width(x) == 32 and width(y) == 64 and is_signed(y)))


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in JAX_BITWISE_OP_RECORDS
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in filter(
          _dtypes_are_compatible_for_bitwise_ops,
          CombosWithReplacement(rec.dtypes, rec.nargs)))
  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    if not FLAGS.jax_enable_x64 and any(
        onp.iinfo(dtype).bits == 64 for dtype in dtypes):
      self.skipTest(""x64 types are disabled by jax_enable_x64"")
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True])
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape)))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2])
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape)))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()])
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes)
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes)
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ])
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape)))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes)
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes)
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ]))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  config.config_with_absl()
  absltest.main()
","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index d275fc2a4198bb86cf8847d07645d10863add1fd..da799b2379d3d3fc3f4c7bc041e28e42ef91be09 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -39,8 +39,7 @@ all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
 int_dtypes = [onp.int32, onp.int64]
-unsigned_dtypes = (
-    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
+unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
@@ -182,6 +181,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           _dtypes_are_compatible_for_bitwise_ops,
           CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    if not FLAGS.jax_enable_x64 and any(
+        onp.iinfo(dtype).bits == 64 for dtype in dtypes):
+      self.skipTest(""x64 types are disabled by jax_enable_x64"")
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
",Dead / unused code,Remove conditional exclusion of uint64 dtype and add test skip for x64 types when jax_enable_x64 is False
ae7df43e9bcdcd4ff70228bfddb1f52973315b8e,Dougal Maclaurin: Fixed bug due to input_shape kwarg not being modified in batching rule for reducers. Fixes b/120595235,jax/interpreters/batching.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from collections import namedtuple

import itertools as it

import numpy as onp

from six.moves import reduce

from .. import core
from ..core import Trace, Tracer, new_master, pack, AbstractTuple, JaxTuple
from ..abstract_arrays import ShapedArray, make_shaped_array, array_types
from ..ad_util import add_jaxvals_p
from ..linear_util import transformation, transformation_with_aux, wrap_init
from ..tree_util import register_pytree_node
from ..util import unzip2, partial, safe_map

map = safe_map


def batch(fun, in_vals, in_dims, out_dim_target):
  sizes = reduce(set.union, map(dimsize, in_dims, in_vals))
  if not sizes:
    return fun.call_wrapped(*in_vals), None  # no mapped dimensions
  elif len(sizes) == 1:
    out_val, out_dim = batch_transform(fun).call_wrapped(in_vals, in_dims)
    return moveaxis(sizes.pop(), out_dim_target, out_dim, out_val)
  else:
    raise TypeError(""got inconsistent map dimension sizes: {}"".format(sizes))


# TODO(mattjj,dougalm): could call batch_subtrace here (a bit redundant)
@transformation
def batch_transform(vals, dims):
  with new_master(BatchTrace) as master:
    trace = BatchTrace(master, core.cur_sublevel())
    in_tracers = map(partial(BatchTracer, trace), vals, dims)
    out_tracer = yield in_tracers
    out_tracer = trace.full_raise(out_tracer)
    out_val, out_dim = out_tracer.val, out_tracer.batch_dim
    del master
  yield (out_val, out_dim)


@transformation_with_aux
def batch_subtrace(master, dims, *vals):
  trace = BatchTrace(master, core.cur_sublevel())
  ans = yield map(partial(BatchTracer, trace), vals, dims)
  out_tracer = trace.full_raise(ans)
  out_val, out_dim = out_tracer.val, out_tracer.batch_dim
  yield out_val, out_dim


### tracer


class BatchTracer(Tracer):
  def __init__(self, trace, val, batch_dim):
    self.trace = trace
    self.val = val
    self.batch_dim = batch_dim

  @property
  def aval(self):
    batched_aval = get_aval(self.val)
    return remove_batch_dim_from_aval(self.batch_dim, batched_aval)

  def unpack(self):
    t = type(self.batch_dim)
    if t is tuple:
      batch_dims = self.batch_dim
    elif t is int:
      batch_dims = [self.batch_dim] * len(self.val)
    elif t is type(None):
      return tuple(self.val)
    else:
      raise TypeError(t)
    return map(partial(BatchTracer, self.trace), self.val, batch_dims)

  def full_lower(self):
    if self.batch_dim is None:
      return core.full_lower(self.val)
    else:
      return self

class BatchTrace(Trace):
  def pure(self, val):
    return BatchTracer(self, val, None)

  def lift(self, val):
    return BatchTracer(self, val, None)

  def sublift(self, val):
    return BatchTracer(self, val.val, val.batch_dim)

  def process_primitive(self, primitive, tracers, params):
    vals_in, dims_in = unzip2((t.val, t.batch_dim) for t in tracers)
    if all(bdim is None for bdim in dims_in):
      return primitive.bind(*vals_in, **params)
    else:
      batched_primitive = get_primitive_batcher(primitive)
      val_out, dim_out = batched_primitive(vals_in, dims_in, **params)
      return BatchTracer(self, val_out, dim_out)

  def process_call(self, call_primitive, f, tracers, params):
    vals, dims = unzip2((t.val, t.batch_dim) for t in tracers)
    if all(bdim is None for bdim in dims):
      return call_primitive.bind(f, *vals, **params)
    else:
      f, dim_out = batch_subtrace(f, self.master, dims)
      val_out = call_primitive.bind(f, *vals, **params)
      return BatchTracer(self, val_out, dim_out())

  def post_process_call(self, _, out_tracer):
    raise NotImplementedError  # TODO(mattjj,dougalm)

  def pack(self, tracers):
    vals = pack([t.val for t in tracers])
    batch_dim = tuple(t.batch_dim for t in tracers)
    return BatchTracer(self, vals, batch_dim)


### abstract values


def get_aval(x):
  if isinstance(x, Tracer):
    return raise_to_shaped(x.aval)
  else:
    return shaped_aval(x)

def shaped_aval(x):
  try:
    return pytype_aval_mappings[type(x)](x)
  except KeyError:
    raise TypeError(""{} is not a valid type for batching"".format(type(x)))

def raise_to_shaped(aval):
  if type(aval) is AbstractTuple:
    return AbstractTuple(map(raise_to_shaped, aval))
  elif isinstance(aval, ShapedArray):
    return ShapedArray(aval.shape, aval.dtype)
  else:
    raise TypeError(type(aval))

def remove_batch_dim_from_aval(bdim, aval):
  t = type(aval)
  if t is AbstractTuple:
    if type(bdim) is tuple:
      return AbstractTuple(map(remove_batch_dim_from_aval, bdim, aval))
    else:
      return AbstractTuple(map(partial(remove_batch_dim_from_aval, bdim), aval))
  elif t is ShapedArray:
    if bdim is None:
      return ShapedArray(aval.shape, aval.dtype)
    else:
      assert 0 <= bdim < aval.ndim
      unbatched_shape = tuple(onp.delete(aval.shape, bdim))
      return ShapedArray(unbatched_shape, aval.dtype)
  else:
    raise TypeError(t)

pytype_aval_mappings = {}

def shaped_jaxtuple(xs):
  return AbstractTuple(map(shaped_aval, xs))

pytype_aval_mappings[JaxTuple] = shaped_jaxtuple

for t in array_types:
  pytype_aval_mappings[t] = make_shaped_array


### primitives


primitive_batchers = {}

def get_primitive_batcher(p):
  try:
    return primitive_batchers[p]
  except KeyError:
    raise NotImplementedError(
        ""Batching rule for '{}' not implemented"".format(p))

def defvectorized(prim):
  primitive_batchers[prim] = partial(vectorized_batcher, prim)

def vectorized_batcher(prim, batched_args, batch_dims, **params):
  assert all(batch_dims[0] == bd for bd in batch_dims[1:])
  return prim.bind(*batched_args, **params), batch_dims[0]

def defbroadcasting(prim):
  primitive_batchers[prim] = partial(broadcast_batcher, prim)

def broadcast_batcher(prim, batched_args, batch_dims, **params):
  args = map(bdim_at_front, batched_args, batch_dims)
  ndim = max(map(onp.ndim, args))  # special case to handle scalar broadcasting
  args = map(partial(handle_scalar_broadcasting, ndim), args, batch_dims)
  return prim.bind(*args, **params), 0

def defreducer(prim):
  primitive_batchers[prim] = partial(reducer_batcher, prim)

def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
  operand, = batched_args
  bdim, = batch_dims
  axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
  bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
  return prim.bind(operand, axes=axes, **kwargs), bdim_out

def add_batched(batched_args, batch_dims):
  xs, ys = batched_args
  bdx, bdy = batch_dims
  if bdx == bdy:
    return add_jaxvals_p.bind(xs, ys), bdx
  else:
    raise NotImplementedError  # TODO(mattjj)
primitive_batchers[add_jaxvals_p] = add_batched


### util


def bdim_at_front(x, bdim, broadcast_size=1):
  if bdim is None:
    return broadcast(x, broadcast_size) if onp.ndim(x) else x
  else:
    return move_dim_to_front(x, bdim)

def move_dim_to_front(x, dim):
  assert 0 <= dim < onp.ndim(x)
  if dim == 0:
    return x
  else:
    perm = (dim,) + tuple(range(dim)) + tuple(range(dim + 1, onp.ndim(x)))
    return x.transpose(perm)

def handle_scalar_broadcasting(nd, x, bdim):
  if bdim is None or nd == onp.ndim(x):
    return x
  else:
    return x.reshape(x.shape + (1,) * (nd - x.ndim))

def dimsize(dim, x):
  if type(x) is JaxTuple:
    return reduce(set.union, map(partial(dimsize, dim), x))
  elif type(dim) is int:
    return {x.shape[dim]}
  elif dim is None:
    return set()
  else:
    raise TypeError(type(dim))

def moveaxis(sz, dst, src, x):
  aval = get_aval(x)
  if type(aval) is AbstractTuple:
    if type(src) is tuple and type(dst) is tuple:
      return pack(map(partial(moveaxis, sz), dst, src, x))
    elif type(src) is tuple:
      return pack(map(partial(moveaxis, sz, dst), src, x))
    else:
      return pack(map(partial(moveaxis, sz, dst, src), x))
  elif isinstance(aval, ShapedArray):
    if src == dst:
      return x
    else:
      if src is None:
        x = broadcast(x, sz)
        src = 0
      if src == dst:
        return x
      else:
        perm = [i for i in range(onp.ndim(x)) if i != src]
        perm.insert(dst, src)
        return x.transpose(perm)
  else:
    raise TypeError(type(aval))

def broadcast(x, sz):
  try:
    return x.broadcast((sz,))
  except AttributeError:
    assert not isinstance(x, Tracer)
    return onp.broadcast_to(x, (sz,) + onp.shape(x))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from collections import namedtuple

import itertools as it

import numpy as onp

from six.moves import reduce

from .. import core
from ..core import Trace, Tracer, new_master, pack, AbstractTuple, JaxTuple
from ..abstract_arrays import ShapedArray, make_shaped_array, array_types
from ..ad_util import add_jaxvals_p
from ..linear_util import transformation, transformation_with_aux, wrap_init
from ..tree_util import register_pytree_node
from ..util import unzip2, partial, safe_map

map = safe_map


def batch(fun, in_vals, in_dims, out_dim_target):
  sizes = reduce(set.union, map(dimsize, in_dims, in_vals))
  if not sizes:
    return fun.call_wrapped(*in_vals), None  # no mapped dimensions
  elif len(sizes) == 1:
    out_val, out_dim = batch_transform(fun).call_wrapped(in_vals, in_dims)
    return moveaxis(sizes.pop(), out_dim_target, out_dim, out_val)
  else:
    raise TypeError(""got inconsistent map dimension sizes: {}"".format(sizes))


# TODO(mattjj,dougalm): could call batch_subtrace here (a bit redundant)
@transformation
def batch_transform(vals, dims):
  with new_master(BatchTrace) as master:
    trace = BatchTrace(master, core.cur_sublevel())
    in_tracers = map(partial(BatchTracer, trace), vals, dims)
    out_tracer = yield in_tracers
    out_tracer = trace.full_raise(out_tracer)
    out_val, out_dim = out_tracer.val, out_tracer.batch_dim
    del master
  yield (out_val, out_dim)


@transformation_with_aux
def batch_subtrace(master, dims, *vals):
  trace = BatchTrace(master, core.cur_sublevel())
  ans = yield map(partial(BatchTracer, trace), vals, dims)
  out_tracer = trace.full_raise(ans)
  out_val, out_dim = out_tracer.val, out_tracer.batch_dim
  yield out_val, out_dim


### tracer


class BatchTracer(Tracer):
  def __init__(self, trace, val, batch_dim):
    self.trace = trace
    self.val = val
    self.batch_dim = batch_dim

  @property
  def aval(self):
    batched_aval = get_aval(self.val)
    return remove_batch_dim_from_aval(self.batch_dim, batched_aval)

  def unpack(self):
    t = type(self.batch_dim)
    if t is tuple:
      batch_dims = self.batch_dim
    elif t is int:
      batch_dims = [self.batch_dim] * len(self.val)
    elif t is type(None):
      return tuple(self.val)
    else:
      raise TypeError(t)
    return map(partial(BatchTracer, self.trace), self.val, batch_dims)

  def full_lower(self):
    if self.batch_dim is None:
      return core.full_lower(self.val)
    else:
      return self

class BatchTrace(Trace):
  def pure(self, val):
    return BatchTracer(self, val, None)

  def lift(self, val):
    return BatchTracer(self, val, None)

  def sublift(self, val):
    return BatchTracer(self, val.val, val.batch_dim)

  def process_primitive(self, primitive, tracers, params):
    vals_in, dims_in = unzip2((t.val, t.batch_dim) for t in tracers)
    if all(bdim is None for bdim in dims_in):
      return primitive.bind(*vals_in, **params)
    else:
      batched_primitive = get_primitive_batcher(primitive)
      val_out, dim_out = batched_primitive(vals_in, dims_in, **params)
      return BatchTracer(self, val_out, dim_out)

  def process_call(self, call_primitive, f, tracers, params):
    vals, dims = unzip2((t.val, t.batch_dim) for t in tracers)
    if all(bdim is None for bdim in dims):
      return call_primitive.bind(f, *vals, **params)
    else:
      f, dim_out = batch_subtrace(f, self.master, dims)
      val_out = call_primitive.bind(f, *vals, **params)
      return BatchTracer(self, val_out, dim_out())

  def post_process_call(self, _, out_tracer):
    raise NotImplementedError  # TODO(mattjj,dougalm)

  def pack(self, tracers):
    vals = pack([t.val for t in tracers])
    batch_dim = tuple(t.batch_dim for t in tracers)
    return BatchTracer(self, vals, batch_dim)


### abstract values


def get_aval(x):
  if isinstance(x, Tracer):
    return raise_to_shaped(x.aval)
  else:
    return shaped_aval(x)

def shaped_aval(x):
  try:
    return pytype_aval_mappings[type(x)](x)
  except KeyError:
    raise TypeError(""{} is not a valid type for batching"".format(type(x)))

def raise_to_shaped(aval):
  if type(aval) is AbstractTuple:
    return AbstractTuple(map(raise_to_shaped, aval))
  elif isinstance(aval, ShapedArray):
    return ShapedArray(aval.shape, aval.dtype)
  else:
    raise TypeError(type(aval))

def remove_batch_dim_from_aval(bdim, aval):
  t = type(aval)
  if t is AbstractTuple:
    if type(bdim) is tuple:
      return AbstractTuple(map(remove_batch_dim_from_aval, bdim, aval))
    else:
      return AbstractTuple(map(partial(remove_batch_dim_from_aval, bdim), aval))
  elif t is ShapedArray:
    if bdim is None:
      return ShapedArray(aval.shape, aval.dtype)
    else:
      assert 0 <= bdim < aval.ndim
      unbatched_shape = tuple(onp.delete(aval.shape, bdim))
      return ShapedArray(unbatched_shape, aval.dtype)
  else:
    raise TypeError(t)

pytype_aval_mappings = {}

def shaped_jaxtuple(xs):
  return AbstractTuple(map(shaped_aval, xs))

pytype_aval_mappings[JaxTuple] = shaped_jaxtuple

for t in array_types:
  pytype_aval_mappings[t] = make_shaped_array


### primitives


primitive_batchers = {}

def get_primitive_batcher(p):
  try:
    return primitive_batchers[p]
  except KeyError:
    raise NotImplementedError(
        ""Batching rule for '{}' not implemented"".format(p))

def defvectorized(prim):
  primitive_batchers[prim] = partial(vectorized_batcher, prim)

def vectorized_batcher(prim, batched_args, batch_dims, **params):
  assert all(batch_dims[0] == bd for bd in batch_dims[1:])
  return prim.bind(*batched_args, **params), batch_dims[0]

def defbroadcasting(prim):
  primitive_batchers[prim] = partial(broadcast_batcher, prim)

def broadcast_batcher(prim, batched_args, batch_dims, **params):
  args = map(bdim_at_front, batched_args, batch_dims)
  ndim = max(map(onp.ndim, args))  # special case to handle scalar broadcasting
  args = map(partial(handle_scalar_broadcasting, ndim), args, batch_dims)
  return prim.bind(*args, **params), 0

def defreducer(prim):
  primitive_batchers[prim] = partial(reducer_batcher, prim)

def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
  operand, = batched_args
  bdim, = batch_dims
  axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
  bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
  if 'input_shape' in kwargs:
    kwargs['input_shape'] = operand.shape
  return prim.bind(operand, axes=axes, **kwargs), bdim_out

def add_batched(batched_args, batch_dims):
  xs, ys = batched_args
  bdx, bdy = batch_dims
  if bdx == bdy:
    return add_jaxvals_p.bind(xs, ys), bdx
  else:
    raise NotImplementedError  # TODO(mattjj)
primitive_batchers[add_jaxvals_p] = add_batched


### util


def bdim_at_front(x, bdim, broadcast_size=1):
  if bdim is None:
    return broadcast(x, broadcast_size) if onp.ndim(x) else x
  else:
    return move_dim_to_front(x, bdim)

def move_dim_to_front(x, dim):
  assert 0 <= dim < onp.ndim(x)
  if dim == 0:
    return x
  else:
    perm = (dim,) + tuple(range(dim)) + tuple(range(dim + 1, onp.ndim(x)))
    return x.transpose(perm)

def handle_scalar_broadcasting(nd, x, bdim):
  if bdim is None or nd == onp.ndim(x):
    return x
  else:
    return x.reshape(x.shape + (1,) * (nd - x.ndim))

def dimsize(dim, x):
  if type(x) is JaxTuple:
    return reduce(set.union, map(partial(dimsize, dim), x))
  elif type(dim) is int:
    return {x.shape[dim]}
  elif dim is None:
    return set()
  else:
    raise TypeError(type(dim))

def moveaxis(sz, dst, src, x):
  aval = get_aval(x)
  if type(aval) is AbstractTuple:
    if type(src) is tuple and type(dst) is tuple:
      return pack(map(partial(moveaxis, sz), dst, src, x))
    elif type(src) is tuple:
      return pack(map(partial(moveaxis, sz, dst), src, x))
    else:
      return pack(map(partial(moveaxis, sz, dst, src), x))
  elif isinstance(aval, ShapedArray):
    if src == dst:
      return x
    else:
      if src is None:
        x = broadcast(x, sz)
        src = 0
      if src == dst:
        return x
      else:
        perm = [i for i in range(onp.ndim(x)) if i != src]
        perm.insert(dst, src)
        return x.transpose(perm)
  else:
    raise TypeError(type(aval))

def broadcast(x, sz):
  try:
    return x.broadcast((sz,))
  except AttributeError:
    assert not isinstance(x, Tracer)
    return onp.broadcast_to(x, (sz,) + onp.shape(x))
","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 4611f0daa319214fee4fd7e64c4b8c95d80449ab..56e76461603115be0dddd2147ae61ea994c695a6 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -224,6 +224,8 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
   bdim, = batch_dims
   axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
   bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
+  if 'input_shape' in kwargs:
+    kwargs['input_shape'] = operand.shape
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
 def add_batched(batched_args, batch_dims):
",Dependency / version mismatch,Add input shape to reducer batcher kwargs when available
ae7df43e9bcdcd4ff70228bfddb1f52973315b8e,Dougal Maclaurin: Fixed bug due to input_shape kwarg not being modified in batching rule for reducers. Fixes b/120595235,jax/lax.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
#   assert False  # TODO

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')
  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])

  starts = onp.zeros((len(operands), t.ndim), dtype=int)
  starts[1:, dimension] = limit_points[:-1]
  limits = onp.tile(t.shape, (len(operands), 1))
  limits[:, dimension] = limit_points

  return [slice(t, start, limit) if o is None else None
          for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
#   assert False  # TODO

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')
  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])

  starts = onp.zeros((len(operands), t.ndim), dtype=int)
  starts[1:, dimension] = limit_points[:-1]
  limits = onp.tile(t.shape, (len(operands), 1))
  limits[:, dimension] = limit_points

  return [slice(t, start, limit) if o is None else None
          for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","diff --git a/jax/lax.py b/jax/lax.py
index 9e5ae12a3816944fbff572504f5515d3747a8ead..5d6d63f58b74ceb9fdf9e68746142e554ef8bf4f 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1790,6 +1790,8 @@ batching.defreducer(reduce_p)
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):
+  assert operand.shape == input_shape, ('{} != {}'
+                                        .format(operand.shape, input_shape))
   return tuple(onp.delete(operand.shape, axes))
 
 def reduce_sum_translation_rule(c, operand, axes, input_shape):
",Boundary condition / off-by-one,Added shape validation to `reduce_sum_shape_rule` to ensure operand and input shapes match.
db748a046f5f8856cb65dbce0d8d7bc99256d65f,Roy Frostig: py3 compatibility fix,jax/test_util.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import re
import itertools as it
import random

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from . import api
from .config import flags
from .util import partial
from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce

FLAGS = flags.FLAGS
flags.DEFINE_enum(
    'jax_test_dut',
    None,
    enum_values=['cpu', 'gpu', 'tpu'],
    help=
    'Describes the device under test in case special consideration is required.'
)

flags.DEFINE_integer(
  'num_generated_cases',
  100,
  help='Number of generated cases to test')

EPS = 1e-4
ATOL = 1e-4
RTOL = 1e-4

_dtype = lambda x: getattr(x, 'dtype', None) or onp.asarray(x).dtype


def numpy_eq(x, y):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    return onp.allclose(x, y, 1e-3, 1e-3)
  else:
    return onp.allclose(x, y)


def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    atol = max(atol, 1e-1)
    rtol = max(rtol, 1e-1)
  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)


def check_eq(xs, ys):
  assert tree_all(tree_multimap(numpy_eq, xs, ys)), \
      '\n{} != \n{}'.format(xs, ys)


def check_close(xs, ys, atol=ATOL, rtol=RTOL):
  close = partial(numpy_close, atol=atol, rtol=rtol)
  assert tree_all(tree_multimap(close, xs, ys)), '\n{} != \n{}'.format(xs, ys)


def inner_prod(xs, ys):
  contract = lambda x, y: onp.real(onp.vdot(x, y))
  return tree_reduce(onp.add, tree_multimap(contract, xs, ys))


add = partial(tree_multimap, onp.add)
sub = partial(tree_multimap, onp.subtract)
conj = partial(tree_map, onp.conj)


def scalar_mul(xs, a):
  return tree_map(lambda x: onp.multiply(x, a, dtype=_dtype(x)), xs)


def rand_like(rng, x):
  shape = onp.shape(x)
  dtype = _dtype(x)
  randn = lambda: onp.asarray(rng.randn(*shape), dtype=dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    return randn() + 1.0j * randn()
  else:
    return randn()


def numerical_jvp(f, primals, tangents, eps=EPS):
  delta = scalar_mul(tangents, EPS)
  f_pos = f(*add(primals, delta))
  f_neg = f(*sub(primals, delta))
  return scalar_mul(sub(f_pos, f_neg), 0.5 / EPS)


def check_jvp(f, f_jvp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  rng = onp.random.RandomState(0)
  tangent = tree_map(partial(rand_like, rng), args)
  v_out, t_out = f_jvp(args, tangent)
  v_out_expected = f(*args)
  t_out_expected = numerical_jvp(f, args, tangent, eps=eps)
  check_eq(v_out, v_out_expected)
  check_close(t_out, t_out_expected, atol=atol, rtol=rtol)


def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  _rand_like = partial(rand_like, onp.random.RandomState(0))
  v_out, vjpfun = f_vjp(*args)
  v_out_expected = f(*args)
  check_eq(v_out, v_out_expected)
  tangent = tree_map(_rand_like, args)
  tangent_out = numerical_jvp(f, args, tangent, eps=EPS)
  cotangent = tree_map(_rand_like, v_out)
  cotangent_out = conj(vjpfun(conj(cotangent)))
  ip = inner_prod(tangent, cotangent_out)
  ip_expected = inner_prod(tangent_out, cotangent)
  check_close(ip, ip_expected, atol=atol, rtol=rtol)


def skip_on_devices(*disabled_devices):
  """"""A decorator for test methods to skip the test on certain devices.""""""
  def skip(test_method):
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      device = FLAGS.jax_test_dut
      if device in disabled_devices:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported on {}.'.format(test_name, device.upper()))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def skip_on_flag(flag_name, skip_value):
  """"""A decorator for test methods to skip the test when flags are set.""""""
  def skip(test_method):        # pylint: disable=missing-docstring
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      flag_value = getattr(FLAGS, flag_name)
      if flag_value == skip_value:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported when FLAGS.{} is {}'.format(
                test_name, flag_name, flag_value))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def format_test_name_suffix(opname, shapes, dtypes):
  arg_descriptions = (format_shape_dtype_string(shape, dtype)
                      for shape, dtype in zip(shapes, dtypes))
  return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))


class _NumpyScalar(object):

  def __len__(self):
    return 0

# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
# identical to 0-D arrays, and we want to write tests that exercise both paths.
NUMPY_SCALAR_SHAPE = _NumpyScalar()


def _dims_of_shape(shape):
  """"""Converts `shape` to a tuple of dimensions.""""""
  return shape if shape != NUMPY_SCALAR_SHAPE else ()


def _cast_to_shape(value, shape, dtype):
  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
  if shape != NUMPY_SCALAR_SHAPE:
    return value
  else:
    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
    # scalar.
    return dtype(value)


def format_shape_dtype_string(shape, dtype):
  typestr = onp.dtype(dtype).name
  if shape == NUMPY_SCALAR_SHAPE:
    return typestr

  if onp.isscalar(shape):
    shapestr = str(shape) + ','
  else:
    shapestr = ','.join(str(dim) for dim in shape)
  return '{}[{}]'.format(typestr, shapestr)


def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
  """"""Produce random values given shape, dtype, scale, and post-processor.

  Args:
    rand: a function for producing random values of a given shape, e.g. a
      bound version of either onp.RandomState.randn or onp.RandomState.rand.
    shape: a shape value as a tuple of positive integers.
    dtype: a numpy dtype.
    scale: optional, a multiplicative scale for the random values (default 1).
    post: optional, a callable for post-processing the random values (default
      identity).

  Returns:
    An ndarray of the given shape and dtype using random values based on a call
    to rand but scaled, converted to the appropriate dtype, and post-processed.
  """"""
  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    vals = r() + 1.0j * r()
  else:
    vals = r()
  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)


def rand_default():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3)


def rand_nonzero():
  post = lambda x: onp.where(x == 0, 1, x)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3, post=post)


def rand_positive():
  post = lambda x: x + 1
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2, post=post)


def rand_small():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=1e-3)


def rand_not_small():
  post = lambda x: x + onp.where(x > 0, 10., -10.)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3., post=post)


def rand_small_positive():
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2e-5)


def rand_some_equal():
  randn = npr.RandomState(0).randn
  rng = npr.RandomState(0)

  def post(x):
    flips = rng.rand(*onp.shape(x)) < 0.5
    return onp.where(flips, x.ravel()[0], x)

  return partial(_rand_dtype, randn, scale=100., post=post)


# TODO(mattjj): doesn't handle complex types
def rand_some_inf():
  """"""Return a random sampler that produces infinities in floating types.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    if not onp.issubdtype(dtype, onp.float):
      # only float types have inf
      return base_rand(shape, dtype)

    dims = _dims_of_shape(shape)
    posinf_flips = rng.rand(*dims) < 0.1
    neginf_flips = rng.rand(*dims) < 0.1

    vals = base_rand(shape, dtype)
    vals = onp.where(posinf_flips, onp.inf, vals)
    vals = onp.where(neginf_flips, -onp.inf, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


# TODO(mattjj): doesn't handle complex types
def rand_some_zero():
  """"""Return a random sampler that produces some zeros.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    dims = _dims_of_shape(shape)
    zeros = rng.rand(*dims) < 0.5

    vals = base_rand(shape, dtype)
    vals = onp.where(zeros, 0, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


def rand_bool():
  rng = npr.RandomState(0)
  def generator(shape, dtype):
    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
  return generator

def check_raises(thunk, err_type, msg):
  try:
    thunk()
    assert False
  except err_type as e:
    assert str(e).startswith(msg), ""\n{}\n\n{}\n"".format(e, msg)

def check_raises_regexp(thunk, err_type, pattern):
  try:
    thunk()
    assert False
  except err_type as e:
    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)

random.seed(0) # TODO: consider managing prng state more carefully

def cases_from_list(xs):
  xs = list(xs)
  k = min(len(xs), FLAGS.num_generated_cases)
  return random.sample(xs, k)

def cases_from_gens(*gens):
  sizes = [1, 3, 10]
  cases_per_size = int(FLAGS.num_generated_cases / len(sizes)) + 1
  for size in sizes:
    for i in xrange(cases_per_size):
      yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)


class JaxTestCase(parameterized.TestCase):
  """"""Base class for JAX tests including numerical checks and boilerplate.""""""

  def assertArraysAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y are close (up to numerical tolerances).""""""
    dtype = lambda x: str(onp.asarray(x).dtype)
    tol = 1e-2 if str(onp.dtype(onp.float32)) in {dtype(x), dtype(y)} else 1e-5
    atol = atol or tol
    rtol = rtol or tol

    if FLAGS.jax_test_dut == 'tpu':
      atol = max(atol, 0.5)
      rtol = max(rtol, 1e-1)

    if not onp.allclose(x, y, atol=atol, rtol=rtol, equal_nan=True):
      msg = ('Arguments x and y not equal to tolerance atol={}, rtol={}:\n'
             'x:\n{}\n'
             'y:\n{}\n').format(atol, rtol, x, y)
      raise self.failureException(msg)

    if check_dtypes:
      self.assertDtypesMatch(x, y)

  def assertDtypesMatch(self, x, y):
    if FLAGS.jax_enable_x64:
      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)

  def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""
    if isinstance(x, (tuple, list)):
      self.assertIsInstance(y, (tuple, list))
      self.assertEqual(len(x), len(y))
      for x_elt, y_elt in zip(x, y):
        self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
    else:
      is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
      self.assertTrue(is_array(x))
      self.assertTrue(is_array(y))
      x = onp.asarray(x)
      y = onp.asarray(y)
      self.assertArraysAllClose(x, y, check_dtypes, atol=atol, rtol=rtol)

  def _CompileAndCheck(self, fun, args_maker, check_dtypes,
                       rtol=None, atol=None):
    """"""Helper method for running JAX compilation and allclose assertions.""""""
    args = args_maker()

    def wrapped_fun(*args):
      self.assertTrue(python_should_be_executing)
      return fun(*args)

    python_should_be_executing = True
    python_ans = fun(*args)

    cfun = api.jit(wrapped_fun)
    python_should_be_executing = True
    monitored_ans = cfun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, monitored_ans, check_dtypes, rtol, atol)
    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

    args = args_maker()

    python_should_be_executing = True
    python_ans = fun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

  def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                         check_dtypes=False, tol=1e-5):
    args = args_maker()
    lax_ans = lax_op(*args)
    numpy_ans = numpy_reference_op(*args)
    self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                        atol=tol, rtol=tol)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import re
import itertools as it
import random

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from six.moves import xrange

from . import api
from .config import flags
from .util import partial
from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce

FLAGS = flags.FLAGS
flags.DEFINE_enum(
    'jax_test_dut',
    None,
    enum_values=['cpu', 'gpu', 'tpu'],
    help=
    'Describes the device under test in case special consideration is required.'
)

flags.DEFINE_integer(
  'num_generated_cases',
  100,
  help='Number of generated cases to test')

EPS = 1e-4
ATOL = 1e-4
RTOL = 1e-4

_dtype = lambda x: getattr(x, 'dtype', None) or onp.asarray(x).dtype


def numpy_eq(x, y):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    return onp.allclose(x, y, 1e-3, 1e-3)
  else:
    return onp.allclose(x, y)


def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    atol = max(atol, 1e-1)
    rtol = max(rtol, 1e-1)
  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)


def check_eq(xs, ys):
  assert tree_all(tree_multimap(numpy_eq, xs, ys)), \
      '\n{} != \n{}'.format(xs, ys)


def check_close(xs, ys, atol=ATOL, rtol=RTOL):
  close = partial(numpy_close, atol=atol, rtol=rtol)
  assert tree_all(tree_multimap(close, xs, ys)), '\n{} != \n{}'.format(xs, ys)


def inner_prod(xs, ys):
  contract = lambda x, y: onp.real(onp.vdot(x, y))
  return tree_reduce(onp.add, tree_multimap(contract, xs, ys))


add = partial(tree_multimap, onp.add)
sub = partial(tree_multimap, onp.subtract)
conj = partial(tree_map, onp.conj)


def scalar_mul(xs, a):
  return tree_map(lambda x: onp.multiply(x, a, dtype=_dtype(x)), xs)


def rand_like(rng, x):
  shape = onp.shape(x)
  dtype = _dtype(x)
  randn = lambda: onp.asarray(rng.randn(*shape), dtype=dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    return randn() + 1.0j * randn()
  else:
    return randn()


def numerical_jvp(f, primals, tangents, eps=EPS):
  delta = scalar_mul(tangents, EPS)
  f_pos = f(*add(primals, delta))
  f_neg = f(*sub(primals, delta))
  return scalar_mul(sub(f_pos, f_neg), 0.5 / EPS)


def check_jvp(f, f_jvp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  rng = onp.random.RandomState(0)
  tangent = tree_map(partial(rand_like, rng), args)
  v_out, t_out = f_jvp(args, tangent)
  v_out_expected = f(*args)
  t_out_expected = numerical_jvp(f, args, tangent, eps=eps)
  check_eq(v_out, v_out_expected)
  check_close(t_out, t_out_expected, atol=atol, rtol=rtol)


def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  _rand_like = partial(rand_like, onp.random.RandomState(0))
  v_out, vjpfun = f_vjp(*args)
  v_out_expected = f(*args)
  check_eq(v_out, v_out_expected)
  tangent = tree_map(_rand_like, args)
  tangent_out = numerical_jvp(f, args, tangent, eps=EPS)
  cotangent = tree_map(_rand_like, v_out)
  cotangent_out = conj(vjpfun(conj(cotangent)))
  ip = inner_prod(tangent, cotangent_out)
  ip_expected = inner_prod(tangent_out, cotangent)
  check_close(ip, ip_expected, atol=atol, rtol=rtol)


def skip_on_devices(*disabled_devices):
  """"""A decorator for test methods to skip the test on certain devices.""""""
  def skip(test_method):
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      device = FLAGS.jax_test_dut
      if device in disabled_devices:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported on {}.'.format(test_name, device.upper()))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def skip_on_flag(flag_name, skip_value):
  """"""A decorator for test methods to skip the test when flags are set.""""""
  def skip(test_method):        # pylint: disable=missing-docstring
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      flag_value = getattr(FLAGS, flag_name)
      if flag_value == skip_value:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported when FLAGS.{} is {}'.format(
                test_name, flag_name, flag_value))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def format_test_name_suffix(opname, shapes, dtypes):
  arg_descriptions = (format_shape_dtype_string(shape, dtype)
                      for shape, dtype in zip(shapes, dtypes))
  return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))


class _NumpyScalar(object):

  def __len__(self):
    return 0

# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
# identical to 0-D arrays, and we want to write tests that exercise both paths.
NUMPY_SCALAR_SHAPE = _NumpyScalar()


def _dims_of_shape(shape):
  """"""Converts `shape` to a tuple of dimensions.""""""
  return shape if shape != NUMPY_SCALAR_SHAPE else ()


def _cast_to_shape(value, shape, dtype):
  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
  if shape != NUMPY_SCALAR_SHAPE:
    return value
  else:
    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
    # scalar.
    return dtype(value)


def format_shape_dtype_string(shape, dtype):
  typestr = onp.dtype(dtype).name
  if shape == NUMPY_SCALAR_SHAPE:
    return typestr

  if onp.isscalar(shape):
    shapestr = str(shape) + ','
  else:
    shapestr = ','.join(str(dim) for dim in shape)
  return '{}[{}]'.format(typestr, shapestr)


def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
  """"""Produce random values given shape, dtype, scale, and post-processor.

  Args:
    rand: a function for producing random values of a given shape, e.g. a
      bound version of either onp.RandomState.randn or onp.RandomState.rand.
    shape: a shape value as a tuple of positive integers.
    dtype: a numpy dtype.
    scale: optional, a multiplicative scale for the random values (default 1).
    post: optional, a callable for post-processing the random values (default
      identity).

  Returns:
    An ndarray of the given shape and dtype using random values based on a call
    to rand but scaled, converted to the appropriate dtype, and post-processed.
  """"""
  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    vals = r() + 1.0j * r()
  else:
    vals = r()
  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)


def rand_default():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3)


def rand_nonzero():
  post = lambda x: onp.where(x == 0, 1, x)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3, post=post)


def rand_positive():
  post = lambda x: x + 1
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2, post=post)


def rand_small():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=1e-3)


def rand_not_small():
  post = lambda x: x + onp.where(x > 0, 10., -10.)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3., post=post)


def rand_small_positive():
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2e-5)


def rand_some_equal():
  randn = npr.RandomState(0).randn
  rng = npr.RandomState(0)

  def post(x):
    flips = rng.rand(*onp.shape(x)) < 0.5
    return onp.where(flips, x.ravel()[0], x)

  return partial(_rand_dtype, randn, scale=100., post=post)


# TODO(mattjj): doesn't handle complex types
def rand_some_inf():
  """"""Return a random sampler that produces infinities in floating types.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    if not onp.issubdtype(dtype, onp.float):
      # only float types have inf
      return base_rand(shape, dtype)

    dims = _dims_of_shape(shape)
    posinf_flips = rng.rand(*dims) < 0.1
    neginf_flips = rng.rand(*dims) < 0.1

    vals = base_rand(shape, dtype)
    vals = onp.where(posinf_flips, onp.inf, vals)
    vals = onp.where(neginf_flips, -onp.inf, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


# TODO(mattjj): doesn't handle complex types
def rand_some_zero():
  """"""Return a random sampler that produces some zeros.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    dims = _dims_of_shape(shape)
    zeros = rng.rand(*dims) < 0.5

    vals = base_rand(shape, dtype)
    vals = onp.where(zeros, 0, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


def rand_bool():
  rng = npr.RandomState(0)
  def generator(shape, dtype):
    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
  return generator

def check_raises(thunk, err_type, msg):
  try:
    thunk()
    assert False
  except err_type as e:
    assert str(e).startswith(msg), ""\n{}\n\n{}\n"".format(e, msg)

def check_raises_regexp(thunk, err_type, pattern):
  try:
    thunk()
    assert False
  except err_type as e:
    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)

random.seed(0) # TODO: consider managing prng state more carefully

def cases_from_list(xs):
  xs = list(xs)
  k = min(len(xs), FLAGS.num_generated_cases)
  return random.sample(xs, k)

def cases_from_gens(*gens):
  sizes = [1, 3, 10]
  cases_per_size = int(FLAGS.num_generated_cases / len(sizes)) + 1
  for size in sizes:
    for i in xrange(cases_per_size):
      yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)


class JaxTestCase(parameterized.TestCase):
  """"""Base class for JAX tests including numerical checks and boilerplate.""""""

  def assertArraysAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y are close (up to numerical tolerances).""""""
    dtype = lambda x: str(onp.asarray(x).dtype)
    tol = 1e-2 if str(onp.dtype(onp.float32)) in {dtype(x), dtype(y)} else 1e-5
    atol = atol or tol
    rtol = rtol or tol

    if FLAGS.jax_test_dut == 'tpu':
      atol = max(atol, 0.5)
      rtol = max(rtol, 1e-1)

    if not onp.allclose(x, y, atol=atol, rtol=rtol, equal_nan=True):
      msg = ('Arguments x and y not equal to tolerance atol={}, rtol={}:\n'
             'x:\n{}\n'
             'y:\n{}\n').format(atol, rtol, x, y)
      raise self.failureException(msg)

    if check_dtypes:
      self.assertDtypesMatch(x, y)

  def assertDtypesMatch(self, x, y):
    if FLAGS.jax_enable_x64:
      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)

  def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""
    if isinstance(x, (tuple, list)):
      self.assertIsInstance(y, (tuple, list))
      self.assertEqual(len(x), len(y))
      for x_elt, y_elt in zip(x, y):
        self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
    else:
      is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
      self.assertTrue(is_array(x))
      self.assertTrue(is_array(y))
      x = onp.asarray(x)
      y = onp.asarray(y)
      self.assertArraysAllClose(x, y, check_dtypes, atol=atol, rtol=rtol)

  def _CompileAndCheck(self, fun, args_maker, check_dtypes,
                       rtol=None, atol=None):
    """"""Helper method for running JAX compilation and allclose assertions.""""""
    args = args_maker()

    def wrapped_fun(*args):
      self.assertTrue(python_should_be_executing)
      return fun(*args)

    python_should_be_executing = True
    python_ans = fun(*args)

    cfun = api.jit(wrapped_fun)
    python_should_be_executing = True
    monitored_ans = cfun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, monitored_ans, check_dtypes, rtol, atol)
    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

    args = args_maker()

    python_should_be_executing = True
    python_ans = fun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

  def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                         check_dtypes=False, tol=1e-5):
    args = args_maker()
    lax_ans = lax_op(*args)
    numpy_ans = numpy_reference_op(*args)
    self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                        atol=tol, rtol=tol)
","diff --git a/jax/test_util.py b/jax/test_util.py
index 21525c462c2ce129edc41d80ade975c4d03bd6dc..a605bb781d3f8604741dfe7204ef2064b0289c3b 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -27,6 +27,8 @@ from absl.testing import parameterized
 import numpy as onp
 import numpy.random as npr
 
+from six.moves import xrange
+
 from . import api
 from .config import flags
 from .util import partial
",Permission / authorization bug,Add import of xrange from six.moves for Python compatibility.
16658b97c4ff676e67560d57040122e4e86f0614,Matthew Johnson: fix typo in wheel path,README.md,"# JAX: Autograd and XLA

![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)

JAX is [Autograd](https://github.com/hips/autograd) and
[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
brought together for high-performance machine learning research.

With its updated version of Autograd, JAX can automatically differentiate native
Python and NumPy functions. It can differentiate through loops, branches,
recursion, and closures, and it can take derivatives of derivatives of
derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)
as well as forward-mode differentiation, and the two can be composed arbitrarily
to any order.

What’s new is that JAX uses
[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
to compile and run your NumPy programs on GPUs and TPUs. Compilation happens
under the hood by default, with library calls getting just-in-time compiled and
executed. But JAX also lets you just-in-time compile your own Python functions
into XLA-optimized kernels using a one-function API,
[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be
composed arbitrarily, so you can express sophisticated algorithms and get
maximal performance without leaving Python.

This is a research project, not an official Google product. Expect bugs and
sharp edges. Please help by trying it out, [reporting
bugs](https://github.com/google/jax/issues), and letting us know what you
think!

```python
import jax.numpy as np
from jax import grad, jit, vmap
from functools import partial

def predict(params, inputs):
  for W, b in params:
    outputs = np.dot(inputs, W) + b
    inputs = np.tanh(outputs)
  return outputs

def logprob_fun(params, inputs, targets):
  preds = predict(params, inputs)
  return np.sum((preds - targets)**2)

grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function
perex_grads = jit(lambda params, inputs, targets:  # fast per-example gradients
                  vmap(partial(grad_fun, params), inputs, targets))
```

JAX started as a research project by [Matt Johnson](https://github.com/mattjj),
[Roy Frostig](https://github.com/froystig), [Dougal
Maclaurin](https://github.com/dougalm), and [Chris
Leary](https://github.com/learyg), and is now developed [in the
open](https://github.com/google/jax) by a growing number of
[contributors](#contributors).

## Quickstart: Colab in the Cloud
Jump right in using [a notebook in your
browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
connected to a Google Cloud GPU.

## Installation
JAX is written in pure Python, but it depends on XLA, which needs to be
compiled and installed as the `jaxlib` package. Use the following instructions
to [build XLA from source](#building-jax-from-source) or [install a binary
package with pip](#pip-installation).

### Building JAX from source
First, obtain the JAX source code:

```bash
git clone https://github.com/google/jax
cd jax
```

To build XLA with CUDA support, you can run

```bash
python build/build.py --enable_cuda
pip install -e build  # install jaxlib
pip install -e .      # install jax (pure Python)
```

See `python build/build.py --help` for configuration options, including ways to
specify the paths to CUDA and CUDNN, which you must have installed. The build
also depends on NumPy, and a compiler toolchain corresponding to that of
Ubuntu 16.04 or newer.

To build XLA without CUDA GPU support (CPU only), drop the `--enable_cuda`:

```bash
python build/build.py
pip install -e build  # install jaxlib
pip install -e .      # install jax
```

To upgrade to the latest version from GitHub, just run `git pull` from the JAX
repository root, and rebuild by running `build.py` if necessary. You shouldn't have
to reinstall because `pip install -e` sets up symbolic links from site-packages
into the repository.

### pip installation

Installing XLA with prebuilt binaries via `pip` is still experimental,
especially with GPU support. Let us know on [the issue
tracker](https://github.com/google/jax/issues) if you run into any errors.

To install a CPU-only version, which might be useful for doing local
development on a laptop, you can run

```bash
pip install jax jaxlib
```

If you want to install JAX with both CPU and GPU support, using existing CUDA
and CUDNN7 installations on your machine (for example, preinstalled on your
cloud VM), you can run

```bash
# install jaxlib
PYTHON_VERSION=py2  # alternatives: py2, py3
CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl

pip install jax  # install jax
```

The library package name must correspond to the version of the existing CUDA
installation you want to use, with `cuda100` for CUDA 10.0, `cuda92` for CUDA
9.2, and `cuda90` for CUDA 9.0. To find your CUDA and CUDNN versions, you can
run command like these, depending on your CUDNN install path:

```bash
nvcc --version
grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
```

## A brief tour

```python
In [1]: import jax.numpy as np

In [2]: from jax import random

In [3]: key = random.PRNGKey(0)

In [4]: x = random.normal(key, (5000, 5000))

In [5]: print(np.dot(x, x.T) / 2)  # fast!
[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...

In [6]: print(np.dot(x, x.T) / 2)  # even faster!
[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...
```

What’s happening behind-the-scenes is that JAX is using XLA to just-in-time
(JIT) compile and execute these individual operations on the GPU. First the
`random.normal` call is compiled and the array referred to by `x` is generated
on the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and
`divide`) is individually JIT-compiled and executed, each keeping its results on
the device.
It’s only when a value needs to be printed, plotted, saved, or passed into a raw
NumPy function that a read-only copy of the value is brought back to the host as
an ndarray and cached. The second call to `dot` is faster because the
JIT-compiled code is cached and reused, saving the compilation time.

The fun really starts when you use `grad` for automatic differentiation and
`jit` to compile your own functions end-to-end. Here’s a more complete toy
example:

```python
from jax import grad, jit
import jax.numpy as np

def sigmoid(x):
    return 0.5 * (np.tanh(x / 2.) + 1)

# Outputs probability of a label being true according to logistic model.
def logistic_predictions(weights, inputs):
    return sigmoid(np.dot(inputs, weights))

# Training loss is the negative log-likelihood of the training labels.
def loss(weights, inputs, targets):
    preds = logistic_predictions(weights, inputs)
    label_probs = preds * targets + (1 - preds) * (1 - targets)
    return -np.sum(np.log(label_probs))

# Build a toy dataset.
inputs = np.array([[0.52, 1.12,  0.77],
                   [0.88, -1.08, 0.15],
                   [0.52, 0.06, -1.30],
                   [0.74, -2.49, 1.39]])
targets = np.array([True, True, False, True])

# Define a compiled function that returns gradients of the training loss
training_gradient_fun = jit(grad(loss))

# Optimize weights using gradient descent.
weights = np.array([0.0, 0.0, 0.0])
print(""Initial loss: {:0.2f}"".format(loss(weights, inputs, targets)))
for i in range(100):
    weights -= 0.1 * training_gradient_fun(weights, inputs, targets)

print(""Trained loss: {:0.2f}"".format(loss(weights, inputs, targets)))
```

To see more, check out the [quickstart
notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb),
a [simple MNIST classifier
example](https://github.com/google/jax/blob/master/examples/mnist_classifier.py)
and the rest of the [JAX
examples](https://github.com/google/jax/blob/master/examples/).

## What's supported

If you’re using JAX just as an accelerator-backed NumPy, without using `grad` or
`jit` in your code, then in principle there are no constraints, though some
NumPy functions haven’t been implemented yet. Generally using `np.dot(A, B)` is
better than `A.dot(B)` because the former gives us more opportunities to run the
computation on the device. NumPy also does a lot of work to cast any array-like
function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
typically requires explicit casting of array arguments, like
`np.sum(np.array([x, y]))`.

For automatic differentiation with `grad`, JAX has the same restrictions
as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
dicts freely: jax doesn't even see them. Using `np.dot(A, B)` rather than
`A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.

For compiling your own functions with `jit` there are a few more requirements.
Because `jit` aims to specialize Python functions only on shapes and dtypes
during tracing, rather than on concrete values, Python control flow that depends
on concrete values won’t be able to execute and will instead raise an error. If
you want compiled control flow, use structured control flow primitives like
lax.cond and lax.while. Some indexing features, like slice-based indexing
`A[i:i+5]` for argument-dependent `i`, or boolean-based indexing `A[bool_ind]`
for argument-dependent `bool_ind`, produce abstract values of unknown shape and
are thus unsupported in `jit` functions.

In general, JAX is intended to be used with a functional style of Python
programming. Functions passed to transformations like `grad` and `jit` are
expected to be free of side-effects. You can write print statements for
debugging but they may only be executed once if they're under a `jit` decorator.

> TLDR **Do use**
>
> *   Functional programming
> *   [Many](https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py) of NumPy’s
>     functions (help us add more!)
> *   [Some](https://github.com/google/jax/tree/master/jax/scipy) SciPy functions
> *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`
> *   Explicit array creation from lists like `A = np.array([x, y])`
>
> **Don’t use**
>
> *   Assignment into arrays like `A[0, 0] = x`
> *   Implicit casting to arrays like `np.sum([x, y])` (use `np.sum(np.array([x,
>     y])` instead)
> *   `A.dot(B)` method syntax for functions of more than one argument (use
>     `np.dot(A, B)` instead)
> *   Side-effects like mutation of arguments or mutation of global variables
> *   The `out` argument of NumPy functions
>
> **For jit functions, also don’t use**
>
> *   Control flow based on dynamic values `if x > 0: ...`. Control flow based
>     on shapes is fine: `if x.shape[0] > 2: ...` and `for subarr in array`.
> *   Slicing `A[i:i+5]` for dynamic index `i` (use `lax.dynamic_slice` instead)
>     or boolean indexing `A[bool_ind]` for traced values `bool_ind`.

You should get loud errors if your code violates any of these.

## Transformations

At its core, JAX is an extensible system for transforming numerical functions.
We currently expose three important transformations: `grad`, `jit`, and `vmap`.

### Automatic differentiation with grad

JAX has roughly the same API as [Autograd](https://github.com/hips/autograd).
The most popular function is `grad` for reverse-mode gradients:

```python
from jax import grad
import jax.numpy as np

def tanh(x):  # Define a function
  y = np.exp(-2.0 * x)
  return (1.0 - y) / (1.0 + y)

grad_tanh = grad(tanh)  # Obtain its gradient function
print(grad_tanh(1.0))   # Evaluate it at x = 1.0
# prints 0.41997434161402603
```

You can differentiate to any order with `grad`.

For more advanced autodiff, you can use `jax.vjp` for reverse-mode
vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector
products. The two can be composed arbitrarily with one another, and with other
JAX transformations. Here's one way to compose
those to make a function that efficiently computes full Hessian matrices:

```python
from jax import jit, jacfwd, jacrev
def hessian(fun):
  return jit(jacfwd(jacrev(fun)))
```

As with Autograd, you're free to use differentiation with Python control
structures:

```python
def abs_val(x):
  if x > 0:
    return x
  else:
    return -x

abs_val_grad = grad(abs_val)
print(abs_val_grad)(1.0)   # prints 1.0
print(abs_val_grad)(-1.0)  # prints -1.0 (abs_val is re-evaluated)
```

### Compilation with jit

You can use XLA to compile your functions end-to-end with `jit`, used either as
an `@jit` decorator or as a higher-order function.

```python
import jax.numpy as np
from jax import jit

def slow_f(x):
  # Element-wise ops see a large benefit from fusion
  return x * x + x * 2.0

x = np.ones((5000, 5000))
fast_f = jit(slow_f)
%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X
%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)
```

You can mix `jit` and `grad` and any other JAX transformation however you like.

### Auto-vectorization with vmap

`vmap` is the vectorizing map.
It has the familiar semantics of mapping a function along array axes, but
instead of keeping the loop on the outside, it pushes the loop down into a
function’s primitive operations for better performance.

Using `vmap` can save you from having to carry around batch dimensions in your
code. For example, consider this simple *unbatched* neural network prediction
function:

```python
def predict(params, input_vec):
  assert input_vec.ndim == 1
  for W, b in params:
    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!
    input_vec = np.tanh(output_vec)
  return output_vec
```

We often instead write `np.dot(inputs, W)` to allow for a batch dimension on the
left side of `inputs`, but we’ve written this particular prediction function to
apply only to single input vectors. If we wanted to apply this function to a
batch of inputs at once, semantically we could just write

```python
from functools import partial
predictions = np.stack(list(map(partial(predict, params), input_batch)))
```

But pushing one example through the network at a time would be slow! It’s better
to vectorize the computation, so that at every layer we’re doing matrix-matrix
multiplies rather than matrix-vector multiplies.

The `vmap` function does that transformation for us. That is, if we write

```python
from jax import vmap
predictions = vmap(partial(predict, params), input_batch)
```

then the `vmap` function will push the outer loop inside the function, and our
machine will end up executing matrix-matrix multiplications exactly as if we’d
done the batching by hand.

It’s easy enough to manually batch a simple neural network without `vmap`, but
in other cases manual vectorization can be impractical or impossible. Take the
problem of efficiently computing per-example gradients: that is, for a fixed set
of parameters, we want to compute the gradient of our loss function evaluated
separately at each example in a batch. With `vmap`, it’s easy:

```python
per_example_gradients = vmap(partial(grad(loss), params), inputs, targets)
```

Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other
JAX transformation! We use `vmap` with both forward- and reverse-mode automatic
differentiation for fast Jacobian and Hessian matrix calculations in
`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.


## Random numbers are different

JAX needs a functional pseudo-random number generator (PRNG) system to provide
reproducible results invariant to compilation boundaries and backends, while
also maximizing performance by enabling vectorized generation and
parallelization across random calls. The `numpy.random` library doesn’t have
those properties. The `jax.random` library meets those needs: it’s functionally
pure, but it doesn’t require you to pass stateful random objects back out of
every function.

The `jax.random` library uses
[count-based PRNGs](http://www.thesalmons.org/john/random123/papers/random123sc11.pdf)
and a functional array-oriented
[splitting model](http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf).
To generate random values, you call a function like `jax.random.normal` and give
it a PRNG key:

```python
import jax.random as random

key = random.PRNGKey(0)
print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
```

If we make the same call again with the same key, we get the same values:

```python
print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
```

The key never gets updated. So how do we get fresh random values? We use
`jax.random.split` to create new keys from existing ones. A common pattern is to
split off a new key for every function call that needs random values:

```python
key = random.PRNGKey(0)

key, subkey = random.split(key)
print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]

key, subkey = random.split(key)
print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]
```

By splitting the PRNG key, not only do we avoid having to thread random states
back out of every function call, but also we can generate multiple random arrays
in parallel because we can avoid unnecessary sequential dependencies.

There's a gotcha here, which is that it's easy to unintentionally reuse a key
without splitting. We intend to add a check for this (a sort of dynamic linear
typing) but for now it's something to be careful about.


## Mini-libraries

JAX provides some small, experimental libraries for machine learning. These
libraries are in part about providing tools and in part about serving as
examples for how to build such libraries using JAX. Each one is only a few
hundred lines of code, so take a look inside and adapt them as you need!

### Neural-net building with Stax

**Stax** is a functional neural network building library. The basic idea is that
a single layer or an entire network can be modeled as an `(init_fun, apply_fun)`
pair. The `init_fun` is used to initialize network parameters and the
`apply_fun` takes parameters and inputs to produce outputs. There are
constructor functions for common basic pairs, like `Conv` and `Relu`, and these
pairs can be composed in series using `stax.serial` or in parallel using
`stax.parallel`.

Here’s an example:

```python
from jax.experimental import stax
from jax.experimental.stax import Conv
from jax.experimental.stax import Dense
from jax.experimental.stax import MaxPool
from jax.experimental.stax import Relu
from jax.experimental.stax import LogSoftmax

# Set up network initialization and evaluation functions
net_init, net_apply = stax.serial(
    Conv(32, (3, 3), padding='SAME'), Relu,
    Conv(64, (3, 3), padding='SAME'), Relu
    MaxPool((2, 2)), Flatten,
    Dense(128), Relu,
    Dense(10), SoftMax,
)

# Initialize parameters, not committing to a batch shape
in_shape = (-1, 28 * 28)
out_shape, net_params = net_init(in_shape)

# Apply network
predictions = net_apply(net_params, inputs)
```

### First-order optimization with Minmax

**Minmax** is an optimization library focused on stochastic first-order
optimizers. Every optimizer is modeled as an `(init_fun, update_fun)` pair. The
`init_fun` is used to initialize the optimizer state, which could include things
like momentum variables, and the `update_fun` accepts a gradient and an
optimizer state to produce a new optimizer state. The parameters being optimized
can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can
store your parameters however you’d like.

Here’s an example, using `jit` to compile the whole update end-to-end:

```python
from jax.experimental import minmax
from jax import jit

# Set up an optimizer
opt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)

# Define a compiled update step
@jit
def step(i, opt_state, batch):
  params = minmax.get_params(opt_state)
  g = grad(loss)(params, batch)
  return opt_update(i, g, opt_state)

# Optimize parameters in a loop
opt_state = opt_init(net_params)
for i in range(num_steps):
  opt_state = step(i, opt_state, next(data_generator))
net_params = minmax.get_params(opt_state)
```

## How it works

Programming in machine learning is about expressing and transforming functions.
Transformations include automatic differentiation, compilation for accelerators,
and automatic batching. High-level languages like Python are great for
expressing functions, but usually all we can do with them is apply them. We lose
access to their internal structure which would let us perform transformations.

JAX is a tool for specializing and translating high-level Python+NumPy functions
into a representation that can be transformed and then lifted back into a Python
function.

![simplified-lifecycle](https://raw.githubusercontent.com/google/jax/master/images/lifecycle.png)

JAX specializes Python functions by tracing. Tracing a function means monitoring
all the basic operations that are applied to its input to produce its output,
and recording these operations and the data-flow between them in a directed
acyclic graph (DAG). To perform tracing, JAX wraps primitive operations, like
basic numerical kernels, so that when they’re called they add themselves to a
list of operations performed along with their inputs and outputs. To keep track
of how data flows between these primitives, values being tracked are wrapped in
instances of the `Tracer` class.

When a Python function is provided to `grad` or `jit`, it’s wrapped for tracing
and returned. When the wrapped function is called, we abstract the concrete
arguments provided into instances of the `AbstractValue` class, box them for
tracing in instances of the `Tracer` class, and call the function on them.
Abstract arguments represent sets of possible values rather than specific
values: for example, `jit` abstracts ndarray arguments to abstract values that
represent all ndarrays with the same shape and dtype. In contrast, `grad`
abstracts ndarray arguments to represent an infinitesimal neighborhood of the
underlying
value. By tracing the Python function on these abstract values, we ensure that
it’s specialized enough so that it’s tractable to transform, and that it’s still
general enough so that the transformed result is useful, and possibly reusable.
These transformed functions are then lifted back into Python callables in a way
that allows them to be traced and transformed again as needed.

The primitive functions that JAX traces are mostly in 1:1 correspondence with
[XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined
in [lax.py](https://github.com/google/jax/blob/master/jax/lax.py). This 1:1
correspondence makes most of the translations to XLA essentially trivial, and
ensures we only have a small set of primitives to cover for other
transformations like automatic differentiation. The [`jax.numpy`
layer](https://github.com/google/jax/blob/master/jax/numpy/) is written in pure
Python simply by expressing NumPy functions in terms of the LAX functions (and
other NumPy functions we’ve already written). That makes `jax.numpy` easy to
extend.

When you use `jax.numpy`, the underlying LAX primitives are `jit`-compiled
behind the scenes, allowing you to write unrestricted Python+Numpy code while
still executing each primitive operation on an accelerator.

But JAX can do more: instead of just compiling and dispatching to a fixed set of
individual primitives, you can use `jit` on larger and larger functions to be
end-to-end compiled and optimized. For example, instead of just compiling and
dispatching a convolution op, you can compile a whole network, or a whole
gradient evaluation and optimizer update step.

The tradeoff is that `jit` functions have to satisfy some additional
specialization requirements: since we want to compile traces that are
specialized on shapes and dtypes, but not specialized all the way to concrete
values, the Python code under a `jit` decorator must be applicable to abstract
values. If we try to evaluate `x > 0` on an abstract `x`, the result is an
abstract value representing the set `{True, False}`, and so a Python branch like
`if x > 0` will raise an error: it doesn’t know which way to go! 
See [What’s supported](#whats-supported) for more
information about `jit` requirements.

The good news about this tradeoff is that `jit` is opt-in: JAX libraries use
`jit` on individual operations and functions behind the scenes, allowing you to
write unrestricted Python+Numpy and still make use of a hardware accelerator.
But when you want to maximize performance, you can often use `jit` in your own
code to compile and end-to-end optimize much bigger functions.

## What we're working on
1. Documentation!
2. Cloud TPU support
3. Multi-GPU and multi-TPU support
4. Full NumPy coverage and some SciPy coverage
5. Full coverage for vmap
6. Make everything faster
    * Lowering the XLA function dispatch overhead
    * Linear algebra routines (MKL on CPU, MAGMA on GPU)
7. `cond` and `while` primitives with efficient automatic differentiation

## Current gotchas

Some things we don't handle that might surprise NumPy users:
1. No in-place mutation syntax. Functional code. Can use lax.dynamic\_update\_slice.
2. PRNG can be awkward, and linearity is not checked with a warning.

## Contributors

So far, JAX includes lots of help and contributions from [Peter
Hawkins](https://github.com/hawkinsp), [Alex
Wiltschko](http://github.com/alexbw), George Dahl, [Eli
Bendersky](https://github.com/eliben), Zak Stone, [Alexey
Radul](https://github.com/axch), Michael Isard, Skye Wanderman-Milne, and many
others.
","# JAX: Autograd and XLA

![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)

JAX is [Autograd](https://github.com/hips/autograd) and
[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
brought together for high-performance machine learning research.

With its updated version of Autograd, JAX can automatically differentiate native
Python and NumPy functions. It can differentiate through loops, branches,
recursion, and closures, and it can take derivatives of derivatives of
derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)
as well as forward-mode differentiation, and the two can be composed arbitrarily
to any order.

What’s new is that JAX uses
[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
to compile and run your NumPy programs on GPUs and TPUs. Compilation happens
under the hood by default, with library calls getting just-in-time compiled and
executed. But JAX also lets you just-in-time compile your own Python functions
into XLA-optimized kernels using a one-function API,
[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be
composed arbitrarily, so you can express sophisticated algorithms and get
maximal performance without leaving Python.

This is a research project, not an official Google product. Expect bugs and
sharp edges. Please help by trying it out, [reporting
bugs](https://github.com/google/jax/issues), and letting us know what you
think!

```python
import jax.numpy as np
from jax import grad, jit, vmap
from functools import partial

def predict(params, inputs):
  for W, b in params:
    outputs = np.dot(inputs, W) + b
    inputs = np.tanh(outputs)
  return outputs

def logprob_fun(params, inputs, targets):
  preds = predict(params, inputs)
  return np.sum((preds - targets)**2)

grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function
perex_grads = jit(lambda params, inputs, targets:  # fast per-example gradients
                  vmap(partial(grad_fun, params), inputs, targets))
```

JAX started as a research project by [Matt Johnson](https://github.com/mattjj),
[Roy Frostig](https://github.com/froystig), [Dougal
Maclaurin](https://github.com/dougalm), and [Chris
Leary](https://github.com/learyg), and is now developed [in the
open](https://github.com/google/jax) by a growing number of
[contributors](#contributors).

## Quickstart: Colab in the Cloud
Jump right in using [a notebook in your
browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
connected to a Google Cloud GPU.

## Installation
JAX is written in pure Python, but it depends on XLA, which needs to be
compiled and installed as the `jaxlib` package. Use the following instructions
to [build XLA from source](#building-jax-from-source) or [install a binary
package with pip](#pip-installation).

### Building JAX from source
First, obtain the JAX source code:

```bash
git clone https://github.com/google/jax
cd jax
```

To build XLA with CUDA support, you can run

```bash
python build/build.py --enable_cuda
pip install -e build  # install jaxlib
pip install -e .      # install jax (pure Python)
```

See `python build/build.py --help` for configuration options, including ways to
specify the paths to CUDA and CUDNN, which you must have installed. The build
also depends on NumPy, and a compiler toolchain corresponding to that of
Ubuntu 16.04 or newer.

To build XLA without CUDA GPU support (CPU only), drop the `--enable_cuda`:

```bash
python build/build.py
pip install -e build  # install jaxlib
pip install -e .      # install jax
```

To upgrade to the latest version from GitHub, just run `git pull` from the JAX
repository root, and rebuild by running `build.py` if necessary. You shouldn't have
to reinstall because `pip install -e` sets up symbolic links from site-packages
into the repository.

### pip installation

Installing XLA with prebuilt binaries via `pip` is still experimental,
especially with GPU support. Let us know on [the issue
tracker](https://github.com/google/jax/issues) if you run into any errors.

To install a CPU-only version, which might be useful for doing local
development on a laptop, you can run

```bash
pip install jax jaxlib
```

If you want to install JAX with both CPU and GPU support, using existing CUDA
and CUDNN7 installations on your machine (for example, preinstalled on your
cloud VM), you can run

```bash
# install jaxlib
PYTHON_VERSION=py2  # alternatives: py2, py3
CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl

pip install jax  # install jax
```

The library package name must correspond to the version of the existing CUDA
installation you want to use, with `cuda100` for CUDA 10.0, `cuda92` for CUDA
9.2, and `cuda90` for CUDA 9.0. To find your CUDA and CUDNN versions, you can
run command like these, depending on your CUDNN install path:

```bash
nvcc --version
grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
```

## A brief tour

```python
In [1]: import jax.numpy as np

In [2]: from jax import random

In [3]: key = random.PRNGKey(0)

In [4]: x = random.normal(key, (5000, 5000))

In [5]: print(np.dot(x, x.T) / 2)  # fast!
[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...

In [6]: print(np.dot(x, x.T) / 2)  # even faster!
[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...
```

What’s happening behind-the-scenes is that JAX is using XLA to just-in-time
(JIT) compile and execute these individual operations on the GPU. First the
`random.normal` call is compiled and the array referred to by `x` is generated
on the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and
`divide`) is individually JIT-compiled and executed, each keeping its results on
the device.
It’s only when a value needs to be printed, plotted, saved, or passed into a raw
NumPy function that a read-only copy of the value is brought back to the host as
an ndarray and cached. The second call to `dot` is faster because the
JIT-compiled code is cached and reused, saving the compilation time.

The fun really starts when you use `grad` for automatic differentiation and
`jit` to compile your own functions end-to-end. Here’s a more complete toy
example:

```python
from jax import grad, jit
import jax.numpy as np

def sigmoid(x):
    return 0.5 * (np.tanh(x / 2.) + 1)

# Outputs probability of a label being true according to logistic model.
def logistic_predictions(weights, inputs):
    return sigmoid(np.dot(inputs, weights))

# Training loss is the negative log-likelihood of the training labels.
def loss(weights, inputs, targets):
    preds = logistic_predictions(weights, inputs)
    label_probs = preds * targets + (1 - preds) * (1 - targets)
    return -np.sum(np.log(label_probs))

# Build a toy dataset.
inputs = np.array([[0.52, 1.12,  0.77],
                   [0.88, -1.08, 0.15],
                   [0.52, 0.06, -1.30],
                   [0.74, -2.49, 1.39]])
targets = np.array([True, True, False, True])

# Define a compiled function that returns gradients of the training loss
training_gradient_fun = jit(grad(loss))

# Optimize weights using gradient descent.
weights = np.array([0.0, 0.0, 0.0])
print(""Initial loss: {:0.2f}"".format(loss(weights, inputs, targets)))
for i in range(100):
    weights -= 0.1 * training_gradient_fun(weights, inputs, targets)

print(""Trained loss: {:0.2f}"".format(loss(weights, inputs, targets)))
```

To see more, check out the [quickstart
notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb),
a [simple MNIST classifier
example](https://github.com/google/jax/blob/master/examples/mnist_classifier.py)
and the rest of the [JAX
examples](https://github.com/google/jax/blob/master/examples/).

## What's supported

If you’re using JAX just as an accelerator-backed NumPy, without using `grad` or
`jit` in your code, then in principle there are no constraints, though some
NumPy functions haven’t been implemented yet. Generally using `np.dot(A, B)` is
better than `A.dot(B)` because the former gives us more opportunities to run the
computation on the device. NumPy also does a lot of work to cast any array-like
function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
typically requires explicit casting of array arguments, like
`np.sum(np.array([x, y]))`.

For automatic differentiation with `grad`, JAX has the same restrictions
as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
dicts freely: jax doesn't even see them. Using `np.dot(A, B)` rather than
`A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.

For compiling your own functions with `jit` there are a few more requirements.
Because `jit` aims to specialize Python functions only on shapes and dtypes
during tracing, rather than on concrete values, Python control flow that depends
on concrete values won’t be able to execute and will instead raise an error. If
you want compiled control flow, use structured control flow primitives like
lax.cond and lax.while. Some indexing features, like slice-based indexing
`A[i:i+5]` for argument-dependent `i`, or boolean-based indexing `A[bool_ind]`
for argument-dependent `bool_ind`, produce abstract values of unknown shape and
are thus unsupported in `jit` functions.

In general, JAX is intended to be used with a functional style of Python
programming. Functions passed to transformations like `grad` and `jit` are
expected to be free of side-effects. You can write print statements for
debugging but they may only be executed once if they're under a `jit` decorator.

> TLDR **Do use**
>
> *   Functional programming
> *   [Many](https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py) of NumPy’s
>     functions (help us add more!)
> *   [Some](https://github.com/google/jax/tree/master/jax/scipy) SciPy functions
> *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`
> *   Explicit array creation from lists like `A = np.array([x, y])`
>
> **Don’t use**
>
> *   Assignment into arrays like `A[0, 0] = x`
> *   Implicit casting to arrays like `np.sum([x, y])` (use `np.sum(np.array([x,
>     y])` instead)
> *   `A.dot(B)` method syntax for functions of more than one argument (use
>     `np.dot(A, B)` instead)
> *   Side-effects like mutation of arguments or mutation of global variables
> *   The `out` argument of NumPy functions
>
> **For jit functions, also don’t use**
>
> *   Control flow based on dynamic values `if x > 0: ...`. Control flow based
>     on shapes is fine: `if x.shape[0] > 2: ...` and `for subarr in array`.
> *   Slicing `A[i:i+5]` for dynamic index `i` (use `lax.dynamic_slice` instead)
>     or boolean indexing `A[bool_ind]` for traced values `bool_ind`.

You should get loud errors if your code violates any of these.

## Transformations

At its core, JAX is an extensible system for transforming numerical functions.
We currently expose three important transformations: `grad`, `jit`, and `vmap`.

### Automatic differentiation with grad

JAX has roughly the same API as [Autograd](https://github.com/hips/autograd).
The most popular function is `grad` for reverse-mode gradients:

```python
from jax import grad
import jax.numpy as np

def tanh(x):  # Define a function
  y = np.exp(-2.0 * x)
  return (1.0 - y) / (1.0 + y)

grad_tanh = grad(tanh)  # Obtain its gradient function
print(grad_tanh(1.0))   # Evaluate it at x = 1.0
# prints 0.41997434161402603
```

You can differentiate to any order with `grad`.

For more advanced autodiff, you can use `jax.vjp` for reverse-mode
vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector
products. The two can be composed arbitrarily with one another, and with other
JAX transformations. Here's one way to compose
those to make a function that efficiently computes full Hessian matrices:

```python
from jax import jit, jacfwd, jacrev
def hessian(fun):
  return jit(jacfwd(jacrev(fun)))
```

As with Autograd, you're free to use differentiation with Python control
structures:

```python
def abs_val(x):
  if x > 0:
    return x
  else:
    return -x

abs_val_grad = grad(abs_val)
print(abs_val_grad)(1.0)   # prints 1.0
print(abs_val_grad)(-1.0)  # prints -1.0 (abs_val is re-evaluated)
```

### Compilation with jit

You can use XLA to compile your functions end-to-end with `jit`, used either as
an `@jit` decorator or as a higher-order function.

```python
import jax.numpy as np
from jax import jit

def slow_f(x):
  # Element-wise ops see a large benefit from fusion
  return x * x + x * 2.0

x = np.ones((5000, 5000))
fast_f = jit(slow_f)
%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X
%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)
```

You can mix `jit` and `grad` and any other JAX transformation however you like.

### Auto-vectorization with vmap

`vmap` is the vectorizing map.
It has the familiar semantics of mapping a function along array axes, but
instead of keeping the loop on the outside, it pushes the loop down into a
function’s primitive operations for better performance.

Using `vmap` can save you from having to carry around batch dimensions in your
code. For example, consider this simple *unbatched* neural network prediction
function:

```python
def predict(params, input_vec):
  assert input_vec.ndim == 1
  for W, b in params:
    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!
    input_vec = np.tanh(output_vec)
  return output_vec
```

We often instead write `np.dot(inputs, W)` to allow for a batch dimension on the
left side of `inputs`, but we’ve written this particular prediction function to
apply only to single input vectors. If we wanted to apply this function to a
batch of inputs at once, semantically we could just write

```python
from functools import partial
predictions = np.stack(list(map(partial(predict, params), input_batch)))
```

But pushing one example through the network at a time would be slow! It’s better
to vectorize the computation, so that at every layer we’re doing matrix-matrix
multiplies rather than matrix-vector multiplies.

The `vmap` function does that transformation for us. That is, if we write

```python
from jax import vmap
predictions = vmap(partial(predict, params), input_batch)
```

then the `vmap` function will push the outer loop inside the function, and our
machine will end up executing matrix-matrix multiplications exactly as if we’d
done the batching by hand.

It’s easy enough to manually batch a simple neural network without `vmap`, but
in other cases manual vectorization can be impractical or impossible. Take the
problem of efficiently computing per-example gradients: that is, for a fixed set
of parameters, we want to compute the gradient of our loss function evaluated
separately at each example in a batch. With `vmap`, it’s easy:

```python
per_example_gradients = vmap(partial(grad(loss), params), inputs, targets)
```

Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other
JAX transformation! We use `vmap` with both forward- and reverse-mode automatic
differentiation for fast Jacobian and Hessian matrix calculations in
`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.


## Random numbers are different

JAX needs a functional pseudo-random number generator (PRNG) system to provide
reproducible results invariant to compilation boundaries and backends, while
also maximizing performance by enabling vectorized generation and
parallelization across random calls. The `numpy.random` library doesn’t have
those properties. The `jax.random` library meets those needs: it’s functionally
pure, but it doesn’t require you to pass stateful random objects back out of
every function.

The `jax.random` library uses
[count-based PRNGs](http://www.thesalmons.org/john/random123/papers/random123sc11.pdf)
and a functional array-oriented
[splitting model](http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf).
To generate random values, you call a function like `jax.random.normal` and give
it a PRNG key:

```python
import jax.random as random

key = random.PRNGKey(0)
print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
```

If we make the same call again with the same key, we get the same values:

```python
print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
```

The key never gets updated. So how do we get fresh random values? We use
`jax.random.split` to create new keys from existing ones. A common pattern is to
split off a new key for every function call that needs random values:

```python
key = random.PRNGKey(0)

key, subkey = random.split(key)
print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]

key, subkey = random.split(key)
print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]
```

By splitting the PRNG key, not only do we avoid having to thread random states
back out of every function call, but also we can generate multiple random arrays
in parallel because we can avoid unnecessary sequential dependencies.

There's a gotcha here, which is that it's easy to unintentionally reuse a key
without splitting. We intend to add a check for this (a sort of dynamic linear
typing) but for now it's something to be careful about.


## Mini-libraries

JAX provides some small, experimental libraries for machine learning. These
libraries are in part about providing tools and in part about serving as
examples for how to build such libraries using JAX. Each one is only a few
hundred lines of code, so take a look inside and adapt them as you need!

### Neural-net building with Stax

**Stax** is a functional neural network building library. The basic idea is that
a single layer or an entire network can be modeled as an `(init_fun, apply_fun)`
pair. The `init_fun` is used to initialize network parameters and the
`apply_fun` takes parameters and inputs to produce outputs. There are
constructor functions for common basic pairs, like `Conv` and `Relu`, and these
pairs can be composed in series using `stax.serial` or in parallel using
`stax.parallel`.

Here’s an example:

```python
from jax.experimental import stax
from jax.experimental.stax import Conv
from jax.experimental.stax import Dense
from jax.experimental.stax import MaxPool
from jax.experimental.stax import Relu
from jax.experimental.stax import LogSoftmax

# Set up network initialization and evaluation functions
net_init, net_apply = stax.serial(
    Conv(32, (3, 3), padding='SAME'), Relu,
    Conv(64, (3, 3), padding='SAME'), Relu
    MaxPool((2, 2)), Flatten,
    Dense(128), Relu,
    Dense(10), SoftMax,
)

# Initialize parameters, not committing to a batch shape
in_shape = (-1, 28 * 28)
out_shape, net_params = net_init(in_shape)

# Apply network
predictions = net_apply(net_params, inputs)
```

### First-order optimization with Minmax

**Minmax** is an optimization library focused on stochastic first-order
optimizers. Every optimizer is modeled as an `(init_fun, update_fun)` pair. The
`init_fun` is used to initialize the optimizer state, which could include things
like momentum variables, and the `update_fun` accepts a gradient and an
optimizer state to produce a new optimizer state. The parameters being optimized
can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can
store your parameters however you’d like.

Here’s an example, using `jit` to compile the whole update end-to-end:

```python
from jax.experimental import minmax
from jax import jit

# Set up an optimizer
opt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)

# Define a compiled update step
@jit
def step(i, opt_state, batch):
  params = minmax.get_params(opt_state)
  g = grad(loss)(params, batch)
  return opt_update(i, g, opt_state)

# Optimize parameters in a loop
opt_state = opt_init(net_params)
for i in range(num_steps):
  opt_state = step(i, opt_state, next(data_generator))
net_params = minmax.get_params(opt_state)
```

## How it works

Programming in machine learning is about expressing and transforming functions.
Transformations include automatic differentiation, compilation for accelerators,
and automatic batching. High-level languages like Python are great for
expressing functions, but usually all we can do with them is apply them. We lose
access to their internal structure which would let us perform transformations.

JAX is a tool for specializing and translating high-level Python+NumPy functions
into a representation that can be transformed and then lifted back into a Python
function.

![simplified-lifecycle](https://raw.githubusercontent.com/google/jax/master/images/lifecycle.png)

JAX specializes Python functions by tracing. Tracing a function means monitoring
all the basic operations that are applied to its input to produce its output,
and recording these operations and the data-flow between them in a directed
acyclic graph (DAG). To perform tracing, JAX wraps primitive operations, like
basic numerical kernels, so that when they’re called they add themselves to a
list of operations performed along with their inputs and outputs. To keep track
of how data flows between these primitives, values being tracked are wrapped in
instances of the `Tracer` class.

When a Python function is provided to `grad` or `jit`, it’s wrapped for tracing
and returned. When the wrapped function is called, we abstract the concrete
arguments provided into instances of the `AbstractValue` class, box them for
tracing in instances of the `Tracer` class, and call the function on them.
Abstract arguments represent sets of possible values rather than specific
values: for example, `jit` abstracts ndarray arguments to abstract values that
represent all ndarrays with the same shape and dtype. In contrast, `grad`
abstracts ndarray arguments to represent an infinitesimal neighborhood of the
underlying
value. By tracing the Python function on these abstract values, we ensure that
it’s specialized enough so that it’s tractable to transform, and that it’s still
general enough so that the transformed result is useful, and possibly reusable.
These transformed functions are then lifted back into Python callables in a way
that allows them to be traced and transformed again as needed.

The primitive functions that JAX traces are mostly in 1:1 correspondence with
[XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined
in [lax.py](https://github.com/google/jax/blob/master/jax/lax.py). This 1:1
correspondence makes most of the translations to XLA essentially trivial, and
ensures we only have a small set of primitives to cover for other
transformations like automatic differentiation. The [`jax.numpy`
layer](https://github.com/google/jax/blob/master/jax/numpy/) is written in pure
Python simply by expressing NumPy functions in terms of the LAX functions (and
other NumPy functions we’ve already written). That makes `jax.numpy` easy to
extend.

When you use `jax.numpy`, the underlying LAX primitives are `jit`-compiled
behind the scenes, allowing you to write unrestricted Python+Numpy code while
still executing each primitive operation on an accelerator.

But JAX can do more: instead of just compiling and dispatching to a fixed set of
individual primitives, you can use `jit` on larger and larger functions to be
end-to-end compiled and optimized. For example, instead of just compiling and
dispatching a convolution op, you can compile a whole network, or a whole
gradient evaluation and optimizer update step.

The tradeoff is that `jit` functions have to satisfy some additional
specialization requirements: since we want to compile traces that are
specialized on shapes and dtypes, but not specialized all the way to concrete
values, the Python code under a `jit` decorator must be applicable to abstract
values. If we try to evaluate `x > 0` on an abstract `x`, the result is an
abstract value representing the set `{True, False}`, and so a Python branch like
`if x > 0` will raise an error: it doesn’t know which way to go! 
See [What’s supported](#whats-supported) for more
information about `jit` requirements.

The good news about this tradeoff is that `jit` is opt-in: JAX libraries use
`jit` on individual operations and functions behind the scenes, allowing you to
write unrestricted Python+Numpy and still make use of a hardware accelerator.
But when you want to maximize performance, you can often use `jit` in your own
code to compile and end-to-end optimize much bigger functions.

## What we're working on
1. Documentation!
2. Cloud TPU support
3. Multi-GPU and multi-TPU support
4. Full NumPy coverage and some SciPy coverage
5. Full coverage for vmap
6. Make everything faster
    * Lowering the XLA function dispatch overhead
    * Linear algebra routines (MKL on CPU, MAGMA on GPU)
7. `cond` and `while` primitives with efficient automatic differentiation

## Current gotchas

Some things we don't handle that might surprise NumPy users:
1. No in-place mutation syntax. Functional code. Can use lax.dynamic\_update\_slice.
2. PRNG can be awkward, and linearity is not checked with a warning.

## Contributors

So far, JAX includes lots of help and contributions from [Peter
Hawkins](https://github.com/hawkinsp), [Alex
Wiltschko](http://github.com/alexbw), George Dahl, [Eli
Bendersky](https://github.com/eliben), Zak Stone, [Alexey
Radul](https://github.com/axch), Michael Isard, Skye Wanderman-Milne, and many
others.
","diff --git a/README.md b/README.md
index 672ffecd6caa1778929fd0b7d71f03602ab44fd7..3ef0705d6ce8a3035c465c055376a1daede63a48 100644
--- a/README.md
+++ b/README.md
@@ -122,7 +122,7 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
 ```
",Dead / unused code,Update README.md to install jaxlib wheel instead of jax wheel
7be7d42484fa97fe54b5e57ab9e1a3dea246edb5,Dougal Maclaurin: Fixed json bug,notebooks/quickstart.ipynb,"{
  ""nbformat"": 4,
  ""nbformat_minor"": 0,
  ""metadata"": {
    ""colab"": {
      ""name"": ""JAX Quickstart.ipynb"",
      ""version"": ""0.3.2"",
      ""provenance"": [],
      ""collapsed_sections"": [],
      ""toc_visible"": true
    },
    ""kernelspec"": {
      ""display_name"": ""Python 3"",
      ""language"": ""python"",
      ""name"": ""python3""
    },
    ""accelerator"": ""GPU""
  },
  ""cells"": [
    {
      ""metadata"": {
        ""id"": ""logZcM_HEnve"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""##### Copyright 2018 Google LLC.\n"",
        ""\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""QwN47xiBEsKz"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
        ""you may not use this file except in compliance with the License.\n"",
        ""You may obtain a copy of the License at\n"",
        ""\n"",
        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
        ""\n"",
        ""Unless required by applicable law or agreed to in writing, software\n"",
        ""distributed under the License is distributed on an \""AS IS\"" BASIS,\n"",
        ""WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n"",
        ""See the License for the specific language governing permissions and\n"",
        ""limitations under the License.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""xtWX4x9DCF5_""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""# JAX Quickstart\n"",
        ""Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary\n"",
        ""\n"",
        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
        ""\n"",
        ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
        ""\n"",
        ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
        ""can automatically differentiate native Python and NumPy code. It can\n"",
        ""differentiate through a large subset of Python’s features, including loops, ifs,\n"",
        ""recursion, and closures, and it can even take derivatives of derivatives of\n"",
        ""derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n"",
        ""to any order.\n"",
        ""\n"",
        ""What’s new is that JAX uses\n"",
        ""[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)\n"",
        ""to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n"",
        ""Compilation happens under the hood by default, with library calls getting\n"",
        ""just-in-time compiled and executed. But JAX even lets you just-in-time compile\n"",
        ""your own Python functions into XLA-optimized kernels using a one-function API.\n"",
        ""Compilation and automatic differentiation can be composed arbitrarily, so you\n"",
        ""can express sophisticated algorithms and get maximal performance without having\n"",
        ""to leave Python.\n""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""PaW85yP_BrCF"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
        ""!pip install jax"",
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""SY8mDvEvCGqk"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from __future__ import print_function, division\n"",
        ""import jax.numpy as np\n"",
        ""from jax import grad, jit, vmap\n"",
        ""from jax import random""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""FQ89jHCYfhpg""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Multiplying Matrices""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""Xpy1dSgNqCP4""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see the readme.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""u0nseKZNqOoH"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""key = random.PRNGKey(0)\n"",
        ""x = random.normal(key, (10,))\n"",
        ""print(x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""hDJF0UPKnuqB""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Let's dive right in and multiply two big matrices.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""eXn8GUl6CG5N"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""size = 3000\n"",
        ""x = random.normal(key, (size, size), dtype=np.float32)\n"",
        ""%timeit np.dot(x, x.T)  # runs on the GPU""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""0AlN7EbonyaR""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX NumPy functions work on regular NumPy arrays.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""ZPl0MuwYrM7t"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""import numpy as onp  # original CPU-backed NumPy\n"",
        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
        ""%timeit np.dot(x, x.T)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""_SrcB2IurUuE""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""That's slower because it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""Jj7M7zyRskF0"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import device_put\n"",
        ""\n"",
        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
        ""x = device_put(x)\n"",
        ""%timeit np.dot(x, x.T)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""clO9djnen8qi""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""The output of `device_put` still acts like an NDArray. By the way, the [implementation](https://github.com/google/jax/blob/c8f93511ecb977d02fa5b0eee17075706fd6fd93/jax/api.py#L162) of `device_put` is just `device_put = jit(lambda x: x)`.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""ghkfKNQttDpg""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""If you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""RzXK8GnIs7VV"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
        ""%timeit onp.dot(x, x.T)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""iOzp0P_GoJhb""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there's three main ones:\n"",
        ""\n"",
        "" - `jit`, for speeding up your code\n"",
        "" - `grad`, for taking derivatives\n"",
        "" - `vmap`, for automatic vectorization or batching.\n"",
        ""\n"",
        ""Let's go over these, one-by-one. We'll also end up composing these in interesting ways.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""bTTrTbWvgLUK""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Using `jit` to speed up functions""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""YrqE32mvE3b7""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md). Let's try that.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""qLGdCtFKFLOR"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def selu(x, alpha=1.67, lmbda=1.05):\n"",
        ""  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n"",
        ""\n"",
        ""x = random.normal(key, (1000000,))\n"",
        ""%timeit selu(x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""a_V8SruVHrD_""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We can speed it up with `@jit`, which will jit-compile the first time `selu` is called and will be cached thereafter.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""fh4w_3NpFYTp"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""selu_jit = jit(selu)\n"",
        ""%timeit selu_jit(x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""HxpBc4WmfsEU""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Taking derivatives with `grad`\n"",
        ""\n"",
        ""In addition to evaluating numerical functions, we also want to transform them. One transformation is [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). In JAX, just like in [Autograd](https://github.com/HIPS/autograd), you can compute gradients with the `grad` function.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""IMAgNJaMJwPD"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def sum_logistic(x):\n"",
        ""  return np.sum(1.0 / (1.0 + np.exp(-x)))\n"",
        ""\n"",
        ""x_small = np.arange(3.)\n"",
        ""derivative_fn = grad(sum_logistic)\n"",
        ""print(derivative_fn(x_small))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""PtNs881Ohioc""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Let's verify with finite differences that our result is correct.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""JXI7_OZuKZVO"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def first_finite_differences(f, x):\n"",
        ""  eps = 1e-3\n"",
        ""  return np.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n"",
        ""                   for v in onp.eye(len(x))])\n"",
        ""\n"",
        ""\n"",
        ""print(first_finite_differences(sum_logistic, x_small))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""Q2CUZjOWNZ-3""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""TO4g8ny-OEi4"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""yCJ5feKvhnBJ""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""Z-JxbiNyhxEW"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import jacfwd, jacrev\n"",
        ""def hessian(fun):\n"",
        ""  return jit(jacfwd(jacrev(fun)))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""TI4nPsGafxbL""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Auto-vectorization with `vmap`""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""PcxkONy5aius""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a function’s primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""TPiX4y-bWLFS""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using `vmap`. Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""8w0Gpsn8WYYj"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""mat = random.normal(key, (150, 100))\n"",
        ""batched_x = random.normal(key, (10, 100))\n"",
        ""\n"",
        ""def apply_matrix(v):\n"",
        ""  return np.dot(mat, v)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""0zWsc0RisQWx"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Given a function such as `apply_matrix`, we can loop over a batch dimension in Python, but usually the performance of doing so is poor.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""KWVc9BsZv0Ki"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def naively_batched_apply_matrix(v_batched):\n"",
        ""  return np.stack([apply_matrix(v) for v in v_batched])\n"",
        ""\n"",
        ""print('Naively batched')\n"",
        ""%timeit naively_batched_apply_matrix(batched_x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""qHfKaLE9stbA"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We know how to batch this operation manually. In this case, `np.dot` handles extra batch dimensions transparently.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""ipei6l8nvrzH"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@jit\n"",
        ""def batched_apply_matrix(v_batched):\n"",
        ""  return np.dot(v_batched, mat.T)\n"",
        ""\n"",
        ""print('Manually batched')\n"",
        ""%timeit batched_apply_matrix(batched_x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""1eF8Nhb-szAb"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""However, suppose we had a more complicated function without batching support. We can use `vmap` to add batching support automatically.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""67Oeknf5vuCl"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@jit\n"",
        ""def vmap_batched_apply_matrix(batched_x):\n"",
        ""  return vmap(apply_matrix, batched_x)\n"",
        ""\n"",
        ""print('Auto-vectorized with vmap')\n"",
        ""%timeit vmap_batched_apply_matrix(batched_x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""pYVl3Z2nbZhO""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation.""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""WwNnjaI4th_8"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""This is just a taste of what JAX can do. We're really excited to see what you do with it!""
      ]
    }
  ]
}
","{
  ""nbformat"": 4,
  ""nbformat_minor"": 0,
  ""metadata"": {
    ""colab"": {
      ""name"": ""JAX Quickstart.ipynb"",
      ""version"": ""0.3.2"",
      ""provenance"": [],
      ""collapsed_sections"": [],
      ""toc_visible"": true
    },
    ""kernelspec"": {
      ""display_name"": ""Python 3"",
      ""language"": ""python"",
      ""name"": ""python3""
    },
    ""accelerator"": ""GPU""
  },
  ""cells"": [
    {
      ""metadata"": {
        ""id"": ""logZcM_HEnve"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""##### Copyright 2018 Google LLC.\n"",
        ""\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""QwN47xiBEsKz"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
        ""you may not use this file except in compliance with the License.\n"",
        ""You may obtain a copy of the License at\n"",
        ""\n"",
        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
        ""\n"",
        ""Unless required by applicable law or agreed to in writing, software\n"",
        ""distributed under the License is distributed on an \""AS IS\"" BASIS,\n"",
        ""WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n"",
        ""See the License for the specific language governing permissions and\n"",
        ""limitations under the License.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""xtWX4x9DCF5_""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""# JAX Quickstart\n"",
        ""Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary\n"",
        ""\n"",
        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
        ""\n"",
        ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
        ""\n"",
        ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
        ""can automatically differentiate native Python and NumPy code. It can\n"",
        ""differentiate through a large subset of Python’s features, including loops, ifs,\n"",
        ""recursion, and closures, and it can even take derivatives of derivatives of\n"",
        ""derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n"",
        ""to any order.\n"",
        ""\n"",
        ""What’s new is that JAX uses\n"",
        ""[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)\n"",
        ""to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n"",
        ""Compilation happens under the hood by default, with library calls getting\n"",
        ""just-in-time compiled and executed. But JAX even lets you just-in-time compile\n"",
        ""your own Python functions into XLA-optimized kernels using a one-function API.\n"",
        ""Compilation and automatic differentiation can be composed arbitrarily, so you\n"",
        ""can express sophisticated algorithms and get maximal performance without having\n"",
        ""to leave Python.\n""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""PaW85yP_BrCF"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
        ""!pip install jax""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""SY8mDvEvCGqk"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from __future__ import print_function, division\n"",
        ""import jax.numpy as np\n"",
        ""from jax import grad, jit, vmap\n"",
        ""from jax import random""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""FQ89jHCYfhpg""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Multiplying Matrices""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""Xpy1dSgNqCP4""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see the readme.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""u0nseKZNqOoH"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""key = random.PRNGKey(0)\n"",
        ""x = random.normal(key, (10,))\n"",
        ""print(x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""hDJF0UPKnuqB""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Let's dive right in and multiply two big matrices.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""eXn8GUl6CG5N"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""size = 3000\n"",
        ""x = random.normal(key, (size, size), dtype=np.float32)\n"",
        ""%timeit np.dot(x, x.T)  # runs on the GPU""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""0AlN7EbonyaR""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX NumPy functions work on regular NumPy arrays.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""ZPl0MuwYrM7t"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""import numpy as onp  # original CPU-backed NumPy\n"",
        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
        ""%timeit np.dot(x, x.T)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""_SrcB2IurUuE""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""That's slower because it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""Jj7M7zyRskF0"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import device_put\n"",
        ""\n"",
        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
        ""x = device_put(x)\n"",
        ""%timeit np.dot(x, x.T)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""clO9djnen8qi""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""The output of `device_put` still acts like an NDArray. By the way, the [implementation](https://github.com/google/jax/blob/c8f93511ecb977d02fa5b0eee17075706fd6fd93/jax/api.py#L162) of `device_put` is just `device_put = jit(lambda x: x)`.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""ghkfKNQttDpg""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""If you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""RzXK8GnIs7VV"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
        ""%timeit onp.dot(x, x.T)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""iOzp0P_GoJhb""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there's three main ones:\n"",
        ""\n"",
        "" - `jit`, for speeding up your code\n"",
        "" - `grad`, for taking derivatives\n"",
        "" - `vmap`, for automatic vectorization or batching.\n"",
        ""\n"",
        ""Let's go over these, one-by-one. We'll also end up composing these in interesting ways.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""bTTrTbWvgLUK""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Using `jit` to speed up functions""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""YrqE32mvE3b7""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md). Let's try that.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""qLGdCtFKFLOR"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def selu(x, alpha=1.67, lmbda=1.05):\n"",
        ""  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n"",
        ""\n"",
        ""x = random.normal(key, (1000000,))\n"",
        ""%timeit selu(x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""a_V8SruVHrD_""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We can speed it up with `@jit`, which will jit-compile the first time `selu` is called and will be cached thereafter.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""fh4w_3NpFYTp"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""selu_jit = jit(selu)\n"",
        ""%timeit selu_jit(x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""HxpBc4WmfsEU""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Taking derivatives with `grad`\n"",
        ""\n"",
        ""In addition to evaluating numerical functions, we also want to transform them. One transformation is [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). In JAX, just like in [Autograd](https://github.com/HIPS/autograd), you can compute gradients with the `grad` function.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""IMAgNJaMJwPD"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def sum_logistic(x):\n"",
        ""  return np.sum(1.0 / (1.0 + np.exp(-x)))\n"",
        ""\n"",
        ""x_small = np.arange(3.)\n"",
        ""derivative_fn = grad(sum_logistic)\n"",
        ""print(derivative_fn(x_small))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""PtNs881Ohioc""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Let's verify with finite differences that our result is correct.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""JXI7_OZuKZVO"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def first_finite_differences(f, x):\n"",
        ""  eps = 1e-3\n"",
        ""  return np.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n"",
        ""                   for v in onp.eye(len(x))])\n"",
        ""\n"",
        ""\n"",
        ""print(first_finite_differences(sum_logistic, x_small))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""Q2CUZjOWNZ-3""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""TO4g8ny-OEi4"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""yCJ5feKvhnBJ""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""Z-JxbiNyhxEW"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import jacfwd, jacrev\n"",
        ""def hessian(fun):\n"",
        ""  return jit(jacfwd(jacrev(fun)))""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""TI4nPsGafxbL""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Auto-vectorization with `vmap`""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""PcxkONy5aius""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a function’s primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""TPiX4y-bWLFS""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using `vmap`. Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""8w0Gpsn8WYYj"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""mat = random.normal(key, (150, 100))\n"",
        ""batched_x = random.normal(key, (10, 100))\n"",
        ""\n"",
        ""def apply_matrix(v):\n"",
        ""  return np.dot(mat, v)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""0zWsc0RisQWx"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Given a function such as `apply_matrix`, we can loop over a batch dimension in Python, but usually the performance of doing so is poor.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""KWVc9BsZv0Ki"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def naively_batched_apply_matrix(v_batched):\n"",
        ""  return np.stack([apply_matrix(v) for v in v_batched])\n"",
        ""\n"",
        ""print('Naively batched')\n"",
        ""%timeit naively_batched_apply_matrix(batched_x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""qHfKaLE9stbA"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""We know how to batch this operation manually. In this case, `np.dot` handles extra batch dimensions transparently.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""ipei6l8nvrzH"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@jit\n"",
        ""def batched_apply_matrix(v_batched):\n"",
        ""  return np.dot(v_batched, mat.T)\n"",
        ""\n"",
        ""print('Manually batched')\n"",
        ""%timeit batched_apply_matrix(batched_x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""1eF8Nhb-szAb"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""However, suppose we had a more complicated function without batching support. We can use `vmap` to add batching support automatically.""
      ]
    },
    {
      ""metadata"": {
        ""colab_type"": ""code"",
        ""id"": ""67Oeknf5vuCl"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@jit\n"",
        ""def vmap_batched_apply_matrix(batched_x):\n"",
        ""  return vmap(apply_matrix, batched_x)\n"",
        ""\n"",
        ""print('Auto-vectorized with vmap')\n"",
        ""%timeit vmap_batched_apply_matrix(batched_x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""colab_type"": ""text"",
        ""id"": ""pYVl3Z2nbZhO""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation.""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""WwNnjaI4th_8"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""This is just a taste of what JAX can do. We're really excited to see what you do with it!""
      ]
    }
  ]
}
","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 3b8c69f1a20303bc537b1e6a2aa1c4fa0d6817c2..962ddb45bf57f520ce70097187f77db5b91d2b36 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -90,7 +90,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax"",
+        ""!pip install jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []
",Typo / wrong variable name,"No meaningful changes were made to the code, as the modification only removed and then immediately re-added the same line. If you"
551601965661c95e406c6e420e06775ac300862d,Stephan Hoyer: Fix link in gufuncs notebook,notebooks/gufuncs.ipynb,"{
  ""nbformat"": 4,
  ""nbformat_minor"": 0,
  ""metadata"": {
    ""colab"": {
      ""name"": ""JAX generalized ufuncs.ipynb"",
      ""version"": ""0.3.2"",
      ""provenance"": [],
      ""collapsed_sections"": []
    },
    ""kernelspec"": {
      ""name"": ""python3"",
      ""display_name"": ""Python 3""
    },
    ""accelerator"": ""GPU""
  },
  ""cells"": [
    {
      ""metadata"": {
        ""id"": ""435_M09vl3NA"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""# Extending JAX's vmap to work like NumPy's gufuncs\n"",
        ""\n"",
        ""by [Stephan Hoyer](https://github.com/shoyer)\n"",
        ""\n"",
        ""## What is a gufunc?\n"",
        ""\n"",
        ""[Generalized universal functions]((https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
        ""- \""core dimensions\"" over which an operation is defined.\n"",
        ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
        ""\n"",
        ""A string [signature](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html#details-of-signature) associated with each gufunc controls how this happens by indicating how core dimensions are mapped between inputs and outputs. The syntax is easiest to understand by looking at a few examples:\n"",
        ""\n"",
        ""- Addition: `(),()->()`\n"",
        ""- 1D inner product: `(i),(i)->()`\n"",
        ""- 1D sum: `(i)->()`\n"",
        ""- Matrix multiplcation: `(m,n),(n,k)->(m,k)`\n"",
        ""\n"",
        ""## Why write gufuncs?\n"",
        ""\n"",
        ""From a user perspective, gufuncs are nice because they're guaranteed to vectorize in a consistent and general fashion. For example, by default gufuncs use the last dimensions of arrays as core dimensions, but you can control that explicitly with the `axis` or `axes` arguments.\n"",
        ""\n"",
        ""From a developer perspective, gufuncs are nice because they simply your work: you only need to think about the core logic of your function, not how it handles arbitrary dimensional input. You can just write that down in a simple, declarative way.\n"",
        ""\n"",
        ""## JAX makes it easy to write high-level performant code\n"",
        ""\n"",
        ""Unfortunately, writing NumPy gufuncs today is somewhat non-trivial. Your options today are:\n"",
        ""\n"",
        ""1. Write the inner loops yourself in C.\n"",
        ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differention.\n"",
        ""\n"",
        ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
        ""\n"",
        ""## A simple example\n"",
        ""\n"",
        ""Consider a simple example from data preprocessing, centering an array.\n"",
        ""\n"",
        ""Here's how we might write a vectorized version using NumPy:\n"",
        ""```python\n"",
        ""def center(array, axis=-1):\n"",
        ""  # array can have any number of dimensions\n"",
        ""  bias = np.mean(array, axis=axis)\n"",
        ""  debiased = array - np.expand_dims(bias, axis)\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""And here's how we could write a vectorized version using JAX gufuncs:\n"",
        ""```python\n"",
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  # array is always a 1D vector\n"",
        ""  bias = np.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""See the difference?\n"",
        ""- Instead of needing to think about broadcasting while writing the entire function, we can write the function assuming the input is always a vector.\n"",
        ""- We get the `axis` argument automatically, without needing to write it ourselves.\n"",
        ""- As a bonus, the decorator makes the function self-documenting: a reader immediately knows that it handles higher dimensional input and output correctly.\n"",
        ""\n"",
        ""For more examples (and the implementation) see below.""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""k40qkuQdkqFg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Implementation\n"",
        ""\n"",
        ""### License\n"",
        ""\n"",
        ""Copyright 2018 Google LLC.\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
        ""\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n"",
        ""\n"",
        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
        ""\n"",
        ""Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""4QrBJNYG5ECU"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Setup and imports""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""2NXj3Dp5270W"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
        ""!pip install -q jax""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""p-rYDdqL1uZP"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import grad, jit, vmap\n"",
        ""import jax.numpy as jnp\n"",
        ""import numpy as np\n"",
        ""import re""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""tU2rwIOZmT0Q"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Copied from `numpy.lib.function_base`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""lBVIP3O2kkqY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# See http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n"",
        ""_DIMENSION_NAME = r'\\w+'\n"",
        ""_CORE_DIMENSION_LIST = '(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)\n"",
        ""_ARGUMENT = r'\\({}\\)'.format(_CORE_DIMENSION_LIST)\n"",
        ""_ARGUMENT_LIST = '{0:}(?:,{0:})*'.format(_ARGUMENT)\n"",
        ""_SIGNATURE = '^{0:}->{0:}$'.format(_ARGUMENT_LIST)\n"",
        ""\n"",
        ""\n"",
        ""def _parse_gufunc_signature(signature):\n"",
        ""    \""\""\""\n"",
        ""    Parse string signatures for a generalized universal function.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    signature : string\n"",
        ""        Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``\n"",
        ""        for ``np.matmul``.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    Tuple of input and output core dimensions parsed from the signature, each\n"",
        ""    of the form List[Tuple[str, ...]].\n"",
        ""    \""\""\""\n"",
        ""    if not re.match(_SIGNATURE, signature):\n"",
        ""        raise ValueError(\n"",
        ""            'not a valid gufunc signature: {}'.format(signature))\n"",
        ""    return tuple([tuple(re.findall(_DIMENSION_NAME, arg))\n"",
        ""                  for arg in re.findall(_ARGUMENT, arg_list)]\n"",
        ""                 for arg_list in signature.split('->'))\n"",
        ""\n"",
        ""\n"",
        ""\n"",
        ""def _update_dim_sizes(dim_sizes, arg, core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Incrementally check and update core dimension sizes for a single argument.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Sizes of existing core dimensions. Will be updated in-place.\n"",
        ""    arg : ndarray\n"",
        ""        Argument to examine.\n"",
        ""    core_dims : Tuple[str, ...]\n"",
        ""        Core dimensions for this argument.\n"",
        ""    \""\""\""\n"",
        ""    if not core_dims:\n"",
        ""        return\n"",
        ""\n"",
        ""    num_core_dims = len(core_dims)\n"",
        ""    if arg.ndim < num_core_dims:\n"",
        ""        raise ValueError(\n"",
        ""            '%d-dimensional argument does not have enough '\n"",
        ""            'dimensions for all core dimensions %r'\n"",
        ""            % (arg.ndim, core_dims))\n"",
        ""\n"",
        ""    core_shape = arg.shape[-num_core_dims:]\n"",
        ""    for dim, size in zip(core_dims, core_shape):\n"",
        ""        if dim in dim_sizes:\n"",
        ""            if size != dim_sizes[dim]:\n"",
        ""                raise ValueError(\n"",
        ""                    'inconsistent size for core dimension %r: %r vs %r'\n"",
        ""                    % (dim, size, dim_sizes[dim]))\n"",
        ""        else:\n"",
        ""            dim_sizes[dim] = size\n"",
        ""\n"",
        ""\n"",
        ""def _parse_input_dimensions(args, input_core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Parse broadcast and core dimensions for vectorize with a signature.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    args : Tuple[ndarray, ...]\n"",
        ""        Tuple of input arguments to examine.\n"",
        ""    input_core_dims : List[Tuple[str, ...]]\n"",
        ""        List of core dimensions corresponding to each input.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    broadcast_shape : Tuple[int, ...]\n"",
        ""        Common shape to broadcast all non-core dimensions to.\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Common sizes for named core dimensions.\n"",
        ""    \""\""\""\n"",
        ""    broadcast_args = []\n"",
        ""    dim_sizes = {}\n"",
        ""    for arg, core_dims in zip(args, input_core_dims):\n"",
        ""        _update_dim_sizes(dim_sizes, arg, core_dims)\n"",
        ""        ndim = arg.ndim - len(core_dims)\n"",
        ""        dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])\n"",
        ""        broadcast_args.append(dummy_array)\n"",
        ""    broadcast_shape = np.lib.stride_tricks._broadcast_shape(*broadcast_args)\n"",
        ""    return broadcast_shape, dim_sizes\n"",
        ""\n"",
        ""\n"",
        ""def _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):\n"",
        ""    \""\""\""Helper for calculating broadcast shapes with core dimensions.\""\""\""\n"",
        ""    return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)\n"",
        ""            for core_dims in list_of_core_dims]\n"",
        ""\n"",
        ""  \n"",
        ""# adapted from np.vectorize (again authored by shoyer@)\n"",
        ""def broadcast_with_core_dims(args, input_core_dims, output_core_dims):\n"",
        ""  if len(args) != len(input_core_dims):\n"",
        ""    raise TypeError('wrong number of positional arguments: '\n"",
        ""                    'expected %r, got %r'\n"",
        ""                    % (len(input_core_dims), len(args)))\n"",
        ""\n"",
        ""  broadcast_shape, dim_sizes = _parse_input_dimensions(\n"",
        ""      args, input_core_dims)\n"",
        ""  input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,\n"",
        ""                                   input_core_dims)\n"",
        ""  args = [jnp.broadcast_to(arg, shape)\n"",
        ""          for arg, shape in zip(args, input_shapes)]\n"",
        ""  return args""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""aa_Gh3K_PQkY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Handle the `axis` argument""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""MLeFHhVoPT4h"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def verify_axis_is_supported(input_core_dims, output_core_dims):\n"",
        ""  all_core_dims = set()\n"",
        ""  for input_or_output_core_dims in [input_core_dims, output_core_dims]:\n"",
        ""    for core_dims in input_or_output_core_dims:\n"",
        ""      all_core_dims.update(core_dims)\n"",
        ""  if len(core_dims) > 1:\n"",
        ""    raise ValueError('only one gufuncs with one core dim support axis')\n"",
        ""\n"",
        ""\n"",
        ""def reorder_inputs(args, axis, input_core_dims):\n"",
        ""  return tuple(jnp.moveaxis(arg, axis, -1) if core_dims else arg\n"",
        ""               for arg, core_dims in zip(args, input_core_dims))\n"",
        ""\n"",
        ""\n"",
        ""def reorder_outputs(result, axis, output_core_dims):\n"",
        ""  if not isinstance(result, tuple):\n"",
        ""    result = (result,)\n"",
        ""  result = tuple(jnp.moveaxis(res, -1, axis) if core_dims else res\n"",
        ""                 for res, core_dims in zip(result, output_core_dims))\n"",
        ""  if len(result) == 1:\n"",
        ""    (result,) = result\n"",
        ""  return result""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""Uik9GA76lZjY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Core implementation\n"",
        ""\n"",
        ""This is the only part that uses `vmap`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""z-FgQaW02_WN"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""import functools\n"",
        ""\n"",
        ""def curried_vmap(func):\n"",
        ""  @functools.wraps(func)\n"",
        ""  def wrapper(*args):\n"",
        ""    return vmap(func, *args)\n"",
        ""  return wrapper\n"",
        ""\n"",
        ""\n"",
        ""def vectorize(signature):\n"",
        ""  \""\""\""Vectorize a function using JAX.\""\""\""\n"",
        ""  input_core_dims, output_core_dims = _parse_gufunc_signature(signature)\n"",
        ""  \n"",
        ""  def decorator(func):\n"",
        ""    @functools.wraps(func)\n"",
        ""    def wrapper(*args, axis=None):\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        verify_axis_is_supported(input_core_dims, output_core_dims)\n"",
        ""        args = reorder_inputs(args, axis, input_core_dims)\n"",
        ""\n"",
        ""      boardcast_args = broadcast_with_core_dims(\n"",
        ""          args, input_core_dims, output_core_dims)\n"",
        ""      num_batch_dims = len(boardcast_args[0].shape) - len(input_core_dims[0])\n"",
        ""\n"",
        ""      vectorized_func = func\n"",
        ""      for _ in range(num_batch_dims):\n"",
        ""        vectorized_func = curried_vmap(vectorized_func)\n"",
        ""      result = vectorized_func(*boardcast_args)\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        result = reorder_outputs(result, axis, output_core_dims)\n"",
        ""\n"",
        ""      return result\n"",
        ""    return wrapper\n"",
        ""  return decorator""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""EWDCFZiqmY9A"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Test cases\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""W-jCowsgj_Tg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### matmul""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""gSJ7G_da4ArE"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""matmat = vectorize('(n,m),(m,k)->(n,k)')(jnp.dot)\n"",
        ""matvec = vectorize('(n,m),(m)->(n)')(jnp.dot)\n"",
        ""vecmat = vectorize('(m),(m,k)->(k)')(jnp.dot)\n"",
        ""vecvec = vectorize('(m),(m)->()')(jnp.dot)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""CI-vJzjMfPXS"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert matmat(np.zeros((2, 3)), np.zeros((3, 4))).shape == (2, 4)\n"",
        ""assert matmat(np.zeros((2, 3)), np.zeros((1, 3, 4))).shape == (1, 2, 4)\n"",
        ""assert matmat(np.zeros((5, 2, 3)), np.zeros((1, 3, 4))).shape == (5, 2, 4)\n"",
        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
        ""# assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
        ""\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((1, 3))).shape == (1, 2)\n"",
        ""assert matvec(np.zeros((4, 2, 3)), np.zeros((1, 3))).shape == (4, 2)\n"",
        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
        ""# assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
        ""\n"",
        ""assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()\n"",
        ""# these raise: AssertionError -- assert False  # unreachable\n"",
        ""# assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""# assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2) ""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""u5xKzwoRkKuR"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### magnitude""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""Rcbol3OHkKUQ"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->()')\n"",
        ""def magnitude(x):\n"",
        ""  return jnp.dot(x, x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""DBtX_QDwkMbI"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert magnitude(np.arange(3.0)).shape == ()\n"",
        ""# these also raise (\""unreachable\"")\n"",
        ""# assert magnitude(np.arange(6.0).reshape(2, 3)).shape == (2,)\n"",
        ""# assert magnitude(np.arange(6.0).reshape(1, 2, 3)).shape == (1, 2,)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""LFlyTMg0kCm5"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### mean""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""m5HrLVmehaHx"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""mean = vectorize('(n)->()')(jnp.mean)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""QBtnkLwnhhJY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert mean(np.zeros((3,))).shape == ()\n"",
        ""assert mean(np.zeros((2, 3,))).shape == (2,)\n"",
        ""assert mean(np.zeros((2, 3,)), axis=0).shape == (3,)\n"",
        ""assert mean(np.zeros((1, 2, 3, 4))).shape == (1, 2, 3)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""hAMhRkK4kEzw"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### center""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""bDsjXm7MitcX"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  bias = jnp.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""MSyxOrUPixDI"",
        ""colab_type"": ""code"",
        ""outputId"": ""e8d05c0c-a337-4a9c-bdc8-8645616b4b93"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 35
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(jnp.arange(3))\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 13,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[-1.  0.  1.] 1.0\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""7UZGOS8FGT_D"",
        ""colab_type"": ""code"",
        ""outputId"": ""fdb7dad9-721c-439a-ffd6-c469445f371a"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""X = jnp.arange(12).reshape(3, 4)\n"",
        ""X""
      ],
      ""execution_count"": 14,
      ""outputs"": [
        {
          ""output_type"": ""execute_result"",
          ""data"": {
            ""text/plain"": [
              ""array([[ 0,  1,  2,  3],\n"",
              ""       [ 4,  5,  6,  7],\n"",
              ""       [ 8,  9, 10, 11]])""
            ]
          },
          ""metadata"": {
            ""tags"": []
          },
          ""execution_count"": 14
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""n2Fz91ptjM_7"",
        ""colab_type"": ""code"",
        ""outputId"": ""0a75386a-8238-4fca-c0c8-1d8ac9573e48"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=1)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 15,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]] [1.5 5.5 9.5]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""G-AyCkAK4RKT"",
        ""colab_type"": ""code"",
        ""outputId"": ""b4738597-d189-43b9-ad52-67a527b4c179"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=0)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 16,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-4. -4. -4. -4.]\n"",
            "" [ 0.  0.  0.  0.]\n"",
            "" [ 4.  4.  4.  4.]] [4. 5. 6. 7.]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""_FhnjYMUjZgI"",
        ""colab_type"": ""code"",
        ""outputId"": ""b81b93ac-5f6a-43ce-ce85-17d6c237891a"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# NOTE: using the wrapped function directly silently gives the wrong result!\n"",
        ""b, a = center.__wrapped__(X)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 17,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-5.5 -4.5 -3.5 -2.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [ 2.5  3.5  4.5  5.5]] 5.5\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""1EeD7aFdENt8"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        """"
      ],
      ""execution_count"": 0,
      ""outputs"": []
    }
  ]
}","{
  ""nbformat"": 4,
  ""nbformat_minor"": 0,
  ""metadata"": {
    ""colab"": {
      ""name"": ""JAX generalized ufuncs.ipynb"",
      ""version"": ""0.3.2"",
      ""provenance"": [],
      ""collapsed_sections"": []
    },
    ""kernelspec"": {
      ""name"": ""python3"",
      ""display_name"": ""Python 3""
    },
    ""accelerator"": ""GPU""
  },
  ""cells"": [
    {
      ""metadata"": {
        ""id"": ""435_M09vl3NA"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""# Extending JAX's vmap to work like NumPy's gufuncs\n"",
        ""\n"",
        ""by [Stephan Hoyer](https://github.com/shoyer)\n"",
        ""\n"",
        ""## What is a gufunc?\n"",
        ""\n"",
        ""[Generalized universal functions](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
        ""- \""core dimensions\"" over which an operation is defined.\n"",
        ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
        ""\n"",
        ""A string [signature](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html#details-of-signature) associated with each gufunc controls how this happens by indicating how core dimensions are mapped between inputs and outputs. The syntax is easiest to understand by looking at a few examples:\n"",
        ""\n"",
        ""- Addition: `(),()->()`\n"",
        ""- 1D inner product: `(i),(i)->()`\n"",
        ""- 1D sum: `(i)->()`\n"",
        ""- Matrix multiplcation: `(m,n),(n,k)->(m,k)`\n"",
        ""\n"",
        ""## Why write gufuncs?\n"",
        ""\n"",
        ""From a user perspective, gufuncs are nice because they're guaranteed to vectorize in a consistent and general fashion. For example, by default gufuncs use the last dimensions of arrays as core dimensions, but you can control that explicitly with the `axis` or `axes` arguments.\n"",
        ""\n"",
        ""From a developer perspective, gufuncs are nice because they simply your work: you only need to think about the core logic of your function, not how it handles arbitrary dimensional input. You can just write that down in a simple, declarative way.\n"",
        ""\n"",
        ""## JAX makes it easy to write high-level performant code\n"",
        ""\n"",
        ""Unfortunately, writing NumPy gufuncs today is somewhat non-trivial. Your options today are:\n"",
        ""\n"",
        ""1. Write the inner loops yourself in C.\n"",
        ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differention.\n"",
        ""\n"",
        ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
        ""\n"",
        ""## A simple example\n"",
        ""\n"",
        ""Consider a simple example from data preprocessing, centering an array.\n"",
        ""\n"",
        ""Here's how we might write a vectorized version using NumPy:\n"",
        ""```python\n"",
        ""def center(array, axis=-1):\n"",
        ""  # array can have any number of dimensions\n"",
        ""  bias = np.mean(array, axis=axis)\n"",
        ""  debiased = array - np.expand_dims(bias, axis)\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""And here's how we could write a vectorized version using JAX gufuncs:\n"",
        ""```python\n"",
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  # array is always a 1D vector\n"",
        ""  bias = np.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""See the difference?\n"",
        ""- Instead of needing to think about broadcasting while writing the entire function, we can write the function assuming the input is always a vector.\n"",
        ""- We get the `axis` argument automatically, without needing to write it ourselves.\n"",
        ""- As a bonus, the decorator makes the function self-documenting: a reader immediately knows that it handles higher dimensional input and output correctly.\n"",
        ""\n"",
        ""For more examples (and the implementation) see below.""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""k40qkuQdkqFg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Implementation\n"",
        ""\n"",
        ""### License\n"",
        ""\n"",
        ""Copyright 2018 Google LLC.\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
        ""\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n"",
        ""\n"",
        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
        ""\n"",
        ""Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""4QrBJNYG5ECU"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Setup and imports""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""2NXj3Dp5270W"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
        ""!pip install -q jax""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""p-rYDdqL1uZP"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import grad, jit, vmap\n"",
        ""import jax.numpy as jnp\n"",
        ""import numpy as np\n"",
        ""import re""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""tU2rwIOZmT0Q"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Copied from `numpy.lib.function_base`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""lBVIP3O2kkqY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# See http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n"",
        ""_DIMENSION_NAME = r'\\w+'\n"",
        ""_CORE_DIMENSION_LIST = '(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)\n"",
        ""_ARGUMENT = r'\\({}\\)'.format(_CORE_DIMENSION_LIST)\n"",
        ""_ARGUMENT_LIST = '{0:}(?:,{0:})*'.format(_ARGUMENT)\n"",
        ""_SIGNATURE = '^{0:}->{0:}$'.format(_ARGUMENT_LIST)\n"",
        ""\n"",
        ""\n"",
        ""def _parse_gufunc_signature(signature):\n"",
        ""    \""\""\""\n"",
        ""    Parse string signatures for a generalized universal function.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    signature : string\n"",
        ""        Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``\n"",
        ""        for ``np.matmul``.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    Tuple of input and output core dimensions parsed from the signature, each\n"",
        ""    of the form List[Tuple[str, ...]].\n"",
        ""    \""\""\""\n"",
        ""    if not re.match(_SIGNATURE, signature):\n"",
        ""        raise ValueError(\n"",
        ""            'not a valid gufunc signature: {}'.format(signature))\n"",
        ""    return tuple([tuple(re.findall(_DIMENSION_NAME, arg))\n"",
        ""                  for arg in re.findall(_ARGUMENT, arg_list)]\n"",
        ""                 for arg_list in signature.split('->'))\n"",
        ""\n"",
        ""\n"",
        ""\n"",
        ""def _update_dim_sizes(dim_sizes, arg, core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Incrementally check and update core dimension sizes for a single argument.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Sizes of existing core dimensions. Will be updated in-place.\n"",
        ""    arg : ndarray\n"",
        ""        Argument to examine.\n"",
        ""    core_dims : Tuple[str, ...]\n"",
        ""        Core dimensions for this argument.\n"",
        ""    \""\""\""\n"",
        ""    if not core_dims:\n"",
        ""        return\n"",
        ""\n"",
        ""    num_core_dims = len(core_dims)\n"",
        ""    if arg.ndim < num_core_dims:\n"",
        ""        raise ValueError(\n"",
        ""            '%d-dimensional argument does not have enough '\n"",
        ""            'dimensions for all core dimensions %r'\n"",
        ""            % (arg.ndim, core_dims))\n"",
        ""\n"",
        ""    core_shape = arg.shape[-num_core_dims:]\n"",
        ""    for dim, size in zip(core_dims, core_shape):\n"",
        ""        if dim in dim_sizes:\n"",
        ""            if size != dim_sizes[dim]:\n"",
        ""                raise ValueError(\n"",
        ""                    'inconsistent size for core dimension %r: %r vs %r'\n"",
        ""                    % (dim, size, dim_sizes[dim]))\n"",
        ""        else:\n"",
        ""            dim_sizes[dim] = size\n"",
        ""\n"",
        ""\n"",
        ""def _parse_input_dimensions(args, input_core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Parse broadcast and core dimensions for vectorize with a signature.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    args : Tuple[ndarray, ...]\n"",
        ""        Tuple of input arguments to examine.\n"",
        ""    input_core_dims : List[Tuple[str, ...]]\n"",
        ""        List of core dimensions corresponding to each input.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    broadcast_shape : Tuple[int, ...]\n"",
        ""        Common shape to broadcast all non-core dimensions to.\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Common sizes for named core dimensions.\n"",
        ""    \""\""\""\n"",
        ""    broadcast_args = []\n"",
        ""    dim_sizes = {}\n"",
        ""    for arg, core_dims in zip(args, input_core_dims):\n"",
        ""        _update_dim_sizes(dim_sizes, arg, core_dims)\n"",
        ""        ndim = arg.ndim - len(core_dims)\n"",
        ""        dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])\n"",
        ""        broadcast_args.append(dummy_array)\n"",
        ""    broadcast_shape = np.lib.stride_tricks._broadcast_shape(*broadcast_args)\n"",
        ""    return broadcast_shape, dim_sizes\n"",
        ""\n"",
        ""\n"",
        ""def _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):\n"",
        ""    \""\""\""Helper for calculating broadcast shapes with core dimensions.\""\""\""\n"",
        ""    return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)\n"",
        ""            for core_dims in list_of_core_dims]\n"",
        ""\n"",
        ""  \n"",
        ""# adapted from np.vectorize (again authored by shoyer@)\n"",
        ""def broadcast_with_core_dims(args, input_core_dims, output_core_dims):\n"",
        ""  if len(args) != len(input_core_dims):\n"",
        ""    raise TypeError('wrong number of positional arguments: '\n"",
        ""                    'expected %r, got %r'\n"",
        ""                    % (len(input_core_dims), len(args)))\n"",
        ""\n"",
        ""  broadcast_shape, dim_sizes = _parse_input_dimensions(\n"",
        ""      args, input_core_dims)\n"",
        ""  input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,\n"",
        ""                                   input_core_dims)\n"",
        ""  args = [jnp.broadcast_to(arg, shape)\n"",
        ""          for arg, shape in zip(args, input_shapes)]\n"",
        ""  return args""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""aa_Gh3K_PQkY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Handle the `axis` argument""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""MLeFHhVoPT4h"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def verify_axis_is_supported(input_core_dims, output_core_dims):\n"",
        ""  all_core_dims = set()\n"",
        ""  for input_or_output_core_dims in [input_core_dims, output_core_dims]:\n"",
        ""    for core_dims in input_or_output_core_dims:\n"",
        ""      all_core_dims.update(core_dims)\n"",
        ""  if len(core_dims) > 1:\n"",
        ""    raise ValueError('only one gufuncs with one core dim support axis')\n"",
        ""\n"",
        ""\n"",
        ""def reorder_inputs(args, axis, input_core_dims):\n"",
        ""  return tuple(jnp.moveaxis(arg, axis, -1) if core_dims else arg\n"",
        ""               for arg, core_dims in zip(args, input_core_dims))\n"",
        ""\n"",
        ""\n"",
        ""def reorder_outputs(result, axis, output_core_dims):\n"",
        ""  if not isinstance(result, tuple):\n"",
        ""    result = (result,)\n"",
        ""  result = tuple(jnp.moveaxis(res, -1, axis) if core_dims else res\n"",
        ""                 for res, core_dims in zip(result, output_core_dims))\n"",
        ""  if len(result) == 1:\n"",
        ""    (result,) = result\n"",
        ""  return result""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""Uik9GA76lZjY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Core implementation\n"",
        ""\n"",
        ""This is the only part that uses `vmap`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""z-FgQaW02_WN"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""import functools\n"",
        ""\n"",
        ""def curried_vmap(func):\n"",
        ""  @functools.wraps(func)\n"",
        ""  def wrapper(*args):\n"",
        ""    return vmap(func, *args)\n"",
        ""  return wrapper\n"",
        ""\n"",
        ""\n"",
        ""def vectorize(signature):\n"",
        ""  \""\""\""Vectorize a function using JAX.\""\""\""\n"",
        ""  input_core_dims, output_core_dims = _parse_gufunc_signature(signature)\n"",
        ""  \n"",
        ""  def decorator(func):\n"",
        ""    @functools.wraps(func)\n"",
        ""    def wrapper(*args, axis=None):\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        verify_axis_is_supported(input_core_dims, output_core_dims)\n"",
        ""        args = reorder_inputs(args, axis, input_core_dims)\n"",
        ""\n"",
        ""      boardcast_args = broadcast_with_core_dims(\n"",
        ""          args, input_core_dims, output_core_dims)\n"",
        ""      num_batch_dims = len(boardcast_args[0].shape) - len(input_core_dims[0])\n"",
        ""\n"",
        ""      vectorized_func = func\n"",
        ""      for _ in range(num_batch_dims):\n"",
        ""        vectorized_func = curried_vmap(vectorized_func)\n"",
        ""      result = vectorized_func(*boardcast_args)\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        result = reorder_outputs(result, axis, output_core_dims)\n"",
        ""\n"",
        ""      return result\n"",
        ""    return wrapper\n"",
        ""  return decorator""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""EWDCFZiqmY9A"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Test cases\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""W-jCowsgj_Tg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### matmul""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""gSJ7G_da4ArE"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""matmat = vectorize('(n,m),(m,k)->(n,k)')(jnp.dot)\n"",
        ""matvec = vectorize('(n,m),(m)->(n)')(jnp.dot)\n"",
        ""vecmat = vectorize('(m),(m,k)->(k)')(jnp.dot)\n"",
        ""vecvec = vectorize('(m),(m)->()')(jnp.dot)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""CI-vJzjMfPXS"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert matmat(np.zeros((2, 3)), np.zeros((3, 4))).shape == (2, 4)\n"",
        ""assert matmat(np.zeros((2, 3)), np.zeros((1, 3, 4))).shape == (1, 2, 4)\n"",
        ""assert matmat(np.zeros((5, 2, 3)), np.zeros((1, 3, 4))).shape == (5, 2, 4)\n"",
        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
        ""# assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
        ""\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((1, 3))).shape == (1, 2)\n"",
        ""assert matvec(np.zeros((4, 2, 3)), np.zeros((1, 3))).shape == (4, 2)\n"",
        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
        ""# assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
        ""\n"",
        ""assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()\n"",
        ""# these raise: AssertionError -- assert False  # unreachable\n"",
        ""# assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""# assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2) ""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""u5xKzwoRkKuR"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### magnitude""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""Rcbol3OHkKUQ"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->()')\n"",
        ""def magnitude(x):\n"",
        ""  return jnp.dot(x, x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""DBtX_QDwkMbI"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert magnitude(np.arange(3.0)).shape == ()\n"",
        ""# these also raise (\""unreachable\"")\n"",
        ""# assert magnitude(np.arange(6.0).reshape(2, 3)).shape == (2,)\n"",
        ""# assert magnitude(np.arange(6.0).reshape(1, 2, 3)).shape == (1, 2,)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""LFlyTMg0kCm5"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### mean""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""m5HrLVmehaHx"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""mean = vectorize('(n)->()')(jnp.mean)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""QBtnkLwnhhJY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert mean(np.zeros((3,))).shape == ()\n"",
        ""assert mean(np.zeros((2, 3,))).shape == (2,)\n"",
        ""assert mean(np.zeros((2, 3,)), axis=0).shape == (3,)\n"",
        ""assert mean(np.zeros((1, 2, 3, 4))).shape == (1, 2, 3)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""hAMhRkK4kEzw"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### center""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""bDsjXm7MitcX"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  bias = jnp.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""MSyxOrUPixDI"",
        ""colab_type"": ""code"",
        ""outputId"": ""e8d05c0c-a337-4a9c-bdc8-8645616b4b93"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 35
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(jnp.arange(3))\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 13,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[-1.  0.  1.] 1.0\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""7UZGOS8FGT_D"",
        ""colab_type"": ""code"",
        ""outputId"": ""fdb7dad9-721c-439a-ffd6-c469445f371a"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""X = jnp.arange(12).reshape(3, 4)\n"",
        ""X""
      ],
      ""execution_count"": 14,
      ""outputs"": [
        {
          ""output_type"": ""execute_result"",
          ""data"": {
            ""text/plain"": [
              ""array([[ 0,  1,  2,  3],\n"",
              ""       [ 4,  5,  6,  7],\n"",
              ""       [ 8,  9, 10, 11]])""
            ]
          },
          ""metadata"": {
            ""tags"": []
          },
          ""execution_count"": 14
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""n2Fz91ptjM_7"",
        ""colab_type"": ""code"",
        ""outputId"": ""0a75386a-8238-4fca-c0c8-1d8ac9573e48"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=1)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 15,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]] [1.5 5.5 9.5]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""G-AyCkAK4RKT"",
        ""colab_type"": ""code"",
        ""outputId"": ""b4738597-d189-43b9-ad52-67a527b4c179"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=0)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 16,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-4. -4. -4. -4.]\n"",
            "" [ 0.  0.  0.  0.]\n"",
            "" [ 4.  4.  4.  4.]] [4. 5. 6. 7.]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""_FhnjYMUjZgI"",
        ""colab_type"": ""code"",
        ""outputId"": ""b81b93ac-5f6a-43ce-ce85-17d6c237891a"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 71
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# NOTE: using the wrapped function directly silently gives the wrong result!\n"",
        ""b, a = center.__wrapped__(X)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 17,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-5.5 -4.5 -3.5 -2.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [ 2.5  3.5  4.5  5.5]] 5.5\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""1EeD7aFdENt8"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        """"
      ],
      ""execution_count"": 0,
      ""outputs"": []
    }
  ]
}
","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 84315e1a24139f73ef90644d4a616ed03ac344b5..51895394266fe382f9d4beb39f64eb1a6543eeed 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -28,7 +28,7 @@
         ""\n"",
         ""## What is a gufunc?\n"",
         ""\n"",
-        ""[Generalized universal functions]((https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
+        ""[Generalized universal functions](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
         ""- \""core dimensions\"" over which an operation is defined.\n"",
         ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
         ""\n"",
@@ -717,4 +717,4 @@
       ""outputs"": []
     }
   ]
-}
\ No newline at end of file
+}
",Incorrect caching / stale cache,Fix link formatting in gufuncs.ipynb notebook
30124b6da1eca136a026eb40c94f8fad6885a467,Dougal Maclaurin: Added jit transformations to generated functions. Fixed bug in comparing numpy arrays for equality.,jax/util.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import itertools as it
from operator import mul

allow_memoize_hash_failures = False


def safe_zip(*args):
  n = len(args[0])
  for arg in args[1:]:
    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
  return list(zip(*args))


def safe_map(f, *args):
  args = list(map(list, args))
  n = len(args[0])
  for arg in args[1:]:
    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
  return list(map(f, *args))


def unzip2(xys):
  xs = []
  ys = []
  for x, y in xys:
    xs.append(x)
    ys.append(y)
  return tuple(xs), tuple(ys)


def unzip3(xyzs):
  xs = []
  ys = []
  zs = []
  for x, y, z in xyzs:
    xs.append(x)
    ys.append(y)
    zs.append(z)
  return tuple(xs), tuple(ys), tuple(zs)


def concatenate(xs):
  return list(it.chain.from_iterable(xs))


def partial(fun, *args, **kwargs):
  wrapped = functools.partial(fun, *args, **kwargs)
  functools.update_wrapper(wrapped, fun)
  wrapped._bound_args = args
  return wrapped

class partialmethod(functools.partial):
  def __get__(self, instance, owner):
    if instance is None:
      return self
    else:
      return partial(self.func, instance,
                     *(self.args or ()), **(self.keywords or {}))

def curry(f):
  return partial(partial, f)

def toposort(end_node):
  child_counts = {}
  stack = [end_node]
  while stack:
    node = stack.pop()
    if id(node) in child_counts:
      child_counts[id(node)] += 1
    else:
      child_counts[id(node)] = 1
      stack.extend(node.parents)

  sorted_nodes = []
  childless_nodes = [end_node]
  while childless_nodes:
    node = childless_nodes.pop()
    sorted_nodes.append(node)
    for parent in node.parents:
      if child_counts[id(parent)] == 1:
        childless_nodes.append(parent)
      else:
        child_counts[id(parent)] -= 1

  return sorted_nodes[::-1]


def split_merge(predicate, xs):
  sides = list(map(predicate, xs))
  lhs = [x for x, s in zip(xs, sides) if s]
  rhs = [x for x, s in zip(xs, sides) if not s]
  def merge(new_lhs, new_rhs):
    out = []
    for s in sides:
      if s:
        out.append(new_lhs[0])
        new_lhs = new_lhs[1:]
      else:
        out.append(new_rhs[0])
        new_rhs = new_rhs[1:]
    assert not new_rhs
    assert not new_lhs
    return out

  return lhs, rhs, merge


def memoize(fun):
  cache = {}
  def memoized_fun(*args, **kwargs):
    key = (args, tuple(kwargs and sorted(kwargs.items())))
    try:
      return cache[key]
    except KeyError:
      ans = cache[key] = fun(*args, **kwargs)
      return ans
    except TypeError:
      if allow_memoize_hash_failures:
        return fun(*args, **kwargs)
      else:
        raise
  return memoized_fun


def prod(xs):
  return functools.reduce(mul, xs, 1)


class WrapHashably(object):
  def __init__(self, val):
    self.val = val

  def __hash__(self):
    return id(self.val)

  def __eq__(self, other):
    return self.val == other.val

","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import itertools as it
from operator import mul

allow_memoize_hash_failures = False


def safe_zip(*args):
  n = len(args[0])
  for arg in args[1:]:
    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
  return list(zip(*args))


def safe_map(f, *args):
  args = list(map(list, args))
  n = len(args[0])
  for arg in args[1:]:
    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
  return list(map(f, *args))


def unzip2(xys):
  xs = []
  ys = []
  for x, y in xys:
    xs.append(x)
    ys.append(y)
  return tuple(xs), tuple(ys)


def unzip3(xyzs):
  xs = []
  ys = []
  zs = []
  for x, y, z in xyzs:
    xs.append(x)
    ys.append(y)
    zs.append(z)
  return tuple(xs), tuple(ys), tuple(zs)


def concatenate(xs):
  return list(it.chain.from_iterable(xs))


def partial(fun, *args, **kwargs):
  wrapped = functools.partial(fun, *args, **kwargs)
  functools.update_wrapper(wrapped, fun)
  wrapped._bound_args = args
  return wrapped

class partialmethod(functools.partial):
  def __get__(self, instance, owner):
    if instance is None:
      return self
    else:
      return partial(self.func, instance,
                     *(self.args or ()), **(self.keywords or {}))

def curry(f):
  return partial(partial, f)

def toposort(end_node):
  child_counts = {}
  stack = [end_node]
  while stack:
    node = stack.pop()
    if id(node) in child_counts:
      child_counts[id(node)] += 1
    else:
      child_counts[id(node)] = 1
      stack.extend(node.parents)

  sorted_nodes = []
  childless_nodes = [end_node]
  while childless_nodes:
    node = childless_nodes.pop()
    sorted_nodes.append(node)
    for parent in node.parents:
      if child_counts[id(parent)] == 1:
        childless_nodes.append(parent)
      else:
        child_counts[id(parent)] -= 1

  return sorted_nodes[::-1]


def split_merge(predicate, xs):
  sides = list(map(predicate, xs))
  lhs = [x for x, s in zip(xs, sides) if s]
  rhs = [x for x, s in zip(xs, sides) if not s]
  def merge(new_lhs, new_rhs):
    out = []
    for s in sides:
      if s:
        out.append(new_lhs[0])
        new_lhs = new_lhs[1:]
      else:
        out.append(new_rhs[0])
        new_rhs = new_rhs[1:]
    assert not new_rhs
    assert not new_lhs
    return out

  return lhs, rhs, merge


def memoize(fun):
  cache = {}
  def memoized_fun(*args, **kwargs):
    key = (args, tuple(kwargs and sorted(kwargs.items())))
    try:
      return cache[key]
    except KeyError:
      ans = cache[key] = fun(*args, **kwargs)
      return ans
    except TypeError:
      if allow_memoize_hash_failures:
        return fun(*args, **kwargs)
      else:
        raise
  return memoized_fun


def prod(xs):
  return functools.reduce(mul, xs, 1)


class WrapHashably(object):
  def __init__(self, val):
    self.val = val

  def __hash__(self):
    return id(self.val)

  def __eq__(self, other):
    return self.val is other.val
","diff --git a/jax/util.py b/jax/util.py
index 38d22f00f8047c0947f1a6270749930a1237ae3c..ea686229bcc35c6fa4ca2755c3949e8752ba123b 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -153,5 +153,4 @@ class WrapHashably(object):
     return id(self.val)
 
   def __eq__(self, other):
-    return self.val == other.val
-
+    return self.val is other.val
",Typo / wrong variable name,"Change `__eq__` in `WrapHashably` to compare objects by identity, not value."
30124b6da1eca136a026eb40c94f8fad6885a467,Dougal Maclaurin: Added jit transformations to generated functions. Fixed bug in comparing numpy arrays for equality.,tests/generated_fun_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from collections import namedtuple
from functools import partial
import numpy.random as npr

from absl.testing import absltest
from absl.testing import parameterized

import itertools as it
import jax.numpy as np
from jax import jit, jvp, vjp
import jax.test_util as jtu
from jax.config import config

config.parse_flags_with_absl()

npr.seed(0)

from jax.util import unzip2, safe_zip, safe_map

map = safe_map
zip = safe_zip

subfun_prob = 0.5
thin_prob = 0.1
size_reduction_factor = 3

Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
Prim = namedtuple('Prim', ['fun'])
ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
Var = namedtuple('Var', ['name', 'vartype'])
Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])

def gen_fun_and_types(size):
  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
  fun, _ = gen_function(size, in_types)
  return fun

def gen_function(size, in_types):
  eqns = []
  in_vars = map(fresh_var, in_types)
  cur_vars = in_vars[:]
  for _ in range(gen_nonneg_int(size)):
    if not cur_vars:
      break
    if npr.rand() < subfun_prob:
      arg_vars = gen_subset(cur_vars)
      arg_types = [v.vartype for v in arg_vars]
      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
      fun = partial(eval_fun, fun)
    else:
      arity = choice(list(primitive_generators))
      arg_vars = gen_sized_subset(cur_vars, arity)
      arg_types = [v.vartype for v in arg_vars]
      prim_gen = weighted_choice(primitive_generators[arity])
      fun, out_type = prim_gen(size, *arg_types)
      fun = wrap_singleton(fun)
      out_types = [out_type]

    out_vars = map(fresh_var, out_types)
    eqns.append(Eqn(arg_vars, out_vars, fun))
    cur_vars.extend(out_vars)
    cur_vars = thin(cur_vars, thin_prob)

  out_vars = gen_subset(cur_vars)
  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]

def eval_fun(fun, *args):
  def read(v):
    return env[v]
  def write(v, x):
    env[v] = x

  env = {}
  map(write, fun.in_vars, args)
  for in_vars, out_vars, f in fun.eqns:
    out_vals = f(*map(read, in_vars))
    map(write, out_vars, out_vals)

  return map(read, fun.out_vars)

counter = it.count()
def fresh_var(ty):
  return Var(next(counter), ty)

def gen_array_type(size):
  # TODO(dougalm): randomize this
  return ArrayType((2,2), np.float32)

def gen_array_val(array_type):
  # TODO(dougalm): different sizes and dtypes
  return npr.randn(*array_type.shape)

def gen_neg(size, t):
  return (lambda x: -x), t

def gen_trig(size, t):
  op = choice([np.sin, np.cos])
  return op, t

def gen_binop(size, t1, t2):
  unifier, t_out = gen_broadcasting_unifier(t1, t2)
  binop = choice([lambda x, y: x + y,
                  lambda x, y: x * y])
  def unify_and_binop(x, y):
    x_, y_ = unifier(x, y)
    return binop(x_, y_)

  return unify_and_binop, t_out

def thin(xs, p):
  return [x for x in xs if npr.rand() > p]

def gen_broadcasting_unifier(t1, t2):
  assert t1.shape == t2.shape
  return lambda x, y: (x,y), t1
  # TODO: generate slices and paddings to match shapes

def wrap_singleton(f):
  return lambda *xs: (f(*xs),)

unary_primitive_generators = [
  (3, gen_trig),
  (1, gen_neg) ]

binary_primitive_generators = [
  (1, gen_binop)]

primitive_generators = { 1: unary_primitive_generators,
                         2: binary_primitive_generators }

def gen_nonneg_int(size):
  return npr.randint(size)

def choice(xs, weights=None):
  # npr.choice isn't actually RS -> [a] -> a
  # because it inspects the components to see if they're array-like
  assert xs
  n = len(xs)
  if weights is None:
    i = npr.randint(n)
  else:
    normalizer = float(sum(weights))
    weights = [w / normalizer for w in weights]
    i = npr.choice(range(n), p=weights)
  return xs[i]

def weighted_choice(weighted_choices):
  weights, choices = unzip2(weighted_choices)
  return choice(choices, weights)

def gen_sized_subset(xs, size):
  return [choice(xs) for _ in range(size)]

def gen_subset(xs):
  if not xs:
    return []

  return gen_sized_subset(xs, npr.randint(len(xs) + 1))

def gen_vals(vs):
  return [gen_array_val(v.vartype) for v in vs]

def inner_prod(xs, ys):
  xys = zip(xs, ys)
  assert all(x.shape == y.shape for x, y in xys)
  return sum(np.sum(x * y) for x, y in xys)

def jvp_fd(fun, args, tangents):
  EPS = 1e-4
  def eval_eps(eps):
    return fun(*[x if t is None else x + eps * t
                 for x, t in zip(args, tangents)])

  ys_neg = eval_eps(-EPS)
  ys_pos = eval_eps(EPS)
  ys = eval_eps(0.0)
  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
  return ys, deriv

def check_all_close(xs, ys, tol=1e-3):
  for x, y in zip(xs, ys):
    check_close(x, y, tol)

def check_close(x, y, tol=1e-3):
  assert np.shape(x) == np.shape(y)
  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
  # assert x.dtype == y.dtype
  assert np.allclose(x, y, rtol=tol, atol=tol), \
     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)

def partial_argnums(f, args, dyn_argnums):
  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
  def f_(*dyn_args):
    args = fixed_args[:]
    for i, arg in zip(dyn_argnums, dyn_args):
      args[i] = arg
    return f(*args)

  dyn_args = [args[i] for i in dyn_argnums]
  return f_, dyn_args


class GeneratedFunTest(jtu.JaxTestCase):
  """"""Tests of transformations on randomly generated functions.""""""

  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
  def testJitIsIdentity(self, fun):
    vals = gen_vals(fun.in_vars)
    fun = partial(eval_fun, fun)
    ans = fun(*vals)
    static_argnums = thin(range(len(vals)), 0.5)
    ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
    try:
      check_all_close(ans, ans_jitted)
    except:
      print(fun)
      raise

  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
  def testJVPMatchesFD(self, fun):
    vals = gen_vals(fun.in_vars)
    tangents = gen_vals(fun.in_vars)
    fun = partial(eval_fun, fun)
    dyn_argnums = thin(range(len(vals)), 0.5)
    tangents = [tangents[i] for i in dyn_argnums]
    fun, vals = partial_argnums(fun, vals, dyn_argnums)
    ans1, deriv1 = jvp_fd(fun, vals, tangents)
    ans2, deriv2 = jvp(fun, vals, tangents)
    check_all_close(ans1, ans2)
    check_all_close(deriv1, deriv2)

  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
  def vjp_matches_fd(self, fun):
    vals = gen_vals(fun.in_vars)
    in_tangents = gen_vals(fun.in_vars)
    in_cotangents = gen_vals(fun.out_vars)
    fun = partial(eval_fun, fun)
    dyn_argnums = thin(range(len(vals)), 0.5)
    in_tangents = [in_tangents[i] for i in dyn_argnums]
    fun, vals = partial_argnums(fun, vals, dyn_argnums)
    ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
    ans2, vjpfun = vjp(fun, *vals)
    out_cotangents = vjpfun(in_cotangents)
    check_all_close(ans1, ans2)
    inner_prod_fd = inner_prod(out_tangents, in_cotangents)
    inner_prod_ad = inner_prod(in_tangents, out_cotangents)
    check_close(inner_prod_fd, inner_prod_ad)


if __name__ == ""__main__"":
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from collections import namedtuple
from functools import partial
import numpy.random as npr

from absl.testing import absltest
from absl.testing import parameterized

import itertools as it
import jax.numpy as np
from jax import jit, jvp, vjp
import jax.test_util as jtu
from jax.config import config

config.parse_flags_with_absl()

npr.seed(0)

from jax.util import unzip2, safe_zip, safe_map

map = safe_map
zip = safe_zip

subfun_prob = 0.5
thin_prob = 0.1
size_reduction_factor = 3

Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
Prim = namedtuple('Prim', ['fun'])
ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
Var = namedtuple('Var', ['name', 'vartype'])
Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])

def gen_fun_and_types(size):
  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
  fun, _ = gen_function(size, in_types)
  return fun

def gen_function(size, in_types):
  eqns = []
  in_vars = map(fresh_var, in_types)
  cur_vars = in_vars[:]
  for _ in range(gen_nonneg_int(size)):
    if not cur_vars:
      break
    if npr.rand() < subfun_prob:
      arg_vars = gen_subset(cur_vars)
      arg_types = [v.vartype for v in arg_vars]
      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
      fun = partial(eval_fun, fun)
      fun = maybe_jit(fun, len(arg_types))
    else:
      arity = choice(list(primitive_generators))
      arg_vars = gen_sized_subset(cur_vars, arity)
      arg_types = [v.vartype for v in arg_vars]
      prim_gen = weighted_choice(primitive_generators[arity])
      fun, out_type = prim_gen(size, *arg_types)
      fun = wrap_singleton(fun)
      out_types = [out_type]

    out_vars = map(fresh_var, out_types)
    eqns.append(Eqn(arg_vars, out_vars, fun))
    cur_vars.extend(out_vars)
    cur_vars = thin(cur_vars, thin_prob)

  out_vars = gen_subset(cur_vars)
  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]

def eval_fun(fun, *args):
  def read(v):
    return env[v]
  def write(v, x):
    env[v] = x

  env = {}
  map(write, fun.in_vars, args)
  for in_vars, out_vars, f in fun.eqns:
    out_vals = f(*map(read, in_vars))
    map(write, out_vars, out_vals)

  return map(read, fun.out_vars)

def maybe_jit(f, num_args):
  static_argnums = thin(range(num_args), 0.5)
  return jit(f, static_argnums=static_argnums)

counter = it.count()
def fresh_var(ty):
  return Var(next(counter), ty)

def gen_array_type(size):
  # TODO(dougalm): randomize this
  return ArrayType((2,2), np.float32)

def gen_array_val(array_type):
  # TODO(dougalm): different sizes and dtypes
  return npr.randn(*array_type.shape)

def gen_neg(size, t):
  return (lambda x: -x), t

def gen_trig(size, t):
  op = choice([np.sin, np.cos])
  return op, t

def gen_binop(size, t1, t2):
  unifier, t_out = gen_broadcasting_unifier(t1, t2)
  binop = choice([lambda x, y: x + y,
                  lambda x, y: x * y])
  def unify_and_binop(x, y):
    x_, y_ = unifier(x, y)
    return binop(x_, y_)

  return unify_and_binop, t_out

def thin(xs, p):
  return [x for x in xs if npr.rand() > p]

def gen_broadcasting_unifier(t1, t2):
  assert t1.shape == t2.shape
  return lambda x, y: (x,y), t1
  # TODO: generate slices and paddings to match shapes

def wrap_singleton(f):
  return lambda *xs: (f(*xs),)

unary_primitive_generators = [
  (3, gen_trig),
  (1, gen_neg) ]

binary_primitive_generators = [
  (1, gen_binop)]

primitive_generators = { 1: unary_primitive_generators,
                         2: binary_primitive_generators }

def gen_nonneg_int(size):
  return npr.randint(size)

def choice(xs, weights=None):
  # npr.choice isn't actually RS -> [a] -> a
  # because it inspects the components to see if they're array-like
  assert xs
  n = len(xs)
  if weights is None:
    i = npr.randint(n)
  else:
    normalizer = float(sum(weights))
    weights = [w / normalizer for w in weights]
    i = npr.choice(range(n), p=weights)
  return xs[i]

def weighted_choice(weighted_choices):
  weights, choices = unzip2(weighted_choices)
  return choice(choices, weights)

def gen_sized_subset(xs, size):
  return [choice(xs) for _ in range(size)]

def gen_subset(xs):
  if not xs:
    return []

  return gen_sized_subset(xs, npr.randint(len(xs) + 1))

def gen_vals(vs):
  return [gen_array_val(v.vartype) for v in vs]

def inner_prod(xs, ys):
  xys = zip(xs, ys)
  assert all(x.shape == y.shape for x, y in xys)
  return sum(np.sum(x * y) for x, y in xys)

def jvp_fd(fun, args, tangents):
  EPS = 1e-4
  def eval_eps(eps):
    return fun(*[x if t is None else x + eps * t
                 for x, t in zip(args, tangents)])

  ys_neg = eval_eps(-EPS)
  ys_pos = eval_eps(EPS)
  ys = eval_eps(0.0)
  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
  return ys, deriv

def check_all_close(xs, ys, tol=1e-3):
  for x, y in zip(xs, ys):
    check_close(x, y, tol)

def check_close(x, y, tol=1e-3):
  assert np.shape(x) == np.shape(y)
  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
  # assert x.dtype == y.dtype
  assert np.allclose(x, y, rtol=tol, atol=tol), \
     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)

def partial_argnums(f, args, dyn_argnums):
  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
  def f_(*dyn_args):
    args = fixed_args[:]
    for i, arg in zip(dyn_argnums, dyn_args):
      args[i] = arg
    return f(*args)

  dyn_args = [args[i] for i in dyn_argnums]
  return f_, dyn_args


class GeneratedFunTest(jtu.JaxTestCase):
  """"""Tests of transformations on randomly generated functions.""""""

  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
  def testJitIsIdentity(self, fun):
    vals = gen_vals(fun.in_vars)
    fun = partial(eval_fun, fun)
    ans = fun(*vals)
    static_argnums = thin(range(len(vals)), 0.5)
    ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
    try:
      check_all_close(ans, ans_jitted)
    except:
      print(fun)
      raise

  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
  def testJVPMatchesFD(self, fun):
    vals = gen_vals(fun.in_vars)
    tangents = gen_vals(fun.in_vars)
    fun = partial(eval_fun, fun)
    dyn_argnums = thin(range(len(vals)), 0.5)
    tangents = [tangents[i] for i in dyn_argnums]
    fun, vals = partial_argnums(fun, vals, dyn_argnums)
    ans1, deriv1 = jvp_fd(fun, vals, tangents)
    ans2, deriv2 = jvp(fun, vals, tangents)
    check_all_close(ans1, ans2)
    check_all_close(deriv1, deriv2)

  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
  def vjp_matches_fd(self, fun):
    vals = gen_vals(fun.in_vars)
    in_tangents = gen_vals(fun.in_vars)
    in_cotangents = gen_vals(fun.out_vars)
    fun = partial(eval_fun, fun)
    dyn_argnums = thin(range(len(vals)), 0.5)
    in_tangents = [in_tangents[i] for i in dyn_argnums]
    fun, vals = partial_argnums(fun, vals, dyn_argnums)
    ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
    ans2, vjpfun = vjp(fun, *vals)
    out_cotangents = vjpfun(in_cotangents)
    check_all_close(ans1, ans2)
    inner_prod_fd = inner_prod(out_tangents, in_cotangents)
    inner_prod_ad = inner_prod(in_tangents, out_cotangents)
    check_close(inner_prod_fd, inner_prod_ad)


if __name__ == ""__main__"":
  absltest.main()
","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 3fbc71ebd4dc342904017e77c27f1189c45baa4c..dec320953bb93fa0e28ed1db1500a9949eb95930 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -65,6 +65,7 @@ def gen_function(size, in_types):
       arg_types = [v.vartype for v in arg_vars]
       fun, out_types = gen_function(size / size_reduction_factor, arg_types)
       fun = partial(eval_fun, fun)
+      fun = maybe_jit(fun, len(arg_types))
     else:
       arity = choice(list(primitive_generators))
       arg_vars = gen_sized_subset(cur_vars, arity)
@@ -96,6 +97,10 @@ def eval_fun(fun, *args):
 
   return map(read, fun.out_vars)
 
+def maybe_jit(f, num_args):
+  static_argnums = thin(range(num_args), 0.5)
+  return jit(f, static_argnums=static_argnums)
+
 counter = it.count()
 def fresh_var(ty):
   return Var(next(counter), ty)
",Missing or broken tests,Add just-in-time compilation to generated functions with partial argument specialization.
10b61e08f76a34898471d0b5b52a36c19483ea03,Matthew Johnson: fix symbolic zero handling in concat transpose,jax/lax.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
#   assert False  # TODO

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')
  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])

  starts = onp.zeros((len(operands), t.ndim), dtype=int)
  starts[1:, dimension] = limit_points[:-1]
  limits = onp.tile(t.shape, (len(operands), 1))
  limits[:, dimension] = limit_points

  return [slice(t, start, limit) if o is None else None
          for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
#   assert False  # TODO

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')

  if t is ad_util.zero:
    return [ad_util.zero if o is None else None for o in operands]
  else:
    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
    starts = onp.zeros((len(operands), t.ndim), dtype=int)
    starts[1:, dimension] = limit_points[:-1]
    limits = onp.tile(t.shape, (len(operands), 1))
    limits[:, dimension] = limit_points

    return [slice(t, start, limit) if o is None else None
            for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","diff --git a/jax/lax.py b/jax/lax.py
index 5d6d63f58b74ceb9fdf9e68746142e554ef8bf4f..fb57be27168584c367697b97aabd09c6373d1130 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1352,15 +1352,18 @@ def concatenate_translation_rule(c, *operands, **kwargs):
 def concatenate_transpose_rule(t, *operands, **kwargs):
   dimension = kwargs.pop('dimension')
   operand_shapes = kwargs.pop('operand_shapes')
-  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim), dtype=int)
-  starts[1:, dimension] = limit_points[:-1]
-  limits = onp.tile(t.shape, (len(operands), 1))
-  limits[:, dimension] = limit_points
+  if t is ad_util.zero:
+    return [ad_util.zero if o is None else None for o in operands]
+  else:
+    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
+    starts = onp.zeros((len(operands), t.ndim), dtype=int)
+    starts[1:, dimension] = limit_points[:-1]
+    limits = onp.tile(t.shape, (len(operands), 1))
+    limits[:, dimension] = limit_points
 
-  return [slice(t, start, limit) if o is None else None
-          for o, start, limit in zip(operands, starts, limits)]
+    return [slice(t, start, limit) if o is None else None
+            for o, start, limit in zip(operands, starts, limits)]
 
 concatenate_p = standard_primitive(
     concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
",Logging / telemetry bug,Add special case to concatenate_transpose_rule for zero input.
499ea19e44ad68626d3ce10550f27feaf03812df,Matthew Johnson: fix 'unreachable' bug in dot batching rule,jax/lax.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert False  # unreachable

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  assert lbd is not None or rbd is not None

  if lbd is not None:
    if lbd != 0:
      lhs = batching.move_dim_to_front(lhs, lbd)
      lbd = 0
  else:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  lhs_contract = tuple(onp.add(1, lhs_contract))
  lhs_batch = (0,) + tuple(onp.add(1, lhs_batch))

  if rbd is not None:
    if rbd != 0:
      rhs = batching.move_dim_to_front(rhs, rbd)
      rbd = 0
  else:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  rhs_contract = tuple(onp.add(1, rhs_contract))
  rhs_batch = (0,) + tuple(onp.add(1, rhs_batch))

  new_dimension_numbers = [(lhs_contract, rhs_contract), (lhs_batch, rhs_batch)]
  batched_out = dot_general(lhs, rhs, new_dimension_numbers)
  return batched_out, 0

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')

  if t is ad_util.zero:
    return [ad_util.zero if o is None else None for o in operands]
  else:
    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
    starts = onp.zeros((len(operands), t.ndim), dtype=int)
    starts[1:, dimension] = limit_points[:-1]
    limits = onp.tile(t.shape, (len(operands), 1))
    limits[:, dimension] = limit_points

    return [slice(t, start, limit) if o is None else None
            for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  # in some cases, we can call dot instead of dot_general
  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert lbd is not None and rbd is not None
    assert lhs.ndim == rhs.ndim == 2  # dot only supports rank 1 and above
    if lbd != 0:
      batching.move_dim_to_front(lhs, lbd)
    if rbd != 0:
      batching.move_dim_to_front(rhs, rbd)
    return dot_general(lhs, rhs, [((1,), (1,)), ((0,), (0,))])

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  assert lbd is not None or rbd is not None

  if lbd is not None:
    if lbd != 0:
      lhs = batching.move_dim_to_front(lhs, lbd)
      lbd = 0
  else:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  lhs_contract = tuple(onp.add(1, lhs_contract))
  lhs_batch = (0,) + tuple(onp.add(1, lhs_batch))

  if rbd is not None:
    if rbd != 0:
      rhs = batching.move_dim_to_front(rhs, rbd)
      rbd = 0
  else:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  rhs_contract = tuple(onp.add(1, rhs_contract))
  rhs_batch = (0,) + tuple(onp.add(1, rhs_batch))

  new_dimension_numbers = [(lhs_contract, rhs_contract), (lhs_batch, rhs_batch)]
  batched_out = dot_general(lhs, rhs, new_dimension_numbers)
  return batched_out, 0

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')

  if t is ad_util.zero:
    return [ad_util.zero if o is None else None for o in operands]
  else:
    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
    starts = onp.zeros((len(operands), t.ndim), dtype=int)
    starts[1:, dimension] = limit_points[:-1]
    limits = onp.tile(t.shape, (len(operands), 1))
    limits[:, dimension] = limit_points

    return [slice(t, start, limit) if o is None else None
            for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","diff --git a/jax/lax.py b/jax/lax.py
index 5aa30b3679b1f50249d74a8b3d145504389e33d7..4a2bc44fc5bc41912cd290a148d5a923bf81ba58 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1112,6 +1112,7 @@ def dot_batch_rule(batched_args, batch_dims):
   lbd, rbd = batch_dims
   T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])
 
+  # in some cases, we can call dot instead of dot_general
   if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
     if rbd is None:
       assert lbd in (0, 1)
@@ -1127,7 +1128,13 @@ def dot_batch_rule(batched_args, batch_dims):
       else:
         return dot(rhs, T(lhs)), 0
 
-    assert False  # unreachable
+    assert lbd is not None and rbd is not None
+    assert lhs.ndim == rhs.ndim == 2  # dot only supports rank 1 and above
+    if lbd != 0:
+      batching.move_dim_to_front(lhs, lbd)
+    if rbd != 0:
+      batching.move_dim_to_front(rhs, rbd)
+    return dot_general(lhs, rhs, [((1,), (1,)), ((0,), (0,))])
 
   if lbd is None:
     assert rbd is not None
",Typo / wrong variable name,Optimize `dot_batch_rule` by using `dot` instead of `dot_general` for 2D inputs with batch
499ea19e44ad68626d3ce10550f27feaf03812df,Matthew Johnson: fix 'unreachable' bug in dot batching rule,tests/batching_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as onp
from absl.testing import absltest
from absl.testing import parameterized

import jax.numpy as np
from jax import test_util as jtu
from jax.abstract_arrays import ShapedArray
from jax import lax
from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
from jax.api import vmap
from jax.config import config
from jax.core import unit
from jax.interpreters import partial_eval as pe
from jax.util import partial

import functools as fn

class BatchingTest(jtu.JaxTestCase):

  def testConstantFunction(self):
    ans = vmap(lambda x: 3, onp.ones(4))
    expected = 3 * onp.ones(4)
    self.assertAllClose(ans, expected, check_dtypes=False)

  def testNestedBatchingMatMat(self):
    def matvec(A, b):
      return vmap(np.vdot, A, b, in_axes=(0, None))

    def matmat(A, B):
      return vmap(matvec, A, B, in_axes=(None, 1), out_axes=1)

    R = onp.random.RandomState(0).randn
    A = R(4, 3)
    B = R(3, 2)

    ans = matmat(A, B)
    expected = onp.dot(A, B)
    self.assertAllClose(ans, expected, check_dtypes=False)

    # this is a crude check that we only call a single dot
    def pv_like(x):
      aval = ShapedArray(onp.shape(x), onp.result_type(x))
      return pe.PartialVal((aval, unit))

    def make_jaxpr(fun, example_args):
      jaxpr, _, _, _ = trace_to_jaxpr(fun, map(pv_like, example_args))
      return jaxpr

    jaxpr = make_jaxpr(matmat, (A, B))
    self.assertEqual(len(jaxpr.eqns), 1)

  def testPerExampleGradients(self):
    def predict(params, inputs):
      for W, b in params:
        outputs = np.dot(W, inputs) + b
        inputs = np.tanh(outputs)
      return outputs

    def loss(params, data):
      inputs, targets = data
      predictions = predict(params, inputs)
      return np.sum((predictions - targets)**2)

    batch_size = 5
    layer_sizes = [3, 2, 4]

    R = onp.random.RandomState(0).randn
    params = [(R(m, n), R(m))
              for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]

    input_vec = R(3)
    target_vec = R(4)
    datum = (input_vec, target_vec)

    input_batch = R(5, 3)
    target_batch = R(5, 4)
    batch = (input_batch, target_batch)

    ans = vmap(partial(grad(loss), params), batch)

    for ans_pair, param_pair in zip(ans, params):
      dW, db = ans_pair
      W, b = param_pair

      self.assertEqual(dW.shape, (batch_size,) + W.shape)
      self.assertEqual(db.shape, (batch_size,) + b.shape)

  def testJacobians(self):
    def jacbwd(f, x):
      y, pullback = vjp(f, x)
      std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
      jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
      return jac_flat.reshape(onp.shape(y) + onp.shape(x))

    def jacfwd(f, x):
      pushfwd = lambda v: jvp(f, (x,), (v,))
      std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
      y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
      return jac_flat.reshape(onp.shape(y) + onp.shape(x))

    R = onp.random.RandomState(0).randn

    A = R(4, 3)
    b = R(4)
    f = lambda x: np.tanh(np.dot(A, x) + b)

    x = R(3)
    self.assertAllClose(jacfwd(f, x), jacbwd(f, x), check_dtypes=False)

  def testBatchOfCompile(self):
    side = []

    @jit
    def f(x):
      side.append(None)
      return x + x

    g = jit(lambda x: vmap(f, x))
    self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
    self.assertEqual(len(side), 1)
    self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
                        check_dtypes=False)
    self.assertEqual(len(side), 1)

  def testSliceLax(self):
    fun = lambda x: lax.slice(x, (2,), (4,))
    R = onp.random.RandomState(0).randn
    x = R(5, 10)

    ans = vmap(fun, x)
    expected_ans = x[:, 2:4]
    self.assertAllClose(ans, expected_ans, check_dtypes=False)

  def testSliceNumpy(self):
    fun = lambda x: x[:, 2]
    R = onp.random.RandomState(0).randn
    x = R(10, 5, 3, 7)

    ans = vmap(fun, x)
    expected_ans = x[:, :, 2]
    self.assertAllClose(ans, expected_ans, check_dtypes=False)

  def testNpMaximum(self):
    fun = lambda x: np.maximum(x, 0.0)
    R = onp.random.RandomState(0).randn
    x = R(10, 5, 3, 7)

    ans = vmap(fun, x)
    expected_ans = onp.maximum(x, 0.0)
    self.assertAllClose(ans, expected_ans, check_dtypes=False)

  def testNpGtrThan(self):
    R = onp.random.RandomState(0).randn
    x = R(10, 5, 3, 7)

    ans = vmap(lambda x: x > 1.0, x)
    expected_ans = x > 1.0
    self.assertAllClose(ans, expected_ans, check_dtypes=True)

  def testNpMaximumPerExampleGrad(self):
    R = onp.random.RandomState(0).randn
    x = R(10, 5)
    W = R(5, 5)

    fun = lambda W, x: np.sum(np.maximum(np.dot(x, W), 0.0) ** 2)

    ans = vmap(fn.partial(grad(fun), W), x)

    W_t = np.transpose(W)
    for i in range(10):
      x_ex = x[i:i + 1]

      expected_ans = 2.0 * np.dot(
          np.maximum(np.dot(W_t, np.transpose(x_ex)), 0.0), x_ex)
      expected_ans = np.transpose(expected_ans)

      self.assertAllClose(ans[i], expected_ans, check_dtypes=False)

  def testDotGeneral(self):
    R = onp.random.RandomState(0).randn
 
    x = R(10, 3, 4, 5)
    y = R(10, 3, 5, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
    self.assertAllClose(ans, expected, check_dtypes=True)
 
    x = R(3, 4, 10, 5)
    y = R(3, 10, 5, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
               in_axes=(2, 1))
    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
    expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
    self.assertAllClose(ans, expected, check_dtypes=True)
 
    x = R(3, 4, 5, 10)
    y = R(3, 5, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
               in_axes=(3, None))
    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
    expected = onp.stack([fun(x[..., i], y) for i in range(10)])
    self.assertAllClose(ans, expected, check_dtypes=True)
 
    x = R(3, 4, 5)
    y = R(3, 5, 10, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
               in_axes=(None, 2))
    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
    expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
    self.assertAllClose(ans, expected, check_dtypes=True)


if __name__ == '__main__':
  config.config_with_absl()
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as onp
from absl.testing import absltest
from absl.testing import parameterized

import jax.numpy as np
from jax import test_util as jtu
from jax.abstract_arrays import ShapedArray
from jax import lax
from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
from jax.api import vmap
from jax.config import config
from jax.core import unit
from jax.interpreters import partial_eval as pe
from jax.util import partial, curry

import functools as fn

class BatchingTest(jtu.JaxTestCase):

  def testConstantFunction(self):
    ans = vmap(lambda x: 3, onp.ones(4))
    expected = 3 * onp.ones(4)
    self.assertAllClose(ans, expected, check_dtypes=False)

  def testNestedBatchingMatMat(self):
    def matvec(A, b):
      return vmap(np.vdot, A, b, in_axes=(0, None))

    def matmat(A, B):
      return vmap(matvec, A, B, in_axes=(None, 1), out_axes=1)

    R = onp.random.RandomState(0).randn
    A = R(4, 3)
    B = R(3, 2)

    ans = matmat(A, B)
    expected = onp.dot(A, B)
    self.assertAllClose(ans, expected, check_dtypes=False)

    # this is a crude check that we only call a single dot
    def pv_like(x):
      aval = ShapedArray(onp.shape(x), onp.result_type(x))
      return pe.PartialVal((aval, unit))

    def make_jaxpr(fun, example_args):
      jaxpr, _, _, _ = trace_to_jaxpr(fun, map(pv_like, example_args))
      return jaxpr

    jaxpr = make_jaxpr(matmat, (A, B))
    self.assertEqual(len(jaxpr.eqns), 1)

  def testPerExampleGradients(self):
    def predict(params, inputs):
      for W, b in params:
        outputs = np.dot(W, inputs) + b
        inputs = np.tanh(outputs)
      return outputs

    def loss(params, data):
      inputs, targets = data
      predictions = predict(params, inputs)
      return np.sum((predictions - targets)**2)

    batch_size = 5
    layer_sizes = [3, 2, 4]

    R = onp.random.RandomState(0).randn
    params = [(R(m, n), R(m))
              for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]

    input_vec = R(3)
    target_vec = R(4)
    datum = (input_vec, target_vec)

    input_batch = R(5, 3)
    target_batch = R(5, 4)
    batch = (input_batch, target_batch)

    ans = vmap(partial(grad(loss), params), batch)

    for ans_pair, param_pair in zip(ans, params):
      dW, db = ans_pair
      W, b = param_pair

      self.assertEqual(dW.shape, (batch_size,) + W.shape)
      self.assertEqual(db.shape, (batch_size,) + b.shape)

  def testJacobians(self):
    def jacbwd(f, x):
      y, pullback = vjp(f, x)
      std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
      jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
      return jac_flat.reshape(onp.shape(y) + onp.shape(x))

    def jacfwd(f, x):
      pushfwd = lambda v: jvp(f, (x,), (v,))
      std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
      y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
      return jac_flat.reshape(onp.shape(y) + onp.shape(x))

    R = onp.random.RandomState(0).randn

    A = R(4, 3)
    b = R(4)
    f = lambda x: np.tanh(np.dot(A, x) + b)

    x = R(3)
    self.assertAllClose(jacfwd(f, x), jacbwd(f, x), check_dtypes=False)

  def testBatchOfCompile(self):
    side = []

    @jit
    def f(x):
      side.append(None)
      return x + x

    g = jit(lambda x: vmap(f, x))
    self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
    self.assertEqual(len(side), 1)
    self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
                        check_dtypes=False)
    self.assertEqual(len(side), 1)

  def testSliceLax(self):
    fun = lambda x: lax.slice(x, (2,), (4,))
    R = onp.random.RandomState(0).randn
    x = R(5, 10)

    ans = vmap(fun, x)
    expected_ans = x[:, 2:4]
    self.assertAllClose(ans, expected_ans, check_dtypes=False)

  def testSliceNumpy(self):
    fun = lambda x: x[:, 2]
    R = onp.random.RandomState(0).randn
    x = R(10, 5, 3, 7)

    ans = vmap(fun, x)
    expected_ans = x[:, :, 2]
    self.assertAllClose(ans, expected_ans, check_dtypes=False)

  def testNpMaximum(self):
    fun = lambda x: np.maximum(x, 0.0)
    R = onp.random.RandomState(0).randn
    x = R(10, 5, 3, 7)

    ans = vmap(fun, x)
    expected_ans = onp.maximum(x, 0.0)
    self.assertAllClose(ans, expected_ans, check_dtypes=False)

  def testNpGtrThan(self):
    R = onp.random.RandomState(0).randn
    x = R(10, 5, 3, 7)

    ans = vmap(lambda x: x > 1.0, x)
    expected_ans = x > 1.0
    self.assertAllClose(ans, expected_ans, check_dtypes=True)

  def testNpMaximumPerExampleGrad(self):
    R = onp.random.RandomState(0).randn
    x = R(10, 5)
    W = R(5, 5)

    fun = lambda W, x: np.sum(np.maximum(np.dot(x, W), 0.0) ** 2)

    ans = vmap(fn.partial(grad(fun), W), x)

    W_t = np.transpose(W)
    for i in range(10):
      x_ex = x[i:i + 1]

      expected_ans = 2.0 * np.dot(
          np.maximum(np.dot(W_t, np.transpose(x_ex)), 0.0), x_ex)
      expected_ans = np.transpose(expected_ans)

      self.assertAllClose(ans[i], expected_ans, check_dtypes=False)

  def testDotGeneral(self):
    R = onp.random.RandomState(0).randn

    x = R(10, 3, 4, 5)
    y = R(10, 3, 5, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
    self.assertAllClose(ans, expected, check_dtypes=True)

    x = R(3, 4, 10, 5)
    y = R(3, 10, 5, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
               in_axes=(2, 1))
    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
    expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
    self.assertAllClose(ans, expected, check_dtypes=True)

    x = R(3, 4, 5, 10)
    y = R(3, 5, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
               in_axes=(3, None))
    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
    expected = onp.stack([fun(x[..., i], y) for i in range(10)])
    self.assertAllClose(ans, expected, check_dtypes=True)

    x = R(3, 4, 5)
    y = R(3, 5, 10, 6)
    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
               in_axes=(None, 2))
    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
    expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
    self.assertAllClose(ans, expected, check_dtypes=True)

  def testDot(self):
    # these tests are based on @shoyer's notebook studying gufuncs
    curried_vmap = curry(vmap)

    def vecvec(a, b):
      dot = np.dot
      for ndim in range(1, max(a.ndim, b.ndim)):
        a_ax = 0 if a.ndim > ndim else None
        b_ax = 0 if b.ndim > ndim else None
        dot = curried_vmap(dot, in_axes=(a_ax, b_ax))
      return dot(a, b)

    assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()
    assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)
    # TODO(mattjj): this fails due to an xla error in dot_general
    # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)


if __name__ == '__main__':
  config.config_with_absl()
  absltest.main()
","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 80c4ea73f0e6656926b16121b1266b8565ff8fc5..f1ed960af7ecbc37b9a9d863f3b5ee0c5f8bc7c3 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -29,7 +29,7 @@ from jax.api import vmap
 from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
-from jax.util import partial
+from jax.util import partial, curry
 
 import functools as fn
 
@@ -196,13 +196,13 @@ class BatchingTest(jtu.JaxTestCase):
 
   def testDotGeneral(self):
     R = onp.random.RandomState(0).randn
- 
+
     x = R(10, 3, 4, 5)
     y = R(10, 3, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
     expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 10, 5)
     y = R(3, 10, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -210,7 +210,7 @@ class BatchingTest(jtu.JaxTestCase):
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 5, 10)
     y = R(3, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -218,7 +218,7 @@ class BatchingTest(jtu.JaxTestCase):
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i], y) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 5)
     y = R(3, 5, 10, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -227,6 +227,23 @@ class BatchingTest(jtu.JaxTestCase):
     expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  def testDot(self):
+    # these tests are based on @shoyer's notebook studying gufuncs
+    curried_vmap = curry(vmap)
+
+    def vecvec(a, b):
+      dot = np.dot
+      for ndim in range(1, max(a.ndim, b.ndim)):
+        a_ax = 0 if a.ndim > ndim else None
+        b_ax = 0 if b.ndim > ndim else None
+        dot = curried_vmap(dot, in_axes=(a_ax, b_ax))
+      return dot(a, b)
+
+    assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()
+    assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)
+    # TODO(mattjj): this fails due to an xla error in dot_general
+    # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
+
 
 if __name__ == '__main__':
   config.config_with_absl()
",Permission / authorization bug,"```
Add testDot method to BatchingTest class and import curry from jax.util
```"
38927153b1bb6c60a659c02cacf6771c3cbe4b14,"Peter Hawkins: Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.",jax/lax.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  # in some cases, we can call dot instead of dot_general
  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert lbd is not None and rbd is not None
    assert lhs.ndim == rhs.ndim == 2  # dot only supports rank 1 and above
    if lbd != 0:
      batching.move_dim_to_front(lhs, lbd)
    if rbd != 0:
      batching.move_dim_to_front(rhs, rbd)
    return dot_general(lhs, rhs, [((1,), (1,)), ((0,), (0,))])

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  assert lbd is not None or rbd is not None

  if lbd is not None:
    if lbd != 0:
      lhs = batching.move_dim_to_front(lhs, lbd)
      lbd = 0
  else:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  lhs_contract = tuple(onp.add(1, lhs_contract))
  lhs_batch = (0,) + tuple(onp.add(1, lhs_batch))

  if rbd is not None:
    if rbd != 0:
      rhs = batching.move_dim_to_front(rhs, rbd)
      rbd = 0
  else:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  rhs_contract = tuple(onp.add(1, rhs_contract))
  rhs_batch = (0,) + tuple(onp.add(1, rhs_batch))

  new_dimension_numbers = [(lhs_contract, rhs_contract), (lhs_batch, rhs_batch)]
  batched_out = dot_general(lhs, rhs, new_dimension_numbers)
  return batched_out, 0

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')

  if t is ad_util.zero:
    return [ad_util.zero if o is None else None for o in operands]
  else:
    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
    starts = onp.zeros((len(operands), t.ndim), dtype=int)
    starts[1:, dimension] = limit_points[:-1]
    limits = onp.tile(t.shape, (len(operands), 1))
    limits[:, dimension] = limit_points

    return [slice(t, start, limit) if o is None else None
            for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
from .util import partial
import itertools
import operator
import six
from six.moves import builtins, xrange
import string

import numpy as onp

from . import core
from . import ad_util
from . import linear_util as lu
from .core import Primitive
from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                              array_types, make_shaped_array)
from .api_util import flatten_fun, tree_to_jaxtuples
from .interpreters import partial_eval as pe
from .interpreters import xla
from .interpreters import ad
from .interpreters import batching
from .util import curry, safe_zip, unzip2, prod
from .tree_util import build_tree
from .lib import xla_bridge

_max = builtins.max
_min = builtins.max

if six.PY3:
  def maketrans(s1, s2):
    return s1.maketrans(s1, s2)
else:
  maketrans = string.maketrans

### traceables

def neg(x): return neg_p.bind(x)
def sign(x): return sign_p.bind(x)
def floor(x): return floor_p.bind(x)
def ceil(x): return ceil_p.bind(x)
def round(x): return round_p.bind(x)

def is_finite(x): return is_finite_p.bind(x)

def exp(x): return exp_p.bind(x)
def expm1(x): return expm1_p.bind(x)
def log(x): return log_p.bind(x)
def log1p(x): return log1p_p.bind(x)
def tanh(x): return tanh_p.bind(x)
def sin(x): return sin_p.bind(x)
def cos(x): return cos_p.bind(x)
def atan2(x, y): return atan2_p.bind(x, y)

def lgamma(x): return lgamma_p.bind(x)
def digamma(x): return digamma_p.bind(x)
def erf(x): return erf_p.bind(x)
def erfc(x): return erfc_p.bind(x)
def erf_inv(x): return erf_inv_p.bind(x)

def real(x): return real_p.bind(x)
def imag(x): return imag_p.bind(x)
def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
def conj(x): return conj_p.bind(x)
def abs(x): return abs_p.bind(x)
def pow(x, y): return pow_p.bind(x, y)

def bitwise_not(x): return not_p.bind(x)
def bitwise_and(x, y): return and_p.bind(x, y)
def bitwise_or(x, y): return or_p.bind(x, y)
def bitwise_xor(x, y): return xor_p.bind(x, y)

def add(x, y): return add_p.bind(x, y)
def sub(x, y): return sub_p.bind(x, y)
def mul(x, y): return mul_p.bind(x, y)
def div(x, y): return div_p.bind(x, y)
def rem(x, y): return rem_p.bind(x, y)

def max(x, y): return max_p.bind(x, y)
def min(x, y): return min_p.bind(x, y)

def shift_left(x, y): return shift_left_p.bind(x, y)
def shift_right_arithmetic(x, y): return shift_right_arithmetic_p.bind(x, y)
def shift_right_logical(x, y): return shift_right_logical_p.bind(x, y)

def eq(x, y): return eq_p.bind(x, y)
def ne(x, y): return ne_p.bind(x, y)
def ge(x, y): return ge_p.bind(x, y)
def gt(x, y): return gt_p.bind(x, y)
def le(x, y): return le_p.bind(x, y)
def lt(x, y): return lt_p.bind(x, y)

def convert_element_type(operand, new_dtype):
  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
  old_dtype = _dtype(operand)
  if old_dtype != new_dtype:
    return convert_element_type_p.bind(
        operand, new_dtype=new_dtype, old_dtype=old_dtype)
  else:
    return operand

def bitcast_convert_type(operand, new_dtype):
  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)

def clamp(min, operand, max):
  return clamp_p.bind(min, operand, max)

def concatenate(operands, dimension):
  return concatenate_p.bind(*operands, dimension=dimension,
                            operand_shapes=tuple(o.shape for o in operands))

def conv(lhs, rhs, window_strides, padding):
  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_with_general_padding(lhs, rhs, window_strides, padding,
                              lhs_dilation, rhs_dilation):
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
      lhs_shape=lhs.shape, rhs_shape=rhs.shape)

def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
                         rhs_dilation, dimension_numbers):
  if isinstance(padding, str):
    perms = conv_general_permutations(dimension_numbers)
    lhs_perm, rhs_perm, _ = perms
    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
                              onp.take(rhs.shape, rhs_perm)[2:],
                              window_strides, padding)
  return conv_general_dilated_p.bind(
      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
      lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
      dimension_numbers=dimension_numbers, lhs_shape=lhs.shape,
      rhs_shape=rhs.shape)

def dot(lhs, rhs): return dot_p.bind(lhs, rhs)

def dot_general(lhs, rhs, dimension_numbers):
  lhs_dims, rhs_dims = dimension_numbers
  dimension_numbers = (tuple(map(tuple, lhs_dims)), tuple(map(tuple, rhs_dims)))
  return dot_general_p.bind(lhs, rhs, dimension_numbers=dimension_numbers)

def broadcast(operand, sizes):
  return broadcast_p.bind(operand, sizes=tuple(sizes))

def broadcast_in_dim(operand, shape, broadcast_dimensions):
  if operand.ndim == len(shape) and not len(broadcast_dimensions):
    return operand
  else:
    return broadcast_in_dim_p.bind(
        operand, shape=tuple(shape),
        broadcast_dimensions=tuple(broadcast_dimensions))

def reshape(operand, new_sizes, dimensions=None):
  same_shape = onp.shape(operand) == tuple(new_sizes)
  same_dims = dimensions is None or tuple(dimensions) == tuple(range(onp.ndim(operand)))
  if same_shape and same_dims:
    return operand
  else:
    return reshape_p.bind(
        operand, new_sizes=tuple(new_sizes),
        dimensions=None if dimensions is None else tuple(dimensions),
        old_sizes=onp.shape(operand))

def pad(operand, padding_value, padding_config):
  return pad_p.bind(operand, padding_value, padding_config=tuple(padding_config))

def rev(operand, dimensions):
  return rev_p.bind(operand, dimensions=tuple(dimensions))

def select(pred, on_true, on_false):
  return select_p.bind(pred, on_true, on_false)

def slice(operand, start_indices, limit_indices, strides=None):
  return slice_p.bind(operand, start_indices=tuple(start_indices),
                      limit_indices=tuple(limit_indices),
                      strides=None if strides is None else tuple(strides),
                      operand_shape=operand.shape)

def dynamic_slice(operand, start_indices, slice_sizes):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_slice_p.bind(
      operand, start_indices, slice_sizes=tuple(slice_sizes),
      operand_shape=operand.shape)

def dynamic_update_slice(operand, update, start_indices):
  start_indices = _dynamic_slice_indices(operand, start_indices)
  return dynamic_update_slice_p.bind(operand, update, start_indices,
                                     update_shape=update.shape)

def index_take(src, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src,) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
  return index_take_p.bind(src, *idxs, axes=tuple(axes),
                           input_shape=src.shape, jaxpr=jaxpr, consts=consts)

def _index_take(axes, src, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(src.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, idxs, out = state
    src_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * src.ndim, zip(axes, src_ind))
    update = dynamic_slice(src, start_indices, slice_sizes)
    update = reshape(update, (1,) + out.shape[1:])
    out = dynamic_update_slice(out, update, [i] + [0] * (out.ndim - 1))
    return src, idxs, out

  out = full_like(src, 0, shape=(n,) + tuple(onp.delete(src.shape, axes)))
  init_val = src, idxs, out
  _, _, out = fori_loop(0, n, body_fun, init_val)
  return out

def index_untake(src, dst, idxs, axes):
  pvals = [_abstractify(arg) for arg in (src, dst) + idxs]
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_untake, axes), pvals)
  return index_untake_p.bind(src, dst, *idxs, axes=tuple(axes),
                             jaxpr=jaxpr, consts=consts)

def _index_untake(axes, src, dst, *idxs):
  n = idxs[0].shape[0]
  slice_sizes = subvals(dst.shape, zip(axes, [1] * len(axes)))

  def body_fun(i, state):
    src, dst, idxs = state
    vals = dynamic_slice(src, [i] + [0] * (src.ndim - 1), (1,) + src.shape[1:])
    vals = reshape(vals, subvals(dst.shape, zip(axes, [1] * len(axes))))
    dst_ind = (dynamic_index_in_dim(x, i, 0, False) for x in idxs)
    start_indices = subvals([0] * dst.ndim, zip(axes, dst_ind))
    update = add(vals, dynamic_slice(dst, start_indices, slice_sizes))
    dst = dynamic_update_slice(dst, update, start_indices)
    return src, dst, idxs

  init_val = src, dst, idxs
  _, dst, _ = fori_loop(0, n, body_fun, init_val)
  return dst

def transpose(operand, permutation):
  return transpose_p.bind(operand, permutation=tuple(permutation))

def reduce(operand, init_value, computation, dimensions):
  monoid_reducer = _get_monoid_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, dimensions)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
                         dimensions=tuple(dimensions))

def _reduction_jaxpr(computation, init_value):
  pval = _abstractify(init_value)
  jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(computation, (pval, pval))
  return jaxpr, consts

def _get_monoid_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_min

def _get_max_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(-onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).min, dtype)

def _get_min_identity(dtype):
  if onp.issubdtype(dtype, onp.floating):
    return onp.array(onp.inf, dtype)
  elif onp.issubdtype(dtype, onp.integer):
    return onp.array(onp.iinfo(dtype).max, dtype)

def _reduce_sum(operand, axes):
  return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)

def _reduce_max(operand, axes):
  return reduce_max_p.bind(operand, axes=tuple(axes))

def _reduce_min(operand, axes):
  return reduce_min_p.bind(operand, axes=tuple(axes))

def reduce_window(operand, init_value, computation, window_dimensions,
                  window_strides, padding):
  monoid_reducer = _get_monoid_window_reducer(computation, init_value)
  if monoid_reducer:
    return monoid_reducer(operand, window_dimensions, window_strides, padding)
  else:
    jaxpr, consts = _reduction_jaxpr(computation, init_value)
    return reduce_window_p.bind(
        operand, init_value, jaxpr=jaxpr, consts=consts,
        window_dimensions=tuple(window_dimensions),
        window_strides=tuple(window_strides), padding=padding)

def _get_monoid_window_reducer(monoid_op, x):
  aval = core.get_aval(x)
  if (type(aval) is ConcreteArray) and aval.shape == ():
    if monoid_op is add:
      return aval.val == 0 and _reduce_window_sum
    elif monoid_op is max:
      return aval.val == _get_max_identity(aval.dtype) and _reduce_window_max
    elif monoid_op is min:
      return aval.val == _get_min_identity(aval.dtype) and _reduce_window_min

def _reduce_window_sum(operand, window_dimensions, window_strides, padding):
  return reduce_window_sum_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding,
      input_shape=operand.shape)

def _reduce_window_max(operand, window_dimensions, window_strides, padding):
  return reduce_window_max_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _reduce_window_min(operand, window_dimensions, window_strides, padding):
  return reduce_window_min_p.bind(
      operand, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter(operand, select, window_dimensions, window_strides,
                        padding, source, init_value, scatter):
  select_jaxpr, select_consts = _reduction_jaxpr(select)
  scatter_jaxpr, scatter_consts = _reduction_jaxpr(scatter)
  return select_and_scatter_p.bind(
      operand, source, init_value, select_jaxpr=select_jaxpr,
      select_consts=select_consts, scatter_jaxpr=scatter_jaxpr,
      scatter_consts=scatter_consts, window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
                            window_strides, padding):
  return select_and_scatter_add_p.bind(
      source, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                           window_strides, padding):
  return select_and_gather_add_p.bind(
      tangents, operand, select_prim=select_prim,
      window_dimensions=tuple(window_dimensions),
      window_strides=tuple(window_strides), padding=padding)

def sort(operand, dimension=-1):
  return sort_p.bind(operand, dimension=-1)

def sort_key_val(keys, values, dimension=-1):
  # TODO new sort_key_val is variadic
  result = sort_key_val_p.bind(keys, values, dimension=dimension)
  sorted_keys, sorted_values = result
  return sorted_keys, sorted_values

def _while_loop(cond_fun, body_fun, init_val):
  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))

  pval_flat = _abstractify(init_val_flat)
  cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))
  body_jaxpr, pvout, body_consts = pe.trace_to_jaxpr(flat_body_fun, (pval_flat,))
  abs_out, _ = pvout

  params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
  out_flat = while_p.bind(init_val_flat, opaque_params=params)
  if out_tree() != in_tree:
    raise TypeError(""body_fun input and output must have identical structure"")
  return build_tree(out_tree(), out_flat)

class OpaqueParam(object):
  __slots__ = [""val"", ""id""]
  def __init__(self, val):
    self.val = val
    self.id = next(opaque_param_ids)
  def __hash__(self):
    return self.id
opaque_param_ids = itertools.count()


### convenience wrappers around traceables


def full_like(x, fill_value, dtype=None, shape=None):
  """"""Create a full array like np.full based on the example array `x`.

  Args:
    x: example array-like, used for shape and dtype information.
    fill_value: a scalar value to fill the entries of the output array.
    dtype: optional, a dtype parameter for the output ndarray.
    shape: optional, a shape parameter for the output ndarray.

  Returns:
    An ndarray with the same shape as `x` with its entries set equal to
    `fill_value`, similar to the output of np.full.
  """"""
  shape = onp.shape(x) if shape is None else shape
  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)


def collapse(operand, start_dimension, stop_dimension):
  lo, hi = start_dimension, stop_dimension
  size = prod(operand.shape[lo:hi])
  new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
  return reshape(operand, new_shape)


def slice_in_dim(operand, start_index, limit_index, stride=1, axis=0):
  """"""Convenience wrapper around slice applying to only one dimension.""""""
  start_indices = [0] * operand.ndim
  limit_indices = list(operand.shape)
  strides = [1] * operand.ndim

  start_indices[axis] = start_index
  limit_indices[axis] = limit_index
  strides[axis] = stride

  return slice(operand, start_indices, limit_indices, strides)


def index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around slice to perform int indexing.""""""
  axis_size = operand.shape[axis]
  wrapped_index = index + axis_size if index < 0 else index
  if not 0 <= wrapped_index < axis_size:
    msg = 'index {} is out of bounds for axis {} with size {}'
    raise IndexError(msg.format(index, axis, axis_size))
  result = slice_in_dim(operand, wrapped_index, wrapped_index + 1, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_slice_in_dim(operand, start_index, slice_size, axis=0):
  """"""Convenience wrapper around dynamic_slice applying to one dimension.""""""
  start_indices = [onp.array([0])] * operand.ndim
  slice_sizes = list(operand.shape)

  start_indices[axis] = reshape(rem(start_index, operand.shape[axis]), [1])
  slice_sizes[axis] = slice_size

  start_indices = concatenate(start_indices, 0)
  return dynamic_slice(operand, start_indices, slice_sizes)


def dynamic_index_in_dim(operand, index, axis=0, keepdims=True):
  """"""Convenience wrapper around dynamic_slice to perform int indexing.""""""
  result = dynamic_slice_in_dim(operand, index, 1, axis)
  if keepdims:
    return result
  else:
    return reshape(result, onp.delete(operand.shape, axis))


def dynamic_update_slice_in_dim(operand, update, start_index, axis):
  start_indices = [0] * _ndim(operand)
  start_indices[axis] = start_index % operand.shape[axis]
  return dynamic_update_slice(operand, update, start_indices)


def dynamic_update_index_in_dim(operand, update, index, axis):
  if _ndim(update) != _ndim(operand):
    assert _ndim(update) + 1 == _ndim(operand)
    ax = axis % _ndim(operand)
    update = reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax:])
  return dynamic_update_slice_in_dim(operand, update, index, axis)


def fori_loop(lower, upper, body_fun, init_val):
  """"""Loop from `lower` to `upper` by reduction to `while_loop`.

  Arguments:
    lower: loop index lower bound (inclusive)
    upper: loop index upper bound (exclusive)
    body_fun: function of type (int, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  # state: (upper limit, index, loop value)
  # The `lt` and `add` functions are added to the namespace programmatically.
  _, _, result = _while_loop(
      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
                         body_fun(upper_i_x[1], upper_i_x[2])),
      (upper, lower, init_val))
  return result


def foreach_loop(sequence, body_fun, init_val):
  """"""Loop over `sequence` by reduction to `while_loop`.

  Arguments:
    sequence: tuple of loop items, each of type U
    body_fun: function of type (U, T) -> T, where T is the type of `init_val`
    init_val: initial loop value, of type T

  Returns:
    Loop value from the final iteration, of type T.
  """"""
  _, result = fori_loop(
      0, len(sequence),
      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
      (sequence, init_val))
  return result


def batch_matmul(lhs, rhs):
  """"""Batch matrix multiplication.""""""
  if _min(lhs.ndim, rhs.ndim) < 2:
    raise ValueError('Arguments to batch_matmul must be at least 2D, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  if lhs.ndim != rhs.ndim:
    raise ValueError('Arguments to batch_matmul must have same ndim, got {}, {}'
                     .format(lhs.ndim, rhs.ndim))
  lhs_contract = (lhs.ndim - 1,)
  rhs_contract = (rhs.ndim - 2,)
  batch = tuple(range(lhs.ndim - 2))
  return dot_general(lhs, rhs, [(lhs_contract, rhs_contract), (batch, batch)])


# These trig functions also exist in the XLA client library, but we treat them
# as non-primitive to maintain a smaller set of autodiff primitives.

def sqrt(x):
  return pow(x, _const(x, 0.5))

def rsqrt(x):
  return pow(x, _const(x, -0.5))

def square(x):
  return mul(x, x)

def reciprocal(x):
  return div(_const(x, 1.), x)

def tan(x):
  return div(sin(x), cos(x))

def asin(x):
  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
  return mul(_const(x, 2.),
             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))

def acos(x):
  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
  return mul(_const(x, 2.),
             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))

def atan(x):
  return atan2(x, _const(x, 1.))

def sinh(x):
  return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))

def cosh(x):
  return mul(_const(x, 0.5), add(exp(x), exp(neg(x))))

def asinh(x):
  # asinh(x) = log(x + sqrt(x**2 + 1))
  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))

def acosh(x):
  # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                        sqrt(sub(x, _const(x, 1.))))))


# Add some methods to ShapedArray that rely on lax primitives

ShapedArray.broadcast = core.aval_method(broadcast)
ShapedArray.transpose = core.aval_method(transpose)  # clobbered by lax_numpy
ShapedArray.reshape = core.aval_method(reshape)      # clobbered by lax_numpy

def _iter(tracer):
  if tracer.ndim == 0:
    raise TypeError(""iteration over a 0-d array"")  # same as numpy error
  else:
    n = tracer.shape[0]
    return (index_in_dim(tracer, i, keepdims=False) for i in xrange(n))
ShapedArray._iter = staticmethod(_iter)

# Add some ad handlers that use (or could use) lax primitives

def zeros_like_array(x):
  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))

for t in itertools.chain(array_types, [xla.DeviceArray]):
  ad_util.jaxval_adders[t] = add
  ad_util.jaxval_zeros_likers[t] = zeros_like_array

batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array


### primitives


_input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
_fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
_complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype

def identity(x): return x


def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
  prim = Primitive(name)
  prim.def_impl(partial(xla.apply_primitive, prim))
  prim.def_abstract_eval(partial(standard_abstract_eval, shape_rule, dtype_rule))
  xla.translations[prim] = translation_rule or partial(standard_translate, name)
  return prim

def standard_abstract_eval(shape_rule, dtype_rule, *args, **kwargs):
  assert all(isinstance(arg, UnshapedArray) for arg in args), args
  least_specialized = _max(
      map(type, args), key=operator.attrgetter('array_abstraction_level'))
  if least_specialized is ConcreteArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is ShapedArray:
    return ShapedArray(shape_rule(*args, **kwargs), dtype_rule(*args, **kwargs))
  elif least_specialized is UnshapedArray:
    return UnshapedArray(dtype_rule(*args, **kwargs))
  else:
    raise TypeError(args, least_specialized)


def standard_translate(name, c, *args, **kwargs):
  xla_opname = ''.join(term.capitalize() for term in name.split('_'))
  return getattr(c, xla_opname)(*args, **kwargs)


def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
  if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
    msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
    typename = str(onp.dtype(aval.dtype).name)
    accepted_typenames = (str(onp.dtype(t).name) for t in accepted_dtypes)
    raise TypeError(msg.format(name, typename, ', '.join(accepted_typenames)))
  return result_dtype(aval.dtype)


def unop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
  batching.defvectorized(prim)
  return prim
standard_unop = partial(unop, identity)


def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
  aval_dtypes = [aval.dtype for aval in avals]
  for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
    if not any(onp.issubdtype(aval_dtype, t) for t in types):
      msg = ('{} does not accept dtype {} at position {}. '
             'Accepted dtypes at position {} are subtypes of {}.')
      typename = str(onp.dtype(aval_dtype).name)
      typenames = ', '.join(str(onp.dtype(t).name) for t in types)
      raise TypeError(msg.format(name, typename, i, i, typenames))
  _check_same_dtypes(name, False, *aval_dtypes)
  return result_dtype(*avals)


def broadcasting_shape_rule(name, *avals):
  shapes = onp.array([aval.shape for aval in avals if aval.shape])
  if not shapes.size:
    return ()
  if len({len(shape) for shape in shapes}) != 1:
    msg = '{} got arrays of different rank: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  min_shape = onp.min(shapes, axis=0)
  max_shape = onp.max(shapes, axis=0)
  result_shape = onp.where(min_shape == 0, 0, max_shape)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    msg = '{} got incompatible shapes for broadcasting: {}.'
    raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
  return tuple(result_shape)


def binop(result_dtype, accepted_dtypes, name):
  dtype_rule = partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
  shape_rule = partial(broadcasting_shape_rule, name)
  prim = standard_primitive(shape_rule, dtype_rule, name)
  batching.defbroadcasting(prim)
  return prim
standard_binop = partial(binop, _input_dtype)


# NOTE(mattjj): this isn't great for orchestrate fwd mode because it means JVPs
# get two extra ops in them: a reshape and a broadcast_in_dim (or sometimes just
# a broadcast). but saving the shape info with the primitives isn't great either
# because then we can't trace these ops without shape data.
def _brcast(x, *others):
  # used in jvprules to make binop broadcasting explicit for transposability.
  # requires shape info during jvp tracing, which isn't strictly necessary.
  shapes = list(filter(None, map(onp.shape, (x,) + others)))
  shape = tuple(shapes and onp.max(shapes, axis=0))
  if onp.shape(x) != shape:
    return _brcast_to(x, shape)
  else:
    return x


def _brcast_to(x, shape):
  x_shape = onp.shape(x)
  assert x_shape != shape
  if x_shape:
    assert len(x_shape) == len(shape)
    broadcast_dimensions, = onp.where(onp.equal(x_shape, shape))
    squeezed_dimensions, = onp.where(onp.not_equal(x_shape, shape))
    inshape = onp.delete(x_shape, squeezed_dimensions)
    return broadcast_in_dim(reshape(x, inshape), shape, broadcast_dimensions)
  else:
    return broadcast(x, shape)


_f32 = {onp.float32}
_float = {onp.floating}
_complex = {onp.complex64}
_int = {onp.integer}
_bool = {onp.bool_}

_num = _int | _float | _complex
_any = _int | _float | _complex | _bool


neg_p = standard_unop(_num, 'neg')
ad.deflinear(neg_p, lambda t: [neg(t)])
batching.defvectorized(neg_p)

sign_p = standard_unop(_num, 'sign')
ad.defjvp_zero(sign_p)

floor_p = standard_unop(_float, 'floor')
ad.defjvp_zero(floor_p)

ceil_p = standard_unop(_float, 'ceil')
ad.defjvp_zero(ceil_p)

round_p = standard_unop(_float, 'round')
ad.defjvp_zero(round_p)

is_finite_p = unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
ad.defjvp_zero(is_finite_p)

exp_p = standard_unop(_float | _complex, 'exp')
ad.defjvp2(exp_p, lambda g, ans, x: mul(g, ans))

log_p = standard_unop(_float | _complex, 'log')
ad.defjvp(log_p, lambda g, x: div(g, x))

expm1_p = standard_unop(_float | _complex, 'expm1')
ad.defjvp2(expm1_p, lambda g, ans, x: mul(g, add(ans, _one(ans))))

log1p_p = standard_unop(_float | _complex, 'log1p')
ad.defjvp(log1p_p, lambda g, x: div(g, add(x, _one(x))))

tanh_p = standard_unop(_float | _complex, 'tanh')
ad.defjvp(tanh_p, lambda g, x: div(g, pow(cosh(x), _two(x))))

sin_p = standard_unop(_float | _complex, 'sin')
ad.defjvp(sin_p, lambda g, x: mul(g, cos(x)))

cos_p = standard_unop(_float | _complex, 'cos')
ad.defjvp(cos_p, lambda g, x: neg(mul(g, sin(x))))

atan2_p = standard_binop([_float, _float], 'atan2')

lgamma_p = standard_unop(_float, 'lgamma')
ad.defjvp(lgamma_p, lambda g, x: mul(g, digamma(x)))

digamma_p = standard_unop(_float, 'digamma')

erf_p = standard_unop(_float, 'erf')
ad.defjvp(erf_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                  mul(g, exp(neg(square(x))))))

erfc_p = standard_unop(_float, 'erfc')
ad.defjvp(erfc_p, lambda g, x: mul(_const(x, 2. / onp.sqrt(onp.pi)),
                                   mul(neg(g), exp(neg(square(x))))))

erf_inv_p = standard_unop(_float, 'erf_inv')
ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                            mul(g, exp(square(ans)))))

real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])

imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])

complex_p = standard_binop([_f32, _f32], 'complex')
ad.deflinear(complex_p, lambda t: [real(t), imag(t)])

# TODO promotes dtypes, need to remember whether we came from float or not
conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
ad.deflinear(conj_p, lambda t: [conj(t)])

abs_p = unop(_complex_basetype, _num, 'abs')
ad.defjvp2(abs_p,
           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
                                 _replace_zero(ans)))
_maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
_maybe_real = lambda x: real(x) if _iscomplex(x) else x

# TODO handle broadcasting
pow_p = standard_binop([_float | _complex, _float | _complex], 'pow')
ad.defjvp(pow_p,
          lambda g, x, y: mul(_brcast(g, y), mul(y, pow(x, select(
              eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))),
          lambda g, x, y: mul(_brcast(g, x),
                              mul(log(_replace_zero(x)), pow(x, y))))
_replace_zero = lambda x: select(eq(x, _const(x, 0)), _ones(x), x)

not_p = standard_unop(_int | _bool, 'not')

and_p = standard_binop([_any, _any], 'and')
ad.defjvp_zero(and_p)

or_p = standard_binop([_any, _any], 'or')
ad.defjvp_zero(or_p)

xor_p = standard_binop([_any, _any], 'xor')
ad.defjvp_zero(xor_p)

add_p = standard_binop([_num, _num], 'add')
ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))

sub_p = standard_binop([_num, _num], 'sub')
ad.defjvp(sub_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: _brcast(neg(g), x))

mul_p = standard_binop([_num, _num], 'mul')
ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO


def div_transpose_rule(cotangent, x, y):
  assert x is None
  res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
  return res, None
div_p = standard_binop([_num, _num], 'div')
ad.defjvp(div_p,
          lambda g, x, y: div(_brcast(g, y), y),
          lambda g, x, y: div(mul(neg(_brcast(g, x)), x), pow(y, _two(y))))
ad.primitive_transposes[div_p] = div_transpose_rule

rem_p = standard_binop([_num, _num], 'rem')
ad.defjvp(rem_p,
          lambda g, x, y: _brcast(g, y),
          lambda g, x, y: mul(neg(g), floor(div(x, y))))


max_p = standard_binop([_any, _any], 'max')
ad.defjvp2(max_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))

min_p = standard_binop([_any, _any], 'min')
ad.defjvp2(min_p,
           lambda g, ans, x, y: mul(_brcast(g, y), _balanced_eq(x, ans, y)),
           lambda g, ans, x, y: mul(_brcast(g, x), _balanced_eq(y, ans, x)))


shift_left_p = standard_binop([_int, _int], 'shift_left')
ad.defjvp_zero(shift_left_p)

shift_right_arithmetic_p = standard_binop([_int, _int], 'shift_right_arithmetic')
ad.defjvp_zero(shift_right_arithmetic_p)

shift_right_logical_p = standard_binop([_int, _int], 'shift_right_logical')
ad.defjvp_zero(shift_right_logical_p)

eq_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
ad.defjvp_zero(eq_p)

ne_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
ad.defjvp_zero(ne_p)

ge_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
ad.defjvp_zero(ge_p)

gt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
ad.defjvp_zero(gt_p)

le_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
ad.defjvp_zero(le_p)

lt_p = binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
ad.defjvp_zero(lt_p)


def convert_element_type_shape_rule(operand, new_dtype, old_dtype):
  return operand.shape

def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
  return new_dtype

def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.ConvertElementType(operand, new_element_type=new_etype)

convert_element_type_p = standard_primitive(
    convert_element_type_shape_rule, convert_element_type_dtype_rule,
    'convert_element_type', convert_element_type_translation_rule)
ad.deflinear(
    convert_element_type_p,
    lambda t, new_dtype, old_dtype: [convert_element_type(t, old_dtype)])
batching.defvectorized(convert_element_type_p)


def bitcast_convert_type_shape_rule(operand, new_dtype):
  return operand.shape

def bitcast_convert_type_dtype_rule(operand, new_dtype):
  return new_dtype

def bitcast_convert_type_translation_rule(c, operand, new_dtype):
  new_etype = xla_bridge.dtype_to_etype(new_dtype)
  return c.BitcastConvertType(operand, new_element_type=new_etype)

bitcast_convert_type_p = standard_primitive(
    bitcast_convert_type_shape_rule, bitcast_convert_type_dtype_rule,
    'bitcast_convert_type', bitcast_convert_type_translation_rule)
ad.defjvp_zero(bitcast_convert_type_p)
batching.defvectorized(bitcast_convert_type_p)


def conv_general_dilated_shape_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers=None, **unused_kwargs):
  if dimension_numbers is None:
    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
                       window_strides)
    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
  else:
    if not isinstance(dimension_numbers, (tuple, list)):
      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
      raise TypeError(msg.format(type(dimension_numbers)))
    if len(dimension_numbers) != 3:
      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
      raise TypeError(msg.format(len(dimension_numbers)))
    if not all(isinstance(elt, str) for elt in dimension_numbers):
      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
            ""got {}."")
      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
    for i, elt in enumerate(dimension_numbers):
      if len(elt) != lhs.ndim:
        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))

    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
    return tuple(onp.take(out_trans, onp.argsort(out_perm)))

def conv_general_dilated_dtype_rule(
    lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                          lhs, rhs)

def conv_general_dilated_transpose_lhs(
    g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec

  padding = _conv_general_vjp_lhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  revd_weights = rev(rhs, rhs_sdims)
  return conv_general_dilated(
      g, revd_weights, window_strides=lhs_dilation, padding=padding,
      lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_transpose_rhs(
    g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, lhs_shape, rhs_shape):
  if dimension_numbers is None:
    nd = len(lhs_shape)
    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
    trans_dimension_numbers = ConvolutionDimensionNumbers(
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)),
        (1, 0) + tuple(range(2, nd)))
  else:
    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
    lhs_spec, rhs_spec, out_spec = dimension_numbers
    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
                               out_spec.translate(maketrans(""NC"", ""IO"")),
                               rhs_spec.translate(maketrans(""IO"", ""NC"")))

  padding = _conv_general_vjp_rhs_padding(
      onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
      window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
      rhs_dilation)
  return conv_general_dilated(
      lhs, g, window_strides=rhs_dilation, padding=padding,
      lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
      dimension_numbers=trans_dimension_numbers)


def conv_general_dilated_translation_rule(
    c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
    dimension_numbers, **unused_kwargs):
  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
    dimension_numbers = _conv_general_proto(dimension_numbers)
  return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                              rhs_dilation, dimension_numbers)

conv_general_dilated_p = standard_primitive(
    conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
    'conv_general_dilated', conv_general_dilated_translation_rule)
ad.defbilinear(conv_general_dilated_p,
               conv_general_dilated_transpose_lhs,
               conv_general_dilated_transpose_rhs)


def dot_shape_rule(lhs, rhs):
  if lhs.ndim == 0 or rhs.ndim == 0:
    msg = ""Dot only supports rank 1 or above, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))
  if lhs.ndim > 2 or rhs.ndim > 2:
    msg = ""Dot only supports rank 2 or less, got shapes {} and {}.""
    raise TypeError(msg.format(lhs.shape, rhs.shape))

  def require(shape_cond):
    if not shape_cond:
      msg = ""Incompatible shapes for dot: got {} and {}.""
      raise TypeError(msg.format(lhs.shape, rhs.shape))

  if lhs.ndim == rhs.ndim == 1:
    require(lhs.shape == rhs.shape)
    return ()
  elif lhs.ndim == rhs.ndim == 2:
    require(lhs.shape[1] == rhs.shape[0])
    return (lhs.shape[0], rhs.shape[1])
  elif rhs.ndim == 1:
    require(lhs.shape[-1] == rhs.shape[0])
    return lhs.shape[:-1]
  else:
    require(lhs.shape[-1] == rhs.shape[-2])
    return lhs.shape[:-1] + rhs.shape[:-2] + rhs.shape[-1:]

def dot_transpose_lhs(t, rhs):
  if onp.ndim(t) == onp.ndim(rhs) == 2:
    return dot(t, transpose(rhs, (1, 0)))
  elif onp.ndim(t) == 1 and onp.ndim(rhs) == 2:
    return dot(rhs, t)
  elif onp.ndim(t) == onp.ndim(rhs) == 1:
    return _outer(t, rhs)
  elif onp.ndim(t) == 0 or onp.ndim(rhs) == 0:
    return mul(t, rhs)
  else:
    raise TypeError

def dot_transpose_rhs(t, lhs):
  if onp.ndim(lhs) == onp.ndim(t) == 2:
    return dot(transpose(lhs, (1, 0)), t)
  elif onp.ndim(lhs) == 2 and onp.ndim(t) == 1:
    return dot(t, lhs)
  elif onp.ndim(t) == onp.ndim(lhs) == 1:
    return _outer(lhs, t)
  elif onp.ndim(t) == 0 or onp.ndim(lhs) == 0:
    return mul(t, lhs)
  else:
    raise TypeError

def _outer(x, y):
  assert onp.ndim(x) == onp.ndim(y) == 1
  return mul(reshape(x, (x.shape[0], 1)), reshape(y, (1, y.shape[0])))

def dot_batch_rule(batched_args, batch_dims):
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])

  # in some cases, we can call dot instead of dot_general
  if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
    if rbd is None:
      assert lbd in (0, 1)
      if lbd == 0:
        return dot(lhs, rhs), 0
      else:
        return dot(T(rhs), lhs), 1

    if lbd is None:
      assert rbd in (0, 1)
      if rbd == onp.ndim(rhs) - 1:
        return dot(lhs, rhs), 1
      else:
        return dot(rhs, T(lhs)), 0

    assert lbd is not None and rbd is not None
    assert lhs.ndim == rhs.ndim == 2  # dot only supports rank 1 and above
    if lbd != 0:
      batching.move_dim_to_front(lhs, lbd)
    if rbd != 0:
      batching.move_dim_to_front(rhs, rbd)
    return dot_general(lhs, rhs, [((1,), (1,)), ((0,), (0,))])

  if lbd is None:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  else:
    lhs = batching.move_dim_to_front(lhs, lbd)
  lhs_batch = (0,)
  lhs_contracting = (onp.ndim(lhs) - 1,)

  if rbd is None:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  else:
    rhs = batching.move_dim_to_front(rhs, rbd)
  rhs_batch = (0,)
  rhs_contracting = (onp.arange(1, onp.ndim(rhs))[-2:][0],)

  dim_nums = [(lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch)]
  return dot_general(lhs, rhs, dim_nums), 0

dot_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
dot_p = standard_primitive(dot_shape_rule, dot_dtype_rule, 'dot')
ad.defbilinear(dot_p, dot_transpose_lhs, dot_transpose_rhs)
batching.primitive_batchers[dot_p] = dot_batch_rule


def dot_general_shape_rule(lhs, rhs, dimension_numbers):
  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
  if len(lhs_batch) != len(rhs_batch):
    msg = (""dot_general requires equal numbers of lhs_batch and rhs_batch ""
           ""dimensions, got lhs_batch {} and rhs_batch {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  if not onp.all(onp.equal(lhs_batch, rhs_batch)):
    msg = (""dot_general requires same lhs and rhs batch dimension numbers, ""
           ""got {} and {}."")
    raise TypeError(msg.format(lhs_batch, rhs_batch))
  lhs_batch_shape = onp.take(lhs.shape, lhs_batch)
  rhs_batch_shape = onp.take(rhs.shape, rhs_batch)
  if not onp.all(onp.equal(lhs_batch_shape, rhs_batch_shape)):
    msg = (""dot_general requires lhs batch dimensions and rhs batch dimensions ""
           ""to have the same shape, got {} and {}."")
    raise TypeError(msg.format(lhs_batch_shape, rhs_batch_shape))
  if tuple(sorted(lhs_batch)) != tuple(range(len(lhs_batch))):
    msg = (""dot_general requires lhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got lhs_batch {}."")
    raise TypeError(msg.format(lhs_batch))
  if tuple(sorted(rhs_batch)) != tuple(range(len(rhs_batch))):
    msg = (""dot_general requires rhs batch dimensions to precede contracting ""
           ""and non-contracting dimensions, got rhs_batch {}."")
    raise TypeError(msg.format(rhs_batch))
  if not len(lhs_contracting) == len(rhs_contracting) == 1:
    msg = (""dot_general accepts exactly one lhs_contracting and ""
           ""rhs_contracting dimension, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting, rhs_contracting))
  lhs_contracting_shape = onp.take(lhs.shape, lhs_contracting)
  rhs_contracting_shape = onp.take(rhs.shape, rhs_contracting)
  if not onp.all(onp.equal(lhs_contracting_shape, rhs_contracting_shape)):
    msg = (""dot_general requires contracting dimensions to have the same ""
           ""shape, got {} and {}."")
    raise TypeError(msg.format(lhs_contracting_shape, rhs_contracting_shape))
  if lhs.ndim > len(lhs_batch) + len(lhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""lhs dimension, got {}."")
    diff = lhs.ndim - len(lhs_batch) - len(lhs_contracting)
    raise TypeError(msg.format(diff))
  if rhs.ndim > len(rhs_batch) + len(rhs_contracting) + 1:
    msg = (""dot_general requires either one or zero non-batch non-contracting ""
           ""rhs dimension, got {}."")
    diff = rhs.ndim - len(rhs_batch) - len(rhs_contracting)
    raise TypeError(msg.format(diff))

  batch_shape = tuple(onp.take(lhs.shape, lhs_batch))
  lhs_contract_or_batch = tuple(lhs_contracting) + tuple(lhs_batch)
  lhs_tensored_shape = tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
  rhs_contract_or_batch = tuple(rhs_contracting) + tuple(rhs_batch)
  rhs_tensored_shape = tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
  return batch_shape + lhs_tensored_shape + rhs_tensored_shape


def dot_general_dtype_rule(lhs, rhs, dimension_numbers):
  return binop_dtype_rule(_input_dtype, [_num, _num], 'dot_general', lhs, rhs)


def dot_general_transpose_lhs(g, y, dimension_numbers, swap_ans=False):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  x_ndim = g.ndim - y.ndim + len(x_batch) + 2 * len(x_contract)
  x_kept = remaining(range(x_ndim), x_contract, x_batch)
  y_kept = remaining(range(y.ndim), y_contract, y_batch)
  if swap_ans:
    ans_batch, ans_y, _ = ranges_like(x_batch, y_kept, x_kept)
  else:
    ans_batch, _, ans_y = ranges_like(x_batch, x_kept, y_kept)
  dims = ((ans_y, y_kept), (ans_batch, y_batch))
  x_contract_sorted_by_y = list(onp.take(x_contract, onp.argsort(y_contract)))
  out_axes = onp.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
  return transpose(dot_general(g, y, dims), tuple(out_axes))

def dot_general_transpose_rhs(g, x, dimension_numbers):
  (x_contract, y_contract), (x_batch, y_batch) = dimension_numbers
  swapped_dimension_numbers = ((y_contract, x_contract), (y_batch, x_batch))
  return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)


def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers
  lhs, rhs = batched_args
  lbd, rbd = batch_dims
  assert lbd is not None or rbd is not None

  if lbd is not None:
    if lbd != 0:
      lhs = batching.move_dim_to_front(lhs, lbd)
      lbd = 0
  else:
    assert rbd is not None
    lhs = broadcast(lhs, (rhs.shape[rbd],))
  lhs_contract = tuple(onp.add(1, lhs_contract))
  lhs_batch = (0,) + tuple(onp.add(1, lhs_batch))

  if rbd is not None:
    if rbd != 0:
      rhs = batching.move_dim_to_front(rhs, rbd)
      rbd = 0
  else:
    assert lbd is not None
    rhs = broadcast(rhs, (lhs.shape[lbd],))
  rhs_contract = tuple(onp.add(1, rhs_contract))
  rhs_batch = (0,) + tuple(onp.add(1, rhs_batch))

  new_dimension_numbers = [(lhs_contract, rhs_contract), (lhs_batch, rhs_batch)]
  batched_out = dot_general(lhs, rhs, new_dimension_numbers)
  return batched_out, 0

dot_general_p = standard_primitive(dot_general_shape_rule,
                                   dot_general_dtype_rule, 'dot_general')
ad.defbilinear(dot_general_p,
               dot_general_transpose_lhs, dot_general_transpose_rhs)
batching.primitive_batchers[dot_general_p] = dot_general_batch_rule


def broadcast_shape_rule(operand, sizes):
  _check_shapelike('broadcast', 'sizes', sizes)
  return tuple(sizes) + operand.shape

def broadcast_batch_rule(batched_args, batch_dims, sizes):
  operand, = batched_args
  bdim, = batch_dims
  new_bdim = None if bdim is None else bdim + len(sizes)
  return broadcast(operand, sizes), new_bdim

broadcast_p = standard_primitive(
    broadcast_shape_rule, _input_dtype, 'broadcast')
ad.deflinear(broadcast_p, lambda t, sizes: [_reduce_sum(t, range(len(sizes)))])
batching.primitive_batchers[broadcast_p] = broadcast_batch_rule


def broadcast_in_dim_shape_rule(operand, shape, broadcast_dimensions):
  _check_shapelike('broadcast_in_dim', 'shape', shape)
  _check_shapelike('broadcast_in_dim', 'broadcast_dimensions',
                   broadcast_dimensions)
  if operand.ndim != len(broadcast_dimensions):
    msg = ('broadcast_in_dim broadcast_dimensions must have length equal to '
           'operand ndim, got broadcast_dimensions for operand ndim {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim))
  if not set(broadcast_dimensions).issubset(set(range(len(shape)))):
    msg = ('broadcast_in_dim broadcast_dimensions must be a subset of output '
           'dimensions, got {} for operand ndim {} and shape {}.')
    raise TypeError(msg.format(broadcast_dimensions, operand.ndim, shape))
  return shape

def broadcast_in_dim_transpose_rule(t, shape, broadcast_dimensions):
  axes = tuple(onp.delete(range(len(shape)), broadcast_dimensions))
  return [_reduce_sum(t, axes)]

def broadcast_in_dim_batch_rule(batched_args, batch_dims, shape,
                                broadcast_dimensions):
  operand, = batched_args
  bdim, = batch_dims
  new_shape = list(shape)
  new_shape.insert(bdim, operand.shape[bdim])
  new_broadcast_dimensions = [d if d < bdim else d + 1 for d in broadcast_dimensions]
  new_broadcast_dimensions.insert(bdim, bdim)
  return broadcast_in_dim(operand, new_shape, new_broadcast_dimensions), bdim


broadcast_in_dim_p = standard_primitive(
    broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
ad.deflinear(broadcast_in_dim_p, broadcast_in_dim_transpose_rule)
batching.primitive_batchers[broadcast_in_dim_p] = broadcast_in_dim_batch_rule


def clamp_shape_rule(min, operand, max):
  if min.shape and min.shape != operand.shape:
    m = ""clamp requires min.shape == operand.shape or min.shape == (), got {}.""
    raise TypeError(m.format(min.shape))
  if max.shape and max.shape != operand.shape:
    m = ""clamp requires max.shape == operand.shape or max.shape == (), got {}.""
    raise TypeError(m.format(max.shape))
  return operand.shape

clamp_dtype_rule = partial(binop_dtype_rule, _input_dtype, [_any, _any, _any],
                           'clamp')

clamp_p = standard_primitive(clamp_shape_rule, clamp_dtype_rule, 'clamp')
ad.defjvp(clamp_p,
          lambda g, min, operand, max:
          select(bitwise_and(gt(min, operand), lt(min, max)),
                 _brcast(g, operand), _zeros(operand)),
          lambda g, min, operand, max:
          select(bitwise_and(gt(operand, min), lt(operand, max)),
                 g, _zeros(operand)),
          lambda g, min, operand, max:
          select(lt(max, operand), _brcast(g, operand), _zeros(operand)))


def concatenate_shape_rule(*operands, **kwargs):
  dimension = kwargs.pop('dimension')
  if not operands:
    msg = ""concatenate expects at least one operand, got 0.""
    raise TypeError(msg)
  if not all(isinstance(operand, UnshapedArray) for operand in operands):
    msg = ""All objects to concatenate must be arrays, got {}.""
    op = next(op for op in operands if not isinstance(op, UnshapedArray))
    raise TypeError(msg.format(type(op)))
  if len(set(operand.ndim for operand in operands)) != 1:
    msg = ""Cannot concatenate arrays with different ranks, got {}.""
    raise TypeError(msg.format("", "".join(str(o.ndim) for o in operands)))
  shapes = onp.array([operand.shape for operand in operands])
  if not 0 <= dimension < shapes.shape[1]:
    msg = ""concatenate dimension out of bounds: dimension {} for shapes {}.""
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))
  if not onp.all(onp.delete(shapes[0] == shapes, dimension, axis=1)):
    msg = (""Cannot concatenate arrays with shapes that differ in dimensions ""
           ""other than the one being concatenated: dimension {} for shapes {}."")
    raise TypeError(msg.format(dimension, "", "".join(map(str, shapes))))

  concat_size = sum(o.shape[dimension] for o in operands)
  ex_shape = operands[0].shape
  return ex_shape[:dimension] + (concat_size,) + ex_shape[dimension+1:]

def concatenate_dtype_rule(*operands, **kwargs):
  _check_same_dtypes('concatenate', False, *(o.dtype for o in operands))
  return operands[0].dtype

def concatenate_translation_rule(c, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  return c.Concatenate(operands, dimension=dimension)

def concatenate_transpose_rule(t, *operands, **kwargs):
  dimension = kwargs.pop('dimension')
  operand_shapes = kwargs.pop('operand_shapes')

  if t is ad_util.zero:
    return [ad_util.zero if o is None else None for o in operands]
  else:
    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
    starts = onp.zeros((len(operands), t.ndim), dtype=int)
    starts[1:, dimension] = limit_points[:-1]
    limits = onp.tile(t.shape, (len(operands), 1))
    limits[:, dimension] = limit_points

    return [slice(t, start, limit) if o is None else None
            for o, start, limit in zip(operands, starts, limits)]

concatenate_p = standard_primitive(
    concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
    concatenate_translation_rule)
ad.deflinear(concatenate_p, concatenate_transpose_rule)
ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule


def pad_shape_rule(operand, padding_value, padding_config):
  if operand.dtype != padding_value.dtype:
    msg = ""pad operand and padding_value must be same dtype: got {} and {}.""
    raise TypeError(msg.format(operand.dtype, padding_value.dtype))

  lo, hi, interior = zip(*padding_config)
  out_shape = onp.add(onp.add(onp.add(lo, hi), operand.shape),
                      onp.multiply(interior, onp.subtract(operand.shape, 1)))
  return tuple(out_shape)

def pad_transpose(t, operand, padding_value, padding_config):
  lo, hi, interior = zip(*padding_config)
  if onp.any(onp.less(lo, 0)) or onp.any(onp.less(hi, 0)):
    msg = ""pad transpose not implemented for negative padding, got {}.""
    raise NotImplementedError(msg.format(padding_config))

  total = lambda x: _reduce_sum(x, list(range(t.ndim)))

  t_op = lambda: slice(t, lo, onp.subtract(t.shape, hi), onp.add(interior, 1))
  t_operand = t_op() if operand is None else None

  if padding_value is None:
    t_operand = t_op() if t_operand is None else t_operand
    t_padv = sub(total(t), total(t_operand))
  else:
    t_padv = None

  return [t_operand, t_padv]

pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
ad.deflinear(pad_p, pad_transpose)
ad.primitive_transposes[pad_p] = pad_transpose


def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
  if not onp.all(onp.greater_equal(new_sizes, 0)):
    msg = 'reshape new_sizes must all be positive, got {}.'
    raise TypeError(msg.format(new_sizes))
  if prod(onp.shape(operand)) != prod(new_sizes):
    msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
    raise TypeError(msg.format(new_sizes, onp.shape(operand)))
  if dimensions is not None:
    if set(dimensions) != set(range(onp.ndim(operand))):
      msg = ('reshape dimensions must be a permutation of operand dimensions, '
             'got dimensions {} for shape {}.')
      raise TypeError(msg.format(dimensions, onp.shape(operand)))
  return tuple(new_sizes)

def reshape_dtype_rule(operand, new_sizes, dimensions, **unused_kwargs):
  return operand.dtype

def reshape_translation_rule(c, operand, new_sizes, dimensions, old_sizes):
  del old_sizes  # Unused.
  return c.Reshape(operand, new_sizes=new_sizes, dimensions=dimensions)

def reshape_transpose_rule(t, new_sizes, dimensions, old_sizes):
  out = reshape(t, old_sizes)
  if dimensions is None:
    return [out]
  else:
    return [transpose(out, onp.argsort(dimensions))]

def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused):
  operand, = batched_args
  bdim, = batch_dims
  operand = batching.move_dim_to_front(operand, bdim)
  if dimensions is not None:
    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
    dimensions = (0,) + tuple(onp.add(1, dimensions))
  return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0

reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                               'reshape', reshape_translation_rule)
ad.deflinear(reshape_p, reshape_transpose_rule)
batching.primitive_batchers[reshape_p] = reshape_batch_rule


def rev_shape_rule(operand, dimensions):
  _check_shapelike('rev', 'dimensions', dimensions)
  if len(set(dimensions)) != len(dimensions):
    msg = 'rev dimensions must be unique, got {}.'
    raise TypeError(msg.format(dimensions))
  if not _max(dimensions) < operand.ndim:
    msg = ('rev dimensions must all be less than operand ndim, got dimensions '
           '{} for operand ndim {}.')
    raise TypeError(msg.format(dimensions, operand.ndim))
  return operand.shape

rev_p = standard_primitive(rev_shape_rule, _input_dtype, 'rev')
ad.deflinear(rev_p, lambda t, dimensions: [rev(t, dimensions)])


def transpose_shape_rule(operand, permutation):
  if not isinstance(permutation, (tuple, list, onp.ndarray)):
    msg = ""transpose permutation must be a tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(type(permutation)))
  if tuple(sorted(permutation)) != tuple(range(operand.ndim)):
    msg = (""transpose permutation isn't a permutation of operand dimensions, ""
           ""got permutation {} for operand shape {}."")
    raise TypeError(msg.format(permutation, operand.shape))
  return tuple(onp.take(operand.shape, permutation))

def transpose_batch_rule(batched_args, batch_dims, permutation):
  operand, = batched_args
  bdim, = batch_dims
  perm = tuple(onp.insert(onp.add(permutation, 1), bdim, 0))
  return transpose(operand, perm), 0

transpose_p = standard_primitive(transpose_shape_rule, _input_dtype,
                                 'transpose')
ad.deflinear(transpose_p,
             lambda t, permutation: [transpose(t, onp.argsort(permutation))])
batching.primitive_batchers[transpose_p] = transpose_batch_rule


def select_shape_rule(pred, on_true, on_false):
  if on_true.shape != on_false.shape:
    msg = ""select on_true and on_false must have the same shape, got {} and {}.""
    raise TypeError(msg.format(on_true.shape, on_false.shape))
  if pred.shape and pred.shape != on_true.shape:
    msg = (""select pred must be scalar or have the same shape as on_true and ""
           ""on_false, got pred shape {} for on_true and on_false of shape {}."")
    raise TypeError(msg.format(pred.shape, on_true.shape))
  return on_true.shape

def select_dtype_rule(pred, on_true, on_false):
  _check_same_dtypes(""select"", False, on_true.dtype, on_false.dtype)
  if not onp.issubdtype(pred.dtype, onp.bool_):
    msg = ""select pred must be boolean type, got {}.""
    raise TypeError(msg.format(pred.dtype))
  return on_true.dtype

def select_transpose_rule(t, pred, on_true, on_false):
  return [None,
          select(pred, t, _zeros(on_false)) if on_true is None else None,
          select(pred, _zeros(on_true), t) if on_false is None else None]

def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
  oprand, on_true, on_false, = batched_args
  pred_bdim, ot_bdim, of_bdim = batch_dims

  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.

  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
  ot = _ones(oprand) * on_true
  of = _ones(oprand) * on_false

  return select(oprand, ot, of), pred_bdim

select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
ad.defjvp(select_p,
          None,
          lambda g, b, x, y: select(b, g, _zeros(g)),
          lambda g, b, x, y: select(b, _zeros(g), g))
ad.primitive_transposes[select_p] = select_transpose_rule
batching.primitive_batchers[select_p] = select_batch_rule

def slice_shape_rule(operand, start_indices, limit_indices, strides,
                     operand_shape):
  _check_shapelike(""slice"", ""start_indices"", start_indices)
  _check_shapelike(""slice"", ""limit_indices"", limit_indices)
  if operand.ndim != len(start_indices):
    msg = (""slice start_indices must have length equal to the number of ""
           ""dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(limit_indices):
    msg = (""slice limit_indices must have the same length as start_indices, ""
           ""got start_inidices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if not onp.all(onp.less_equal(limit_indices, operand.shape)):
    msg = (""slice limit_indices must be less than or equal to operand shape, ""
           ""got limit_indices {} for operand shape {}."")
    raise TypeError(msg.format(limit_indices, operand.shape))
  if not onp.all(onp.greater_equal(start_indices, 0)):
    msg = (""slice start_indices must be greater than or equal to zero, ""
           ""got start_indices of {}."")
    raise TypeError(msg.format(start_indices))
  if not onp.all(onp.greater_equal(limit_indices, start_indices)):
    msg = (""slice limit_indices must be greater than or equal to start_indices,""
           "" got start_indices {} and limit_indices {}."")
    raise TypeError(msg.format(start_indices, limit_indices))
  if strides is None:
    strides = onp.ones(operand.ndim, onp.int32)
  else:
    _check_shapelike(""slice"", ""strides"", strides)
    if len(strides) != operand.ndim:
      msg = (""slice strides must have length equal to the number of dimensions ""
             ""of the operand, got strides {} for operand shape {}."")
      raise TypeError(msg.format(strides, operand.shape))
    if not onp.all(onp.greater(strides, 0)):
      msg = ""slice strides must be positive, got {}""
      raise TypeError(msg.format(strides))

  result_shape = onp.floor_divide(
      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
  return tuple(result_shape)

def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
                           operand_shape):
  return c.Slice(operand, start_indices, limit_indices, strides)

def slice_transpose_rule(t, start_indices, limit_indices, strides,
                         operand_shape):
  if strides is None or onp.all(onp.equal(strides, 1)):
    pads = zip(start_indices, onp.subtract(operand_shape, limit_indices),
               (0,) * len(start_indices))
  else:
    real_limits = onp.add(onp.add(start_indices, 1),
                          onp.multiply(onp.subtract(t.shape, 1), strides))
    pads = zip(start_indices, onp.subtract(operand_shape, real_limits),
               onp.subtract(strides, 1))
  result = pad(t, _const(t, 0), pads)
  assert result.shape == operand_shape
  return [result]

def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
                        strides, **unused_kwargs):
  operand, = batched_args
  bdim, = batch_dims

  new_start_indices = list(start_indices)
  new_start_indices.insert(bdim, 0)

  new_limit_indices = list(limit_indices)
  new_limit_indices.insert(bdim, operand.shape[bdim])

  if strides is None:
    new_strides = None
  else:
    new_strides = list(strides)
    new_strides.insert(bdim, 1)

  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
  return out, bdim

slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                             slice_translation_rule)
ad.deflinear(slice_p, slice_transpose_rule)
batching.primitive_batchers[slice_p] = slice_batching_rule


def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,
                             operand_shape):
  if operand.ndim != len(start_indices):
    msg = (""dynamic_slice start_indices must have length equal to the number ""
           ""of dimensions of the operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if len(start_indices) != len(slice_sizes):
    msg = (""dynamic_slice slice_sizes must have the same length as ""
           ""start_indices, got start_inidices length {} and slice_sizes {}."")
    raise TypeError(msg.format(len(start_indices), slice_sizes))
  if not onp.all(onp.less_equal(slice_sizes, operand.shape)):
    msg = (""slice slice_sizes must be less than or equal to operand shape, ""
           ""got slice_sizes {} for operand shape {}."")
    raise TypeError(msg.format(slice_sizes, operand.shape))
  if not onp.all(onp.greater_equal(slice_sizes, 0)):
    msg = (""slice slice_sizes must be greater than or equal to zero, ""
           ""got slice_sizes of {}."")
    raise TypeError(msg.format(slice_sizes))
  return tuple(slice_sizes)

def dynamic_slice_translation_rule(c, operand, start_indices, slice_sizes,
                                   operand_shape):
  return c.DynamicSlice(operand, start_indices, slice_sizes)

def dynamic_slice_jvp_rule(g, operand, start_indices, slice_sizes,
                           operand_shape):
  return dynamic_slice(g, start_indices, slice_sizes)

def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                 operand_shape):
  assert operand is None
  zeros = broadcast(_const(t, 0), operand_shape)
  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]

dynamic_slice_p = standard_primitive(
    dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
    dynamic_slice_translation_rule)
ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule


def dynamic_update_slice_shape_rule(operand, update, start_indices,
                                    update_shape):
  if operand.ndim != update.ndim:
    msg = (""dynamic_update_slice update must have the same rank as operand, ""
           ""got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  if operand.ndim != len(start_indices):
    msg = (""dynamic_update_slice start_indices must have length equal to the ""
           ""rank of operand, got indices {} for operand shape {}."")
    raise TypeError(msg.format(start_indices, operand.shape))
  if not onp.all(onp.less_equal(update.shape, operand.shape)):
    msg = (""dynamic_update_slice update shape must be smaller than operand ""
           ""shape, got update shape {} for operand shape {}."")
    raise TypeError(msg.format(update.shape, operand.shape))
  return operand.shape

def dynamic_update_slice_dtype_rule(operand, update, start_indices,
                                    update_shape):
  _check_same_dtypes(""dynamic_update_slice"", False, operand.dtype, update.dtype)
  return operand.dtype

def dynamic_update_slice_jvp(primals, tangents, update_shape):
  operand, update, start_indices = primals
  g_operand, g_update, g_start_indices = tangents
  assert g_start_indices is ad_util.zero
  val_out = dynamic_update_slice(operand, update, start_indices)
  if g_operand is ad_util.zero and g_update is ad_util.zero:
    tangent_out = ad_util.zero
  else:
    g_operand = ad.instantiate_zeros(operand, g_operand)
    g_update = ad.instantiate_zeros(update, g_update)
    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
  return val_out, tangent_out

def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
                                        update_shape):
  assert start_indices is not None
  dus = dynamic_update_slice
  ds = dynamic_slice
  zeros = _zeros(t, shape=update_shape)
  operand_t = dus(t, zeros, start_indices) if operand is None else None
  update_t = ds(t, start_indices, update_shape) if update is None else None
  return [operand_t, update_t, None]

def dynamic_update_slice_translation_rule(c, operand, update, start_indices,
                                          update_shape):
  return c.DynamicUpdateSlice(operand, update, start_indices)

dynamic_update_slice_p = standard_primitive(
    dynamic_update_slice_shape_rule, dynamic_update_slice_dtype_rule,
    'dynamic_update_slice', dynamic_update_slice_translation_rule)
ad.primitive_jvps[dynamic_update_slice_p] = dynamic_update_slice_jvp
ad.primitive_transposes[dynamic_update_slice_p] = \
    dynamic_update_slice_transpose_rule


def index_take_shape_rule(src, *idxs, **kwargs):
  axes = kwargs['axes']
  return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))

def index_take_translation_rule(c, src, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src,) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src,) + idxs)

def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
  src = primals[0]
  idxs = tuple(primals[1:])
  g = ad.instantiate_zeros(src, tangents[0])
  return index_take(src, idxs, axes), index_take(g, idxs, axes)

def index_take_transpose_rule(t, src, *idxs, **kwargs):
  assert src is None
  axes = kwargs['axes']
  input_shape = kwargs['input_shape']
  t_src = index_untake(t, _zeros(t, shape=input_shape), idxs, axes)
  return [t_src] + [None] * len(idxs)

index_take_p = standard_primitive(index_take_shape_rule, _input_dtype,
                                  'index_take', index_take_translation_rule)
ad.primitive_jvps[index_take_p] = index_take_jvp
ad.primitive_transposes[index_take_p] = index_take_transpose_rule


def index_untake_shape_rule(src, dst, *idxs, **kwargs):
  return dst.shape

def index_untake_translation_rule(c, src, dst, *idxs, **kwargs):
  jaxpr = kwargs['jaxpr']
  consts = kwargs['consts']
  shapes = map(c.GetShape, (src, dst) + idxs)
  xla_computation = xla.jaxpr_computation(jaxpr, consts, (), *shapes)
  return c.Call(xla_computation, (src, dst) + idxs)

def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
  src, dst = primals[0], primals[1]
  idxs = tuple(primals[2:])
  g_src, g_dst = tangents[0], tangents[1]
  g_src = ad.instantiate_zeros(src, g_src)
  g_dst = ad.instantiate_zeros(dst, g_dst)
  val_out = index_untake(src, dst, idxs, axes)
  tangent_out = index_untake(g_src, g_dst, idxs, axes)
  return val_out, tangent_out

def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
  axes = kwargs['axes']
  if src is None:
    t_src = index_take(t, idxs, axes)
  if dst is None:
    t_dst = t
  return [t_src, t_dst] + [None] * len(idxs)

index_untake_p = standard_primitive(
    index_untake_shape_rule, _input_dtype, 'index_untake',
    index_untake_translation_rule)
ad.primitive_jvps[index_untake_p] = index_untake_jvp
ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule


def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
  return tuple(onp.delete(operand.shape, dimensions))

def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.Reduce(operand, init_value, xla_computation, dimensions)

def _reduction_computation(c, jaxpr, consts, init_value):
  shape = c.GetShape(init_value)
  return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)

reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                              reduce_translation_rule)
batching.defreducer(reduce_p)


def reduce_sum_shape_rule(operand, axes, input_shape):
  assert operand.shape == input_shape, ('{} != {}'
                                        .format(operand.shape, input_shape))
  return tuple(onp.delete(operand.shape, axes))

def reduce_sum_translation_rule(c, operand, axes, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(onp.array(0, dtype)),
                  xla.primitive_computation(add_p, scalar, scalar),
                  axes)

def reduce_sum_transpose_rule(cotangent, input_shape, axes):
  broadcast_dimensions = tuple(onp.delete(onp.arange(len(input_shape)), axes))
  result = broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
  assert result.shape == input_shape
  return [result]

reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                  'reduce_sum', reduce_sum_translation_rule)
ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
batching.defreducer(reduce_sum_p)


def reduce_chooser_shape_rule(operand, axes):
  return tuple(onp.delete(operand.shape, axes))

def reduce_chooser_translation_rule(prim, identity, c, operand, axes):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.Reduce(operand, c.Constant(identity(dtype)),
                  xla.primitive_computation(prim, scalar, scalar), axes)

def reduce_chooser_jvp_rule(g, ans, operand, axes):
  # TODO(mattjj): an alternative is to use variadic reduce to compute the chosen
  # locations in a single pass (rather than comparing equality) and use a
  # gather, and/or even push along the chosen elements of g (b/112040122)
  shape = [1 if i in axes else d for i, d in enumerate(operand.shape)]
  location_indicators = convert_element_type(
      _eq_meet(operand, reshape(ans, shape)), g.dtype)
  counts = _reduce_sum(location_indicators, axes)
  return div(_reduce_sum(mul(g, location_indicators), axes), counts)

reduce_max_translation_rule = partial(reduce_chooser_translation_rule, max_p,
                                      _get_max_identity)
reduce_max_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_max', reduce_max_translation_rule)
ad.defjvp2(reduce_max_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_max_p)


reduce_min_translation_rule = partial(
    reduce_chooser_translation_rule, min_p, _get_min_identity)
reduce_min_p = standard_primitive(reduce_chooser_shape_rule, _input_dtype,
                                  'reduce_min', reduce_min_translation_rule)
ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
batching.defreducer(reduce_min_p)


def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                             window_dimensions, window_strides, padding):
  if operand.dtype != init_value.dtype:
    msg = (""reduce_window got inconsistent dtypes for operand and init_value: ""
           "" got operand dtype {} and init_value dtype {}."")
    raise TypeError(msg.format(operand.dtype, init_value.dtype))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_translation_rule(c, operand, init_value, jaxpr, consts,
                                   window_dimensions, window_strides, padding):
  xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
  return c.ReduceWindow(operand, init_value, xla_computation, window_dimensions,
                        window_strides, padding)

reduce_window_p = standard_primitive(
    reduce_window_shape_rule, _input_dtype, 'reduce_window',
    reduce_window_translation_rule)


def reduce_window_sum_shape_rule(operand, window_dimensions, window_strides,
                                 padding, input_shape):
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def reduce_window_sum_translation_rule(c, operand, window_dimensions,
                                       window_strides, padding, input_shape):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(onp.array(0, dtype)),
                        xla.primitive_computation(add_p, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                                     window_strides, padding, input_shape):
  in_pads = padtype_to_pads(input_shape, window_dimensions, window_strides,
                            padding)
  ones = [1] * len(input_shape)
  pads = _conv_general_vjp_lhs_padding(
      input_shape, window_dimensions, window_strides, cotangent.shape, in_pads,
      ones, ones)
  padding_config = [(lo, hi, stride - 1)
                    for (lo, hi), stride in zip(pads, window_strides)]
  pad_cotangent = pad(cotangent, _zero(cotangent), padding_config)
  result = _reduce_window_sum(pad_cotangent, window_dimensions, ones,
                              xla_bridge.get_xla_client().PaddingType.VALID)
  assert result.shape == input_shape
  return [result]

reduce_window_sum_p = standard_primitive(
    reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
    reduce_window_sum_translation_rule)
ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)


def reduce_window_chooser_translation_rule(
    prim, identity, c, operand, window_dimensions, window_strides, padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  return c.ReduceWindow(operand, c.Constant(identity(dtype)),
                        xla.primitive_computation(prim, scalar, scalar),
                        window_dimensions, window_strides, padding)

def reduce_window_chooser_jvp_rule(prim, g, operand, window_dimensions,
                                   window_strides, padding):
  assert prim is max_p or prim is min_p
  select_prim = ge_p if prim is max_p else le_p
  return _select_and_gather_add(g, operand, select_prim, window_dimensions,
                                window_strides, padding)


def common_reduce_window_shape_rule(operand, window_dimensions, window_strides,
                                    padding):
  _check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""reduce_window"", ""window_strides"", window_strides)
  if operand.ndim != len(window_dimensions):
    msg = (""reduce_window got the wrong number of window_dimensions for ""
           ""operand: got operand shape {} with window_dimensions {}."")
    raise TypeError(msg.format(operand.shape, window_dimensions))
  if len(window_strides) != len(window_dimensions):
    msg = (""reduce_window got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))

  return reduce_window_shape_tuple(operand.shape, window_dimensions,
                                   window_strides, padding)

def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                              padding):
  pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
  operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
  t = onp.floor_divide(
      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
  return tuple(t)


reduce_window_max_translation_rule = partial(
    reduce_window_chooser_translation_rule, max_p, _get_max_identity)
reduce_window_max_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
    reduce_window_max_translation_rule)
ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))


reduce_window_min_translation_rule = partial(
    reduce_window_chooser_translation_rule, min_p, _get_min_identity)
reduce_window_min_p = standard_primitive(
    common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min',
    reduce_window_min_translation_rule)
ad.defjvp(reduce_window_min_p, partial(reduce_window_chooser_jvp_rule, min_p))


def select_and_scatter_shape_rule(
    operand, source, init_value, select_jaxpr, select_consts, scatter_jaxpr,
    scatter_consts, window_dimensions, window_strides, padding):
  _check_shapelike(""select_and_scatter"", ""window_dimensions"", window_dimensions)
  _check_shapelike(""select_and_scatter"", ""window_strides"", window_strides)
  if len(window_dimensions) != len(window_strides):
    msg = (""select_and_scatter got inconsistent window_strides and ""
           ""window_dimensions: got window_strides {} and window_dimensions {}."")
    raise TypeError(msg.format(window_strides, window_dimensions))
  return operand.shape

def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
                                   select_consts, scatter_jaxpr, scatter_consts,
                                   window_dimensions, window_strides, padding):
  select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
  scatter = _reduction_computation(c, scatter_jaxpr, scatter_consts, init_value)
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, init_value, scatter)

select_and_scatter_p = standard_primitive(
    select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter',
    select_and_scatter_translation)


def select_and_scatter_add_shape_rule(
    source, operand, select_prim, window_dimensions, window_strides, padding):
  return operand.shape

def select_and_scatter_add_translation(
    c, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  dtype = c.GetShape(operand).numpy_dtype()
  scalar = xla_bridge.Shape.array_shape(dtype, ())
  select = xla.primitive_computation(select_prim, scalar, scalar)
  scatter = xla.primitive_computation(add_p, scalar, scalar)
  zero = c.Constant(onp.array(0, dtype))
  return c.SelectAndScatter(operand, select, window_dimensions, window_strides,
                            padding, source, zero, scatter)

def select_and_scatter_add_transpose(
    t, source, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert source is None and operand is not None
  result = _select_and_gather_add(t, operand, select_prim, window_dimensions,
                                  window_strides, padding)
  return [result, None]

select_and_scatter_add_p = standard_primitive(
    select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
    select_and_scatter_add_translation)
ad.primitive_transposes[select_and_scatter_add_p] = \
    select_and_scatter_add_transpose


def select_and_gather_add_shape_rule(
    tangents, operand, select_prim, window_dimensions, window_strides, padding):
  if tangents.shape != operand.shape:
    msg = (""select_and_gather_add tangents and operand shapes must match, ""
           ""got {} and {}."")
    raise TypeError(msg.format(tangents.shape, operand.shape))
  return common_reduce_window_shape_rule(operand, window_dimensions,
                                         window_strides, padding)

def select_and_gather_add_translation(
    c, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  raise NotImplementedError(""No efficient translation."")

def select_and_gather_add_transpose(
    t, tangents, operand, select_prim, window_dimensions, window_strides,
    padding):
  assert tangents is None and operand is not None
  result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                   window_strides, padding)
  return [result, None]

select_and_gather_add_p = standard_primitive(
    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
    select_and_gather_add_translation)
ad.primitive_transposes[select_and_gather_add_p] = \
    select_and_gather_add_transpose


sort_shape = lambda operand, dimension: operand.shape

def sort_jvp_rule(g, operand, dimension):
  _, g_out = sort_key_val(operand, g, dimension)
  return g_out

sort_p = standard_primitive(sort_shape, _input_dtype, 'sort')
ad.defjvp(sort_p, sort_jvp_rule)


def sort_key_val_abstract_eval(keys, values, dimension):
  return core.AbstractTuple((keys, values))

def sort_key_val_impl(keys, values, dimension):
  out = xla.apply_primitive(sort_key_val_p, keys, values, dimension=dimension)
  sorted_keys, sorted_values = out
  return core.pack((sorted_keys, sorted_values))

def sort_key_val_jvp(primals, tangents, dimension):
  # NOTE(mattjj): this re-sorts three times, but if we had a variadic
  # sort_key_val, or if we could apply a fixed permutation efficiently, we could
  # implement this jvp rule with a single sort. The apply_permutation primitive
  # would make the jvp (and corresponding transpose rule) faster and easier.
  # This would also be cleaner if we didn't get the sorted keys out.
  # TODO(mattjj): make sort_key_val variadic, no sorted keys out by default
  keys, values = primals
  keys_tangents, values_tangents = tangents

  val_out = sort_key_val(keys, values, dimension)

  if keys_tangents is ad_util.zero:
    keys_tangents_out = ad_util.zero
  else:
    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)

  if values_tangents is ad_util.zero:
    values_tangents_out = ad_util.zero
  else:
    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)

  tangents_out = keys_tangents_out, values_tangents_out
  return core.pack(val_out), core.pack(tangents_out)

def sort_key_val_transpose_rule(t, keys, values, dimension):
  t_keys, t_values = t
  assert t_keys is ad_util.zero
  broadcasted_iota = broadcast_in_dim(
      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
  _, perm = sort_key_val(keys, broadcasted_iota)
  keys_result = ad_util.zero if keys is None else None
  values_result = sort_key_val(perm, t_values)[1] if values is None else None
  return [keys_result, values_result]

sort_key_val_p = Primitive('sort_key_val')
sort_key_val_p.def_impl(sort_key_val_impl)
sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule


def while_loop_abstract_eval(init_val, opaque_params):
  abs_out = opaque_params.val[0]
  return maybe_tracer_tuple_to_abstract_tuple(abs_out)

def while_loop_translation_rule(c, init_val, opaque_params):
  shape = c.GetShape(init_val)
  abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts = opaque_params.val
  cond_computation = xla.jaxpr_computation(cond_jaxpr, cond_consts, (), shape)
  body_computation = xla.jaxpr_computation(body_jaxpr, body_consts, (), shape)
  return c.While(cond_computation, body_computation, init_val)

while_p = Primitive('while')
while_p.def_impl(partial(xla.apply_primitive, while_p))
while_p.def_abstract_eval(while_loop_abstract_eval)
xla.translations[while_p] = while_loop_translation_rule


### util


def _dilate_shape(shape, dilation):
  """"""Utility function for computing the shape resulting from a dilation.""""""
  if not onp.all(onp.greater(dilation, 0)):
    msg = ""All dilations must be positive, got {}.""
    raise TypeError(msg.format(dilation))
  dilation = (1,) * (len(shape) - len(dilation)) + tuple(dilation)
  return onp.multiply(dilation, onp.subtract(shape, 1)) + 1



def padtype_to_pads(in_shape, window_shape, window_strides, padding):
  """"""Convert padding string to list of pairs of pad values.""""""
  PaddingType = xla_bridge.get_xla_client().PaddingType

  if isinstance(padding, str):
    mapping = {'VALID': PaddingType.VALID, 'SAME': PaddingType.SAME}
    try:
      padding = mapping[padding.upper()]
    except KeyError:
      msg = ""Unrecognized padding type: expected 'VALID' or 'SAME', got {}.""
      raise RuntimeError(msg.format(padding))

  if padding == PaddingType.SAME:
    out_shape = onp.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
    pad_sizes = [_max((out_size - 1) * stride + window_shape - in_size, 0)
                 for out_size, stride, window_shape, in_size
                 in zip(out_shape, window_strides, window_shape, in_shape)]
    return [(pad_size // 2, pad_size - pad_size // 2) for pad_size in pad_sizes]
  elif padding == PaddingType.VALID:
    return [(0, 0)] * len(in_shape)
  else:
    msg = ""Unknown padding type: {}.""
    raise TypeError(msg.format(padding))


def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
  """"""Check that dtypes agree, possibly ignoring float precision.""""""
  # the `ignore_fp_precision` flag exists because the XLA shape inference logic
  # allows mixed floating point precision, but the HLO verifier often rejects it
  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
  if ignore_fp_precision:
    dtypes = [
        onp.floating if onp.issubdtype(dtype, onp.floating)
        else onp.complexfloating if onp.issubdtype(dtype, onp.complexfloating)
        else dtype for dtype in dtypes]
  if len({xla_bridge.canonicalize_dtype(t) for t in dtypes}) != 1:
    if ignore_fp_precision:
      msg = (""{} requires arguments to have same dtypes up to floating point ""
             ""precision, got {}."")
    else:
      msg = ""{} requires arguments to have the same dtypes, got {}.""
    raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))


def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
  """"""Check that conv shapes are valid and are consistent with window_strides.""""""
  if len(lhs_shape) != len(rhs_shape):
    msg = ""Arguments to {} must have same rank, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if len(lhs_shape) < 2:
    msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
  if lhs_shape[1] != rhs_shape[1]:
    msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
  _check_shapelike(name, ""window_strides"", window_strides)
  if not onp.all(onp.greater(window_strides, 0)):
    msg = ""All elements of window_strides must be positive, got {}.""
    raise TypeError(msg.format(window_strides))
  if len(window_strides) != len(lhs_shape) - 2:
    msg = ""{} window_strides has wrong length: expected {}, got {}.""
    expected_length = len(lhs_shape) - 2
    raise TypeError(msg.format(name, expected_length, len(window_strides)))


def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
  """"""Compute the shape tuple of a conv given input shapes in canonical order.""""""
  if isinstance(pads, str):
    pads = padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
  if len(pads) != len(lhs_shape) - 2:
    msg = ""Wrong number of explicit pads for convolution: expected {}, got {}.""
    raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))

  lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
  out_space = onp.floor_divide(
      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
  out_space = onp.maximum(0, out_space)
  out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
  return tuple(out_shape)


def conv_general_shape_tuple(lhs_shape, rhs_shape, window_strides, padding,
                             dimension_numbers):
  lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
  lhs_trans = onp.take(lhs_shape, lhs_perm)
  rhs_trans = onp.take(rhs_shape, rhs_perm)
  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
  return tuple(onp.take(out_trans, onp.argsort(out_perm)))


def _check_shapelike(fun_name, arg_name, obj):
  """"""Check that `obj` is a shape-like value (e.g. tuple of nonnegative ints).""""""
  if not isinstance(obj, (tuple, list, onp.ndarray)):
    msg = ""{} {} must be of type tuple/list/ndarray, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, type(obj)))
  # bool(obj) for an ndarray raises an error, so we check len
  if not len(obj):  # pylint: disable=g-explicit-length-test
    return
  obj_arr = onp.array(obj)
  if obj_arr.ndim != 1:
    msg = ""{} {} must be rank 1, got {}.""
    raise TypeError(msg.format(obj_arr.ndim))
  if not onp.issubdtype(obj_arr.dtype, onp.integer):
    msg = ""{} {} must have every element be an integer type, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj))))
  if not (obj_arr >= 0).all():
    msg = ""{} {} must have every element be nonnegative, got {}.""
    raise TypeError(msg.format(fun_name, arg_name, obj))


def conv_general_permutations(dimension_numbers):
  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
  for i, (a, b) in enumerate(charpairs):
    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
      msg = (""convolution dimension_numbers[{}] must contain the characters ""
             ""'{}' and '{}' exatly once, got {}."")
      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
             ""characters, got {}."")
      raise TypeError(msg.format(i, dimension_numbers[i]))
  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
          set(out_spec) - set(out_char)):
    msg = (""convolution dimension_numbers elements must each have the same ""
           ""set of spatial characters, got {}."")
    raise TypeError(msg.format(dimension_numbers))

  def getperm(spec, charpair):
    spatial = (i for i, c in enumerate(spec) if c not in charpair)
    if spec is not rhs_spec:
      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)

  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
  return lhs_perm, rhs_perm, out_perm


def _dynamic_slice_indices(operand, start_indices):
  if isinstance(start_indices, (tuple, list)):
    start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))


_const = lambda example, val: onp.array(val, _dtype(example))
_zeros = partial(full_like, fill_value=0)
_zero = partial(full_like, shape=(), fill_value=0)
_ones = partial(full_like, fill_value=1)
_one = partial(full_like, shape=(), fill_value=1)
_twos = partial(full_like, fill_value=2)
_two = partial(full_like, shape=(), fill_value=2)

_dtype = onp.result_type
_iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)


def ranges_like(*xs):
  start = 0
  for x in xs:
    x_len = len(x)
    yield range(start, start + x_len)
    start += x_len


def remaining(original, *removed_lists):
  blacklist = set(itertools.chain(*removed_lists))
  return [i for i in original if i not in blacklist]


def _charswap(a, b, s):
  return s.translate(maketrans(a + b, b + a))


def _get_sdims(dimension_numbers):
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(lhs_spec[i]))
  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
                     key=lambda i: rhs_spec.index(out_spec[i]))
  return lhs_sdims, rhs_sdims, out_sdims


ConvolutionDimensionNumbers = collections.namedtuple(
    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])

def _conv_general_proto(dimension_numbers):
  assert type(dimension_numbers) is ConvolutionDimensionNumbers
  lhs_spec, rhs_spec, out_spec = dimension_numbers
  proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
  proto.input_batch_dimension = lhs_spec[0]
  proto.input_feature_dimension = lhs_spec[1]
  proto.output_batch_dimension = out_spec[0]
  proto.output_feature_dimension = out_spec[1]
  proto.kernel_output_feature_dimension = rhs_spec[0]
  proto.kernel_input_feature_dimension = rhs_spec[1]
  proto.input_spatial_dimensions.extend(lhs_spec[2:])
  proto.kernel_spatial_dimensions.extend(rhs_spec[2:])
  proto.output_spatial_dimensions.extend(out_spec[2:])
  return proto


def _conv_general_vjp_lhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  pad_before = onp.subtract(window_dimensions, [lo for lo, _ in padding]) - 1
  pad_after = (onp.add(lhs_dilated_shape, window_dimensions) - 1
               - out_dilated_shape - pad_before)
  return zip(pad_before, pad_after)


def _conv_general_vjp_rhs_padding(
    in_shape, window_dimensions, window_strides, out_shape, padding,
    lhs_dilation, rhs_dilation):
  lhs_dilated_shape = _dilate_shape(in_shape, lhs_dilation)
  rhs_dilated_shape = _dilate_shape(window_dimensions, rhs_dilation)
  out_dilated_shape = _dilate_shape(out_shape, window_strides)
  total_in_pad = out_dilated_shape + rhs_dilated_shape - lhs_dilated_shape - 1
  return [(pad[0], tot - pad[0]) for pad, tot in zip(padding, total_in_pad)]


def _balanced_eq(x, z, y):
  return div(select(_eq_meet(x, z), _ones(z), _zeros(z)),
             select(_eq_meet(y, z), _twos(z), _ones(z)))


def _eq_meet(a, b):
  a_dtype, b_dtype = _dtype(a), _dtype(b)
  if a_dtype != b_dtype:
    higher_dtype = onp.promote_types(a_dtype, b_dtype)
    if higher_dtype == a_dtype:
      a = convert_element_type(a, b_dtype)
    else:
      b = convert_element_type(b, a_dtype)
  return eq(a, b)


def maybe_tracer_tuple_to_abstract_tuple(tup):
  if isinstance(tup, pe.JaxprTracerTuple):
    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
  elif isinstance(tup, core.AbstractValue):
    return tup
  elif tup is None:
    return core.AbstractTuple(())  # TODO(dougalm): check this
  else:
    raise TypeError(tup)


def subvals(lst, replace):
  lst = list(lst)
  for i, v in replace:
    lst[i] = v
  return tuple(lst)


def _abstractify(x):
  # abstractify wrapper used internally for primitives like _while_loop
  if isinstance(x, core.Tracer):
    # TODO(mattjj,dougalm): check that it's at least ShapedArray
    return pe.PartialVal((x.aval, core.unit))
  else:
    return pe.PartialVal((xla.abstractify(x), core.unit))
","diff --git a/jax/lax.py b/jax/lax.py
index 4a2bc44fc5bc41912cd290a148d5a923bf81ba58..3d83b755a6768716f9b7956cd13b53461dd12bae 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -692,7 +692,9 @@ def broadcasting_shape_rule(name, *avals):
   if len({len(shape) for shape in shapes}) != 1:
     msg = '{} got arrays of different rank: {}.'
     raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
-  result_shape = onp.max(shapes, axis=0)
+  min_shape = onp.min(shapes, axis=0)
+  max_shape = onp.max(shapes, axis=0)
+  result_shape = onp.where(min_shape == 0, 0, max_shape)
   if not onp.all((shapes == result_shape) | (shapes == 1)):
     msg = '{} got incompatible shapes for broadcasting: {}.'
     raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
",Resource leak (files/sockets),Modify broadcasting shape rule to handle zero-sized dimensions correctly
38927153b1bb6c60a659c02cacf6771c3cbe4b14,"Peter Hawkins: Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.",jax/numpy/lax_numpy.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
import jax.lax as lax
from ..util import memoize
from ..lib import xla_bridge

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  result_shape = onp.max(shapes, axis=0)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    from_dtypes = (_dtype(x) for x in args)
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange
eye = onp.eye


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


@_wraps(onp.repeat)
def repeat(a, repeats, axis=None):
  if not isscalar(repeats):
    raise NotImplementedError(
        ""np.repeat implementation only supports scalar repeats"")
  if axis is None or isscalar(a):
    a = ravel(a)
    axis = 0
  a_shape = list(shape(a))
  num_dims = len(a_shape)
  if axis < 0:
    axis = axis + num_dims

  if axis < 0 or axis >= num_dims:
    raise ValueError(
        ""axis {} is out of bounds for array of dimension {}"".format(
            axis, num_dims))

  # Broadcasts to [..., X, repeats, ...] and reshapes to [..., X * repeats, ...]
  broadcast_shape = list(a_shape)
  broadcast_shape.insert(axis + 1, repeats)
  broadcast_dims = onp.concatenate((onp.arange(0, axis + 1),
                                    onp.arange(axis + 2, num_dims + 1)))
  a_shape[axis] *= repeats
  return lax.reshape(
      lax.broadcast_in_dim(a, broadcast_shape, broadcast_dims),
      a_shape)


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


def _not_implemented(fun):
  def wrapped(*args, **kwargs):
    raise Exception(""Numpy function {} not yet implemented"".format(fun))
  return wrapped

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), False otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, False otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from six.moves import builtins

import six
import numpy as onp

from .. import core
from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
from ..interpreters.xla import DeviceArray
import jax.lax as lax
from ..util import memoize
from ..lib import xla_bridge

# To provide the same module-level names as Numpy, we need to redefine builtins
# and also use some common names (like 'shape' and 'dtype') at the top-level.
# pylint: disable=redefined-builtin,redefined-outer-name

# There might be a pylint bug with tuple unpacking.
# pylint: disable=unbalanced-tuple-unpacking

# We get docstrings from the underlying numpy functions.
# pylint: disable=missing-docstring


# We replace some builtin names to follow Numpy's API, so we capture here.
_all = builtins.all
_any = builtins.any
_max = builtins.max
_min = builtins.min
_sum = builtins.sum

# We need some numpy scalars
# TODO(mattjj): handle constants in an indirected, less explicit way?
pi = onp.pi
e = onp.e
inf = onp.inf
nan = onp.nan

# We want isinstance(x, np.ndarray) checks in user code to work with the our
# array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
# array base class). We can override the isinstance behavior directly, without
# having the complexity of multiple inheritance on those classes, by defining
# the ndarray class to have a metaclass with special __instancecheck__ behavior.
_arraylike_types = (onp.ndarray, UnshapedArray, DeviceArray)

class _ArrayMeta(type(onp.ndarray)):
  """"""Metaclass for overriding ndarray isinstance checks.""""""

  def __instancecheck__(self, instance):
    try:
      return isinstance(instance.aval, _arraylike_types)
    except AttributeError:
      return isinstance(instance, _arraylike_types)

# pylint: disable=invalid-name
class ndarray(six.with_metaclass(_ArrayMeta, onp.ndarray)):
  pass
# pylint: enable=invalid-name


isscalar = onp.isscalar
iscomplexobj = onp.iscomplexobj
result_type = onp.result_type
shape = _shape = onp.shape
ndim = _ndim = onp.ndim
size = onp.size
_dtype = lax._dtype

uint32 = onp.uint32
int32 = onp.int32
uint64 = onp.uint64
int64 = onp.int64
float32 = onp.float32
float64 = onp.float64
complex64 = onp.complex64


### utility functions


def _promote_shapes(*args):
  """"""Prepend implicit leading singleton dimensions for Numpy broadcasting.""""""
  if len(args) < 2:
    return args
  else:
    shapes = [shape(arg) for arg in args]
    nd = len(_broadcast_shapes(*shapes))
    return [lax.reshape(arg, (1,) * (nd - len(shp)) + shp)
            if len(shp) != nd else arg for arg, shp in zip(args, shapes)]


@memoize
def _broadcast_shapes(*shapes):
  """"""Apply Numpy broadcasting rules to the given shapes.""""""
  if len(shapes) == 1:
    return shapes[0]
  ndim = _max(len(shape) for shape in shapes)
  shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
  min_shape = onp.min(shapes, axis=0)
  max_shape = onp.max(shapes, axis=0)
  result_shape = onp.where(min_shape == 0, 0, max_shape)
  if not onp.all((shapes == result_shape) | (shapes == 1)):
    raise ValueError(""Incompatible shapes for broadcasting: {}""
                     .format(tuple(map(tuple, shapes))))
  return tuple(result_shape)


def _promote_dtypes(*args):
  """"""Convenience function to apply Numpy argument dtype promotion.""""""
  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
  if len(args) < 2:
    return args
  else:
    from_dtypes = (_dtype(x) for x in args)
    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
    return [lax.convert_element_type(x, to_dtype)
            if _dtype(x) != to_dtype else x for x in args]


def _promote_to_result_dtype(op, *args):
  """"""Convenience function to promote args directly to the op's result dtype.""""""
  to_dtype = _result_dtype(op, *args)
  return [lax.convert_element_type(arg, to_dtype) for arg in args]


def _result_dtype(op, *args):
  """"""Compute result dtype of applying op to arguments with given dtypes.""""""
  args = (onp.ones((0,) * ndim(arg), _dtype(arg)) for arg in args)
  return _dtype(op(*args))


def _check_arraylike(fun_name, *args):
  """"""Check if all args fit JAX's definition of arraylike (ndarray or scalar).""""""
  not_array = lambda x: not isinstance(x, ndarray) and not onp.isscalar(x)
  if _any(not_array(arg) for arg in args):
    pos, arg = next((i, arg) for i, arg in enumerate(args) if not_array(arg))
    msg = ""{} requires ndarray or scalar arguments, got {} at position {}.""
    raise TypeError(msg.format(fun_name, type(arg), pos))


def _promote_args(fun_name, *args):
  """"""Convenience function to apply Numpy argument shape and dtype promotion.""""""
  _check_arraylike(fun_name, *args)
  return _promote_shapes(*_promote_dtypes(*args))


def _promote_args_like(op, *args):
  """"""Convenience function to apply shape and dtype promotion to result type.""""""
  _check_arraylike(op.__name__, *args)
  return _promote_shapes(*_promote_to_result_dtype(op, *args))


def _constant_like(x, const):
  return onp.array(const, dtype=_dtype(x))


def _wraps(fun):
  """"""Like functools.wraps but works with numpy.ufuncs.""""""
  docstr = """"""
  LAX-backed implementation of {fun}. Original docstring below.

  {np_doc}
  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
  def wrap(op):
    try:
      op.__name__ = fun.__name__
      op.__doc__ = docstr
    finally:
      return op
  return wrap


### implementations of numpy functions in terms of lax


def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
  if promote_to_result_dtype:
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
  else:
    name = numpy_fn.__name__
    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
  return _wraps(numpy_fn)(promoted_lax_fn)

absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
add = _one_to_one_op(onp.add, lax.add)
bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
ceil = _one_to_one_op(onp.ceil, lax.ceil)
equal = _one_to_one_op(onp.equal, lax.eq)
expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
exp = _one_to_one_op(onp.exp, lax.exp, True)
floor = _one_to_one_op(onp.floor, lax.floor)
greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
greater = _one_to_one_op(onp.greater, lax.gt)
isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
less_equal = _one_to_one_op(onp.less_equal, lax.le)
less = _one_to_one_op(onp.less, lax.lt)
log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
log = _one_to_one_op(onp.log, lax.log, True)
maximum = _one_to_one_op(onp.maximum, lax.max)
minimum = _one_to_one_op(onp.minimum, lax.min)
multiply = _one_to_one_op(onp.multiply, lax.mul)
negative = _one_to_one_op(onp.negative, lax.neg)
not_equal = _one_to_one_op(onp.not_equal, lax.ne)
power = _one_to_one_op(onp.power, lax.pow, True)
sign = _one_to_one_op(onp.sign, lax.sign)
subtract = _one_to_one_op(onp.subtract, lax.sub)
tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
sort = _one_to_one_op(onp.sort, lax.sort)


def _logical_op(np_op, bitwise_op):
  @_wraps(np_op)
  def op(*args):
    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
    args = (x if onp.issubdtype(_dtype(x), onp.bool_) else lax.ne(x, zero(x))
            for x in args)
    return bitwise_op(*_promote_args(np_op.__name__, *args))
  return op

logical_and = _logical_op(onp.logical_and, lax.bitwise_and)
logical_not = _logical_op(onp.logical_not, lax.bitwise_not)
logical_or = _logical_op(onp.logical_or, lax.bitwise_or)
logical_xor = _logical_op(onp.logical_xor, lax.bitwise_xor)


@_wraps(onp.true_divide)
def true_divide(x1, x2):
  x1, x2 = _promote_shapes(x1, x2)
  result_dtype = _result_dtype(onp.true_divide, x1, x2)
  return lax.div(lax.convert_element_type(x1, result_dtype),
                 lax.convert_element_type(x2, result_dtype))


@_wraps(onp.divide)
def divide(x1, x2):
  # decide whether to perform integer division based on Numpy result dtype, as a
  # way to check whether Python 3 style division is active in Numpy
  result_dtype = _result_dtype(onp.divide, x1, x2)
  if onp.issubdtype(result_dtype, onp.integer):
    return floor_divide(x1, x2)
  else:
    return true_divide(x1, x2)


@_wraps(onp.floor_divide)
def floor_divide(x1, x2):
  x1, x2 = _promote_args(""floor_divide"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    quotient = lax.div(x1, x2)
    select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
    # TODO(mattjj): investigate why subtracting a scalar was causing promotion
    return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
  else:
    return _float_divmod(x1, x2)[0]


@_wraps(onp.divmod)
def divmod(x1, x2):
  x1, x2 = _promote_args(""divmod"", x1, x2)
  if onp.issubdtype(_dtype(x1), onp.integer):
    return floor_divide(x1, x2), remainder(x1, x2)
  else:
    return _float_divmod(x1, x2)


def _float_divmod(x1, x2):
  # see float_divmod in floatobject.c of CPython
  mod = lax.rem(x1, x2)
  div = lax.div(lax.sub(x1, mod), x2)

  ind = lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
  mod = lax.select(ind, mod + x1, mod)
  div = lax.select(ind, div - _constant_like(div, 1), div)

  return lax.round(div), mod


def logaddexp(x1, x2):
  x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
  amax = lax.max(x1, x2)
  return lax.add(amax, lax.log(lax.add(lax.exp(lax.sub(x1, amax)),
                                       lax.exp(lax.sub(x2, amax)))))


@_wraps(onp.remainder)
def remainder(x1, x2):
  x1, x2 = _promote_args(""remainder"", x1, x2)
  return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
mod = remainder
fmod = lax.rem


def sqrt(x):
  x, = _promote_to_result_dtype(onp.sqrt, x)
  return power(x, _constant_like(x, 0.5))


@_wraps(onp.transpose)
def transpose(x, axis=None):
  axis = onp.arange(ndim(x))[::-1] if axis is None else axis
  return lax.transpose(x, axis)


@_wraps(onp.sinh)
def sinh(x):
  x, = _promote_to_result_dtype(onp.sinh, x)
  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.cosh)
def cosh(x):
  x, = _promote_to_result_dtype(onp.cosh, x)
  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))


@_wraps(onp.sin)
def sin(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.sin(x)


@_wraps(onp.cos)
def cos(x):
  x, = _promote_to_result_dtype(onp.sin, x)
  return lax.cos(x)


@_wraps(onp.conjugate)
def conjugate(x):
  return lax.conj(x) if iscomplexobj(x) else x
conj = conjugate


@_wraps(onp.imag)
def imag(x):
  return lax.imag(x) if iscomplexobj(x) else x


@_wraps(onp.real)
def real(x):
  return lax.real(x) if iscomplexobj(x) else x


@_wraps(onp.angle)
def angle(x):
  if iscomplexobj(x):
    return lax.atan2(lax.imag(x), lax.real(x))
  else:
    return zeros_like(x)


@_wraps(onp.reshape)
def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
  if order == ""C"" or order is None:
    dims = None
  elif order == ""F"":
    dims = onp.arange(ndim(a))[::-1]
  elif order == ""A"":
    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
  else:
    raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))

  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
  computed_newshape = onp.reshape(dummy_val, newshape).shape
  return lax.reshape(a, computed_newshape, dims)


@_wraps(onp.ravel)
def ravel(a, order=""C""):
  if order == ""K"":
    raise NotImplementedError(""Ravel not implemented for order='K'."")
  return reshape(a, (size(a),), order)


@_wraps(onp.squeeze)
def squeeze(a, axis=None):
  if 1 not in shape(a):
    return a
  if axis is None:
    newshape = [d for d in shape(a) if d != 1]
  else:
    axis = frozenset(onp.mod(axis, ndim(a)).reshape(-1))
    newshape = [d for i, d in enumerate(shape(a))
                if d != 1 or i not in axis]
  return lax.reshape(a, newshape)


@_wraps(onp.expand_dims)
def expand_dims(a, axis):
  shape = _shape(a)
  axis = axis % (ndim(a) + 1)  # pylint: disable=g-no-augmented-assignment
  return lax.reshape(a, shape[:axis] + (1,) + shape[axis:])


@_wraps(onp.swapaxes)
def swapaxes(a, axis1, axis2):
  perm = onp.arange(ndim(a))
  perm[axis1], perm[axis2] = perm[axis2], perm[axis1]
  return lax.transpose(a, perm)


@_wraps(onp.moveaxis)
def moveaxis(a, source, destination):
  source = onp.mod(source, ndim(a)).reshape(-1)
  destination = onp.mod(destination, ndim(a)).reshape(-1)
  if len(source) != len(destination):
    raise ValueError(""Inconsistent number of elements: {} vs {}""
                     .format(len(source), len(destination)))
  perm = [i for i in range(ndim(a)) if i not in source]
  for dest, src in sorted(zip(destination, source)):
    perm.insert(dest, src)
  return lax.transpose(a, perm)


@_wraps(onp.isclose)
def isclose(a, b, rtol=1e-05, atol=1e-08):
  a, b = _promote_args(""isclose"", a, b)
  rtol = lax.convert_element_type(rtol, _dtype(a))
  atol = lax.convert_element_type(atol, _dtype(a))
  return lax.le(lax.abs(lax.sub(a, b)),
                lax.add(atol, lax.mul(rtol, lax.abs(b))))


@_wraps(onp.where)
def where(condition, x=None, y=None):
  if x is None or y is None:
    raise ValueError(""Must use the three-argument form of where()."")
  if not onp.issubdtype(_dtype(condition), onp.bool_):
    condition = lax.ne(condition, zeros_like(condition))
  condition, x, y = broadcast_arrays(condition, x, y)
  return lax.select(condition, *_promote_dtypes(x, y))


def broadcast_arrays(*args):
  """"""Like Numpy's broadcast_arrays but doesn't return views.""""""
  shapes = [shape(arg) for arg in args]
  if len(set(shapes)) == 1:
    return [arg if isinstance(arg, ndarray) or isscalar(arg) else array(arg)
            for arg in args]
  result_shape = _broadcast_shapes(*shapes)
  return [broadcast_to(arg, result_shape) for arg in args]


def broadcast_to(arr, shape):
  """"""Like Numpy's broadcast_to but doesn't necessarily return views.""""""
  arr = arr if isinstance(arr, ndarray) or isscalar(arr) else array(arr)
  if _shape(arr) != shape:
    # TODO(mattjj): revise this to call lax.broadcast_in_dim rather than
    # lax.broadcast and lax.transpose
    _broadcast_shapes(shape, _shape(arr))  # error checking
    nlead = len(shape) - len(_shape(arr))
    diff, = onp.where(onp.not_equal(shape[nlead:], _shape(arr)))

    new_dims = tuple(range(nlead)) + tuple(nlead + diff)
    kept_dims = tuple(onp.delete(onp.arange(len(shape)), new_dims))
    perm = onp.argsort(new_dims + kept_dims)

    broadcast_dims = onp.take(shape, new_dims)
    squeezed_array = squeeze(arr, diff)
    return lax.transpose(lax.broadcast(squeezed_array, broadcast_dims), perm)
  else:
    return arr


@_wraps(onp.split)
def split(ary, indices_or_sections, axis=0):
  dummy_val = onp.broadcast_to(0, ary.shape)  # zero strides
  subarrays = onp.split(dummy_val, indices_or_sections, axis)  # shapes
  split_indices = onp.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
  starts, ends = [0] * ndim(ary), shape(ary)
  _subval = lambda x, i, v: lax.subvals(x, [(i, v)])
  return [lax.slice(ary, _subval(starts, axis, start), _subval(ends, axis, end))
          for start, end in zip(split_indices[:-1], split_indices[1:])]


@_wraps(onp.clip)
def clip(a, a_min=None, a_max=None):
  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
  if _dtype(a_min) != _dtype(a):
    a_min = lax.convert_element_type(a_min, _dtype(a))
  if _dtype(a_max) != _dtype(a):
    a_max = lax.convert_element_type(a_max, _dtype(a))
  return lax.clamp(a_min, a, a_max)


def _dtype_info(dtype):
  """"""Helper function for to get dtype info needed for clipping.""""""
  if onp.issubdtype(dtype, onp.integer):
    return onp.iinfo(dtype)
  return onp.finfo(dtype)


@_wraps(onp.round)
def round(a, decimals=0):
  if onp.issubdtype(_dtype(a), onp.integer):
    return a  # no-op on integer types

  if decimals == 0:
    return lax.round(a)

  factor = _constant_like(a, 10 ** decimals)
  return lax.div(lax.round(lax.mul(a, factor)), factor)
around = round


### Reducers


def _make_reduction(np_fun, op, init_val):
  """"""Creates reduction function given a binary operation and monoid identity.""""""

  @_wraps(op)
  def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
    if out is not None:
      raise ValueError(""reduction does not support `out` argument."")

    a = a if isinstance(a, ndarray) else asarray(a)
    dims = _reduction_dims(a, axis)
    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
    if _dtype(a) != result_dtype:
      a = lax.convert_element_type(a, result_dtype)
    result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
    if keepdims:
      shape_with_singletons = lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
      result = lax.reshape(result, shape_with_singletons)
    if dtype and onp.dtype(dtype) != onp.dtype(result_dtype):
      result = lax.convert_element_type(result, dtype)
    return result

  return reduction


def _reduction_dims(a, axis):
  if axis is None:
    return onp.arange(ndim(a))
  elif isinstance(axis, (onp.ndarray, tuple, list)):
    return onp.mod(onp.asarray(axis), ndim(a))
  elif isinstance(axis, int):
    return onp.mod([axis], ndim(a))
  else:
    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))


def _reduction_init_val(a, init_val):
  a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
  try:
    return onp.array(init_val, dtype=a_dtype)
  except OverflowError:
    assert onp.issubdtype(a_dtype, onp.integer)
    sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
    return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)


sum = _make_reduction(onp.sum, lax.add, 0)
prod = _make_reduction(onp.prod, lax.mul, 1)
max = _make_reduction(onp.max, lax.max, -onp.inf)
min = _make_reduction(onp.min, lax.min, onp.inf)
all = _make_reduction(onp.all, logical_and, True)
any = _make_reduction(onp.any, logical_or, False)


@_wraps(onp.mean)
def mean(a, axis=None, keepdims=False):
  if axis is None:
    normalizer = size(a)
  else:
    normalizer = onp.prod(onp.take(shape(a), axis))
  if onp.issubdtype(_dtype(a), onp.bool_):
    a = lax.convert_element_type(a, onp.int32)
  return true_divide(sum(a, axis, keepdims=keepdims),
                     _constant_like(a, normalizer))


@_wraps(onp.var)
def var(a, axis=None, keepdims=False, ddof=0):
  if ddof != 0:
    raise NotImplementedError(""Only implemented for ddof=0."")
  centered = subtract(a, mean(a, axis, keepdims=True))
  if iscomplexobj(centered):
    centered = lax.abs(centered)
  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)


@_wraps(onp.std)
def std(a, axis=None, keepdims=False, ddof=0):
  return sqrt(var(a, axis, keepdims, ddof))


@_wraps(onp.allclose)
def allclose(a, b, rtol=1e-05, atol=1e-08):
  return all(isclose(a, b, rtol, atol))


### Array-creation functions


arange = onp.arange
eye = onp.eye


@_wraps(onp.stack)
def stack(arrays):
  if not arrays:
    raise ValueError(""Need at least one array to stack."")
  new_arrays = [reshape(x, (-1,) + onp.shape(x)) for x in arrays]
  return reshape(concatenate(new_arrays), (len(arrays),) + arrays[0].shape)


@_wraps(onp.concatenate)
def concatenate(arrays, axis=0):
  if not arrays:
    raise ValueError(""Need at least one array to concatenate."")
  return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))


@_wraps(onp.vstack)
def vstack(tup):
  return concatenate([atleast_2d(m) for m in tup], axis=0)
row_stack = vstack


@_wraps(onp.hstack)
def hstack(tup):
  arrs = [atleast_1d(m) for m in tup]
  if arrs[0].ndim == 1:
    return concatenate(arrs, 0)
  return concatenate(arrs, 1)


@_wraps(onp.column_stack)
def column_stack(tup):
  arrays = []
  for v in tup:
    arr = array(v)
    if arr.ndim < 2:
      arr = arr.reshape((-1, 1))
    arrays.append(arr)
  return concatenate(arrays, 1)


@_wraps(onp.atleast_1d)
def atleast_1d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 1 else arr.reshape(-1)
  else:
    return [atleast_1d(arr) for arr in arys]


@_wraps(onp.atleast_2d)
def atleast_2d(*arys):
  if len(arys) == 1:
    arr = array(arys[0])
    return arr if arr.ndim >= 2 else arr.reshape((1, -1))
  else:
    return [atleast_2d(arr) for arr in arys]


# TODO(mattjj): can this be simplified?
@_wraps(onp.array)
def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
  del copy  # Unused.
  if ndmin != 0 or order != ""K"":
    raise NotImplementedError(""Only implemented for order='K', ndmin=0."")

  if hasattr(object, '__asarray__'):
    return object.__asarray__(dtype)
  elif isinstance(object, ndarray):
    if dtype and _dtype(object) != dtype:
      return lax.convert_element_type(object, dtype)
    else:
      return object
  elif isinstance(object, (list, tuple)):
    if object:
      subarrays = [expand_dims(array(elt, dtype=dtype), 0) for elt in object]
      return concatenate(subarrays)
    else:
      return onp.array([], dtype)
  elif isscalar(object):
    out = lax.reshape(object, ())
    if dtype and _dtype(out) != dtype:
      return lax.convert_element_type(out, dtype)
    else:
      return out
  else:
    raise TypeError(""Unexpected input type for array: {}"".format(type(object)))
asarray = array


@_wraps(onp.zeros_like)
def zeros_like(x, dtype=None):
  return zeros(_shape(x), dtype or _dtype(x))


@_wraps(onp.ones_like)
def ones_like(x, dtype=None):
  return ones(_shape(x), dtype or _dtype(x))


@_wraps(onp.full)
def full(shape, fill_value, dtype=None):
  if dtype:
    fill_value = lax.convert_element_type(fill_value, dtype)
  return lax.broadcast(fill_value, tuple(shape))


@_wraps(onp.zeros)
def zeros(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))


@_wraps(onp.ones)
def ones(shape, dtype=onp.dtype(""float64"")):
  shape = (shape,) if onp.isscalar(shape) else shape
  dtype = xla_bridge.canonicalize_dtype(dtype)
  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))


@_wraps(onp.repeat)
def repeat(a, repeats, axis=None):
  if not isscalar(repeats):
    raise NotImplementedError(
        ""np.repeat implementation only supports scalar repeats"")
  if axis is None or isscalar(a):
    a = ravel(a)
    axis = 0
  a_shape = list(shape(a))
  num_dims = len(a_shape)
  if axis < 0:
    axis = axis + num_dims

  if axis < 0 or axis >= num_dims:
    raise ValueError(
        ""axis {} is out of bounds for array of dimension {}"".format(
            axis, num_dims))

  # Broadcasts to [..., X, repeats, ...] and reshapes to [..., X * repeats, ...]
  broadcast_shape = list(a_shape)
  broadcast_shape.insert(axis + 1, repeats)
  broadcast_dims = onp.concatenate((onp.arange(0, axis + 1),
                                    onp.arange(axis + 2, num_dims + 1)))
  a_shape[axis] *= repeats
  return lax.reshape(
      lax.broadcast_in_dim(a, broadcast_shape, broadcast_dims),
      a_shape)


### Tensor contraction operations


@_wraps(onp.dot)
def dot(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""dot"", a, b)
  a, b = _promote_dtypes(a, b)
  a_ndim, b_ndim = ndim(a), ndim(b)
  if a_ndim == 0 or b_ndim == 0:
    return lax.mul(a, b)
  if _max(a_ndim, b_ndim) <= 2:
    return lax.dot(a, b)
  a_reshaped = reshape(a, (-1, shape(a)[-1]))
  if _ndim(b) in {1, 2}:
    out = lax.dot(a_reshaped, b)
  else:
    b_reshaped = reshape(moveaxis(b, -2, 0), (shape(b)[-2], -1))
    out = lax.dot(a_reshaped, b_reshaped)
  return lax.reshape(out, a.shape[:-1] + b.shape[:-2] + b.shape[-2:][1:])


@_wraps(onp.matmul)
def matmul(a, b):  # pylint: disable=missing-docstring
  _check_arraylike(""matmul"", a, b)
  a_is_vec, b_is_vec = (ndim(a) == 1), (ndim(b) == 1)
  a = lax.reshape(a, (1,) + shape(a)) if a_is_vec else a
  b = lax.reshape(b, shape(b) + (1,)) if b_is_vec else b

  a, b = _promote_dtypes(a, b)
  batch_shape = _broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
  a = broadcast_to(a, batch_shape + shape(a)[-2:])
  b = broadcast_to(b, batch_shape + shape(b)[-2:])
  batch_dims = tuple(range(len(batch_shape)))
  result = lax.dot_general(a, b, (((ndim(a) - 1,), (ndim(b) - 2,)),
                                  (batch_dims, batch_dims)))

  if a_is_vec or b_is_vec:
    m, n = shape(result)[-2:]
    new_m = () if a_is_vec else (m,)
    new_n = () if b_is_vec else (n,)
    return lax.reshape(result, batch_shape + new_m + new_n)
  else:
    return result


@_wraps(onp.vdot)
def vdot(a, b):
  if onp.issubdtype(_dtype(a), onp.complexfloating):
    a = conj(a)
  return dot(a.ravel(), b.ravel())


### Misc


@_wraps(onp.argmax)
def argmax(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(max, a, axis)


@_wraps(onp.argmin)
def argmin(a, axis=None):
  if axis is None:
    a = ravel(a)
    axis = 0
  return _argminmax(min, a, axis)


# TODO(mattjj): redo this lowering with a call to variadic lax.reduce
def _argminmax(op, a, axis):
  shape = [1] * a.ndim
  shape[axis] = a.shape[axis]
  idxs = onp.arange(a.shape[axis]).reshape(shape)
  maxval = onp.iinfo(xla_bridge.canonicalize_dtype(idxs.dtype)).max
  mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
  return min(mask_idxs, axis)


def _not_implemented(fun):
  def wrapped(*args, **kwargs):
    raise Exception(""Numpy function {} not yet implemented"".format(fun))
  return wrapped

argpartition = _not_implemented(onp.argpartition)
argsort = _not_implemented(onp.argsort)
compress = _not_implemented(onp.compress)
cumprod = _not_implemented(onp.cumprod)
cumsum = _not_implemented(onp.cumsum)
delete = _not_implemented(onp.delete)
diagonal = _not_implemented(onp.diagonal)
insert = _not_implemented(onp.insert)
linspace = _not_implemented(onp.linspace)
nonzero = _not_implemented(onp.nonzero)
ptp = _not_implemented(onp.ptp)
searchsorted = _not_implemented(onp.searchsorted)
take = _not_implemented(onp.take)
trace = _not_implemented(onp.trace)


### Indexing


def _rewriting_take(arr, idx, axis=0):
  """"""A function like numpy.take that handles boxes and rewrites to LAX.""""""

  # Handle special indexers: (), Ellipsis, slice(None), and None.
  # TODO(mattjj): don't compare empty tuple identity (though works for CPython)
  if idx is () or idx is Ellipsis or _is_slice_none(idx):  # pylint: disable=literal-comparison
    return arr
  elif idx is None:
    return expand_dims(arr, 0)


  # Handle int index
  _int = lambda aval: not aval.shape and onp.issubdtype(aval.dtype, onp.integer)
  try:
    abstract_idx = core.get_aval(idx)
  except TypeError:
    abstract_idx = None

  if isinstance(abstract_idx, ConcreteArray) and _int(abstract_idx):
    return lax.index_in_dim(arr, idx, axis, False)
  elif isinstance(abstract_idx, ShapedArray) and _int(abstract_idx):
    idx = mod(idx, arr.shape[axis])
    return lax.dynamic_index_in_dim(arr, idx, axis, False)

  # Handle slice index (only static, otherwise an error is raised)
  elif isinstance(idx, slice):
    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
                for elt in (idx.start, idx.stop, idx.step)):
      msg = (""Array slice indices must have static start/stop/step to be used ""
             ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
      raise IndexError(msg)
    else:
      start, limit, stride, needs_rev = _static_idx(idx, arr.shape[axis])
      result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
      return lax.rev(result, [axis]) if needs_rev else result

  # Handle non-advanced tuple indices by recursing once
  elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
    canonical_idx = _canonicalize_tuple_index(arr, idx)
    result, axis = arr, 0
    for elt in (elt for elt in canonical_idx if elt is not None):
      result = _rewriting_take(result, elt, axis=axis)
      axis += isinstance(elt, slice)   # advance axis index if not eliminated
    unexpanded_shape_itr = iter(result.shape)
    result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
                         for elt in canonical_idx if not isinstance(elt, int))
    return lax.reshape(result, result_shape)

  # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
  # or a tuple with at least one sequence object).
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  # https://gist.github.com/seberg/976373b6a2b7c4188591

  # Handle integer array indexing *without* ellipsis/slices/nones
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
  if _is_advanced_int_indexer_without_slices(idx):
    if isinstance(idx, list):
      if _any(_shape(e) for e in idx):
        # At least one sequence element in the index list means broadcasting.
        idx = broadcast_arrays(*idx)
      else:
        # The index list is a flat list of integers.
        idx = [lax.concatenate([lax.reshape(e, (1,)) for e in idx], 0)]
    else:
      # The indexer is just a single integer array.
      idx = [idx]

    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
    out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
    return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])

  # Handle integer array indexing *with* ellipsis/slices/nones by recursing once
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
  elif _is_advanced_int_indexer(idx):
    canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
    arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))

    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
    idx_advanced, axes = zip(*advanced_pairs)
    idx_advanced = broadcast_arrays(*idx_advanced)

    flat_idx = tuple(mod(ravel(x), arr_sliced.shape[i])
                     for i, x in zip(axes, idx_advanced))
    out = lax.index_take(arr_sliced, flat_idx, axes)
    shape_suffix = tuple(onp.delete(_shape(arr_sliced), axes))
    out = lax.reshape(out, idx_advanced[0].shape + shape_suffix)

    axes_are_contiguous = onp.all(onp.diff(axes) == 1)
    if axes_are_contiguous:
      start = axes[0]
      naxes = idx_advanced[0].ndim
      out = moveaxis(out, list(range(naxes)), list(range(start, start + naxes)))
    return out

  msg = ""Indexing mode not yet supported. Open a feature request!\n{}""
  raise IndexError(msg.format(idx))


def _is_slice_none(idx):
  """"""Return True if idx is equal to slice(None), False otherwise.""""""
  if isinstance(idx, slice):
    return idx.start is None and idx.stop is None and idx.step is None


def _is_advanced_int_indexer(idx):
  """"""Returns True if idx should trigger int array indexing, False otherwise.""""""
  # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
  if isinstance(idx, (tuple, list)):
    # We assume this check comes *after* the check for non-advanced tuple index,
    # and hence we already know at least one element is a sequence
    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
                for e in idx)
  else:
    return _is_int(idx)


def _is_advanced_int_indexer_without_slices(idx):
  """"""Returns True iff idx is an advanced int idx without slice/ellipsis/none.""""""
  if _is_advanced_int_indexer(idx):
    if isinstance(idx, (tuple, list)):
      return not _any(e is None or e is Ellipsis or isinstance(e, slice)
                      for e in idx)
    else:
      return True


def _is_int(x):
  """"""Returns True if x is array-like with integer dtype, False otherwise.""""""
  return (isinstance(x, int) and not isinstance(x, bool)
          or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))


def _canonicalize_tuple_index(arr, idx):
  """"""Helper to remove Ellipsis and add in the implicit trailing slice(None).""""""
  len_without_none = _sum(1 for e in idx if e is not None and e is not Ellipsis)
  if len_without_none > arr.ndim:
    msg = ""Too many indices for array: {} non-None/Ellipsis indices for dim {}.""
    raise IndexError(msg.format(len_without_none, arr.ndim))
  ellipses = (i for i, elt in enumerate(idx) if elt is Ellipsis)
  ellipsis_index = next(ellipses, None)
  if ellipsis_index is not None:
    if next(ellipses, None) is not None:
      msg = ""Multiple ellipses (...) not supported: {}.""
      raise IndexError(msg.format(list(map(type, idx))))
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
  elif len_without_none < arr.ndim:
    colons = (slice(None),) * (arr.ndim - len_without_none)
    idx = tuple(idx) + colons
  return idx


def _static_idx(idx, size):
  """"""Helper function to compute the static slice start/limit/stride values.""""""
  indices = onp.arange(size)[idx]  # get shape statically
  if not len(indices):  # pylint: disable=g-explicit-length-test
    return 0, 0, 1, False  # sliced to size zero
  start, stop_inclusive = indices[0], indices[-1]
  step = 1 if idx.step is None else idx.step
  if step > 0:
    end = _min(stop_inclusive + step, size)
    return start, end, step, False
  else:
    end = _min(start - step, size)
    return stop_inclusive, end, -step, True


### add method and operator overloads to arraylike classes

# We add operator overloads to DeviceArray and ShapedArray. These method and
# operator overloads mainly just forward calls to the corresponding lax_numpy
# functions, which can themselves handle instances from any of these classes.


def _swap_args(f):
  return lambda x, y: f(y, x)

_operators = {
    ""astype"": lax.convert_element_type,
    ""getitem"": _rewriting_take,
    ""neg"": negative,
    ""eq"": equal,
    ""ne"": not_equal,
    ""lt"": less,
    ""le"": less_equal,
    ""gt"": greater,
    ""ge"": greater_equal,
    ""abs"": abs,
    ""add"": add,
    ""radd"": add,
    ""sub"": subtract,
    ""rsub"": _swap_args(subtract),
    ""mul"": multiply,
    ""rmul"": multiply,
    ""div"": divide,
    ""rdiv"": _swap_args(divide),
    ""truediv"": true_divide,
    ""rtruediv"": _swap_args(true_divide),
    ""floordiv"": floor_divide,
    ""rfloordiv"": _swap_args(floor_divide),
    ""divmod"": divmod,
    ""rdivmod"": _swap_args(divmod),
    ""mod"": mod,
    ""rmod"": _swap_args(mod),
    ""pow"": power,
    ""rpow"": _swap_args(power),
    ""matmul"": matmul,
    ""rmatmul"": _swap_args(matmul),
    ""and"": bitwise_and,
    ""rand"": bitwise_and,
    ""or"": bitwise_or,
    ""ror"": bitwise_or,
    ""xor"": bitwise_xor,
    ""rxor"": bitwise_xor,
    ""invert"": bitwise_not,
    ""lshift"": left_shift,
    ""rshift"": right_shift,
}

# These numpy.ndarray methods are just refs to an equivalent numpy function
_nondiff_methods = [""all"", ""any"", ""argmax"", ""argmin"", ""argpartition"", ""argsort"",
                    ""nonzero"", ""searchsorted"", ""round""]
_diff_methods = [""clip"", ""compress"", ""conj"", ""conjugate"", ""cumprod"", ""cumsum"",
                 ""diagonal"", ""dot"", ""max"", ""mean"", ""min"", ""prod"", ""ptp"",
                 ""ravel"", ""repeat"", ""reshape"", ""sort"", ""squeeze"", ""std"", ""sum"",
                 ""swapaxes"", ""take"", ""trace"", ""transpose"", ""var""]


# Set up operator, method, and property forwarding on Tracer instances containing
# ShapedArray avals by following the forwarding conventions for Tracer.
# Forward operators using a single-underscore-prefix naming convention:
for operator_name, function in _operators.items():
  setattr(ShapedArray, ""_{}"".format(operator_name), staticmethod(function))
# Forward methods and properties using core.aval_method and core.aval_property:
for method_name in _nondiff_methods + _diff_methods:
  setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
setattr(ShapedArray, ""T"", core.aval_property(transpose))


# Forward operators, methods, and properies on DeviceArray to lax_numpy
# functions (with no Tracers involved; this forwarding is direct)
for operator_name, function in _operators.items():
  setattr(DeviceArray, ""__{}__"".format(operator_name), function)
for method_name in _nondiff_methods + _diff_methods:
  setattr(DeviceArray, method_name, globals()[method_name])
setattr(DeviceArray, ""flatten"", ravel)
setattr(DeviceArray, ""T"", property(transpose))


# Extra methods that are handy
setattr(DeviceArray, ""broadcast"", lax.broadcast)
","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7da6fb38a04fa648fbf7da767b11676586b08590..15d6cbbf4529c436df17e466c1b1f2b33a7ddc97 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -113,7 +113,9 @@ def _broadcast_shapes(*shapes):
     return shapes[0]
   ndim = _max(len(shape) for shape in shapes)
   shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
-  result_shape = onp.max(shapes, axis=0)
+  min_shape = onp.min(shapes, axis=0)
+  max_shape = onp.max(shapes, axis=0)
+  result_shape = onp.where(min_shape == 0, 0, max_shape)
   if not onp.all((shapes == result_shape) | (shapes == 1)):
     raise ValueError(""Incompatible shapes for broadcasting: {}""
                      .format(tuple(map(tuple, shapes))))
",Incorrect caching / stale cache,Update broadcasting logic to handle zero-sized dimensions correctly
38927153b1bb6c60a659c02cacf6771c3cbe4b14,"Peter Hawkins: Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.",jax/test_util.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import re
import itertools as it
import random

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from six.moves import xrange

from . import api
from .config import flags
from .util import partial
from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce

FLAGS = flags.FLAGS
flags.DEFINE_enum(
    'jax_test_dut',
    None,
    enum_values=['cpu', 'gpu', 'tpu'],
    help=
    'Describes the device under test in case special consideration is required.'
)

flags.DEFINE_integer(
  'num_generated_cases',
  100,
  help='Number of generated cases to test')

EPS = 1e-4
ATOL = 1e-4
RTOL = 1e-4

_dtype = lambda x: getattr(x, 'dtype', None) or onp.asarray(x).dtype


def numpy_eq(x, y):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    return onp.allclose(x, y, 1e-3, 1e-3)
  else:
    return onp.allclose(x, y)


def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    atol = max(atol, 1e-1)
    rtol = max(rtol, 1e-1)
  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)


def check_eq(xs, ys):
  assert tree_all(tree_multimap(numpy_eq, xs, ys)), \
      '\n{} != \n{}'.format(xs, ys)


def check_close(xs, ys, atol=ATOL, rtol=RTOL):
  close = partial(numpy_close, atol=atol, rtol=rtol)
  assert tree_all(tree_multimap(close, xs, ys)), '\n{} != \n{}'.format(xs, ys)


def inner_prod(xs, ys):
  contract = lambda x, y: onp.real(onp.vdot(x, y))
  return tree_reduce(onp.add, tree_multimap(contract, xs, ys))


add = partial(tree_multimap, onp.add)
sub = partial(tree_multimap, onp.subtract)
conj = partial(tree_map, onp.conj)


def scalar_mul(xs, a):
  return tree_map(lambda x: onp.multiply(x, a, dtype=_dtype(x)), xs)


def rand_like(rng, x):
  shape = onp.shape(x)
  dtype = _dtype(x)
  randn = lambda: onp.asarray(rng.randn(*shape), dtype=dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    return randn() + 1.0j * randn()
  else:
    return randn()


def numerical_jvp(f, primals, tangents, eps=EPS):
  delta = scalar_mul(tangents, EPS)
  f_pos = f(*add(primals, delta))
  f_neg = f(*sub(primals, delta))
  return scalar_mul(sub(f_pos, f_neg), 0.5 / EPS)


def check_jvp(f, f_jvp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  rng = onp.random.RandomState(0)
  tangent = tree_map(partial(rand_like, rng), args)
  v_out, t_out = f_jvp(args, tangent)
  v_out_expected = f(*args)
  t_out_expected = numerical_jvp(f, args, tangent, eps=eps)
  check_eq(v_out, v_out_expected)
  check_close(t_out, t_out_expected, atol=atol, rtol=rtol)


def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  _rand_like = partial(rand_like, onp.random.RandomState(0))
  v_out, vjpfun = f_vjp(*args)
  v_out_expected = f(*args)
  check_eq(v_out, v_out_expected)
  tangent = tree_map(_rand_like, args)
  tangent_out = numerical_jvp(f, args, tangent, eps=EPS)
  cotangent = tree_map(_rand_like, v_out)
  cotangent_out = conj(vjpfun(conj(cotangent)))
  ip = inner_prod(tangent, cotangent_out)
  ip_expected = inner_prod(tangent_out, cotangent)
  check_close(ip, ip_expected, atol=atol, rtol=rtol)


def skip_on_devices(*disabled_devices):
  """"""A decorator for test methods to skip the test on certain devices.""""""
  def skip(test_method):
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      device = FLAGS.jax_test_dut
      if device in disabled_devices:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported on {}.'.format(test_name, device.upper()))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def skip_on_flag(flag_name, skip_value):
  """"""A decorator for test methods to skip the test when flags are set.""""""
  def skip(test_method):        # pylint: disable=missing-docstring
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      flag_value = getattr(FLAGS, flag_name)
      if flag_value == skip_value:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported when FLAGS.{} is {}'.format(
                test_name, flag_name, flag_value))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def format_test_name_suffix(opname, shapes, dtypes):
  arg_descriptions = (format_shape_dtype_string(shape, dtype)
                      for shape, dtype in zip(shapes, dtypes))
  return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))


class _NumpyScalar(object):

  def __len__(self):
    return 0

# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
# identical to 0-D arrays, and we want to write tests that exercise both paths.
NUMPY_SCALAR_SHAPE = _NumpyScalar()


def _dims_of_shape(shape):
  """"""Converts `shape` to a tuple of dimensions.""""""
  return shape if shape != NUMPY_SCALAR_SHAPE else ()


def _cast_to_shape(value, shape, dtype):
  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
  if shape != NUMPY_SCALAR_SHAPE:
    return value
  else:
    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
    # scalar.
    return dtype(value)


def format_shape_dtype_string(shape, dtype):
  typestr = onp.dtype(dtype).name
  if shape == NUMPY_SCALAR_SHAPE:
    return typestr

  if onp.isscalar(shape):
    shapestr = str(shape) + ','
  else:
    shapestr = ','.join(str(dim) for dim in shape)
  return '{}[{}]'.format(typestr, shapestr)


def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
  """"""Produce random values given shape, dtype, scale, and post-processor.

  Args:
    rand: a function for producing random values of a given shape, e.g. a
      bound version of either onp.RandomState.randn or onp.RandomState.rand.
    shape: a shape value as a tuple of positive integers.
    dtype: a numpy dtype.
    scale: optional, a multiplicative scale for the random values (default 1).
    post: optional, a callable for post-processing the random values (default
      identity).

  Returns:
    An ndarray of the given shape and dtype using random values based on a call
    to rand but scaled, converted to the appropriate dtype, and post-processed.
  """"""
  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    vals = r() + 1.0j * r()
  else:
    vals = r()
  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)


def rand_default():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3)


def rand_nonzero():
  post = lambda x: onp.where(x == 0, 1, x)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3, post=post)


def rand_positive():
  post = lambda x: x + 1
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2, post=post)


def rand_small():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=1e-3)


def rand_not_small():
  post = lambda x: x + onp.where(x > 0, 10., -10.)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3., post=post)


def rand_small_positive():
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2e-5)


def rand_some_equal():
  randn = npr.RandomState(0).randn
  rng = npr.RandomState(0)

  def post(x):
    flips = rng.rand(*onp.shape(x)) < 0.5
    return onp.where(flips, x.ravel()[0], x)

  return partial(_rand_dtype, randn, scale=100., post=post)


# TODO(mattjj): doesn't handle complex types
def rand_some_inf():
  """"""Return a random sampler that produces infinities in floating types.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    if not onp.issubdtype(dtype, onp.float):
      # only float types have inf
      return base_rand(shape, dtype)

    dims = _dims_of_shape(shape)
    posinf_flips = rng.rand(*dims) < 0.1
    neginf_flips = rng.rand(*dims) < 0.1

    vals = base_rand(shape, dtype)
    vals = onp.where(posinf_flips, onp.inf, vals)
    vals = onp.where(neginf_flips, -onp.inf, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


# TODO(mattjj): doesn't handle complex types
def rand_some_zero():
  """"""Return a random sampler that produces some zeros.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    dims = _dims_of_shape(shape)
    zeros = rng.rand(*dims) < 0.5

    vals = base_rand(shape, dtype)
    vals = onp.where(zeros, 0, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


def rand_bool():
  rng = npr.RandomState(0)
  def generator(shape, dtype):
    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
  return generator

def check_raises(thunk, err_type, msg):
  try:
    thunk()
    assert False
  except err_type as e:
    assert str(e).startswith(msg), ""\n{}\n\n{}\n"".format(e, msg)

def check_raises_regexp(thunk, err_type, pattern):
  try:
    thunk()
    assert False
  except err_type as e:
    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)

random.seed(0) # TODO: consider managing prng state more carefully

def cases_from_list(xs):
  xs = list(xs)
  k = min(len(xs), FLAGS.num_generated_cases)
  return random.sample(xs, k)

def cases_from_gens(*gens):
  sizes = [1, 3, 10]
  cases_per_size = int(FLAGS.num_generated_cases / len(sizes)) + 1
  for size in sizes:
    for i in xrange(cases_per_size):
      yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)


class JaxTestCase(parameterized.TestCase):
  """"""Base class for JAX tests including numerical checks and boilerplate.""""""

  def assertArraysAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y are close (up to numerical tolerances).""""""
    dtype = lambda x: str(onp.asarray(x).dtype)
    tol = 1e-2 if str(onp.dtype(onp.float32)) in {dtype(x), dtype(y)} else 1e-5
    atol = atol or tol
    rtol = rtol or tol

    if FLAGS.jax_test_dut == 'tpu':
      atol = max(atol, 0.5)
      rtol = max(rtol, 1e-1)

    if not onp.allclose(x, y, atol=atol, rtol=rtol, equal_nan=True):
      msg = ('Arguments x and y not equal to tolerance atol={}, rtol={}:\n'
             'x:\n{}\n'
             'y:\n{}\n').format(atol, rtol, x, y)
      raise self.failureException(msg)

    if check_dtypes:
      self.assertDtypesMatch(x, y)

  def assertDtypesMatch(self, x, y):
    if FLAGS.jax_enable_x64:
      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)

  def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""
    if isinstance(x, (tuple, list)):
      self.assertIsInstance(y, (tuple, list))
      self.assertEqual(len(x), len(y))
      for x_elt, y_elt in zip(x, y):
        self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
    else:
      is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
      self.assertTrue(is_array(x))
      self.assertTrue(is_array(y))
      x = onp.asarray(x)
      y = onp.asarray(y)
      self.assertArraysAllClose(x, y, check_dtypes, atol=atol, rtol=rtol)

  def _CompileAndCheck(self, fun, args_maker, check_dtypes,
                       rtol=None, atol=None):
    """"""Helper method for running JAX compilation and allclose assertions.""""""
    args = args_maker()

    def wrapped_fun(*args):
      self.assertTrue(python_should_be_executing)
      return fun(*args)

    python_should_be_executing = True
    python_ans = fun(*args)

    cfun = api.jit(wrapped_fun)
    python_should_be_executing = True
    monitored_ans = cfun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, monitored_ans, check_dtypes, rtol, atol)
    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

    args = args_maker()

    python_should_be_executing = True
    python_ans = fun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

  def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                         check_dtypes=False, tol=1e-5):
    args = args_maker()
    lax_ans = lax_op(*args)
    numpy_ans = numpy_reference_op(*args)
    self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                        atol=tol, rtol=tol)
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import functools
import re
import itertools as it
import random

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp
import numpy.random as npr

from six.moves import xrange

from . import api
from .config import flags
from .util import partial
from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce

FLAGS = flags.FLAGS
flags.DEFINE_enum(
    'jax_test_dut',
    None,
    enum_values=['cpu', 'gpu', 'tpu'],
    help=
    'Describes the device under test in case special consideration is required.'
)

flags.DEFINE_integer(
  'num_generated_cases',
  100,
  help='Number of generated cases to test')

EPS = 1e-4
ATOL = 1e-4
RTOL = 1e-4

_dtype = lambda x: getattr(x, 'dtype', None) or onp.asarray(x).dtype


def numpy_eq(x, y):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    return onp.allclose(x, y, 1e-3, 1e-3)
  else:
    return onp.allclose(x, y)


def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
  testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
  testing_x32 = not FLAGS.jax_enable_x64
  if testing_tpu or testing_x32:
    atol = max(atol, 1e-1)
    rtol = max(rtol, 1e-1)
  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)


def check_eq(xs, ys):
  assert tree_all(tree_multimap(numpy_eq, xs, ys)), \
      '\n{} != \n{}'.format(xs, ys)


def check_close(xs, ys, atol=ATOL, rtol=RTOL):
  close = partial(numpy_close, atol=atol, rtol=rtol)
  assert tree_all(tree_multimap(close, xs, ys)), '\n{} != \n{}'.format(xs, ys)


def inner_prod(xs, ys):
  contract = lambda x, y: onp.real(onp.vdot(x, y))
  return tree_reduce(onp.add, tree_multimap(contract, xs, ys))


add = partial(tree_multimap, onp.add)
sub = partial(tree_multimap, onp.subtract)
conj = partial(tree_map, onp.conj)


def scalar_mul(xs, a):
  return tree_map(lambda x: onp.multiply(x, a, dtype=_dtype(x)), xs)


def rand_like(rng, x):
  shape = onp.shape(x)
  dtype = _dtype(x)
  randn = lambda: onp.asarray(rng.randn(*shape), dtype=dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    return randn() + 1.0j * randn()
  else:
    return randn()


def numerical_jvp(f, primals, tangents, eps=EPS):
  delta = scalar_mul(tangents, EPS)
  f_pos = f(*add(primals, delta))
  f_neg = f(*sub(primals, delta))
  return scalar_mul(sub(f_pos, f_neg), 0.5 / EPS)


def check_jvp(f, f_jvp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  rng = onp.random.RandomState(0)
  tangent = tree_map(partial(rand_like, rng), args)
  v_out, t_out = f_jvp(args, tangent)
  v_out_expected = f(*args)
  t_out_expected = numerical_jvp(f, args, tangent, eps=eps)
  check_eq(v_out, v_out_expected)
  check_close(t_out, t_out_expected, atol=atol, rtol=rtol)


def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
  _rand_like = partial(rand_like, onp.random.RandomState(0))
  v_out, vjpfun = f_vjp(*args)
  v_out_expected = f(*args)
  check_eq(v_out, v_out_expected)
  tangent = tree_map(_rand_like, args)
  tangent_out = numerical_jvp(f, args, tangent, eps=EPS)
  cotangent = tree_map(_rand_like, v_out)
  cotangent_out = conj(vjpfun(conj(cotangent)))
  ip = inner_prod(tangent, cotangent_out)
  ip_expected = inner_prod(tangent_out, cotangent)
  check_close(ip, ip_expected, atol=atol, rtol=rtol)


def skip_on_devices(*disabled_devices):
  """"""A decorator for test methods to skip the test on certain devices.""""""
  def skip(test_method):
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      device = FLAGS.jax_test_dut
      if device in disabled_devices:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported on {}.'.format(test_name, device.upper()))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def skip_on_flag(flag_name, skip_value):
  """"""A decorator for test methods to skip the test when flags are set.""""""
  def skip(test_method):        # pylint: disable=missing-docstring
    @functools.wraps(test_method)
    def test_method_wrapper(self, *args, **kwargs):
      flag_value = getattr(FLAGS, flag_name)
      if flag_value == skip_value:
        test_name = getattr(test_method, '__name__', '[unknown test]')
        return absltest.unittest.skip(
            '{} not supported when FLAGS.{} is {}'.format(
                test_name, flag_name, flag_value))
      return test_method(self, *args, **kwargs)
    return test_method_wrapper
  return skip


def format_test_name_suffix(opname, shapes, dtypes):
  arg_descriptions = (format_shape_dtype_string(shape, dtype)
                      for shape, dtype in zip(shapes, dtypes))
  return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))


class _NumpyScalar(object):

  def __len__(self):
    return 0

# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
# identical to 0-D arrays, and we want to write tests that exercise both paths.
NUMPY_SCALAR_SHAPE = _NumpyScalar()


def _dims_of_shape(shape):
  """"""Converts `shape` to a tuple of dimensions.""""""
  return shape if shape != NUMPY_SCALAR_SHAPE else ()


def _cast_to_shape(value, shape, dtype):
  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
  if shape != NUMPY_SCALAR_SHAPE:
    return value
  else:
    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
    # scalar.
    return dtype(value)


def format_shape_dtype_string(shape, dtype):
  typestr = onp.dtype(dtype).name
  if shape == NUMPY_SCALAR_SHAPE:
    return typestr

  if onp.isscalar(shape):
    shapestr = str(shape) + ','
  else:
    shapestr = ','.join(str(dim) for dim in shape)
  return '{}[{}]'.format(typestr, shapestr)


def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
  """"""Produce random values given shape, dtype, scale, and post-processor.

  Args:
    rand: a function for producing random values of a given shape, e.g. a
      bound version of either onp.RandomState.randn or onp.RandomState.rand.
    shape: a shape value as a tuple of positive integers.
    dtype: a numpy dtype.
    scale: optional, a multiplicative scale for the random values (default 1).
    post: optional, a callable for post-processing the random values (default
      identity).

  Returns:
    An ndarray of the given shape and dtype using random values based on a call
    to rand but scaled, converted to the appropriate dtype, and post-processed.
  """"""
  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
  if onp.issubdtype(dtype, onp.complexfloating):
    vals = r() + 1.0j * r()
  else:
    vals = r()
  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)


def rand_default():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3)


def rand_nonzero():
  post = lambda x: onp.where(x == 0, 1, x)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3, post=post)


def rand_positive():
  post = lambda x: x + 1
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2, post=post)


def rand_small():
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=1e-3)


def rand_not_small():
  post = lambda x: x + onp.where(x > 0, 10., -10.)
  randn = npr.RandomState(0).randn
  return partial(_rand_dtype, randn, scale=3., post=post)


def rand_small_positive():
  rand = npr.RandomState(0).rand
  return partial(_rand_dtype, rand, scale=2e-5)


def rand_some_equal():
  randn = npr.RandomState(0).randn
  rng = npr.RandomState(0)

  def post(x):
    x_ravel = x.ravel()
    if len(x_ravel) == 0:
      return x
    flips = rng.rand(*onp.shape(x)) < 0.5
    return onp.where(flips, x_ravel[0], x)

  return partial(_rand_dtype, randn, scale=100., post=post)


# TODO(mattjj): doesn't handle complex types
def rand_some_inf():
  """"""Return a random sampler that produces infinities in floating types.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    if not onp.issubdtype(dtype, onp.float):
      # only float types have inf
      return base_rand(shape, dtype)

    dims = _dims_of_shape(shape)
    posinf_flips = rng.rand(*dims) < 0.1
    neginf_flips = rng.rand(*dims) < 0.1

    vals = base_rand(shape, dtype)
    vals = onp.where(posinf_flips, onp.inf, vals)
    vals = onp.where(neginf_flips, -onp.inf, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


# TODO(mattjj): doesn't handle complex types
def rand_some_zero():
  """"""Return a random sampler that produces some zeros.""""""
  rng = npr.RandomState(1)
  base_rand = rand_default()

  def rand(shape, dtype):
    """"""The random sampler function.""""""
    dims = _dims_of_shape(shape)
    zeros = rng.rand(*dims) < 0.5

    vals = base_rand(shape, dtype)
    vals = onp.where(zeros, 0, vals)

    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)

  return rand


def rand_bool():
  rng = npr.RandomState(0)
  def generator(shape, dtype):
    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
  return generator

def check_raises(thunk, err_type, msg):
  try:
    thunk()
    assert False
  except err_type as e:
    assert str(e).startswith(msg), ""\n{}\n\n{}\n"".format(e, msg)

def check_raises_regexp(thunk, err_type, pattern):
  try:
    thunk()
    assert False
  except err_type as e:
    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)

random.seed(0) # TODO: consider managing prng state more carefully

def cases_from_list(xs):
  xs = list(xs)
  k = min(len(xs), FLAGS.num_generated_cases)
  return random.sample(xs, k)

def cases_from_gens(*gens):
  sizes = [1, 3, 10]
  cases_per_size = int(FLAGS.num_generated_cases / len(sizes)) + 1
  for size in sizes:
    for i in xrange(cases_per_size):
      yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)


class JaxTestCase(parameterized.TestCase):
  """"""Base class for JAX tests including numerical checks and boilerplate.""""""

  def assertArraysAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y are close (up to numerical tolerances).""""""
    dtype = lambda x: str(onp.asarray(x).dtype)
    tol = 1e-2 if str(onp.dtype(onp.float32)) in {dtype(x), dtype(y)} else 1e-5
    atol = atol or tol
    rtol = rtol or tol

    if FLAGS.jax_test_dut == 'tpu':
      atol = max(atol, 0.5)
      rtol = max(rtol, 1e-1)

    if not onp.allclose(x, y, atol=atol, rtol=rtol, equal_nan=True):
      msg = ('Arguments x and y not equal to tolerance atol={}, rtol={}:\n'
             'x:\n{}\n'
             'y:\n{}\n').format(atol, rtol, x, y)
      raise self.failureException(msg)

    if check_dtypes:
      self.assertDtypesMatch(x, y)

  def assertDtypesMatch(self, x, y):
    if FLAGS.jax_enable_x64:
      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)

  def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
    """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""
    if isinstance(x, (tuple, list)):
      self.assertIsInstance(y, (tuple, list))
      self.assertEqual(len(x), len(y))
      for x_elt, y_elt in zip(x, y):
        self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
    else:
      is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
      self.assertTrue(is_array(x))
      self.assertTrue(is_array(y))
      x = onp.asarray(x)
      y = onp.asarray(y)
      self.assertArraysAllClose(x, y, check_dtypes, atol=atol, rtol=rtol)

  def _CompileAndCheck(self, fun, args_maker, check_dtypes,
                       rtol=None, atol=None):
    """"""Helper method for running JAX compilation and allclose assertions.""""""
    args = args_maker()

    def wrapped_fun(*args):
      self.assertTrue(python_should_be_executing)
      return fun(*args)

    python_should_be_executing = True
    python_ans = fun(*args)

    cfun = api.jit(wrapped_fun)
    python_should_be_executing = True
    monitored_ans = cfun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, monitored_ans, check_dtypes, rtol, atol)
    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

    args = args_maker()

    python_should_be_executing = True
    python_ans = fun(*args)

    python_should_be_executing = False
    compiled_ans = cfun(*args)

    self.assertAllClose(python_ans, compiled_ans, check_dtypes, rtol, atol)

  def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                         check_dtypes=False, tol=1e-5):
    args = args_maker()
    lax_ans = lax_op(*args)
    numpy_ans = numpy_reference_op(*args)
    self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                        atol=tol, rtol=tol)
","diff --git a/jax/test_util.py b/jax/test_util.py
index a605bb781d3f8604741dfe7204ef2064b0289c3b..62189102a0a0eab05e80bb54096a877e08daafdb 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -274,8 +274,11 @@ def rand_some_equal():
   rng = npr.RandomState(0)
 
   def post(x):
+    x_ravel = x.ravel()
+    if len(x_ravel) == 0:
+      return x
     flips = rng.rand(*onp.shape(x)) < 0.5
-    return onp.where(flips, x.ravel()[0], x)
+    return onp.where(flips, x_ravel[0], x)
 
   return partial(_rand_dtype, randn, scale=100., post=post)
 
",Boundary condition / off-by-one,Fix `rand_some_equal` function to handle empty arrays correctly
38927153b1bb6c60a659c02cacf6771c3cbe4b14,"Peter Hawkins: Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.",tests/lax_numpy_test.py,"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

config.parse_flags_with_absl()
FLAGS = config.FLAGS

array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]

all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
unsigned_dtypes = [onp.uint32, onp.uint64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
                                               ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
]

JAX_BITWISE_OP_RECORDS = [
    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
              jtu.rand_bool(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


def _dtypes_are_compatible_for_bitwise_ops(args):
  if len(args) <= 1:
    return True
  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
  width = lambda dtype: onp.iinfo(dtype).bits
  x, y = args
  if width(x) > width(y):
    x, y = y, x
  # The following condition seems a little ad hoc, but seems to capture what
  # numpy actually implements.
  return (
      is_signed(x) == is_signed(y)
      or (width(x) == 32 and width(y) == 32)
      or (width(x) == 32 and width(y) == 64 and is_signed(y)))


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in JAX_BITWISE_OP_RECORDS
      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
      for dtypes in filter(
          _dtypes_are_compatible_for_bitwise_ops,
          CombosWithReplacement(rec.dtypes, rec.nargs))))
  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    if not FLAGS.jax_enable_x64 and any(
        onp.iinfo(dtype).bits == 64 for dtype in dtypes):
      self.skipTest(""x64 types are disabled by jax_enable_x64"")
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True]))
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in all_shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)]))
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2]))
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape))))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_shape=[{}]_axis={}_repeats={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, repeats),
       ""axis"": axis, ""shape"": shape, ""dtype"": dtype, ""repeats"": repeats,
       ""rng"": jtu.rand_default()}
      for repeats in [0, 1, 2]
      for dtype in default_dtypes
      for shape in all_shapes
      for axis in [None] + list(range(-len(shape), len(shape)))))
  def testRepeat(self, axis, shape, dtype, repeats, rng):
    onp_fun = lambda arg: onp.repeat(arg, repeats=repeats, axis=axis)
    lnp_fun = lambda arg: lnp.repeat(arg, repeats=repeats, axis=axis)

    args_maker = lambda: [rng(shape, dtype)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()]))
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes))
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes))
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ]))
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape))))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes))
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes))
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ])))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  absltest.main()
","# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import functools
import itertools

from absl.testing import absltest
from absl.testing import parameterized

import numpy as onp

from jax import api
from jax import numpy as lnp
from jax import test_util as jtu
from jax.config import config

config.parse_flags_with_absl()
FLAGS = config.FLAGS

nonempty_array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
empty_array_shapes = [(0,), (0, 4), (3, 0),]

scalar_shapes = [jtu.NUMPY_SCALAR_SHAPE]
array_shapes = nonempty_array_shapes + empty_array_shapes
nonempty_shapes = scalar_shapes + nonempty_array_shapes
all_shapes =  scalar_shapes + array_shapes

float_dtypes = [onp.float32, onp.float64]
complex_dtypes = [onp.complex64]
int_dtypes = [onp.int32, onp.int64]
unsigned_dtypes = [onp.uint32, onp.uint64]
bool_dtypes = [onp.bool_]
default_dtypes = float_dtypes + int_dtypes
numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes


OpRecord = collections.namedtuple(
  ""OpRecord"",
  [""name"", ""nargs"", ""dtypes"", ""shapes"", ""rng"", ""diff_modes"", ""test_name""])


def op_record(name, nargs, dtypes, shapes, rng, diff_modes, test_name=None):
  test_name = test_name or name
  return OpRecord(name, nargs, dtypes, shapes, rng, diff_modes, test_name)

JAX_ONE_TO_ONE_OP_RECORDS = [
    op_record(""abs"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""add"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""ceil"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
    op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
    #op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
    op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
    op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
    op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
    op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
    op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
    op_record(""logical_and"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
    op_record(""logical_not"", 1, default_dtypes, all_shapes, jtu.rand_bool(), []),
    op_record(""logical_or"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
    op_record(""logical_xor"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
    op_record(""maximum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
    op_record(""minimum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
    op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
    #op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
    op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
]

JAX_COMPOUND_OP_RECORDS = [
    op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
              test_name=""expm1_large""),
    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
    op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
    op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
              test_name=""log1p_large""),
    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
    op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
    op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
    op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
    op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
    op_record(""where"", 3, (onp.float32, onp.int64), all_shapes, jtu.rand_some_zero(), []),
]

JAX_BITWISE_OP_RECORDS = [
    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes, all_shapes,
              jtu.rand_bool(), []),
    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes, all_shapes,
              jtu.rand_bool(), []),
    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes, all_shapes,
              jtu.rand_bool(), []),
    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes, all_shapes,
              jtu.rand_bool(), []),
]

JAX_REDUCER_RECORDS = [
    op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
    op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
    op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
    #op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
    op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
    #op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
    #op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
]

JAX_ARGMINMAX_RECORDS = [
    op_record(""argmin"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
    op_record(""argmax"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
]

CombosWithReplacement = itertools.combinations_with_replacement


def _dtypes_are_compatible_for_bitwise_ops(args):
  if len(args) <= 1:
    return True
  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
  width = lambda dtype: onp.iinfo(dtype).bits
  x, y = args
  if width(x) > width(y):
    x, y = y, x
  # The following condition seems a little ad hoc, but seems to capture what
  # numpy actually implements.
  return (
      is_signed(x) == is_signed(y)
      or (width(x) == 32 and width(y) == 32)
      or (width(x) == 32 and width(y) == 64 and is_signed(y)))

def _shapes_are_broadcast_compatible(shapes):
  accumulator = onp.zeros([])
  for shape in shapes:
    try:
      accumulator = accumulator + onp.zeros(shape)
    except ValueError:
      return False
  return True


class LaxBackedNumpyTests(jtu.JaxTestCase):
  """"""Tests for LAX-backed Numpy implementation.""""""

  def _GetArgsMaker(self, rng, shapes, dtypes):
    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                 JAX_COMPOUND_OP_RECORDS)
      for shapes in filter(
        _shapes_are_broadcast_compatible,
        CombosWithReplacement(rec.shapes, rec.nargs))
      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                    dtypes),
       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
      for rec in JAX_BITWISE_OP_RECORDS
      for shapes in filter(
        _shapes_are_broadcast_compatible,
        CombosWithReplacement(rec.shapes, rec.nargs))
      for dtypes in filter(
        _dtypes_are_compatible_for_bitwise_ops,
        CombosWithReplacement(rec.dtypes, rec.nargs))))
  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
    if not FLAGS.jax_enable_x64 and any(
        onp.iinfo(dtype).bits == 64 for dtype in dtypes):
      self.skipTest(""x64 types are disabled by jax_enable_x64"")
    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis, ""keepdims"": keepdims}
      for rec in JAX_REDUCER_RECORDS
      for shape in rec.shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))
      for keepdims in [False, True]))
  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
          rec.test_name.capitalize(),
          jtu.format_shape_dtype_string(shape, dtype), axis),
       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
       ""axis"": axis}
      for rec in JAX_ARGMINMAX_RECORDS
      for shape in rec.shapes for dtype in rec.dtypes
      for axis in range(-len(shape), len(shape))))
  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):

    def onp_fun(array_to_reduce):
      return onp_op(array_to_reduce, axis)

    def lnp_fun(array_to_reduce):
      return lnp_op(array_to_reduce, axis)

    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""matrix-scalar"", (3, 3), ()),
          (""scalar-matrix"", (), (3, 3)),
          (""matrix-vector"", (4, 5), (5,)),
          (""vector-matrix"", (6,), (6, 4)),
          (""matrix-matrix"", (3, 4), (4, 5)),
          (""tensor-vector"", (4, 3, 2), (2,)),
          (""vector-tensor"", (2,), (3, 2, 4)),
          (""tensor-matrix"", (4, 3, 2), (2, 5)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_{}_{}"".format(
          name,
          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
       ""rng"": rng}
      for rng in [jtu.rand_default()]
      for name, lhs_shape, rhs_shape in [
          (""vector-vector"", (3,), (3,)),
          (""matrix-vector"", (3, 3), (3,)),
          (""vector-matrix"", (3,), (3, 3)),
          (""matrix-matrix"", (3, 3), (3, 3)),
          (""vector-tensor"", (3,), (5, 3, 2)),
          (""tensor-vector"", (5, 3, 2), (2,)),
          (""matrix-tensor"", (5, 2), (3, 2, 4)),
          (""tensor-matrix"", (5, 2, 3), (3, 2)),
          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                            check_dtypes=True)
    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)]))
  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_decimals={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), decimals),
       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
       ""rng"": jtu.rand_default()}
      for shape in all_shapes for dtype in float_dtypes
      for decimals in [0, 1, -2]))
  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
    onp_fun = lambda x: onp.round(x, decimals=decimals)
    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
          axis, "","".join(str(d) for d in base_shape),
          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
       ""rng"": jtu.rand_default()}
      for num_arrs in [3]
      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
      for base_shape in [(4,), (3, 4), (2, 3, 4)]
      for axis in range(-len(base_shape)+1, len(base_shape))))
  def testConcatenate(self, axis, base_shape, dtypes, rng):
    wrapped_axis = axis % len(base_shape)
    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)

    def args_maker():
      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_shape=[{}]_axis={}_repeats={}"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, repeats),
       ""axis"": axis, ""shape"": shape, ""dtype"": dtype, ""repeats"": repeats,
       ""rng"": jtu.rand_default()}
      for repeats in [0, 1, 2]
      for dtype in default_dtypes
      for shape in all_shapes
      for axis in [None] + list(range(-len(shape), len(shape)))))
  def testRepeat(self, axis, shape, dtype, repeats, rng):
    onp_fun = lambda arg: onp.repeat(arg, repeats=repeats, axis=axis)
    lnp_fun = lambda arg: lnp.repeat(arg, repeats=repeats, axis=axis)

    args_maker = lambda: [rng(shape, dtype)]

    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}"".format(
          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
      for dtypes in [
        [onp.float32],
        [onp.float32, onp.float32],
        [onp.float32, onp.int32, onp.float32],
        [onp.float32, onp.int64, onp.float32],
        [onp.float32, onp.int32, onp.float64],
      ]
      for shape in [(), (2,), (3, 4), (1, 100)]
      for rng in [jtu.rand_default()]))
  def testStack(self, shape, dtypes, rng):
    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
          jtu.format_shape_dtype_string(shape, fill_value_dtype),
          onp.dtype(out_dtype).name),
       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
      for shape in array_shapes
      for fill_value_dtype in default_dtypes
      for out_dtype in default_dtypes))
  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
    args_maker = lambda: [rng((), fill_value_dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
       ""dtype"": dtype, ""rng"": jtu.rand_default()}
      for shape, axis, num_sections in [
          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
      for dtype in default_dtypes))
  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
    args_maker = lambda: [rng(shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_outshape={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype),
          jtu.format_shape_dtype_string(out_shape, dtype)),
       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
       ""rng"": jtu.rand_default()}
      for dtype in default_dtypes
      for arg_shape, out_shape in [
          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
          ((), (1, 1, 1)),
          ((7, 0), (0, 42, 101)),
          ((3, 4), 12),
          ((3, 4), (12,)),
          ((3, 4), -1),
          ((2, 1, 4), (-1,)),
          ((2, 2, 4), (2, 8))
      ]))
  def testReshape(self, arg_shape, out_shape, dtype, rng):
    onp_fun = lambda x: onp.reshape(x, out_shape)
    lnp_fun = lambda x: lnp.reshape(x, out_shape)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
       ""rng"": jtu.rand_default()}
      for arg_shape in [(), (3,), (3, 4)]
      for dtype in default_dtypes
      for dim in range(-len(arg_shape)+1, len(arg_shape))))
  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
    onp_fun = lambda x: onp.expand_dims(x, dim)
    lnp_fun = lambda x: lnp.expand_dims(x, dim)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax1, ax2 in [
          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
      for dtype in default_dtypes))
  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_inshape={}_axis={}"".format(
          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
       ""rng"": jtu.rand_default()}
      for arg_shape, ax in [
          ((3, 1), None),
          ((3, 1), 1),
          ((1, 3, 1), (0, 2)),
          ((1, 4, 1), (0,))]
      for dtype in default_dtypes))
  def testSqueeze(self, arg_shape, dtype, ax, rng):
    onp_fun = lambda x: onp.squeeze(x, ax)
    lnp_fun = lambda x: lnp.squeeze(x, ax)
    args_maker = lambda: [rng(arg_shape, dtype)]
    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)

  @parameterized.named_parameters(jtu.cases_from_list(
      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
      for i, arg in enumerate([
          [1, 2, 3], [1., 2., 3.],
          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
          [[3, onp.array(2), 1], onp.arange(3.)],
      ])))
  def testArray(self, arg):
    args_maker = lambda: [arg]
    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)

  def testArrayAsarrayMethod(self):
    class arraylike(object):
      def __asarray__(self, dtype=None):
        return 3.
    a = arraylike()
    ans = lnp.array(a)
    assert ans == 3.

  def testAllClose(self):
    rng = onp.random.RandomState(0)
    x = rng.randn(2, 2)
    y = rng.randn(2)

    def same(list1, list2):
      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
      elements_close = list(map(allclose, list1, list2))
      return lnp.all(lnp.array(elements_close))

    csame = api.jit(same)

    a1 = same((x, y), (x, y))
    a2 = csame((x, y), (x, y))
    a3 = csame((x, y), (x, 2 * y))

    self.assertTrue(a1)
    self.assertTrue(a2)
    self.assertFalse(a3)

  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
  def DISABLED_testOnesBroadcastingConstantHandler(self):
    # TODO(mattjj): update this test for jax3

    def fun(x):
      ones = lnp.ones((3, 4))
      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)

      # To check that the constant handler generates a Broadcast for stride-zero
      # arrays, we monkey-patch the client instance.
      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
      # we can check the HLO more directly.
      c = x._node.c
      Broadcast = c.Broadcast  # pylint: disable=invalid-name
      was_called = []
      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
      out = x + ones  # the ndarray constant handler should call Broadcast here
      assert was_called, ""Broadcast was not called.""

      return out

    fun = api.jit(fun)
    out_val = fun(lnp.ones(4))
    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)

  def testZeroStridesConstantHandler(self):
    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))

    def fun(x):
      return x * const

    fun = api.jit(fun)
    out_val = fun(3.)
    self.assertAllClose(out_val, 3. * const, check_dtypes=False)

  def testIsInstanceNdarrayDuringTracing(self):
    arr = onp.ones(3)

    @api.jit
    def f(x):
      self.assertIsInstance(x, lnp.ndarray)
      return lnp.sum(x)

    f(arr)


  def testNonArrayErrorMessage(self):
    x = [1., 2.]
    y = onp.array([3., 4.])

    def g(x, y):
      return lnp.add(x, y)

    def f(x, y):
      return lnp.dot(x, y)

    self.assertRaises(TypeError, lambda: g(x, y))
    self.assertRaises(TypeError, lambda: f(x, y))
    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))

  def testAbstractionErrorMessage(self):

    @api.jit
    def f(x, n):
      for _ in range(n):
        x = x * x
      return x

    self.assertRaises(TypeError, lambda: f(3., 3))

    @api.jit
    def g(x):
      if x > 0.:
        return x * 2
      else:
        return x + 2

    self.assertRaises(TypeError, lambda: g(3.))

  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
    # TODO(mattjj): update this for jax3
    foo = lnp._not_implemented(lambda x: x)

    # No error if there's no tracing.
    foo(onp.arange(3))

    cfoo = api.jit(foo)
    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))

  # TODO(mattjj): test infix operator overrides

  def DISABLED_testRavel(self):
    # TODO(mattjj): support this method-based syntax?
    rng = onp.random.RandomState(0)
    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)

  # TODO(mattjj): test other ndarray-like method overrides


if __name__ == ""__main__"":
  absltest.main()
","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 2f9b823f55704dad21abbfe5407cca237f79a497..ea6ef11a62d2031663b3200cf814e458acdfba3f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -33,9 +33,13 @@ from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
-array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+empty_array_shapes = [(0,), (0, 4), (3, 0),]
 
-all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
+scalar_shapes = [jtu.NUMPY_SCALAR_SHAPE]
+array_shapes = nonempty_array_shapes + empty_array_shapes
+nonempty_shapes = scalar_shapes + nonempty_array_shapes
+all_shapes =  scalar_shapes + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -46,90 +50,91 @@ default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
 
 
-OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
-                                               ""diff_modes"", ""test_name""])
+OpRecord = collections.namedtuple(
+  ""OpRecord"",
+  [""name"", ""nargs"", ""dtypes"", ""shapes"", ""rng"", ""diff_modes"", ""test_name""])
 
 
-def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+def op_record(name, nargs, dtypes, shapes, rng, diff_modes, test_name=None):
   test_name = test_name or name
-  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+  return OpRecord(name, nargs, dtypes, shapes, rng, diff_modes, test_name)
 
 JAX_ONE_TO_ONE_OP_RECORDS = [
-    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
-    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
-    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
-    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
-    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
-    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""abs"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""ceil"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    #op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
+    #op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+    op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
-    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
-    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
-    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
-    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
-    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
-    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), all_shapes, jtu.rand_some_zero(), []),
 ]
 
 JAX_BITWISE_OP_RECORDS = [
-    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
 ]
 
 JAX_REDUCER_RECORDS = [
-    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
-    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
-    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    #op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    #op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    #op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
-    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmin"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
 ]
 
 CombosWithReplacement = itertools.combinations_with_replacement
@@ -150,6 +155,15 @@ def _dtypes_are_compatible_for_bitwise_ops(args):
       or (width(x) == 32 and width(y) == 32)
       or (width(x) == 32 and width(y) == 64 and is_signed(y)))
 
+def _shapes_are_broadcast_compatible(shapes):
+  accumulator = onp.zeros([])
+  for shape in shapes:
+    try:
+      accumulator = accumulator + onp.zeros(shape)
+    except ValueError:
+      return False
+  return True
+
 
 class LaxBackedNumpyTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed Numpy implementation.""""""
@@ -164,7 +178,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
       for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                  JAX_COMPOUND_OP_RECORDS)
-      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for shapes in filter(
+        _shapes_are_broadcast_compatible,
+        CombosWithReplacement(rec.shapes, rec.nargs))
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
@@ -177,10 +193,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
       for rec in JAX_BITWISE_OP_RECORDS
-      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for shapes in filter(
+        _shapes_are_broadcast_compatible,
+        CombosWithReplacement(rec.shapes, rec.nargs))
       for dtypes in filter(
-          _dtypes_are_compatible_for_bitwise_ops,
-          CombosWithReplacement(rec.dtypes, rec.nargs))))
+        _dtypes_are_compatible_for_bitwise_ops,
+        CombosWithReplacement(rec.dtypes, rec.nargs))))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     if not FLAGS.jax_enable_x64 and any(
         onp.iinfo(dtype).bits == 64 for dtype in dtypes):
@@ -197,7 +215,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis, ""keepdims"": keepdims}
       for rec in JAX_REDUCER_RECORDS
-      for shape in all_shapes for dtype in rec.dtypes
+      for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
@@ -215,7 +233,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis}
       for rec in JAX_ARGMINMAX_RECORDS
-      for shape in all_shapes for dtype in rec.dtypes
+      for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))))
   def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
 
",Race condition / concurrency,"Add shape parameter to OpRecord and update op_record function

* Modified OpRecord namedtuple to include a ""shapes"" parameter"
72b77b5bbe3f181d0efb1cfd9cb88b477f4a4a84,Matthew Johnson: fix typo in notebook text (closes #22),notebooks/gufuncs.ipynb,"{
  ""nbformat"": 4,
  ""nbformat_minor"": 0,
  ""metadata"": {
    ""colab"": {
      ""name"": ""JAX generalized ufuncs.ipynb"",
      ""version"": ""0.3.2"",
      ""provenance"": [],
      ""collapsed_sections"": []
    },
    ""kernelspec"": {
      ""name"": ""python3"",
      ""display_name"": ""Python 3""
    },
    ""accelerator"": ""GPU""
  },
  ""cells"": [
    {
      ""metadata"": {
        ""id"": ""435_M09vl3NA"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""# Extending JAX's vmap to work like NumPy's gufuncs\n"",
        ""\n"",
        ""by [Stephan Hoyer](https://github.com/shoyer)\n"",
        ""\n"",
        ""## What is a gufunc?\n"",
        ""\n"",
        ""[Generalized universal functions](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
        ""- \""core dimensions\"" over which an operation is defined.\n"",
        ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
        ""\n"",
        ""A string [signature](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html#details-of-signature) associated with each gufunc controls how this happens by indicating how core dimensions are mapped between inputs and outputs. The syntax is easiest to understand by looking at a few examples:\n"",
        ""\n"",
        ""- Addition: `(),()->()`\n"",
        ""- 1D inner product: `(i),(i)->()`\n"",
        ""- 1D sum: `(i)->()`\n"",
        ""- Matrix multiplcation: `(m,n),(n,k)->(m,k)`\n"",
        ""\n"",
        ""## Why write gufuncs?\n"",
        ""\n"",
        ""From a user perspective, gufuncs are nice because they're guaranteed to vectorize in a consistent and general fashion. For example, by default gufuncs use the last dimensions of arrays as core dimensions, but you can control that explicitly with the `axis` or `axes` arguments.\n"",
        ""\n"",
        ""From a developer perspective, gufuncs are nice because they simply your work: you only need to think about the core logic of your function, not how it handles arbitrary dimensional input. You can just write that down in a simple, declarative way.\n"",
        ""\n"",
        ""## JAX makes it easy to write high-level performant code\n"",
        ""\n"",
        ""Unfortunately, writing NumPy gufuncs today is somewhat non-trivial. Your options today are:\n"",
        ""\n"",
        ""1. Write the inner loops yourself in C.\n"",
        ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differention.\n"",
        ""\n"",
        ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
        ""\n"",
        ""## A simple example\n"",
        ""\n"",
        ""Consider a simple example from data preprocessing, centering an array.\n"",
        ""\n"",
        ""Here's how we might write a vectorized version using NumPy:\n"",
        ""```python\n"",
        ""def center(array, axis=-1):\n"",
        ""  # array can have any number of dimensions\n"",
        ""  bias = np.mean(array, axis=axis)\n"",
        ""  debiased = array - np.expand_dims(bias, axis)\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""And here's how we could write a vectorized version using JAX gufuncs:\n"",
        ""```python\n"",
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  # array is always a 1D vector\n"",
        ""  bias = np.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""See the difference?\n"",
        ""- Instead of needing to think about broadcasting while writing the entire function, we can write the function assuming the input is always a vector.\n"",
        ""- We get the `axis` argument automatically, without needing to write it ourselves.\n"",
        ""- As a bonus, the decorator makes the function self-documenting: a reader immediately knows that it handles higher dimensional input and output correctly.\n"",
        ""\n"",
        ""For more examples (and the implementation) see below.""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""k40qkuQdkqFg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Implementation\n"",
        ""\n"",
        ""### License\n"",
        ""\n"",
        ""Copyright 2018 Google LLC.\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
        ""\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n"",
        ""\n"",
        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
        ""\n"",
        ""Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""4QrBJNYG5ECU"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Setup and imports""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""2NXj3Dp5270W"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
        ""!pip install -q jax""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""p-rYDdqL1uZP"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import grad, jit, vmap\n"",
        ""import jax.numpy as jnp\n"",
        ""import numpy as np\n"",
        ""import re""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""tU2rwIOZmT0Q"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Copied from `numpy.lib.function_base`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""lBVIP3O2kkqY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# See http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n"",
        ""_DIMENSION_NAME = r'\\w+'\n"",
        ""_CORE_DIMENSION_LIST = '(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)\n"",
        ""_ARGUMENT = r'\\({}\\)'.format(_CORE_DIMENSION_LIST)\n"",
        ""_ARGUMENT_LIST = '{0:}(?:,{0:})*'.format(_ARGUMENT)\n"",
        ""_SIGNATURE = '^{0:}->{0:}$'.format(_ARGUMENT_LIST)\n"",
        ""\n"",
        ""\n"",
        ""def _parse_gufunc_signature(signature):\n"",
        ""    \""\""\""\n"",
        ""    Parse string signatures for a generalized universal function.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    signature : string\n"",
        ""        Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``\n"",
        ""        for ``np.matmul``.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    Tuple of input and output core dimensions parsed from the signature, each\n"",
        ""    of the form List[Tuple[str, ...]].\n"",
        ""    \""\""\""\n"",
        ""    if not re.match(_SIGNATURE, signature):\n"",
        ""        raise ValueError(\n"",
        ""            'not a valid gufunc signature: {}'.format(signature))\n"",
        ""    return tuple([tuple(re.findall(_DIMENSION_NAME, arg))\n"",
        ""                  for arg in re.findall(_ARGUMENT, arg_list)]\n"",
        ""                 for arg_list in signature.split('->'))\n"",
        ""\n"",
        ""\n"",
        ""\n"",
        ""def _update_dim_sizes(dim_sizes, arg, core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Incrementally check and update core dimension sizes for a single argument.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Sizes of existing core dimensions. Will be updated in-place.\n"",
        ""    arg : ndarray\n"",
        ""        Argument to examine.\n"",
        ""    core_dims : Tuple[str, ...]\n"",
        ""        Core dimensions for this argument.\n"",
        ""    \""\""\""\n"",
        ""    if not core_dims:\n"",
        ""        return\n"",
        ""\n"",
        ""    num_core_dims = len(core_dims)\n"",
        ""    if arg.ndim < num_core_dims:\n"",
        ""        raise ValueError(\n"",
        ""            '%d-dimensional argument does not have enough '\n"",
        ""            'dimensions for all core dimensions %r'\n"",
        ""            % (arg.ndim, core_dims))\n"",
        ""\n"",
        ""    core_shape = arg.shape[-num_core_dims:]\n"",
        ""    for dim, size in zip(core_dims, core_shape):\n"",
        ""        if dim in dim_sizes:\n"",
        ""            if size != dim_sizes[dim]:\n"",
        ""                raise ValueError(\n"",
        ""                    'inconsistent size for core dimension %r: %r vs %r'\n"",
        ""                    % (dim, size, dim_sizes[dim]))\n"",
        ""        else:\n"",
        ""            dim_sizes[dim] = size\n"",
        ""\n"",
        ""\n"",
        ""def _parse_input_dimensions(args, input_core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Parse broadcast and core dimensions for vectorize with a signature.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    args : Tuple[ndarray, ...]\n"",
        ""        Tuple of input arguments to examine.\n"",
        ""    input_core_dims : List[Tuple[str, ...]]\n"",
        ""        List of core dimensions corresponding to each input.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    broadcast_shape : Tuple[int, ...]\n"",
        ""        Common shape to broadcast all non-core dimensions to.\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Common sizes for named core dimensions.\n"",
        ""    \""\""\""\n"",
        ""    broadcast_args = []\n"",
        ""    dim_sizes = {}\n"",
        ""    for arg, core_dims in zip(args, input_core_dims):\n"",
        ""        _update_dim_sizes(dim_sizes, arg, core_dims)\n"",
        ""        ndim = arg.ndim - len(core_dims)\n"",
        ""        dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])\n"",
        ""        broadcast_args.append(dummy_array)\n"",
        ""    broadcast_shape = np.lib.stride_tricks._broadcast_shape(*broadcast_args)\n"",
        ""    return broadcast_shape, dim_sizes\n"",
        ""\n"",
        ""\n"",
        ""def _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):\n"",
        ""    \""\""\""Helper for calculating broadcast shapes with core dimensions.\""\""\""\n"",
        ""    return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)\n"",
        ""            for core_dims in list_of_core_dims]\n"",
        ""\n"",
        ""  \n"",
        ""# adapted from np.vectorize (again authored by shoyer@)\n"",
        ""def broadcast_with_core_dims(args, input_core_dims, output_core_dims):\n"",
        ""  if len(args) != len(input_core_dims):\n"",
        ""    raise TypeError('wrong number of positional arguments: '\n"",
        ""                    'expected %r, got %r'\n"",
        ""                    % (len(input_core_dims), len(args)))\n"",
        ""\n"",
        ""  broadcast_shape, dim_sizes = _parse_input_dimensions(\n"",
        ""      args, input_core_dims)\n"",
        ""  input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,\n"",
        ""                                   input_core_dims)\n"",
        ""  args = [jnp.broadcast_to(arg, shape)\n"",
        ""          for arg, shape in zip(args, input_shapes)]\n"",
        ""  return args""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""aa_Gh3K_PQkY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Handle the `axis` argument""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""MLeFHhVoPT4h"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def verify_axis_is_supported(input_core_dims, output_core_dims):\n"",
        ""  all_core_dims = set()\n"",
        ""  for input_or_output_core_dims in [input_core_dims, output_core_dims]:\n"",
        ""    for core_dims in input_or_output_core_dims:\n"",
        ""      all_core_dims.update(core_dims)\n"",
        ""  if len(core_dims) > 1:\n"",
        ""    raise ValueError('only one gufuncs with one core dim support axis')\n"",
        ""\n"",
        ""\n"",
        ""def reorder_inputs(args, axis, input_core_dims):\n"",
        ""  return tuple(jnp.moveaxis(arg, axis, -1) if core_dims else arg\n"",
        ""               for arg, core_dims in zip(args, input_core_dims))\n"",
        ""\n"",
        ""\n"",
        ""def reorder_outputs(result, axis, output_core_dims):\n"",
        ""  if not isinstance(result, tuple):\n"",
        ""    result = (result,)\n"",
        ""  result = tuple(jnp.moveaxis(res, -1, axis) if core_dims else res\n"",
        ""                 for res, core_dims in zip(result, output_core_dims))\n"",
        ""  if len(result) == 1:\n"",
        ""    (result,) = result\n"",
        ""  return result""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""Uik9GA76lZjY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Core implementation\n"",
        ""\n"",
        ""This is the only part that uses `vmap`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""z-FgQaW02_WN"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""import functools\n"",
        ""\n"",
        ""def curried_vmap(func):\n"",
        ""  @functools.wraps(func)\n"",
        ""  def wrapper(*args):\n"",
        ""    return vmap(func, *args)\n"",
        ""  return wrapper\n"",
        ""\n"",
        ""\n"",
        ""def vectorize(signature):\n"",
        ""  \""\""\""Vectorize a function using JAX.\""\""\""\n"",
        ""  input_core_dims, output_core_dims = _parse_gufunc_signature(signature)\n"",
        ""  \n"",
        ""  def decorator(func):\n"",
        ""    @functools.wraps(func)\n"",
        ""    def wrapper(*args, axis=None):\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        verify_axis_is_supported(input_core_dims, output_core_dims)\n"",
        ""        args = reorder_inputs(args, axis, input_core_dims)\n"",
        ""\n"",
        ""      boardcast_args = broadcast_with_core_dims(\n"",
        ""          args, input_core_dims, output_core_dims)\n"",
        ""      num_batch_dims = len(boardcast_args[0].shape) - len(input_core_dims[0])\n"",
        ""\n"",
        ""      vectorized_func = func\n"",
        ""      for _ in range(num_batch_dims):\n"",
        ""        vectorized_func = curried_vmap(vectorized_func)\n"",
        ""      result = vectorized_func(*boardcast_args)\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        result = reorder_outputs(result, axis, output_core_dims)\n"",
        ""\n"",
        ""      return result\n"",
        ""    return wrapper\n"",
        ""  return decorator""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""EWDCFZiqmY9A"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Test cases\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""W-jCowsgj_Tg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### matmul""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""gSJ7G_da4ArE"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""matmat = vectorize('(n,m),(m,k)->(n,k)')(jnp.dot)\n"",
        ""matvec = vectorize('(n,m),(m)->(n)')(jnp.dot)\n"",
        ""vecmat = vectorize('(m),(m,k)->(k)')(jnp.dot)\n"",
        ""vecvec = vectorize('(m),(m)->()')(jnp.dot)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""CI-vJzjMfPXS"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert matmat(np.zeros((2, 3)), np.zeros((3, 4))).shape == (2, 4)\n"",
        ""assert matmat(np.zeros((2, 3)), np.zeros((1, 3, 4))).shape == (1, 2, 4)\n"",
        ""assert matmat(np.zeros((5, 2, 3)), np.zeros((1, 3, 4))).shape == (5, 2, 4)\n"",
        ""assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
        ""\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((1, 3))).shape == (1, 2)\n"",
        ""assert matvec(np.zeros((4, 2, 3)), np.zeros((1, 3))).shape == (4, 2)\n"",
        ""assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
        ""\n"",
        ""assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()\n"",
        ""# these next two don't work yet\n"",
        ""# assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""# assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2) ""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""u5xKzwoRkKuR"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### magnitude""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""Rcbol3OHkKUQ"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->()')\n"",
        ""def magnitude(x):\n"",
        ""  return jnp.dot(x, x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""DBtX_QDwkMbI"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert magnitude(np.arange(3.0)).shape == ()\n"",
        ""# these next two don't work yet\n"",
        ""# assert magnitude(np.arange(6.0).reshape(2, 3)).shape == (2,)\n"",
        ""# assert magnitude(np.arange(6.0).reshape(1, 2, 3)).shape == (1, 2,)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""LFlyTMg0kCm5"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### mean""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""m5HrLVmehaHx"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""mean = vectorize('(n)->()')(jnp.mean)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""QBtnkLwnhhJY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert mean(np.zeros((3,))).shape == ()\n"",
        ""assert mean(np.zeros((2, 3,))).shape == (2,)\n"",
        ""assert mean(np.zeros((2, 3,)), axis=0).shape == (3,)\n"",
        ""assert mean(np.zeros((1, 2, 3, 4))).shape == (1, 2, 3)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""hAMhRkK4kEzw"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### center""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""bDsjXm7MitcX"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  bias = jnp.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""MSyxOrUPixDI"",
        ""colab_type"": ""code"",
        ""outputId"": ""65ff7d75-6658-402b-85f7-558924ebe934"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 34
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(jnp.arange(3))\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 13,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[-1.  0.  1.] 1.0\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""7UZGOS8FGT_D"",
        ""colab_type"": ""code"",
        ""outputId"": ""28dd7afe-2bbe-485a-ac6e-f8ca6bac9f37"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""X = jnp.arange(12).reshape(3, 4)\n"",
        ""X""
      ],
      ""execution_count"": 14,
      ""outputs"": [
        {
          ""output_type"": ""execute_result"",
          ""data"": {
            ""text/plain"": [
              ""array([[ 0,  1,  2,  3],\n"",
              ""       [ 4,  5,  6,  7],\n"",
              ""       [ 8,  9, 10, 11]])""
            ]
          },
          ""metadata"": {
            ""tags"": []
          },
          ""execution_count"": 14
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""n2Fz91ptjM_7"",
        ""colab_type"": ""code"",
        ""outputId"": ""9ea07d50-7cbc-423c-bffb-92047dcca62c"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=1)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 15,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]] [1.5 5.5 9.5]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""G-AyCkAK4RKT"",
        ""colab_type"": ""code"",
        ""outputId"": ""dccce2b9-a7ec-4634-d45d-d440ddc1e1fe"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=0)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 16,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-4. -4. -4. -4.]\n"",
            "" [ 0.  0.  0.  0.]\n"",
            "" [ 4.  4.  4.  4.]] [4. 5. 6. 7.]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""_FhnjYMUjZgI"",
        ""colab_type"": ""code"",
        ""outputId"": ""96c71fea-b30e-4057-9634-22f4d62218e2"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# NOTE: using the wrapped function directly silently gives the wrong result!\n"",
        ""b, a = center.__wrapped__(X)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 17,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-5.5 -4.5 -3.5 -2.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [ 2.5  3.5  4.5  5.5]] 5.5\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""1EeD7aFdENt8"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        """"
      ],
      ""execution_count"": 0,
      ""outputs"": []
    }
  ]
}","{
  ""nbformat"": 4,
  ""nbformat_minor"": 0,
  ""metadata"": {
    ""colab"": {
      ""name"": ""JAX generalized ufuncs.ipynb"",
      ""version"": ""0.3.2"",
      ""provenance"": [],
      ""collapsed_sections"": []
    },
    ""kernelspec"": {
      ""name"": ""python3"",
      ""display_name"": ""Python 3""
    },
    ""accelerator"": ""GPU""
  },
  ""cells"": [
    {
      ""metadata"": {
        ""id"": ""435_M09vl3NA"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""# Extending JAX's vmap to work like NumPy's gufuncs\n"",
        ""\n"",
        ""by [Stephan Hoyer](https://github.com/shoyer)\n"",
        ""\n"",
        ""## What is a gufunc?\n"",
        ""\n"",
        ""[Generalized universal functions](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
        ""- \""core dimensions\"" over which an operation is defined.\n"",
        ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
        ""\n"",
        ""A string [signature](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html#details-of-signature) associated with each gufunc controls how this happens by indicating how core dimensions are mapped between inputs and outputs. The syntax is easiest to understand by looking at a few examples:\n"",
        ""\n"",
        ""- Addition: `(),()->()`\n"",
        ""- 1D inner product: `(i),(i)->()`\n"",
        ""- 1D sum: `(i)->()`\n"",
        ""- Matrix multiplcation: `(m,n),(n,k)->(m,k)`\n"",
        ""\n"",
        ""## Why write gufuncs?\n"",
        ""\n"",
        ""From a user perspective, gufuncs are nice because they're guaranteed to vectorize in a consistent and general fashion. For example, by default gufuncs use the last dimensions of arrays as core dimensions, but you can control that explicitly with the `axis` or `axes` arguments.\n"",
        ""\n"",
        ""From a developer perspective, gufuncs are nice because they simply your work: you only need to think about the core logic of your function, not how it handles arbitrary dimensional input. You can just write that down in a simple, declarative way.\n"",
        ""\n"",
        ""## JAX makes it easy to write high-level performant code\n"",
        ""\n"",
        ""Unfortunately, writing NumPy gufuncs today is somewhat non-trivial. Your options today are:\n"",
        ""\n"",
        ""1. Write the inner loops yourself in C.\n"",
        ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differentiation.\n"",
        ""\n"",
        ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
        ""\n"",
        ""## A simple example\n"",
        ""\n"",
        ""Consider a simple example from data preprocessing, centering an array.\n"",
        ""\n"",
        ""Here's how we might write a vectorized version using NumPy:\n"",
        ""```python\n"",
        ""def center(array, axis=-1):\n"",
        ""  # array can have any number of dimensions\n"",
        ""  bias = np.mean(array, axis=axis)\n"",
        ""  debiased = array - np.expand_dims(bias, axis)\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""And here's how we could write a vectorized version using JAX gufuncs:\n"",
        ""```python\n"",
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  # array is always a 1D vector\n"",
        ""  bias = np.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased\n"",
        ""```\n"",
        ""\n"",
        ""See the difference?\n"",
        ""- Instead of needing to think about broadcasting while writing the entire function, we can write the function assuming the input is always a vector.\n"",
        ""- We get the `axis` argument automatically, without needing to write it ourselves.\n"",
        ""- As a bonus, the decorator makes the function self-documenting: a reader immediately knows that it handles higher dimensional input and output correctly.\n"",
        ""\n"",
        ""For more examples (and the implementation) see below.""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""k40qkuQdkqFg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Implementation\n"",
        ""\n"",
        ""### License\n"",
        ""\n"",
        ""Copyright 2018 Google LLC.\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
        ""\n"",
        ""Licensed under the Apache License, Version 2.0 (the \""License\""); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n"",
        ""\n"",
        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
        ""\n"",
        ""Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \""AS IS\"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""4QrBJNYG5ECU"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Setup and imports""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""2NXj3Dp5270W"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
        ""!pip install -q jax""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""p-rYDdqL1uZP"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""from jax import grad, jit, vmap\n"",
        ""import jax.numpy as jnp\n"",
        ""import numpy as np\n"",
        ""import re""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""tU2rwIOZmT0Q"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Copied from `numpy.lib.function_base`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""lBVIP3O2kkqY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# See http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html\n"",
        ""_DIMENSION_NAME = r'\\w+'\n"",
        ""_CORE_DIMENSION_LIST = '(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)\n"",
        ""_ARGUMENT = r'\\({}\\)'.format(_CORE_DIMENSION_LIST)\n"",
        ""_ARGUMENT_LIST = '{0:}(?:,{0:})*'.format(_ARGUMENT)\n"",
        ""_SIGNATURE = '^{0:}->{0:}$'.format(_ARGUMENT_LIST)\n"",
        ""\n"",
        ""\n"",
        ""def _parse_gufunc_signature(signature):\n"",
        ""    \""\""\""\n"",
        ""    Parse string signatures for a generalized universal function.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    signature : string\n"",
        ""        Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``\n"",
        ""        for ``np.matmul``.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    Tuple of input and output core dimensions parsed from the signature, each\n"",
        ""    of the form List[Tuple[str, ...]].\n"",
        ""    \""\""\""\n"",
        ""    if not re.match(_SIGNATURE, signature):\n"",
        ""        raise ValueError(\n"",
        ""            'not a valid gufunc signature: {}'.format(signature))\n"",
        ""    return tuple([tuple(re.findall(_DIMENSION_NAME, arg))\n"",
        ""                  for arg in re.findall(_ARGUMENT, arg_list)]\n"",
        ""                 for arg_list in signature.split('->'))\n"",
        ""\n"",
        ""\n"",
        ""\n"",
        ""def _update_dim_sizes(dim_sizes, arg, core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Incrementally check and update core dimension sizes for a single argument.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Sizes of existing core dimensions. Will be updated in-place.\n"",
        ""    arg : ndarray\n"",
        ""        Argument to examine.\n"",
        ""    core_dims : Tuple[str, ...]\n"",
        ""        Core dimensions for this argument.\n"",
        ""    \""\""\""\n"",
        ""    if not core_dims:\n"",
        ""        return\n"",
        ""\n"",
        ""    num_core_dims = len(core_dims)\n"",
        ""    if arg.ndim < num_core_dims:\n"",
        ""        raise ValueError(\n"",
        ""            '%d-dimensional argument does not have enough '\n"",
        ""            'dimensions for all core dimensions %r'\n"",
        ""            % (arg.ndim, core_dims))\n"",
        ""\n"",
        ""    core_shape = arg.shape[-num_core_dims:]\n"",
        ""    for dim, size in zip(core_dims, core_shape):\n"",
        ""        if dim in dim_sizes:\n"",
        ""            if size != dim_sizes[dim]:\n"",
        ""                raise ValueError(\n"",
        ""                    'inconsistent size for core dimension %r: %r vs %r'\n"",
        ""                    % (dim, size, dim_sizes[dim]))\n"",
        ""        else:\n"",
        ""            dim_sizes[dim] = size\n"",
        ""\n"",
        ""\n"",
        ""def _parse_input_dimensions(args, input_core_dims):\n"",
        ""    \""\""\""\n"",
        ""    Parse broadcast and core dimensions for vectorize with a signature.\n"",
        ""\n"",
        ""    Arguments\n"",
        ""    ---------\n"",
        ""    args : Tuple[ndarray, ...]\n"",
        ""        Tuple of input arguments to examine.\n"",
        ""    input_core_dims : List[Tuple[str, ...]]\n"",
        ""        List of core dimensions corresponding to each input.\n"",
        ""\n"",
        ""    Returns\n"",
        ""    -------\n"",
        ""    broadcast_shape : Tuple[int, ...]\n"",
        ""        Common shape to broadcast all non-core dimensions to.\n"",
        ""    dim_sizes : Dict[str, int]\n"",
        ""        Common sizes for named core dimensions.\n"",
        ""    \""\""\""\n"",
        ""    broadcast_args = []\n"",
        ""    dim_sizes = {}\n"",
        ""    for arg, core_dims in zip(args, input_core_dims):\n"",
        ""        _update_dim_sizes(dim_sizes, arg, core_dims)\n"",
        ""        ndim = arg.ndim - len(core_dims)\n"",
        ""        dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])\n"",
        ""        broadcast_args.append(dummy_array)\n"",
        ""    broadcast_shape = np.lib.stride_tricks._broadcast_shape(*broadcast_args)\n"",
        ""    return broadcast_shape, dim_sizes\n"",
        ""\n"",
        ""\n"",
        ""def _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):\n"",
        ""    \""\""\""Helper for calculating broadcast shapes with core dimensions.\""\""\""\n"",
        ""    return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)\n"",
        ""            for core_dims in list_of_core_dims]\n"",
        ""\n"",
        ""  \n"",
        ""# adapted from np.vectorize (again authored by shoyer@)\n"",
        ""def broadcast_with_core_dims(args, input_core_dims, output_core_dims):\n"",
        ""  if len(args) != len(input_core_dims):\n"",
        ""    raise TypeError('wrong number of positional arguments: '\n"",
        ""                    'expected %r, got %r'\n"",
        ""                    % (len(input_core_dims), len(args)))\n"",
        ""\n"",
        ""  broadcast_shape, dim_sizes = _parse_input_dimensions(\n"",
        ""      args, input_core_dims)\n"",
        ""  input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,\n"",
        ""                                   input_core_dims)\n"",
        ""  args = [jnp.broadcast_to(arg, shape)\n"",
        ""          for arg, shape in zip(args, input_shapes)]\n"",
        ""  return args""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""aa_Gh3K_PQkY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Handle the `axis` argument""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""MLeFHhVoPT4h"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""def verify_axis_is_supported(input_core_dims, output_core_dims):\n"",
        ""  all_core_dims = set()\n"",
        ""  for input_or_output_core_dims in [input_core_dims, output_core_dims]:\n"",
        ""    for core_dims in input_or_output_core_dims:\n"",
        ""      all_core_dims.update(core_dims)\n"",
        ""  if len(core_dims) > 1:\n"",
        ""    raise ValueError('only one gufuncs with one core dim support axis')\n"",
        ""\n"",
        ""\n"",
        ""def reorder_inputs(args, axis, input_core_dims):\n"",
        ""  return tuple(jnp.moveaxis(arg, axis, -1) if core_dims else arg\n"",
        ""               for arg, core_dims in zip(args, input_core_dims))\n"",
        ""\n"",
        ""\n"",
        ""def reorder_outputs(result, axis, output_core_dims):\n"",
        ""  if not isinstance(result, tuple):\n"",
        ""    result = (result,)\n"",
        ""  result = tuple(jnp.moveaxis(res, -1, axis) if core_dims else res\n"",
        ""                 for res, core_dims in zip(result, output_core_dims))\n"",
        ""  if len(result) == 1:\n"",
        ""    (result,) = result\n"",
        ""  return result""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""Uik9GA76lZjY"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### Core implementation\n"",
        ""\n"",
        ""This is the only part that uses `vmap`""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""z-FgQaW02_WN"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""import functools\n"",
        ""\n"",
        ""def curried_vmap(func):\n"",
        ""  @functools.wraps(func)\n"",
        ""  def wrapper(*args):\n"",
        ""    return vmap(func, *args)\n"",
        ""  return wrapper\n"",
        ""\n"",
        ""\n"",
        ""def vectorize(signature):\n"",
        ""  \""\""\""Vectorize a function using JAX.\""\""\""\n"",
        ""  input_core_dims, output_core_dims = _parse_gufunc_signature(signature)\n"",
        ""  \n"",
        ""  def decorator(func):\n"",
        ""    @functools.wraps(func)\n"",
        ""    def wrapper(*args, axis=None):\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        verify_axis_is_supported(input_core_dims, output_core_dims)\n"",
        ""        args = reorder_inputs(args, axis, input_core_dims)\n"",
        ""\n"",
        ""      boardcast_args = broadcast_with_core_dims(\n"",
        ""          args, input_core_dims, output_core_dims)\n"",
        ""      num_batch_dims = len(boardcast_args[0].shape) - len(input_core_dims[0])\n"",
        ""\n"",
        ""      vectorized_func = func\n"",
        ""      for _ in range(num_batch_dims):\n"",
        ""        vectorized_func = curried_vmap(vectorized_func)\n"",
        ""      result = vectorized_func(*boardcast_args)\n"",
        ""\n"",
        ""      if axis is not None:\n"",
        ""        result = reorder_outputs(result, axis, output_core_dims)\n"",
        ""\n"",
        ""      return result\n"",
        ""    return wrapper\n"",
        ""  return decorator""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""EWDCFZiqmY9A"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""## Test cases\n""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""W-jCowsgj_Tg"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### matmul""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""gSJ7G_da4ArE"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""matmat = vectorize('(n,m),(m,k)->(n,k)')(jnp.dot)\n"",
        ""matvec = vectorize('(n,m),(m)->(n)')(jnp.dot)\n"",
        ""vecmat = vectorize('(m),(m,k)->(k)')(jnp.dot)\n"",
        ""vecvec = vectorize('(m),(m)->()')(jnp.dot)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""CI-vJzjMfPXS"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert matmat(np.zeros((2, 3)), np.zeros((3, 4))).shape == (2, 4)\n"",
        ""assert matmat(np.zeros((2, 3)), np.zeros((1, 3, 4))).shape == (1, 2, 4)\n"",
        ""assert matmat(np.zeros((5, 2, 3)), np.zeros((1, 3, 4))).shape == (5, 2, 4)\n"",
        ""assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
        ""\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""assert matvec(np.zeros((2, 3)), np.zeros((1, 3))).shape == (1, 2)\n"",
        ""assert matvec(np.zeros((4, 2, 3)), np.zeros((1, 3))).shape == (4, 2)\n"",
        ""assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
        ""\n"",
        ""assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()\n"",
        ""# these next two don't work yet\n"",
        ""# assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
        ""# assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2) ""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""u5xKzwoRkKuR"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### magnitude""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""Rcbol3OHkKUQ"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->()')\n"",
        ""def magnitude(x):\n"",
        ""  return jnp.dot(x, x)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""DBtX_QDwkMbI"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert magnitude(np.arange(3.0)).shape == ()\n"",
        ""# these next two don't work yet\n"",
        ""# assert magnitude(np.arange(6.0).reshape(2, 3)).shape == (2,)\n"",
        ""# assert magnitude(np.arange(6.0).reshape(1, 2, 3)).shape == (1, 2,)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""LFlyTMg0kCm5"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### mean""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""m5HrLVmehaHx"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""mean = vectorize('(n)->()')(jnp.mean)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""QBtnkLwnhhJY"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""assert mean(np.zeros((3,))).shape == ()\n"",
        ""assert mean(np.zeros((2, 3,))).shape == (2,)\n"",
        ""assert mean(np.zeros((2, 3,)), axis=0).shape == (3,)\n"",
        ""assert mean(np.zeros((1, 2, 3, 4))).shape == (1, 2, 3)""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""hAMhRkK4kEzw"",
        ""colab_type"": ""text""
      },
      ""cell_type"": ""markdown"",
      ""source"": [
        ""### center""
      ]
    },
    {
      ""metadata"": {
        ""id"": ""bDsjXm7MitcX"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""@vectorize('(n)->(),(n)')\n"",
        ""def center(array):\n"",
        ""  bias = jnp.mean(array)\n"",
        ""  debiased = array - bias\n"",
        ""  return bias, debiased""
      ],
      ""execution_count"": 0,
      ""outputs"": []
    },
    {
      ""metadata"": {
        ""id"": ""MSyxOrUPixDI"",
        ""colab_type"": ""code"",
        ""outputId"": ""65ff7d75-6658-402b-85f7-558924ebe934"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 34
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(jnp.arange(3))\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 13,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[-1.  0.  1.] 1.0\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""7UZGOS8FGT_D"",
        ""colab_type"": ""code"",
        ""outputId"": ""28dd7afe-2bbe-485a-ac6e-f8ca6bac9f37"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""X = jnp.arange(12).reshape(3, 4)\n"",
        ""X""
      ],
      ""execution_count"": 14,
      ""outputs"": [
        {
          ""output_type"": ""execute_result"",
          ""data"": {
            ""text/plain"": [
              ""array([[ 0,  1,  2,  3],\n"",
              ""       [ 4,  5,  6,  7],\n"",
              ""       [ 8,  9, 10, 11]])""
            ]
          },
          ""metadata"": {
            ""tags"": []
          },
          ""execution_count"": 14
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""n2Fz91ptjM_7"",
        ""colab_type"": ""code"",
        ""outputId"": ""9ea07d50-7cbc-423c-bffb-92047dcca62c"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=1)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 15,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]] [1.5 5.5 9.5]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""G-AyCkAK4RKT"",
        ""colab_type"": ""code"",
        ""outputId"": ""dccce2b9-a7ec-4634-d45d-d440ddc1e1fe"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""b, a = center(X, axis=0)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 16,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-4. -4. -4. -4.]\n"",
            "" [ 0.  0.  0.  0.]\n"",
            "" [ 4.  4.  4.  4.]] [4. 5. 6. 7.]\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""_FhnjYMUjZgI"",
        ""colab_type"": ""code"",
        ""outputId"": ""96c71fea-b30e-4057-9634-22f4d62218e2"",
        ""colab"": {
          ""base_uri"": ""https://localhost:8080/"",
          ""height"": 68
        }
      },
      ""cell_type"": ""code"",
      ""source"": [
        ""# NOTE: using the wrapped function directly silently gives the wrong result!\n"",
        ""b, a = center.__wrapped__(X)\n"",
        ""print(np.array(a), np.array(b))""
      ],
      ""execution_count"": 17,
      ""outputs"": [
        {
          ""output_type"": ""stream"",
          ""text"": [
            ""[[-5.5 -4.5 -3.5 -2.5]\n"",
            "" [-1.5 -0.5  0.5  1.5]\n"",
            "" [ 2.5  3.5  4.5  5.5]] 5.5\n""
          ],
          ""name"": ""stdout""
        }
      ]
    },
    {
      ""metadata"": {
        ""id"": ""1EeD7aFdENt8"",
        ""colab_type"": ""code"",
        ""colab"": {}
      },
      ""cell_type"": ""code"",
      ""source"": [
        """"
      ],
      ""execution_count"": 0,
      ""outputs"": []
    }
  ]
}
","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 805ab23d09a3ef042dcc10c9840f0d4a9b1fd457..7b02d00b71404f0e0211df86fa4263c84fbbecc0 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -51,7 +51,7 @@
         ""\n"",
         ""1. Write the inner loops yourself in C.\n"",
         ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
-        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differention.\n"",
+        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differentiation.\n"",
         ""\n"",
         ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
         ""\n"",
@@ -715,4 +715,4 @@
       ""outputs"": []
     }
   ]
-}
\ No newline at end of file
+}
",Blocking I/O on main thread,"Fix typo in gufuncs.ipynb: ""automatic differention"" -> ""automatic differentiation"""
