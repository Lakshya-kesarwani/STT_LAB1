{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0cf108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from fileinput import filename\n",
    "from unittest import case\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pydriller import Repository\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d246e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://github.com/jax-ml/jax\"]\n",
    "max_limit = 500\n",
    "bugs = [\"bug\", \"fix\", \"issue\", \"error\", \"problem\", \"fail\", \"exception\", \"crash\", \"fault\", \"defect\", \"refactor\", \"resolved\"]\n",
    "repo_path = \"../LAB4/repos/JAX\"\n",
    "# Compile regex pattern (word boundaries, case-insensitive)\n",
    "bug_pattern = re.compile(r\"\\b(\" + \"|\".join(bugs) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "def is_bug_fix_commit(message: str) -> bool:\n",
    "    \"\"\"Return True if commit message relates to bug fixing.\"\"\"\n",
    "    return bool(bug_pattern.search(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14200df1",
   "metadata": {},
   "source": [
    "Identify bug-fixing commits from the repository. The strategy to define the\n",
    "notion of a bug as well as how to identify the corresponding commit, should\n",
    "be defined by you.\n",
    "-  For each bug-fixing commit, store the following information (in csv format).\n",
    "    - Hash | Message | Hashes of parents | Is a merge commit? | List of modified files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae24c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMIT_HISTORY = \"bug_fixing_commit.csv\"\n",
    "data = {\n",
    "    'Hash': [],\n",
    "    'Parent Hash': [],\n",
    "    'Message': [],\n",
    "    'Is a merge commit?': [],\n",
    "    'Modified File': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fd1f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated bug_fixing_commit.csv with 13108 bug fix commit entries.\n"
     ]
    }
   ],
   "source": [
    "for i, commit in enumerate(Repository(repo_path).traverse_commits()):\n",
    "    if is_bug_fix_commit(commit.msg):\n",
    "        parent_hashes = commit.parents or [\"<no-parent>\"]\n",
    "        modified_files = [f.filename for f in commit.modified_files] or [\"<no-file>\"]\n",
    "\n",
    "        for p in parent_hashes:\n",
    "            for mf in modified_files:\n",
    "                data['Hash'].append(commit.hash)\n",
    "                data['Parent Hash'].append(p)\n",
    "                data['Message'].append(commit.author.name + \": \" + commit.msg)\n",
    "                data['Is a merge commit?'].append(commit.merge)\n",
    "                data['Modified File'].append(mf)\n",
    "\n",
    "# Create the DataFrame and save it to a CSV file\n",
    "if data['Hash']:\n",
    "    data_df = pd.DataFrame(data)\n",
    "    data_df.to_csv(COMMIT_HISTORY, index=False)\n",
    "    print(f\"\\nSuccessfully generated {COMMIT_HISTORY} with {len(data['Hash'])} bug fix commit entries.\")\n",
    "else:\n",
    "    print(\"\\nNo bug fix commits found within the specified limit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefbef4",
   "metadata": {},
   "source": [
    "Diff Extraction and Analyses:\n",
    "- For each modified file (in the previous step), store the following (as csv).\n",
    "\n",
    "Hash\n",
    "\n",
    "Message \n",
    "\n",
    "Filename\n",
    "\n",
    "Source\n",
    "Code\n",
    "(before)\n",
    "\n",
    "Source\n",
    "Code\n",
    "(current)\n",
    "\n",
    "Diff LLM Inference\n",
    "(fix type)\n",
    "\n",
    "Rectified\n",
    "Message\n",
    "... ... ... ... ... ... ... ...\n",
    "... ... ... ... ... ... ... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3690216",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_EXTRACTION = \"COMMIT_DIFF_ANALYSIS.csv\"\n",
    "data = {\n",
    "    \"Hash\": [],\n",
    "    \"Message\": [],\n",
    "    \"Filename\": [],\n",
    "    \"Source Code (before)\": [],\n",
    "    \"Source Code (current)\": [],\n",
    "    \"Diff\": [],\n",
    "    # \"LLM_Inference\": [],\n",
    "    # \"Rectifier\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38dd6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_content(commit_hash, filepath):\n",
    "    \"\"\"Get file content at a specific commit hash (text or binary).\"\"\"\n",
    "    if not filepath:\n",
    "        return \"\"\n",
    "    try:\n",
    "        raw = subprocess.check_output(\n",
    "            [\"git\", \"-C\", repo_path, \"show\", f\"{commit_hash}:{filepath}\"],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        # Try UTF-8 first\n",
    "        try:\n",
    "            return raw.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            return \"<binary file>\"\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"\"  # File may not exist in that commit\n",
    "\n",
    "def get_diff(parent_sha, commit_sha, old_path, new_path):\n",
    "    \"\"\"Get diff between parent and commit for the given file.\"\"\"\n",
    "    if not old_path and not new_path:\n",
    "        return \"\"\n",
    "    try:\n",
    "        raw = subprocess.check_output(\n",
    "            [\n",
    "                \"git\", \"-C\", repo_path, \"diff\", \"--diff-algorithm=histogram\", \"--full-index\",\n",
    "                f\"{parent_sha}:{old_path or new_path}\",\n",
    "                f\"{commit_sha}:{new_path or old_path}\"\n",
    "            ],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        return raw.decode(\"utf-8\", errors=\"replace\")  # avoid crash on weird encodings\n",
    "    except subprocess.CalledProcessError:\n",
    "        return \"\"  # Diff not available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "218e1a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing commits: 100%|██████████| 200/200 [00:01<00:00, 106.22commit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully generated COMMIT_DIFF_ANALYSIS.csv with 46 file-level entries.\n"
     ]
    }
   ],
   "source": [
    "commits = list(Repository(repo_path).traverse_commits())[:200]\n",
    "\n",
    "for commit in tqdm(commits, desc=\"Analyzing commits\", unit=\"commit\"):\n",
    "    if is_bug_fix_commit(commit.msg):\n",
    "        parent_hashes = commit.parents or [\"<no-parent>\"]\n",
    "        for mod in commit.modified_files:\n",
    "            for parent_sha in parent_hashes:\n",
    "                old_code = get_file_content(parent_sha, mod.old_path)\n",
    "                new_code = get_file_content(commit.hash, mod.new_path)\n",
    "                diff = get_diff(parent_sha, commit.hash, mod.old_path, mod.new_path)\n",
    "\n",
    "                data[\"Hash\"].append(commit.hash)\n",
    "                data[\"Message\"].append(commit.author.name + \": \" + commit.msg)\n",
    "                data[\"Filename\"].append(mod.new_path or mod.old_path or \"<unknown>\")\n",
    "                data[\"Source Code (before)\"].append(old_code)\n",
    "                data[\"Source Code (current)\"].append(new_code)\n",
    "                data[\"Diff\"].append(diff)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "if data[\"Hash\"]:\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(DIFF_EXTRACTION, index=False)\n",
    "    print(f\"\\n✅ Successfully generated {DIFF_EXTRACTION} with {len(data['Hash'])} file-level entries.\")\n",
    "else:\n",
    "    print(\"\\n No bug fix commit details found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85789e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 46/46 [01:26<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Saved big_commits_with_inference.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "\n",
    "# Batch inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_input_len = 256\n",
    "max_output_len = 64\n",
    "\n",
    "# Function for single inference\n",
    "def run_inference(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_output_len)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply with tqdm\n",
    "df[\"LLM Inference\"] = [\n",
    "    run_inference(diff) for diff in tqdm(df[\"Diff\"].fillna(\"\"), desc=\"Running inference\", total=len(df))\n",
    "]\n",
    "\n",
    "# Save output\n",
    "output_csv = \"big_commits_with_inference.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"✅ Done. Saved {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35009b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "\n",
    "# Your Groq API key (move to env variable in real use for safety!)\n",
    "GROQ_KEY = \"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\"\n",
    "\n",
    "# Initialize Groq client once\n",
    "groq_client = groq.Groq(api_key=GROQ_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42e3d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_commit_msg(diff_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Groq API to propose a commit message from a given diff.\n",
    "    If the API fails, fallback with a generic message.\n",
    "    \"\"\"\n",
    "    if not isinstance(diff_text, str) or not diff_text.strip():\n",
    "        return \"Minor update\"\n",
    "\n",
    "    try:\n",
    "        # Limit diff length to avoid long prompts\n",
    "        snippet = diff_text[:3500]  # slightly smaller cutoff than roommate's code\n",
    "\n",
    "        # Craft instruction\n",
    "        user_prompt = (\n",
    "            \"Please summarize the following code modifications into a clear, \"\n",
    "            \"concise commit message:\\n\\n\"\n",
    "            f\"{snippet}\\n\\n\"\n",
    "            \"Commit message:\"\n",
    "        )\n",
    "\n",
    "        # Call Groq LLM\n",
    "        resp = groq_client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",   # different model (faster + cheaper)\n",
    "            messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "            max_tokens=25,           # allow longer summaries\n",
    "            temperature=0.2,         # make more deterministic\n",
    "            top_p=1.0\n",
    "        )\n",
    "\n",
    "        # Extract output safely\n",
    "        msg = resp.choices[0].message.content.strip()\n",
    "        return msg.strip('\"') if msg else \"Code changes applied\"\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"[Groq Error] {err}\")\n",
    "        return \"Code changes applied\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0701114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rectifying with Groq:  11%|█         | 5/46 [00:00<00:03, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rectifying with Groq:  30%|███       | 14/46 [00:00<00:01, 24.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rectifying with Groq:  50%|█████     | 23/46 [00:00<00:00, 31.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rectifying with Groq:  74%|███████▍  | 34/46 [00:01<00:00, 41.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rectifying with Groq: 100%|██████████| 46/46 [00:01<00:00, 31.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "[Groq Error] Error code: 400 - {'error': {'message': 'The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
      "✅ Finished. Saved big_commits_with_rectifier.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"big_commits_with_inference.csv\")\n",
    "\n",
    "# Generate Rectifier column with tqdm progress\n",
    "rectified_msgs = []\n",
    "for diff_val in tqdm(df[\"Diff\"].fillna(\"\"), desc=\"Rectifying with Groq\", total=len(df)):\n",
    "    rectified_msgs.append(suggest_commit_msg(diff_val))\n",
    "\n",
    "df[\"Rectifier\"] = rectified_msgs\n",
    "\n",
    "# Save the updated DataFrame\n",
    "output_file = \"big_commits_with_rectifier.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Finished. Saved {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6218bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Hash                   46 non-null     object\n",
      " 1   Message                46 non-null     object\n",
      " 2   Filename               46 non-null     object\n",
      " 3   Source Code (before)   46 non-null     object\n",
      " 4   Source Code (current)  45 non-null     object\n",
      " 5   Diff                   45 non-null     object\n",
      " 6   LLM Inference          45 non-null     object\n",
      " 7   Rectifier              46 non-null     object\n",
      "dtypes: object(8)\n",
      "memory usage: 3.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e90d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
