old_filepath,new_filepath,commitSHA,parentcommitSHA,commit_message,diff_myers1,diff_hist2,Discrepancy
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,1d902436ce39b8caf1e8e5e3dc2f27990b9fda0b,a30e858e59d7184b9e54dc3f3955238221d70439,change xla_data_pb2 import,"diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 4dc7f2bb8..6ec5e8fb0 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -29,7 +29,7 @@ import warnings
 from absl import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
-from tensorflow.compiler.xla import xla_data_pb2
+from . import xla_data_pb2
 from . import xla_client
 
 ","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 4dc7f2bb8..6ec5e8fb0 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -29,7 +29,7 @@ import warnings
 from absl import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
-from tensorflow.compiler.xla import xla_data_pb2
+from . import xla_data_pb2
 from . import xla_client
 
 ",No
WORKSPACE,build/WORKSPACE,fd8d83dca27fd8e8b642743e669bda6285f66e9e,f78c05195d4edab4baf9664acc3f4aab9f09281f,tweak build file,"diff --git a/build/WORKSPACE b/build/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/build/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/build/WORKSPACE b/build/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/build/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
build/build_jax.sh,build/build_jax.sh,fd8d83dca27fd8e8b642743e669bda6285f66e9e,f78c05195d4edab4baf9664acc3f4aab9f09281f,tweak build file,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index dcdec3484..708f78e16 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,15 @@
 #!/bin/bash
 set -exv
 
+init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
+if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
+then
+  (>&2 echo ""must be executed from jax repo root"")
+  exit 1
+fi
+
+cp build/WORKSPACE .
+
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -75,5 +84,6 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
+rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}","diff --git a/build/build_jax.sh b/build/build_jax.sh
index dcdec3484..708f78e16 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,15 @@
 #!/bin/bash
 set -exv
 
+init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
+if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
+then
+  (>&2 echo ""must be executed from jax repo root"")
+  exit 1
+fi
+
+cp build/WORKSPACE .
+
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -75,5 +84,6 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
+rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}",No
jax/examples/__init__.py,examples/__init__.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/__init__.py b/examples/__init__.py
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/examples/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.","diff --git a/examples/__init__.py b/examples/__init__.py
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/examples/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.",No
jax/examples/datasets.py,examples/datasets.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/datasets.py b/examples/datasets.py
new file mode 100644
index 000000000..4178017fa
--- /dev/null
+++ b/examples/datasets.py
@@ -0,0 +1,96 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Datasets used in examples.""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import array
+import gzip
+import os
+from os import path
+import struct
+import urllib2
+
+import numpy as np
+
+
+_DATA = ""/tmp/jax_example_data/""
+
+
+def _download(url, filename):
+  """"""Download a url to a file in the JAX data temp directory.""""""
+  if not path.exists(_DATA):
+    os.makedirs(_DATA)
+  out_file = path.join(_DATA, filename)
+  if not path.isfile(out_file):
+    with open(out_file, ""wb"") as f:
+      f.write(urllib2.urlopen(url, out_file).read())
+      print(""downloaded {} to {}"".format(url, _DATA))
+
+
+def _partial_flatten(x):
+  """"""Flatten all but the first dimension of an ndarray.""""""
+  return np.reshape(x, (x.shape[0], -1))
+
+
+def _one_hot(x, k, dtype=np.float32):
+  """"""Create a one-hot encoding of x of size k.""""""
+  return np.array(x[:, None] == np.arange(k), dtype)
+
+
+def mnist_raw():
+  """"""Download and parse the raw MNIST dataset.""""""
+  base_url = ""http://yann.lecun.com/exdb/mnist/""
+
+  def parse_labels(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _ = struct.unpack("">II"", fh.read(8))
+      return np.array(array.array(""B"", fh.read()), dtype=np.uint8)
+
+  def parse_images(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _, num_data, rows, cols = struct.unpack("">IIII"", fh.read(16))
+      return np.array(array.array(""B"", fh.read()),
+                      dtype=np.uint8).reshape(num_data, rows, cols)
+
+  for filename in [""train-images-idx3-ubyte.gz"", ""train-labels-idx1-ubyte.gz"",
+                   ""t10k-images-idx3-ubyte.gz"", ""t10k-labels-idx1-ubyte.gz""]:
+    _download(base_url + filename, filename)
+
+  train_images = parse_images(path.join(_DATA, ""train-images-idx3-ubyte.gz""))
+  train_labels = parse_labels(path.join(_DATA, ""train-labels-idx1-ubyte.gz""))
+  test_images = parse_images(path.join(_DATA, ""t10k-images-idx3-ubyte.gz""))
+  test_labels = parse_labels(path.join(_DATA, ""t10k-labels-idx1-ubyte.gz""))
+
+  return train_images, train_labels, test_images, test_labels
+
+
+def mnist(permute_train=False):
+  """"""Download, parse and process MNIST data to unit scale and one-hot labels.""""""
+  train_images, train_labels, test_images, test_labels = mnist_raw()
+
+  train_images = _partial_flatten(train_images) / np.float32(255.)
+  test_images = _partial_flatten(test_images) / np.float32(255.)
+  train_labels = _one_hot(train_labels, 10)
+  test_labels = _one_hot(test_labels, 10)
+
+  if permute_train:
+    perm = np.random.RandomState(0).permutation(train_images.shape[0])
+    train_images = train_images[perm]
+    train_labels = train_labels[perm]
+
+  return train_images, train_labels, test_images, test_labels","diff --git a/examples/datasets.py b/examples/datasets.py
new file mode 100644
index 000000000..4178017fa
--- /dev/null
+++ b/examples/datasets.py
@@ -0,0 +1,96 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Datasets used in examples.""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import array
+import gzip
+import os
+from os import path
+import struct
+import urllib2
+
+import numpy as np
+
+
+_DATA = ""/tmp/jax_example_data/""
+
+
+def _download(url, filename):
+  """"""Download a url to a file in the JAX data temp directory.""""""
+  if not path.exists(_DATA):
+    os.makedirs(_DATA)
+  out_file = path.join(_DATA, filename)
+  if not path.isfile(out_file):
+    with open(out_file, ""wb"") as f:
+      f.write(urllib2.urlopen(url, out_file).read())
+      print(""downloaded {} to {}"".format(url, _DATA))
+
+
+def _partial_flatten(x):
+  """"""Flatten all but the first dimension of an ndarray.""""""
+  return np.reshape(x, (x.shape[0], -1))
+
+
+def _one_hot(x, k, dtype=np.float32):
+  """"""Create a one-hot encoding of x of size k.""""""
+  return np.array(x[:, None] == np.arange(k), dtype)
+
+
+def mnist_raw():
+  """"""Download and parse the raw MNIST dataset.""""""
+  base_url = ""http://yann.lecun.com/exdb/mnist/""
+
+  def parse_labels(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _ = struct.unpack("">II"", fh.read(8))
+      return np.array(array.array(""B"", fh.read()), dtype=np.uint8)
+
+  def parse_images(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _, num_data, rows, cols = struct.unpack("">IIII"", fh.read(16))
+      return np.array(array.array(""B"", fh.read()),
+                      dtype=np.uint8).reshape(num_data, rows, cols)
+
+  for filename in [""train-images-idx3-ubyte.gz"", ""train-labels-idx1-ubyte.gz"",
+                   ""t10k-images-idx3-ubyte.gz"", ""t10k-labels-idx1-ubyte.gz""]:
+    _download(base_url + filename, filename)
+
+  train_images = parse_images(path.join(_DATA, ""train-images-idx3-ubyte.gz""))
+  train_labels = parse_labels(path.join(_DATA, ""train-labels-idx1-ubyte.gz""))
+  test_images = parse_images(path.join(_DATA, ""t10k-images-idx3-ubyte.gz""))
+  test_labels = parse_labels(path.join(_DATA, ""t10k-labels-idx1-ubyte.gz""))
+
+  return train_images, train_labels, test_images, test_labels
+
+
+def mnist(permute_train=False):
+  """"""Download, parse and process MNIST data to unit scale and one-hot labels.""""""
+  train_images, train_labels, test_images, test_labels = mnist_raw()
+
+  train_images = _partial_flatten(train_images) / np.float32(255.)
+  test_images = _partial_flatten(test_images) / np.float32(255.)
+  train_labels = _one_hot(train_labels, 10)
+  test_labels = _one_hot(test_labels, 10)
+
+  if permute_train:
+    perm = np.random.RandomState(0).permutation(train_images.shape[0])
+    train_images = train_images[perm]
+    train_labels = train_labels[perm]
+
+  return train_images, train_labels, test_images, test_labels",No
jax/examples/interactive.py,examples/interactive.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/interactive.py b/examples/interactive.py
new file mode 100644
index 000000000..e0c061aed
--- /dev/null
+++ b/examples/interactive.py
@@ -0,0 +1,32 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+
+from absl import app
+
+import IPython
+import numpy as onp
+
+from jax import lax
+from jax import numpy as np
+from jax import jit, grad, vmap
+
+
+def main(unused_argv):
+  IPython.embed(user_ns=dict(globals(), **locals()))
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/interactive.py b/examples/interactive.py
new file mode 100644
index 000000000..e0c061aed
--- /dev/null
+++ b/examples/interactive.py
@@ -0,0 +1,32 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+
+from absl import app
+
+import IPython
+import numpy as onp
+
+from jax import lax
+from jax import numpy as np
+from jax import jit, grad, vmap
+
+
+def main(unused_argv):
+  IPython.embed(user_ns=dict(globals(), **locals()))
+
+if __name__ == ""__main__"":
+  app.run(main)",No
jax/examples/mnist_classifier.py,examples/mnist_classifier.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
new file mode 100644
index 000000000..58b660024
--- /dev/null
+++ b/examples/mnist_classifier.py
@@ -0,0 +1,99 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic MNIST example using Numpy and JAX.
+
+The primary aim here is simplicity and minimal dependencies.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import time
+
+from absl import app
+import numpy.random as npr
+
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.scipy.misc import logsumexp
+import jax.numpy as np
+
+
+def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
+  return [(scale * rng.randn(m, n), scale * rng.randn(n))
+          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
+
+def predict(params, inputs):
+  for w, b in params:
+    outputs = np.dot(inputs, w) + b
+    inputs = np.tanh(outputs)
+  return outputs - logsumexp(outputs, axis=1, keepdims=True)
+
+def loss(params, batch):
+  inputs, targets = batch
+  preds = predict(params, inputs)
+  return -np.mean(preds * targets)
+
+def accuracy(params, batch):
+  inputs, targets = batch
+  target_class = np.argmax(targets, axis=1)
+  predicted_class = np.argmax(predict(params, inputs), axis=1)
+  return np.mean(predicted_class == target_class)
+
+
+def main(unused_argv):
+  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
+  param_scale = 0.1
+  step_size = 0.001
+  num_epochs = 10
+  batch_size = 32
+
+  train_images, train_labels, test_images, test_labels = datasets.mnist()
+  num_train = train_images.shape[0]
+  num_complete_batches, leftover = divmod(num_train, batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  def data_stream():
+    rng = npr.RandomState(0)
+    while True:
+      perm = rng.permutation(num_train)
+      for i in range(num_batches):
+        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
+        yield train_images[batch_idx], train_labels[batch_idx]
+  batches = data_stream()
+
+  @jit
+  def update(params, batch):
+    grads = grad(loss)(params, batch)
+    return [(w - step_size * dw, b - step_size * db)
+            for (w, b), (dw, db) in zip(params, grads)]
+
+  params = init_random_params(param_scale, layer_sizes)
+  for epoch in range(num_epochs):
+    start_time = time.time()
+    for _ in range(num_batches):
+      params = update(params, next(batches))
+    epoch_time = time.time() - start_time
+
+    train_acc = accuracy(params, (train_images, train_labels))
+    test_acc = accuracy(params, (test_images, test_labels))
+    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
+    print(""Training set accuracy {}"".format(train_acc))
+    print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
new file mode 100644
index 000000000..58b660024
--- /dev/null
+++ b/examples/mnist_classifier.py
@@ -0,0 +1,99 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic MNIST example using Numpy and JAX.
+
+The primary aim here is simplicity and minimal dependencies.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import time
+
+from absl import app
+import numpy.random as npr
+
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.scipy.misc import logsumexp
+import jax.numpy as np
+
+
+def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
+  return [(scale * rng.randn(m, n), scale * rng.randn(n))
+          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
+
+def predict(params, inputs):
+  for w, b in params:
+    outputs = np.dot(inputs, w) + b
+    inputs = np.tanh(outputs)
+  return outputs - logsumexp(outputs, axis=1, keepdims=True)
+
+def loss(params, batch):
+  inputs, targets = batch
+  preds = predict(params, inputs)
+  return -np.mean(preds * targets)
+
+def accuracy(params, batch):
+  inputs, targets = batch
+  target_class = np.argmax(targets, axis=1)
+  predicted_class = np.argmax(predict(params, inputs), axis=1)
+  return np.mean(predicted_class == target_class)
+
+
+def main(unused_argv):
+  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
+  param_scale = 0.1
+  step_size = 0.001
+  num_epochs = 10
+  batch_size = 32
+
+  train_images, train_labels, test_images, test_labels = datasets.mnist()
+  num_train = train_images.shape[0]
+  num_complete_batches, leftover = divmod(num_train, batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  def data_stream():
+    rng = npr.RandomState(0)
+    while True:
+      perm = rng.permutation(num_train)
+      for i in range(num_batches):
+        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
+        yield train_images[batch_idx], train_labels[batch_idx]
+  batches = data_stream()
+
+  @jit
+  def update(params, batch):
+    grads = grad(loss)(params, batch)
+    return [(w - step_size * dw, b - step_size * db)
+            for (w, b), (dw, db) in zip(params, grads)]
+
+  params = init_random_params(param_scale, layer_sizes)
+  for epoch in range(num_epochs):
+    start_time = time.time()
+    for _ in range(num_batches):
+      params = update(params, next(batches))
+    epoch_time = time.time() - start_time
+
+    train_acc = accuracy(params, (train_images, train_labels))
+    test_acc = accuracy(params, (test_images, test_labels))
+    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
+    print(""Training set accuracy {}"".format(train_acc))
+    print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
jax/examples/mnist_vae.py,examples/mnist_vae.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
new file mode 100644
index 000000000..163e59b38
--- /dev/null
+++ b/examples/mnist_vae.py
@@ -0,0 +1,145 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic variational autoencoder (VAE) on binarized MNIST using Numpy and JAX.
+
+This file uses the stax network definition library and the minmax optimization
+library.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import time
+
+from absl import app
+import matplotlib.pyplot as plt
+
+from jax import lax, random
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.experimental import minmax
+from jax.experimental import stax
+from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import jax.numpy as np
+
+
+def gaussian_kl(mu, sigmasq):
+  """"""KL divergence from a diagonal Gaussian to the standard Gaussian.""""""
+  return -0.5 * np.sum(1. + np.log(sigmasq) - mu**2. - sigmasq)
+
+def gaussian_sample(rng, mu, sigmasq):
+  """"""Sample a diagonal Gaussian.""""""
+  return mu + np.sqrt(sigmasq) * random.normal(rng, mu.shape)
+
+def bernoulli_logpdf(logits, x):
+  """"""Bernoulli log pdf of data x given logits.""""""
+  return -np.sum(np.logaddexp(0., np.where(x, -1., 1.) * logits))
+
+def elbo(rng, params, images):
+  """"""Monte Carlo estimate of the negative evidence lower bound.""""""
+  enc_params, dec_params = params
+  mu_z, sigmasq_z = encode(enc_params, images)
+  logits_x = decode(dec_params, gaussian_sample(rng, mu_z, sigmasq_z))
+  return bernoulli_logpdf(logits_x, images) - gaussian_kl(mu_z, sigmasq_z)
+
+def image_sample(rng, params, nrow, ncol):
+  """"""Sample images from the generative model.""""""
+  _, dec_params = params
+  code_rng, img_rng = random.split(rng)
+  logits = decode(dec_params, random.normal(code_rng, (nrow * ncol, 10)))
+  sampled_images = random.bernoulli(img_rng, np.logaddexp(0., logits))
+  return image_grid(nrow, ncol, sampled_images, (28, 28))
+
+def image_grid(nrow, ncol, imagevecs, imshape):
+  """"""Reshape a stack of image vectors into an image grid for plotting.""""""
+  images = iter(imagevecs.reshape((-1,) + imshape))
+  return np.vstack([np.hstack([next(images).T for _ in range(ncol)][::-1])
+                    for _ in range(nrow)]).T
+
+
+encoder_init, encode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    FanOut(2),
+    stax.parallel(Dense(10), stax.serial(Dense(10), Softplus)),
+)
+
+decoder_init, decode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    Dense(28 * 28),
+)
+
+
+def main(unused_argv):
+  step_size = 0.001
+  num_epochs = 100
+  batch_size = 32
+  nrow, ncol = 10, 10  # sampled image grid size
+  rng = random.PRNGKey(0)
+
+  test_rng = random.PRNGKey(1)  # fixed prng key for evaluation
+  imfile = os.path.join(os.getenv(""TMPDIR"", ""/tmp/""), ""mnist_vae_{:03d}.png"")
+
+  train_images, _, test_images, _ = datasets.mnist(permute_train=True)
+  num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  # TODO(mattjj): automatically keep large closed-over consts device-persistent
+  train_images = jit(lambda x: x)(train_images)  # dataset on device
+
+  _, init_encoder_params = encoder_init((batch_size, 28 * 28))
+  _, init_decoder_params = decoder_init((batch_size, 10))
+  init_params = init_encoder_params, init_decoder_params
+
+  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)
+
+  def binarize_batch(rng, i, images):
+    i = i % num_batches
+    batch = lax.dynamic_slice_in_dim(images, i * batch_size, batch_size)
+    return random.bernoulli(rng, batch)
+
+  @jit
+  def run_epoch(rng, opt_state, images):
+    def body_fun(i, (rng, opt_state, images)):
+      rng, elbo_rng, data_rng = random.split(rng, 3)
+      batch = binarize_batch(data_rng, i, images)
+      loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
+      g = grad(loss)(minmax.get_params(opt_state))
+      return rng, opt_update(i, g, opt_state), images
+    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))
+
+  @jit
+  def evaluate(opt_state, images):
+    params = minmax.get_params(opt_state)
+    elbo_rng, data_rng, image_rng = random.split(test_rng, 3)
+    binarized_test = random.bernoulli(data_rng, images)
+    test_elbo = elbo(elbo_rng, params, binarized_test) / images.shape[0]
+    sampled_images = image_sample(image_rng, params, nrow, ncol)
+    return test_elbo, sampled_images
+
+  opt_state = opt_init(init_params)
+  for epoch in range(num_epochs):
+    tic = time.time()
+    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
+    test_elbo, images = evaluate(opt_state, test_images)
+    print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
+    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
new file mode 100644
index 000000000..163e59b38
--- /dev/null
+++ b/examples/mnist_vae.py
@@ -0,0 +1,145 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic variational autoencoder (VAE) on binarized MNIST using Numpy and JAX.
+
+This file uses the stax network definition library and the minmax optimization
+library.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import time
+
+from absl import app
+import matplotlib.pyplot as plt
+
+from jax import lax, random
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.experimental import minmax
+from jax.experimental import stax
+from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import jax.numpy as np
+
+
+def gaussian_kl(mu, sigmasq):
+  """"""KL divergence from a diagonal Gaussian to the standard Gaussian.""""""
+  return -0.5 * np.sum(1. + np.log(sigmasq) - mu**2. - sigmasq)
+
+def gaussian_sample(rng, mu, sigmasq):
+  """"""Sample a diagonal Gaussian.""""""
+  return mu + np.sqrt(sigmasq) * random.normal(rng, mu.shape)
+
+def bernoulli_logpdf(logits, x):
+  """"""Bernoulli log pdf of data x given logits.""""""
+  return -np.sum(np.logaddexp(0., np.where(x, -1., 1.) * logits))
+
+def elbo(rng, params, images):
+  """"""Monte Carlo estimate of the negative evidence lower bound.""""""
+  enc_params, dec_params = params
+  mu_z, sigmasq_z = encode(enc_params, images)
+  logits_x = decode(dec_params, gaussian_sample(rng, mu_z, sigmasq_z))
+  return bernoulli_logpdf(logits_x, images) - gaussian_kl(mu_z, sigmasq_z)
+
+def image_sample(rng, params, nrow, ncol):
+  """"""Sample images from the generative model.""""""
+  _, dec_params = params
+  code_rng, img_rng = random.split(rng)
+  logits = decode(dec_params, random.normal(code_rng, (nrow * ncol, 10)))
+  sampled_images = random.bernoulli(img_rng, np.logaddexp(0., logits))
+  return image_grid(nrow, ncol, sampled_images, (28, 28))
+
+def image_grid(nrow, ncol, imagevecs, imshape):
+  """"""Reshape a stack of image vectors into an image grid for plotting.""""""
+  images = iter(imagevecs.reshape((-1,) + imshape))
+  return np.vstack([np.hstack([next(images).T for _ in range(ncol)][::-1])
+                    for _ in range(nrow)]).T
+
+
+encoder_init, encode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    FanOut(2),
+    stax.parallel(Dense(10), stax.serial(Dense(10), Softplus)),
+)
+
+decoder_init, decode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    Dense(28 * 28),
+)
+
+
+def main(unused_argv):
+  step_size = 0.001
+  num_epochs = 100
+  batch_size = 32
+  nrow, ncol = 10, 10  # sampled image grid size
+  rng = random.PRNGKey(0)
+
+  test_rng = random.PRNGKey(1)  # fixed prng key for evaluation
+  imfile = os.path.join(os.getenv(""TMPDIR"", ""/tmp/""), ""mnist_vae_{:03d}.png"")
+
+  train_images, _, test_images, _ = datasets.mnist(permute_train=True)
+  num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  # TODO(mattjj): automatically keep large closed-over consts device-persistent
+  train_images = jit(lambda x: x)(train_images)  # dataset on device
+
+  _, init_encoder_params = encoder_init((batch_size, 28 * 28))
+  _, init_decoder_params = decoder_init((batch_size, 10))
+  init_params = init_encoder_params, init_decoder_params
+
+  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)
+
+  def binarize_batch(rng, i, images):
+    i = i % num_batches
+    batch = lax.dynamic_slice_in_dim(images, i * batch_size, batch_size)
+    return random.bernoulli(rng, batch)
+
+  @jit
+  def run_epoch(rng, opt_state, images):
+    def body_fun(i, (rng, opt_state, images)):
+      rng, elbo_rng, data_rng = random.split(rng, 3)
+      batch = binarize_batch(data_rng, i, images)
+      loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
+      g = grad(loss)(minmax.get_params(opt_state))
+      return rng, opt_update(i, g, opt_state), images
+    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))
+
+  @jit
+  def evaluate(opt_state, images):
+    params = minmax.get_params(opt_state)
+    elbo_rng, data_rng, image_rng = random.split(test_rng, 3)
+    binarized_test = random.bernoulli(data_rng, images)
+    test_elbo = elbo(elbo_rng, params, binarized_test) / images.shape[0]
+    sampled_images = image_sample(image_rng, params, nrow, ncol)
+    return test_elbo, sampled_images
+
+  opt_state = opt_init(init_params)
+  for epoch in range(num_epochs):
+    tic = time.time()
+    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
+    test_elbo, images = evaluate(opt_state, test_images)
+    print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
+    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
jax/tests/api_test.py,tests/api_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/api_test.py b/tests/api_test.py
new file mode 100644
index 000000000..00adfbae2
--- /dev/null
+++ b/tests/api_test.py
@@ -0,0 +1,213 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as onp
+from absl.testing import absltest
+from jax import test_util as jtu
+
+import jax.numpy as np
+from jax import jit, grad
+from jax.core import Primitive
+from jax.interpreters.partial_eval import def_abstract_eval
+from jax.interpreters.ad import defjvp
+from jax.abstract_arrays import concretization_err_msg
+
+class APITest(jtu.JaxTestCase):
+
+  def test_grad_argnums(self):
+    def f(x, y, z, flag=False):
+      assert flag
+      return 1.0 * x + 2.0 * y + 3.0 * z
+
+    assert grad(f)(1.0, 1.0, 1.0, flag=True) == 1.0
+    assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
+    assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
+
+
+  def test_jit_static_args(self):
+    side = []
+
+    def f(x, y, z, flag=False, flag2=False):
+      assert flag
+      side.append(None)
+      return 100*x + 10*y + z
+
+    f1 = jit(f)
+    assert f1(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True) == 213
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True, flag2=True) == 213
+    assert len(side) == 2
+
+    side[:] = []
+    f2 = jit(f, static_argnums=[0,2])
+    assert f2(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f2(1, 3, 3, flag=True) == 133
+    assert len(side) == 1
+    assert f2(2, 2, 3, flag=True) == 223
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True) == 243
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True, flag2=True) == 243
+    assert len(side) == 3
+    assert f2(2, 5, 3, flag=True, flag2=True) == 253
+    assert len(side) == 3
+
+  def test_grad_of_jit(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    assert grad(f)(1.0) == 2.0
+    assert len(side) == 1
+    assert grad(f)(2.0) == 4.0
+    assert len(side) == 1
+
+  def test_jit_of_grad(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    g = jit(grad(f))
+    assert g(1.0) == 2.0
+    assert len(side) == 1
+    assert g(2.0) == 4.0
+    assert len(side) == 1
+
+
+  def test_bad_input(self):
+    def f(x):
+      return x
+
+    jtu.check_raises(lambda: grad(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+    jtu.check_raises(lambda: jit(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+  # TODO(dougalm): enable when we remove 'None' from pytree nodes
+  # def test_bad_output(self):
+  #   def f(x):
+  #     pass
+
+  #   grad(f)(onp.zeros(3))
+  #   jit(f)(onp.zeros(3))
+  #   assert False
+
+  def test_grad_tuple_output(self):
+    jtu.check_raises(lambda: grad(lambda x: (x,x))(1.0), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: (1.0, 1.0)"")
+
+  def test_grad_unit_output(self):
+    jtu.check_raises(lambda: grad(lambda x: ())(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: ()"")
+
+  def test_grad_nonscalar_output(self):
+    jtu.check_raises(lambda: grad(lambda x: x)(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: [ 0.  0.  0.]"")
+
+  def test_unwrapped_numpy(self):
+    def f(x):
+      return onp.exp(x)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3)), Exception,
+                     ""Tracer can't be used with raw numpy functions. ""
+                     ""You might have\n  import numpy as np\ninstead of\n""
+                     ""  import jax.numpy as np"")
+
+  def test_binop_mismatch(self):
+    def f(x, y):
+      return x + y
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     ValueError,
+                     ""Incompatible shapes for broadcasting: ((3,), (4,))"")
+
+  def test_dot_mismatch(self):
+    def f(x, y):
+      return np.dot(x, y)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     TypeError,
+                     ""Incompatible shapes for dot: got (3,) and (4,)."")
+
+  def test_switch_value_jit(self):
+    def f(x):
+      y = x > 0
+      if y:
+        return x
+      else:
+        return -x
+
+    assert grad(f)(1.0) == 1.0
+    assert grad(f)(-1.0) == -1.0
+    jtu.check_raises(lambda: jit(f)(1), TypeError, concretization_err_msg(bool))
+
+  def test_range_err(self):
+    def f(x, n):
+      for i in range(n):
+        x = x + i
+      return x
+
+    assert jit(f, static_argnums=(1,))(0, 5) == 10
+    jtu.check_raises(lambda: jit(f)(0, 5), TypeError, concretization_err_msg(int))
+
+  def test_casts(self):
+    for castfun in [float, int, long, complex, hex, oct]:
+      f = lambda x: castfun(x)
+      jtu.check_raises(lambda: jit(f)(0), TypeError, concretization_err_msg(castfun))
+
+  def test_unimplemented_interpreter_rules(self):
+    foo_p = Primitive('foo')
+    def foo(x):
+      return foo_p.bind(x)
+
+    jtu.check_raises(lambda: foo(1.0), NotImplementedError,
+                     ""Evaluation rule for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""Abstract evaluation for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Forward-mode differentiation rule for 'foo' not implemented"")
+
+    def_abstract_eval(foo_p, lambda x: x)
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""XLA translation rule for 'foo' not implemented"")
+
+    foo_p.def_impl(lambda x: x)
+    defjvp(foo_p, lambda g, x: foo(g))
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Reverse-mode differentiation rule for 'foo' not implemented"")
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
new file mode 100644
index 000000000..00adfbae2
--- /dev/null
+++ b/tests/api_test.py
@@ -0,0 +1,213 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as onp
+from absl.testing import absltest
+from jax import test_util as jtu
+
+import jax.numpy as np
+from jax import jit, grad
+from jax.core import Primitive
+from jax.interpreters.partial_eval import def_abstract_eval
+from jax.interpreters.ad import defjvp
+from jax.abstract_arrays import concretization_err_msg
+
+class APITest(jtu.JaxTestCase):
+
+  def test_grad_argnums(self):
+    def f(x, y, z, flag=False):
+      assert flag
+      return 1.0 * x + 2.0 * y + 3.0 * z
+
+    assert grad(f)(1.0, 1.0, 1.0, flag=True) == 1.0
+    assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
+    assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
+
+
+  def test_jit_static_args(self):
+    side = []
+
+    def f(x, y, z, flag=False, flag2=False):
+      assert flag
+      side.append(None)
+      return 100*x + 10*y + z
+
+    f1 = jit(f)
+    assert f1(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True) == 213
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True, flag2=True) == 213
+    assert len(side) == 2
+
+    side[:] = []
+    f2 = jit(f, static_argnums=[0,2])
+    assert f2(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f2(1, 3, 3, flag=True) == 133
+    assert len(side) == 1
+    assert f2(2, 2, 3, flag=True) == 223
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True) == 243
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True, flag2=True) == 243
+    assert len(side) == 3
+    assert f2(2, 5, 3, flag=True, flag2=True) == 253
+    assert len(side) == 3
+
+  def test_grad_of_jit(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    assert grad(f)(1.0) == 2.0
+    assert len(side) == 1
+    assert grad(f)(2.0) == 4.0
+    assert len(side) == 1
+
+  def test_jit_of_grad(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    g = jit(grad(f))
+    assert g(1.0) == 2.0
+    assert len(side) == 1
+    assert g(2.0) == 4.0
+    assert len(side) == 1
+
+
+  def test_bad_input(self):
+    def f(x):
+      return x
+
+    jtu.check_raises(lambda: grad(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+    jtu.check_raises(lambda: jit(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+  # TODO(dougalm): enable when we remove 'None' from pytree nodes
+  # def test_bad_output(self):
+  #   def f(x):
+  #     pass
+
+  #   grad(f)(onp.zeros(3))
+  #   jit(f)(onp.zeros(3))
+  #   assert False
+
+  def test_grad_tuple_output(self):
+    jtu.check_raises(lambda: grad(lambda x: (x,x))(1.0), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: (1.0, 1.0)"")
+
+  def test_grad_unit_output(self):
+    jtu.check_raises(lambda: grad(lambda x: ())(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: ()"")
+
+  def test_grad_nonscalar_output(self):
+    jtu.check_raises(lambda: grad(lambda x: x)(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: [ 0.  0.  0.]"")
+
+  def test_unwrapped_numpy(self):
+    def f(x):
+      return onp.exp(x)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3)), Exception,
+                     ""Tracer can't be used with raw numpy functions. ""
+                     ""You might have\n  import numpy as np\ninstead of\n""
+                     ""  import jax.numpy as np"")
+
+  def test_binop_mismatch(self):
+    def f(x, y):
+      return x + y
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     ValueError,
+                     ""Incompatible shapes for broadcasting: ((3,), (4,))"")
+
+  def test_dot_mismatch(self):
+    def f(x, y):
+      return np.dot(x, y)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     TypeError,
+                     ""Incompatible shapes for dot: got (3,) and (4,)."")
+
+  def test_switch_value_jit(self):
+    def f(x):
+      y = x > 0
+      if y:
+        return x
+      else:
+        return -x
+
+    assert grad(f)(1.0) == 1.0
+    assert grad(f)(-1.0) == -1.0
+    jtu.check_raises(lambda: jit(f)(1), TypeError, concretization_err_msg(bool))
+
+  def test_range_err(self):
+    def f(x, n):
+      for i in range(n):
+        x = x + i
+      return x
+
+    assert jit(f, static_argnums=(1,))(0, 5) == 10
+    jtu.check_raises(lambda: jit(f)(0, 5), TypeError, concretization_err_msg(int))
+
+  def test_casts(self):
+    for castfun in [float, int, long, complex, hex, oct]:
+      f = lambda x: castfun(x)
+      jtu.check_raises(lambda: jit(f)(0), TypeError, concretization_err_msg(castfun))
+
+  def test_unimplemented_interpreter_rules(self):
+    foo_p = Primitive('foo')
+    def foo(x):
+      return foo_p.bind(x)
+
+    jtu.check_raises(lambda: foo(1.0), NotImplementedError,
+                     ""Evaluation rule for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""Abstract evaluation for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Forward-mode differentiation rule for 'foo' not implemented"")
+
+    def_abstract_eval(foo_p, lambda x: x)
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""XLA translation rule for 'foo' not implemented"")
+
+    foo_p.def_impl(lambda x: x)
+    defjvp(foo_p, lambda g, x: foo(g))
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Reverse-mode differentiation rule for 'foo' not implemented"")
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/batching_test.py,tests/batching_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/batching_test.py b/tests/batching_test.py
new file mode 100644
index 000000000..6020079ef
--- /dev/null
+++ b/tests/batching_test.py
@@ -0,0 +1,140 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import jax.numpy as np
+from jax import test_util as jtu
+from jax.abstract_arrays import ShapedArray
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
+from jax.api import vmap
+from jax.core import unit
+from jax.interpreters import partial_eval as pe
+from jax.util import partial
+
+
+class BatchingTest(jtu.JaxTestCase):
+
+  def testConstantFunction(self):
+    ans = vmap(lambda x: 3, onp.ones(4))
+    expected = 3 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testNestedBatchingMatMat(self):
+    def matvec(A, b):
+      return vmap(np.vdot, A, b, in_bdims=(0, None))
+
+    def matmat(A, B):
+      return vmap(matvec, A, B, in_bdims=(None, 1), out_bdim=1)
+
+    R = onp.random.RandomState(0).randn
+    A = R(4, 3)
+    B = R(3, 2)
+
+    ans = matmat(A, B)
+    expected = onp.dot(A, B)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+    # this is a crude check that we only call a single dot
+    def pv_like(x):
+      aval = ShapedArray(onp.shape(x), onp.result_type(x))
+      return pe.PartialVal((aval, unit))
+
+    def make_jaxpr(fun, example_args):
+      jaxpr, _, _, _ = trace_to_jaxpr(fun, map(pv_like, example_args))
+      return jaxpr
+
+    jaxpr = make_jaxpr(matmat, (A, B))
+    self.assertEqual(len(jaxpr.eqns), 1)
+
+  def testPerExampleGradients(self):
+    def predict(params, inputs):
+      for W, b in params:
+        outputs = np.dot(W, inputs) + b
+        inputs = np.tanh(outputs)
+      return outputs
+
+    def loss(params, data):
+      inputs, targets = data
+      predictions = predict(params, inputs)
+      return np.sum((predictions - targets)**2)
+
+    batch_size = 5
+    layer_sizes = [3, 2, 4]
+
+    R = onp.random.RandomState(0).randn
+    params = [(R(m, n), R(m))
+              for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]
+
+    input_vec = R(3)
+    target_vec = R(4)
+    datum = (input_vec, target_vec)
+
+    input_batch = R(5, 3)
+    target_batch = R(5, 4)
+    batch = (input_batch, target_batch)
+
+    ans = vmap(partial(grad(loss), params), batch)
+
+    for ans_pair, param_pair in zip(ans, params):
+      dW, db = ans_pair
+      W, b = param_pair
+
+      self.assertEqual(dW.shape, (batch_size,) + W.shape)
+      self.assertEqual(db.shape, (batch_size,) + b.shape)
+
+  def testJacobians(self):
+    def jacbwd(f, x):
+      y, pullback = vjp(f, x)
+      std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
+      jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    def jacfwd(f, x):
+      pushfwd = lambda v: jvp(f, (x,), (v,))
+      std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
+      y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    R = onp.random.RandomState(0).randn
+
+    A = R(4, 3)
+    b = R(4)
+    f = lambda x: np.tanh(np.dot(A, x) + b)
+
+    x = R(3)
+    self.assertAllClose(jacfwd(f, x), jacbwd(f, x), check_dtypes=False)
+
+  def testBatchOfCompile(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x + x
+
+    g = jit(lambda x: vmap(f, x))
+    self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
+    self.assertEqual(len(side), 1)
+    self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
+                        check_dtypes=False)
+    self.assertEqual(len(side), 1)
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
new file mode 100644
index 000000000..6020079ef
--- /dev/null
+++ b/tests/batching_test.py
@@ -0,0 +1,140 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import jax.numpy as np
+from jax import test_util as jtu
+from jax.abstract_arrays import ShapedArray
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
+from jax.api import vmap
+from jax.core import unit
+from jax.interpreters import partial_eval as pe
+from jax.util import partial
+
+
+class BatchingTest(jtu.JaxTestCase):
+
+  def testConstantFunction(self):
+    ans = vmap(lambda x: 3, onp.ones(4))
+    expected = 3 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testNestedBatchingMatMat(self):
+    def matvec(A, b):
+      return vmap(np.vdot, A, b, in_bdims=(0, None))
+
+    def matmat(A, B):
+      return vmap(matvec, A, B, in_bdims=(None, 1), out_bdim=1)
+
+    R = onp.random.RandomState(0).randn
+    A = R(4, 3)
+    B = R(3, 2)
+
+    ans = matmat(A, B)
+    expected = onp.dot(A, B)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+    # this is a crude check that we only call a single dot
+    def pv_like(x):
+      aval = ShapedArray(onp.shape(x), onp.result_type(x))
+      return pe.PartialVal((aval, unit))
+
+    def make_jaxpr(fun, example_args):
+      jaxpr, _, _, _ = trace_to_jaxpr(fun, map(pv_like, example_args))
+      return jaxpr
+
+    jaxpr = make_jaxpr(matmat, (A, B))
+    self.assertEqual(len(jaxpr.eqns), 1)
+
+  def testPerExampleGradients(self):
+    def predict(params, inputs):
+      for W, b in params:
+        outputs = np.dot(W, inputs) + b
+        inputs = np.tanh(outputs)
+      return outputs
+
+    def loss(params, data):
+      inputs, targets = data
+      predictions = predict(params, inputs)
+      return np.sum((predictions - targets)**2)
+
+    batch_size = 5
+    layer_sizes = [3, 2, 4]
+
+    R = onp.random.RandomState(0).randn
+    params = [(R(m, n), R(m))
+              for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]
+
+    input_vec = R(3)
+    target_vec = R(4)
+    datum = (input_vec, target_vec)
+
+    input_batch = R(5, 3)
+    target_batch = R(5, 4)
+    batch = (input_batch, target_batch)
+
+    ans = vmap(partial(grad(loss), params), batch)
+
+    for ans_pair, param_pair in zip(ans, params):
+      dW, db = ans_pair
+      W, b = param_pair
+
+      self.assertEqual(dW.shape, (batch_size,) + W.shape)
+      self.assertEqual(db.shape, (batch_size,) + b.shape)
+
+  def testJacobians(self):
+    def jacbwd(f, x):
+      y, pullback = vjp(f, x)
+      std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
+      jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    def jacfwd(f, x):
+      pushfwd = lambda v: jvp(f, (x,), (v,))
+      std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
+      y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    R = onp.random.RandomState(0).randn
+
+    A = R(4, 3)
+    b = R(4)
+    f = lambda x: np.tanh(np.dot(A, x) + b)
+
+    x = R(3)
+    self.assertAllClose(jacfwd(f, x), jacbwd(f, x), check_dtypes=False)
+
+  def testBatchOfCompile(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x + x
+
+    g = jit(lambda x: vmap(f, x))
+    self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
+    self.assertEqual(len(side), 1)
+    self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
+                        check_dtypes=False)
+    self.assertEqual(len(side), 1)
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/core_test.py,tests/core_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/core_test.py b/tests/core_test.py
new file mode 100644
index 000000000..a69bd4bd3
--- /dev/null
+++ b/tests/core_test.py
@@ -0,0 +1,325 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import namedtuple
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import api
+from jax import core
+from jax import numpy as np
+from jax import test_util as jtu
+from jax.api import jvp, linearize, vjp, jit
+from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
+from jax.tree_util import tree_flatten, tree_multimap
+from jax.util import partial
+from jax.interpreters import partial_eval as pe
+from jax.interpreters import xla
+
+_ = pe.PartialVal((UnshapedArray(onp.float32), core.unit))
+__ = pe.PartialVal((ShapedArray((), onp.float32), core.unit))
+
+def call(f, *args):
+  return jit(f)(*args)
+
+def simple_fun(x, y):
+  return np.sin(x * y)
+
+def simple_fun_fanout(x, y):
+  return np.sin(x * y) * x
+
+def fun_with_call(x):
+  return call(np.sin, x)
+
+def fun_with_nested_calls(x):
+  def f(y):
+    y2 = np.sin(y) + 1.0 + (2.0 * x)
+
+    @jit
+    def g(z):
+      return y2 * z * x + (x * y)
+
+    return call(g, y)
+
+  return call(f, x)
+
+def error(*args):
+  def f(*args):
+    assert False
+  return f
+
+def fun_with_nested_calls_2(x):
+  def bar(y):
+    def baz(w):
+      q = call(lambda x: y, x)
+      q = q + call(lambda: y)
+      q = q + call(lambda y: w + y, y)
+      return call(lambda w: call(np.sin, x) * y, 1.0) + q
+      # return call(lambda w: call(error(np.sin), x) * y, 1.0) + q
+    p, t = jvp(baz, (x + 1.0,), (y,))
+    return t + (x * p)
+  return call(bar, x)
+
+def fun_call_jitted(x):
+  @jit
+  def g(z):
+    return x * z
+
+  return call(g, x)
+
+def fun_with_two_calls(x):
+  return call(np.sin, x) + call(np.cos, x)
+
+def fun_with_call_closure(x):
+  def foo(y, z):
+    return (x * x) * np.sin(y) * z
+
+  return call(foo, x, np.cos(x)) + x
+
+def product_io_fun(x, y):
+  xa = x['a']
+  xb = x['b']
+  y1, (y2, y3) = y
+  return np.sin(xa + y2), [xb, (y1, y3)]
+
+
+R = onp.random.randn
+TestSpec = namedtuple('TestSpec', ['fun', 'args'])
+test_specs_base = [
+    TestSpec(simple_fun, (R(3, 2), R(3, 2))),
+    TestSpec(simple_fun_fanout, (R(3, 2), R(3, 2))),
+    TestSpec(product_io_fun, ({'a': R(2, 2), 'b': R(2, 2)},
+                              (R(2, 2), (R(2, 2), R(2, 2))))),
+    TestSpec(fun_with_call, (R(3, 2),)),
+    TestSpec(fun_with_two_calls, (R(3, 2),)),
+    TestSpec(fun_with_call_closure, (R(3, 2),)),
+    TestSpec(fun_call_jitted, (R(1,),)),
+    TestSpec(fun_with_nested_calls, (R(),)),
+    TestSpec(fun_with_nested_calls, (R(3, 2),)),
+    TestSpec(fun_with_nested_calls_2, (R(1, 2),)),
+]
+
+def jvp_unlinearized(f, primals, tangents):
+  out, jvp = linearize(f, *primals)
+  return out, jvp(tangents)
+
+test_specs = []
+for ts in test_specs_base:
+  test_specs.append(ts)
+  test_specs.append(TestSpec(partial(jvp, ts.fun), (ts.args, ts.args)))
+  test_specs.append(TestSpec(jit(ts.fun), ts.args))
+  test_specs.append(TestSpec(jit(jit(ts.fun)), ts.args))
+  test_specs.append(TestSpec(partial(jvp_unlinearized, ts.fun),
+                             (ts.args, ts.args)))
+
+
+def fwd_deriv(f):
+  def df(x):
+    return jvp(f, (x,), (1.0,))[1]
+
+  return df
+
+def check_trace_eval(f, pvals, vals, expected_out_pval):
+  jaxpr, consts, out_pval, _ = api.trace_to_jaxpr(f, pvals)
+  assert expected_out_pval == out_pval, (expected_out_pval, out_pval)
+  output_traced = core.eval_jaxpr(jaxpr, consts, (), *vals)
+  output_traced = pe.merge_pvals(output_traced, out_pval)
+  output_eval = f(*vals)
+  assert onp.allclose(output_traced, output_eval), \
+      '\neval:         {}\ntrace + eval: {}'.format(output_eval, output_traced)
+
+
+class CoreTest(jtu.JaxTestCase):
+
+  def DISABLED_test_pack_unpack(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    y = onp.array(1.0)
+    def foo(x):
+      x1, y1 = core.pack((x, y))
+      assert y1 is y, (y1, y)
+      return x1
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def DISABLED_test_tup_add(self):
+    # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
+    y = onp.array(1.0)
+    def foo(x):
+      return np.tup_add(core.pack((x, y)))
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def test_tree_multimap(self):
+    xs = ({'a': 1}, [2, 3])
+    ys = ({'a': 10}, [20, 30])
+    ys_bad = ({'a': 10, 'b': 10}, [20, 30])
+    zs = ({'a': 11}, [22, 33])
+
+    f = lambda x, y: x + y
+    assert tree_multimap(f, xs, ys) == zs
+    try:
+      tree_multimap(f, xs, ys_bad)
+      assert False
+    except TypeError:
+      pass
+
+  def DIABLED_test_print_jaxpr_compound(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
+    print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
+
+  def test_tree_flatten(self):
+    flat, _ = tree_flatten(({'a': 1}, [2, 3], 4))
+    assert flat == [1, 2, 3, 4]
+
+  @parameterized.parameters(test_specs)
+  def test_jit(self, f, args):
+    jtu.check_eq(jit(f)(*args), f(*args))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp, f), args)
+
+  def test_jvp_zeros(self):
+    def foo(x):
+      def bar(y):
+        x1, y1 = core.pack((x, y))
+        return np.sin(x1 * y1)
+      return jvp(bar, (3 * x,), (2 * x,))
+
+    jtu.check_eq(jit(foo)(0.5), foo(0.5))
+
+  def test_dynamic_subfun_context(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(np.sin(x), y)
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  def DISABLED_test_nested_grad(self):
+    def foo(x):
+      print(type(x), x)
+      def bar(y):
+        return np.cos(y) * x
+      print(x * x)
+      return call(bar, x*x)
+
+    print(api.trace_to_jaxpr(api.grad(foo), (__,)))
+
+  def test_nested(self):
+    def foo(x):
+      def bar(y):
+        def baz(w):
+          q = call(lambda x: y, x) + call(lambda: y)
+          return call(lambda w: call(np.sin, x) * y, 1.0) + q
+        p, t = jvp(baz, (x + 1.0,), (y,))
+        return t + (x * p)
+
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp_linearized(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp_unlinearized, f), args)
+
+  @parameterized.parameters(test_specs)
+  def test_vjp(self, f, args):
+    print(f)
+    jtu.check_vjp(f, partial(vjp, f), args)
+
+  def test_jvp_closure(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(x, y)
+      return jvp(bar, (3.0,), (1.0,))[1]
+    ans = jvp(foo, (1.0,), (2.0,))
+    assert ans == (1.0, 2.0), ans
+
+  def test_jit_closure(self):
+    def foo(x):
+      @jit
+      def bar(y):
+        return x + y
+      return bar(0.0)
+    assert jvp(foo, (1.0,), (2.0,)) == (1.0, 2.0)
+
+  def test_simple_trace(self):
+    def foo(x):
+      return np.sin(x) + np.cos(x)
+    pval = pe.PartialVal((ShapedArray((3, 2), onp.float32), core.unit))
+    check_trace_eval(foo, (pval,), (onp.random.randn(3, 2),), pval)
+
+  def test_nullary_trace(self):
+    def foo():
+      return 1.2
+    check_trace_eval(foo, (), (), (None, 1.2))
+
+  def test_simple_jit(self):
+    def foo(x):
+      if x.shape == ():
+        return x + 1.
+      else:
+        return x + 2.
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    x1, y1 = onp.array(1.0), onp.array(2.0)
+    assert foo(x1) == y1
+    assert foo2(x1) == y1
+    assert foo3(x1) == y1
+
+    x2, y2 = onp.array([1.0, 2.0]), onp.array([3.0, 4.0])
+    assert onp.all(foo(x2) == y2)
+    assert onp.all(foo2(x2) == y2)
+    assert onp.all(foo3(x2) == y2)
+
+  def test_product_jit(self):
+    def foo(x, tup):
+      y, z = tup
+      w = x + z
+      return (w, {'x': y}), z
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    args = (1.0, (2.0, 3.0))
+    expected_output = ((4.0, {'x': 2.0}), 3.0)
+
+    assert foo(*args) == expected_output
+    assert foo2(*args) == expected_output
+    assert foo3(*args) == foo(*args)
+
+  def test_jvp_2(self):
+    d_sin = fwd_deriv(np.sin)
+    d2_sin = fwd_deriv(d_sin)
+    d3_sin = fwd_deriv(d2_sin)
+
+    assert d_sin(0.0) == 1.0
+    assert d2_sin(0.0) == 0.0
+    assert d3_sin(0.0) == -1.0
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/core_test.py b/tests/core_test.py
new file mode 100644
index 000000000..a69bd4bd3
--- /dev/null
+++ b/tests/core_test.py
@@ -0,0 +1,325 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import namedtuple
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import api
+from jax import core
+from jax import numpy as np
+from jax import test_util as jtu
+from jax.api import jvp, linearize, vjp, jit
+from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
+from jax.tree_util import tree_flatten, tree_multimap
+from jax.util import partial
+from jax.interpreters import partial_eval as pe
+from jax.interpreters import xla
+
+_ = pe.PartialVal((UnshapedArray(onp.float32), core.unit))
+__ = pe.PartialVal((ShapedArray((), onp.float32), core.unit))
+
+def call(f, *args):
+  return jit(f)(*args)
+
+def simple_fun(x, y):
+  return np.sin(x * y)
+
+def simple_fun_fanout(x, y):
+  return np.sin(x * y) * x
+
+def fun_with_call(x):
+  return call(np.sin, x)
+
+def fun_with_nested_calls(x):
+  def f(y):
+    y2 = np.sin(y) + 1.0 + (2.0 * x)
+
+    @jit
+    def g(z):
+      return y2 * z * x + (x * y)
+
+    return call(g, y)
+
+  return call(f, x)
+
+def error(*args):
+  def f(*args):
+    assert False
+  return f
+
+def fun_with_nested_calls_2(x):
+  def bar(y):
+    def baz(w):
+      q = call(lambda x: y, x)
+      q = q + call(lambda: y)
+      q = q + call(lambda y: w + y, y)
+      return call(lambda w: call(np.sin, x) * y, 1.0) + q
+      # return call(lambda w: call(error(np.sin), x) * y, 1.0) + q
+    p, t = jvp(baz, (x + 1.0,), (y,))
+    return t + (x * p)
+  return call(bar, x)
+
+def fun_call_jitted(x):
+  @jit
+  def g(z):
+    return x * z
+
+  return call(g, x)
+
+def fun_with_two_calls(x):
+  return call(np.sin, x) + call(np.cos, x)
+
+def fun_with_call_closure(x):
+  def foo(y, z):
+    return (x * x) * np.sin(y) * z
+
+  return call(foo, x, np.cos(x)) + x
+
+def product_io_fun(x, y):
+  xa = x['a']
+  xb = x['b']
+  y1, (y2, y3) = y
+  return np.sin(xa + y2), [xb, (y1, y3)]
+
+
+R = onp.random.randn
+TestSpec = namedtuple('TestSpec', ['fun', 'args'])
+test_specs_base = [
+    TestSpec(simple_fun, (R(3, 2), R(3, 2))),
+    TestSpec(simple_fun_fanout, (R(3, 2), R(3, 2))),
+    TestSpec(product_io_fun, ({'a': R(2, 2), 'b': R(2, 2)},
+                              (R(2, 2), (R(2, 2), R(2, 2))))),
+    TestSpec(fun_with_call, (R(3, 2),)),
+    TestSpec(fun_with_two_calls, (R(3, 2),)),
+    TestSpec(fun_with_call_closure, (R(3, 2),)),
+    TestSpec(fun_call_jitted, (R(1,),)),
+    TestSpec(fun_with_nested_calls, (R(),)),
+    TestSpec(fun_with_nested_calls, (R(3, 2),)),
+    TestSpec(fun_with_nested_calls_2, (R(1, 2),)),
+]
+
+def jvp_unlinearized(f, primals, tangents):
+  out, jvp = linearize(f, *primals)
+  return out, jvp(tangents)
+
+test_specs = []
+for ts in test_specs_base:
+  test_specs.append(ts)
+  test_specs.append(TestSpec(partial(jvp, ts.fun), (ts.args, ts.args)))
+  test_specs.append(TestSpec(jit(ts.fun), ts.args))
+  test_specs.append(TestSpec(jit(jit(ts.fun)), ts.args))
+  test_specs.append(TestSpec(partial(jvp_unlinearized, ts.fun),
+                             (ts.args, ts.args)))
+
+
+def fwd_deriv(f):
+  def df(x):
+    return jvp(f, (x,), (1.0,))[1]
+
+  return df
+
+def check_trace_eval(f, pvals, vals, expected_out_pval):
+  jaxpr, consts, out_pval, _ = api.trace_to_jaxpr(f, pvals)
+  assert expected_out_pval == out_pval, (expected_out_pval, out_pval)
+  output_traced = core.eval_jaxpr(jaxpr, consts, (), *vals)
+  output_traced = pe.merge_pvals(output_traced, out_pval)
+  output_eval = f(*vals)
+  assert onp.allclose(output_traced, output_eval), \
+      '\neval:         {}\ntrace + eval: {}'.format(output_eval, output_traced)
+
+
+class CoreTest(jtu.JaxTestCase):
+
+  def DISABLED_test_pack_unpack(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    y = onp.array(1.0)
+    def foo(x):
+      x1, y1 = core.pack((x, y))
+      assert y1 is y, (y1, y)
+      return x1
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def DISABLED_test_tup_add(self):
+    # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
+    y = onp.array(1.0)
+    def foo(x):
+      return np.tup_add(core.pack((x, y)))
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def test_tree_multimap(self):
+    xs = ({'a': 1}, [2, 3])
+    ys = ({'a': 10}, [20, 30])
+    ys_bad = ({'a': 10, 'b': 10}, [20, 30])
+    zs = ({'a': 11}, [22, 33])
+
+    f = lambda x, y: x + y
+    assert tree_multimap(f, xs, ys) == zs
+    try:
+      tree_multimap(f, xs, ys_bad)
+      assert False
+    except TypeError:
+      pass
+
+  def DIABLED_test_print_jaxpr_compound(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
+    print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
+
+  def test_tree_flatten(self):
+    flat, _ = tree_flatten(({'a': 1}, [2, 3], 4))
+    assert flat == [1, 2, 3, 4]
+
+  @parameterized.parameters(test_specs)
+  def test_jit(self, f, args):
+    jtu.check_eq(jit(f)(*args), f(*args))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp, f), args)
+
+  def test_jvp_zeros(self):
+    def foo(x):
+      def bar(y):
+        x1, y1 = core.pack((x, y))
+        return np.sin(x1 * y1)
+      return jvp(bar, (3 * x,), (2 * x,))
+
+    jtu.check_eq(jit(foo)(0.5), foo(0.5))
+
+  def test_dynamic_subfun_context(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(np.sin(x), y)
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  def DISABLED_test_nested_grad(self):
+    def foo(x):
+      print(type(x), x)
+      def bar(y):
+        return np.cos(y) * x
+      print(x * x)
+      return call(bar, x*x)
+
+    print(api.trace_to_jaxpr(api.grad(foo), (__,)))
+
+  def test_nested(self):
+    def foo(x):
+      def bar(y):
+        def baz(w):
+          q = call(lambda x: y, x) + call(lambda: y)
+          return call(lambda w: call(np.sin, x) * y, 1.0) + q
+        p, t = jvp(baz, (x + 1.0,), (y,))
+        return t + (x * p)
+
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp_linearized(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp_unlinearized, f), args)
+
+  @parameterized.parameters(test_specs)
+  def test_vjp(self, f, args):
+    print(f)
+    jtu.check_vjp(f, partial(vjp, f), args)
+
+  def test_jvp_closure(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(x, y)
+      return jvp(bar, (3.0,), (1.0,))[1]
+    ans = jvp(foo, (1.0,), (2.0,))
+    assert ans == (1.0, 2.0), ans
+
+  def test_jit_closure(self):
+    def foo(x):
+      @jit
+      def bar(y):
+        return x + y
+      return bar(0.0)
+    assert jvp(foo, (1.0,), (2.0,)) == (1.0, 2.0)
+
+  def test_simple_trace(self):
+    def foo(x):
+      return np.sin(x) + np.cos(x)
+    pval = pe.PartialVal((ShapedArray((3, 2), onp.float32), core.unit))
+    check_trace_eval(foo, (pval,), (onp.random.randn(3, 2),), pval)
+
+  def test_nullary_trace(self):
+    def foo():
+      return 1.2
+    check_trace_eval(foo, (), (), (None, 1.2))
+
+  def test_simple_jit(self):
+    def foo(x):
+      if x.shape == ():
+        return x + 1.
+      else:
+        return x + 2.
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    x1, y1 = onp.array(1.0), onp.array(2.0)
+    assert foo(x1) == y1
+    assert foo2(x1) == y1
+    assert foo3(x1) == y1
+
+    x2, y2 = onp.array([1.0, 2.0]), onp.array([3.0, 4.0])
+    assert onp.all(foo(x2) == y2)
+    assert onp.all(foo2(x2) == y2)
+    assert onp.all(foo3(x2) == y2)
+
+  def test_product_jit(self):
+    def foo(x, tup):
+      y, z = tup
+      w = x + z
+      return (w, {'x': y}), z
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    args = (1.0, (2.0, 3.0))
+    expected_output = ((4.0, {'x': 2.0}), 3.0)
+
+    assert foo(*args) == expected_output
+    assert foo2(*args) == expected_output
+    assert foo3(*args) == foo(*args)
+
+  def test_jvp_2(self):
+    d_sin = fwd_deriv(np.sin)
+    d2_sin = fwd_deriv(d_sin)
+    d3_sin = fwd_deriv(d2_sin)
+
+    assert d_sin(0.0) == 1.0
+    assert d2_sin(0.0) == 0.0
+    assert d3_sin(0.0) == -1.0
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/lapax_test.py,tests/lapax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lapax_test.py b/tests/lapax_test.py
new file mode 100644
index 000000000..07ef0ad20
--- /dev/null
+++ b/tests/lapax_test.py
@@ -0,0 +1,205 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the LAPAX linear algebra module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import itertools
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import jit
+from jax import test_util as jtu
+from jax.experimental import lapax
+
+
+class LapaxTest(jtu.JaxTestCase):
+
+  def testSolveLowerTriangularVec(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3))
+    rhs = npr.randn(3, 1)
+    rhs2 = npr.randn(3, 1)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularMat(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(4, 4))
+    lhs2 = onp.tril(npr.randn(4, 4))
+    rhs = npr.randn(4, 3)
+    rhs2 = npr.randn(4, 3)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularBroadcasting(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3, 3))
+    rhs = npr.randn(3, 3, 2)
+    rhs2 = npr.randn(3, 3, 2)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(4, 4))
+    arr2 = square(npr.randn(4, 4))
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(lapax.cholesky)
+    check(fun, arr)
+    check(fun, arr2)
+
+  def testBlockedCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(11, 11))
+    arr2 = square(npr.randn(11, 11))
+
+    chol = lambda x: lapax.cholesky(x, block_size=3)
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(chol)
+    check(fun, arr)
+    check(fun, arr2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 4, 4), (2, 4, 6) if left else (2, 6, 4)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangular(self, lower, left_side, transpose_a, lhs_shape,
+                          rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 8, 8), (2, 8, 10) if left else (2, 10, 8)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangularBlocked(self, lower, left_side, transpose_a, lhs_shape,
+                                 rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A).astype(A.dtype)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a,
+        block_size=3)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
new file mode 100644
index 000000000..07ef0ad20
--- /dev/null
+++ b/tests/lapax_test.py
@@ -0,0 +1,205 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the LAPAX linear algebra module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import itertools
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import jit
+from jax import test_util as jtu
+from jax.experimental import lapax
+
+
+class LapaxTest(jtu.JaxTestCase):
+
+  def testSolveLowerTriangularVec(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3))
+    rhs = npr.randn(3, 1)
+    rhs2 = npr.randn(3, 1)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularMat(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(4, 4))
+    lhs2 = onp.tril(npr.randn(4, 4))
+    rhs = npr.randn(4, 3)
+    rhs2 = npr.randn(4, 3)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularBroadcasting(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3, 3))
+    rhs = npr.randn(3, 3, 2)
+    rhs2 = npr.randn(3, 3, 2)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(4, 4))
+    arr2 = square(npr.randn(4, 4))
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(lapax.cholesky)
+    check(fun, arr)
+    check(fun, arr2)
+
+  def testBlockedCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(11, 11))
+    arr2 = square(npr.randn(11, 11))
+
+    chol = lambda x: lapax.cholesky(x, block_size=3)
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(chol)
+    check(fun, arr)
+    check(fun, arr2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 4, 4), (2, 4, 6) if left else (2, 6, 4)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangular(self, lower, left_side, transpose_a, lhs_shape,
+                          rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 8, 8), (2, 8, 10) if left else (2, 10, 8)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangularBlocked(self, lower, left_side, transpose_a, lhs_shape,
+                                 rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A).astype(A.dtype)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a,
+        block_size=3)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
new file mode 100644
index 000000000..eb3329dd9
--- /dev/null
+++ b/tests/lax_numpy_indexing_test.py
@@ -0,0 +1,586 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import collections
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import lax
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+# We disable the whitespace continuation check in this file because otherwise it
+# makes the test name formatting unwieldy.
+# pylint: disable=bad-continuation
+
+
+float_dtypes = [onp.float32, onp.float64]
+int_dtypes = [onp.int32, onp.int64]
+bool_types = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + int_dtypes + bool_types
+
+IndexSpec = collections.namedtuple(""IndexTest"", [""shape"", ""indexer""])
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+class IndexingTest(jtu.JaxTestCase):
+  """"""Tests for Numpy indexing translation rules.""""""
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(
+              name, jtu.format_shape_dtype_string( shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(10,), indexer=slice(1, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(-3, None)),
+      ]),
+      (""OneSliceIndexNegativeStride"", [
+          IndexSpec(shape=(10,), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(1, 8, -1)),  # empty result
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(0, 8, -1)),  # empty result
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -1)),
+      ]),
+      (""OneSliceIndexNonUnitStride"", [
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 2)),
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, 3, 2)),
+          IndexSpec(shape=(10,), indexer=slice(1, None, 2)),
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 8, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, 2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -2)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(10, 8), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(
+              shape=(10, 8), indexer=(slice(None, None, -1), slice(None, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(10, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(10, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      (""EmptyIndex"", [
+          IndexSpec(shape=(), indexer=()),
+          IndexSpec(shape=(3,), indexer=()),
+          IndexSpec(shape=(3, 4), indexer=()),
+      ]),
+  ] for shape, indexer in index_specs for dtype in all_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: x[indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(name,
+                                            jtu.format_shape_dtype_string(
+                                                shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(5,), indexer=slice(1, 3)),
+          IndexSpec(shape=(5,), indexer=slice(1, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(None, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(-3, None)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      # TODO(mattjj): these fail for uninteresting dtype reasons
+      # (""EmptyIndex"",
+      #  [IndexSpec(shape=(), indexer=()),
+      #   IndexSpec(shape=(3,), indexer=()),
+      #   IndexSpec(shape=(3, 4), indexer=()),
+      #   ]),
+  ] for shape, indexer in index_specs for dtype in float_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  def _ReplaceSlicesWithTuples(self, idx):
+    """"""Helper method to replace slices with tuples for dynamic indexing args.""""""
+    if isinstance(idx, slice):
+      triple = idx.start, idx.stop, idx.step
+      isnone = [i for i, elt in enumerate(triple) if elt is None]
+      zeros = itertools.repeat(0)
+      nones = itertools.repeat(None)
+      out = lax.subvals(triple, zip(isnone, zeros))
+      return out, lambda out: slice(*lax.subvals(out, zip(isnone, nones)))
+    elif isinstance(idx, (tuple, list)) and idx:
+      t = type(idx)
+      elts, packs = zip(*map(self._ReplaceSlicesWithTuples, idx))
+      return elts, lambda elts: t((pack(i) for pack, i in zip(packs, elts)))
+    else:
+      return idx, lambda x: x
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneSliceIndex"",
+           [IndexSpec(shape=(5,), indexer=slice(1, 3)),
+            IndexSpec(shape=(5, 4), indexer=slice(1, 3))]),
+          (""TwoSliceIndices"",
+           [IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+            IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2)))]),
+          (""NonUnitStrides"", [
+              IndexSpec(shape=(3,), indexer=slice(None, None, -1)),
+              IndexSpec(shape=(3, 3), indexer=slice(0, 3, -2)),
+              IndexSpec(shape=(3, 4, 5), indexer=slice(0, 4, 2))
+          ]),
+          (""OnlyStartOrStopDynamic"", [
+              IndexSpec(shape=(5, 4), indexer=(slice(None, 3), slice(0, 2))),
+              IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None)))
+          ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self.assertRaises(IndexError, lambda: fun(*args_maker()))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2)]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2))]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2),
+            ]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+            ]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
+    # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(unpacked_indexer, x):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    arr = rng(shape, dtype)
+    check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype), indexer]
+    fun = lambda x, idx: x[idx]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""SlicesAndOneIntArrayIndex"",
+           [IndexSpec(shape=(2, 3), indexer=(onp.array([0, 1]), slice(1, 2))),
+            IndexSpec(shape=(2, 3), indexer=(slice(0, 2),
+                                             onp.array([0, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                slice(None))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([[0, 2], [1, 1]]),
+                                                slice(None))),
+            ]),
+          (""SlicesAndTwoIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                Ellipsis,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                Ellipsis)),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                slice(1, 3))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                slice(1, 3),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2, -2]),
+                                                slice(None, None, 2),
+                                                onp.array([-1, 2, -1]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([[0, 2], [2, 0]]),
+                                                Ellipsis,
+                                                onp.array([[1, 0], [1, 0]]))),
+            ]),
+          (""NonesAndIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2]),
+                                                None,
+                                                onp.array([-1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
+                            for e in indexer]
+    substitutes = [(i, e) for i, e in enumerate(indexer)
+                   if not isinstance(e, onp.ndarray)]
+    args_maker = lambda: [rng(shape, dtype), indexer_with_dummies]
+
+    def fun(x, indexer_with_dummies):
+      idx = type(indexer)(lax.subvals(indexer_with_dummies, substitutes))
+      return x[idx]
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  def testAdvancedIndexingManually(self):
+    x = onp.random.RandomState(0).randn(3, 4, 5)
+    index_array = onp.array([0, 2, -1, 0])
+
+    op = lambda x, index_array: x[..., index_array, :]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[..., index_array, :, index_array, None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[index_array, ..., index_array[:, None], None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+  def testUnpacking(self):
+
+    def foo(x):
+      a, b, c = x
+      return a + b + c
+
+    cfoo = api.jit(foo)
+
+    a1 = foo(onp.arange(3))
+    a2 = cfoo(onp.arange(3))
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
new file mode 100644
index 000000000..eb3329dd9
--- /dev/null
+++ b/tests/lax_numpy_indexing_test.py
@@ -0,0 +1,586 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import collections
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import lax
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+# We disable the whitespace continuation check in this file because otherwise it
+# makes the test name formatting unwieldy.
+# pylint: disable=bad-continuation
+
+
+float_dtypes = [onp.float32, onp.float64]
+int_dtypes = [onp.int32, onp.int64]
+bool_types = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + int_dtypes + bool_types
+
+IndexSpec = collections.namedtuple(""IndexTest"", [""shape"", ""indexer""])
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+class IndexingTest(jtu.JaxTestCase):
+  """"""Tests for Numpy indexing translation rules.""""""
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(
+              name, jtu.format_shape_dtype_string( shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(10,), indexer=slice(1, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(-3, None)),
+      ]),
+      (""OneSliceIndexNegativeStride"", [
+          IndexSpec(shape=(10,), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(1, 8, -1)),  # empty result
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(0, 8, -1)),  # empty result
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -1)),
+      ]),
+      (""OneSliceIndexNonUnitStride"", [
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 2)),
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, 3, 2)),
+          IndexSpec(shape=(10,), indexer=slice(1, None, 2)),
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 8, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, 2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -2)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(10, 8), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(
+              shape=(10, 8), indexer=(slice(None, None, -1), slice(None, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(10, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(10, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      (""EmptyIndex"", [
+          IndexSpec(shape=(), indexer=()),
+          IndexSpec(shape=(3,), indexer=()),
+          IndexSpec(shape=(3, 4), indexer=()),
+      ]),
+  ] for shape, indexer in index_specs for dtype in all_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: x[indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(name,
+                                            jtu.format_shape_dtype_string(
+                                                shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(5,), indexer=slice(1, 3)),
+          IndexSpec(shape=(5,), indexer=slice(1, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(None, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(-3, None)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      # TODO(mattjj): these fail for uninteresting dtype reasons
+      # (""EmptyIndex"",
+      #  [IndexSpec(shape=(), indexer=()),
+      #   IndexSpec(shape=(3,), indexer=()),
+      #   IndexSpec(shape=(3, 4), indexer=()),
+      #   ]),
+  ] for shape, indexer in index_specs for dtype in float_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  def _ReplaceSlicesWithTuples(self, idx):
+    """"""Helper method to replace slices with tuples for dynamic indexing args.""""""
+    if isinstance(idx, slice):
+      triple = idx.start, idx.stop, idx.step
+      isnone = [i for i, elt in enumerate(triple) if elt is None]
+      zeros = itertools.repeat(0)
+      nones = itertools.repeat(None)
+      out = lax.subvals(triple, zip(isnone, zeros))
+      return out, lambda out: slice(*lax.subvals(out, zip(isnone, nones)))
+    elif isinstance(idx, (tuple, list)) and idx:
+      t = type(idx)
+      elts, packs = zip(*map(self._ReplaceSlicesWithTuples, idx))
+      return elts, lambda elts: t((pack(i) for pack, i in zip(packs, elts)))
+    else:
+      return idx, lambda x: x
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneSliceIndex"",
+           [IndexSpec(shape=(5,), indexer=slice(1, 3)),
+            IndexSpec(shape=(5, 4), indexer=slice(1, 3))]),
+          (""TwoSliceIndices"",
+           [IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+            IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2)))]),
+          (""NonUnitStrides"", [
+              IndexSpec(shape=(3,), indexer=slice(None, None, -1)),
+              IndexSpec(shape=(3, 3), indexer=slice(0, 3, -2)),
+              IndexSpec(shape=(3, 4, 5), indexer=slice(0, 4, 2))
+          ]),
+          (""OnlyStartOrStopDynamic"", [
+              IndexSpec(shape=(5, 4), indexer=(slice(None, 3), slice(0, 2))),
+              IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None)))
+          ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self.assertRaises(IndexError, lambda: fun(*args_maker()))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2)]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2))]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2),
+            ]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+            ]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
+    # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(unpacked_indexer, x):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    arr = rng(shape, dtype)
+    check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype), indexer]
+    fun = lambda x, idx: x[idx]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""SlicesAndOneIntArrayIndex"",
+           [IndexSpec(shape=(2, 3), indexer=(onp.array([0, 1]), slice(1, 2))),
+            IndexSpec(shape=(2, 3), indexer=(slice(0, 2),
+                                             onp.array([0, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                slice(None))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([[0, 2], [1, 1]]),
+                                                slice(None))),
+            ]),
+          (""SlicesAndTwoIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                Ellipsis,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                Ellipsis)),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                slice(1, 3))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                slice(1, 3),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2, -2]),
+                                                slice(None, None, 2),
+                                                onp.array([-1, 2, -1]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([[0, 2], [2, 0]]),
+                                                Ellipsis,
+                                                onp.array([[1, 0], [1, 0]]))),
+            ]),
+          (""NonesAndIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2]),
+                                                None,
+                                                onp.array([-1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
+                            for e in indexer]
+    substitutes = [(i, e) for i, e in enumerate(indexer)
+                   if not isinstance(e, onp.ndarray)]
+    args_maker = lambda: [rng(shape, dtype), indexer_with_dummies]
+
+    def fun(x, indexer_with_dummies):
+      idx = type(indexer)(lax.subvals(indexer_with_dummies, substitutes))
+      return x[idx]
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  def testAdvancedIndexingManually(self):
+    x = onp.random.RandomState(0).randn(3, 4, 5)
+    index_array = onp.array([0, 2, -1, 0])
+
+    op = lambda x, index_array: x[..., index_array, :]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[..., index_array, :, index_array, None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[index_array, ..., index_array[:, None], None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+  def testUnpacking(self):
+
+    def foo(x):
+      a, b, c = x
+      return a + b + c
+
+    cfoo = api.jit(foo)
+
+    a1 = foo(onp.arange(3))
+    a2 = cfoo(onp.arange(3))
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_numpy_test.py,tests/lax_numpy_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
new file mode 100644
index 000000000..7b0c918fd
--- /dev/null
+++ b/tests/lax_numpy_test.py
@@ -0,0 +1,528 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_ONE_TO_ONE_OP_RECORDS = [
+    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
+    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+]
+
+JAX_COMPOUND_OP_RECORDS = [
+    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""expm1_large""),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""log1p_large""),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+]
+
+JAX_REDUCER_RECORDS = [
+    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+]
+
+JAX_ARGMINMAX_RECORDS = [
+    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedNumpyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Numpy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
+                                 JAX_COMPOUND_OP_RECORDS)
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis, ""keepdims"": keepdims}
+      for rec in JAX_REDUCER_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
+    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
+    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis}
+      for rec in JAX_ARGMINMAX_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape)))
+  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
+
+    def onp_fun(array_to_reduce):
+      return onp_op(array_to_reduce, axis)
+
+    def lnp_fun(array_to_reduce):
+      return lnp_op(array_to_reduce, axis)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""matrix-scalar"", (3, 3), ()),
+          (""scalar-matrix"", (), (3, 3)),
+          (""matrix-vector"", (4, 5), (5,)),
+          (""vector-matrix"", (6,), (6, 4)),
+          (""matrix-matrix"", (3, 4), (4, 5)),
+          (""tensor-vector"", (4, 3, 2), (2,)),
+          (""vector-tensor"", (2,), (3, 2, 4)),
+          (""tensor-matrix"", (4, 3, 2), (2, 5)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""vector-vector"", (3,), (3,)),
+          (""matrix-vector"", (3, 3), (3,)),
+          (""vector-matrix"", (3,), (3, 3)),
+          (""matrix-matrix"", (3, 3), (3, 3)),
+          (""vector-tensor"", (3,), (5, 3, 2)),
+          (""tensor-vector"", (5, 3, 2), (2,)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-matrix"", (5, 2, 3), (3, 2)),
+          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
+          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
+                            check_dtypes=True)
+    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
+       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
+  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
+    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
+    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_decimals={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), decimals),
+       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for decimals in [0, 1, -2])
+  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
+    onp_fun = lambda x: onp.round(x, decimals=decimals)
+    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
+          axis, "","".join(str(d) for d in base_shape),
+          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
+       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
+       ""rng"": jtu.rand_default()}
+      for num_arrs in [3]
+      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for axis in range(-len(base_shape)+1, len(base_shape)))
+  def testConcatenate(self, axis, base_shape, dtypes, rng):
+    wrapped_axis = axis % len(base_shape)
+    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
+    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
+    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)
+
+    def args_maker():
+      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
+          ""_"".join(str(d) for d in shape),
+          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
+       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
+      for shape in all_shapes
+      for fill_value_dtype in default_dtypes
+      for out_dtype in default_dtypes)
+  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
+    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
+    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
+    args_maker = lambda: [rng((), fill_value_dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
+       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
+       ""dtype"": dtype, ""rng"": jtu.rand_default()}
+      for shape, axis, num_sections in [
+          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
+          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
+      for dtype in default_dtypes)
+  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
+    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
+    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          ((3, 4), 12),
+          ((3, 4), (12,)),
+          ((3, 4), -1),
+          ((2, 1, 4), (-1,)),
+          ((2, 2, 4), (2, 8))
+      ])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    onp_fun = lambda x: onp.reshape(x, out_shape)
+    lnp_fun = lambda x: lnp.reshape(x, out_shape)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
+       ""rng"": jtu.rand_default()}
+      for arg_shape in [(), (3,), (3, 4)]
+      for dtype in default_dtypes
+      for dim in range(-len(arg_shape)+1, len(arg_shape)))
+  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
+    onp_fun = lambda x: onp.expand_dims(x, dim)
+    lnp_fun = lambda x: lnp.expand_dims(x, dim)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax1, ax2 in [
+          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
+          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
+      for dtype in default_dtypes)
+  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
+    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
+    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax in [
+          ((3, 1), None),
+          ((3, 1), 1),
+          ((1, 3, 1), (0, 2)),
+          ((1, 4, 1), (0,))]
+      for dtype in default_dtypes)
+  def testSqueeze(self, arg_shape, dtype, ax, rng):
+    onp_fun = lambda x: onp.squeeze(x, ax)
+    lnp_fun = lambda x: lnp.squeeze(x, ax)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
+      for i, arg in enumerate([
+          [1, 2, 3], [1., 2., 3.],
+          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
+          [[3, onp.array(2), 1], onp.arange(3.)],
+      ]))
+  def testArray(self, arg):
+    args_maker = lambda: [arg]
+    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)
+
+  def testAllClose(self):
+    rng = onp.random.RandomState(0)
+    x = rng.randn(2, 2)
+    y = rng.randn(2)
+
+    def same(list1, list2):
+      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
+      elements_close = list(map(allclose, list1, list2))
+      return lnp.all(lnp.array(elements_close))
+
+    csame = api.jit(same)
+
+    a1 = same((x, y), (x, y))
+    a2 = csame((x, y), (x, y))
+    a3 = csame((x, y), (x, 2 * y))
+
+    self.assertTrue(a1)
+    self.assertTrue(a2)
+    self.assertFalse(a3)
+
+  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
+  def DISABLED_testOnesBroadcastingConstantHandler(self):
+    # TODO(mattjj): update this test for jax3
+
+    def fun(x):
+      ones = lnp.ones((3, 4))
+      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)
+
+      # To check that the constant handler generates a Broadcast for stride-zero
+      # arrays, we monkey-patch the client instance.
+      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
+      # we can check the HLO more directly.
+      c = x._node.c
+      Broadcast = c.Broadcast  # pylint: disable=invalid-name
+      was_called = []
+      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
+      out = x + ones  # the ndarray constant handler should call Broadcast here
+      assert was_called, ""Broadcast was not called.""
+
+      return out
+
+    fun = api.jit(fun)
+    out_val = fun(lnp.ones(4))
+    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)
+
+  def testZeroStridesConstantHandler(self):
+    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
+    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))
+
+    def fun(x):
+      return x * const
+
+    fun = api.jit(fun)
+    out_val = fun(3.)
+    self.assertAllClose(out_val, 3. * const, check_dtypes=False)
+
+  def testIsInstanceNdarrayDuringTracing(self):
+    arr = onp.ones(3)
+
+    @api.jit
+    def f(x):
+      self.assertIsInstance(x, lnp.ndarray)
+      return lnp.sum(x)
+
+    f(arr)
+
+
+  def testNonArrayErrorMessage(self):
+    x = [1., 2.]
+    y = onp.array([3., 4.])
+
+    def g(x, y):
+      return lnp.add(x, y)
+
+    def f(x, y):
+      return lnp.dot(x, y)
+
+    self.assertRaises(TypeError, lambda: g(x, y))
+    self.assertRaises(TypeError, lambda: f(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))
+
+  def testAbstractionErrorMessage(self):
+
+    @api.jit
+    def f(x, n):
+      for _ in range(n):
+        x = x * x
+      return x
+
+    self.assertRaises(TypeError, lambda: f(3., 3))
+
+    @api.jit
+    def g(x):
+      if x > 0.:
+        return x * 2
+      else:
+        return x + 2
+
+    self.assertRaises(TypeError, lambda: g(3.))
+
+  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
+    # TODO(mattjj): update this for jax3
+    foo = lnp._not_implemented(lambda x: x)
+
+    # No error if there's no tracing.
+    foo(onp.arange(3))
+
+    cfoo = api.jit(foo)
+    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))
+
+  # TODO(mattjj): test infix operator overrides
+
+  def DISABLED_testRavel(self):
+    # TODO(mattjj): support this method-based syntax?
+    rng = onp.random.RandomState(0)
+    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
+    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test other ndarray-like method overrides
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
new file mode 100644
index 000000000..7b0c918fd
--- /dev/null
+++ b/tests/lax_numpy_test.py
@@ -0,0 +1,528 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_ONE_TO_ONE_OP_RECORDS = [
+    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
+    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+]
+
+JAX_COMPOUND_OP_RECORDS = [
+    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""expm1_large""),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""log1p_large""),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+]
+
+JAX_REDUCER_RECORDS = [
+    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+]
+
+JAX_ARGMINMAX_RECORDS = [
+    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedNumpyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Numpy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
+                                 JAX_COMPOUND_OP_RECORDS)
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis, ""keepdims"": keepdims}
+      for rec in JAX_REDUCER_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
+    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
+    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis}
+      for rec in JAX_ARGMINMAX_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape)))
+  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
+
+    def onp_fun(array_to_reduce):
+      return onp_op(array_to_reduce, axis)
+
+    def lnp_fun(array_to_reduce):
+      return lnp_op(array_to_reduce, axis)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""matrix-scalar"", (3, 3), ()),
+          (""scalar-matrix"", (), (3, 3)),
+          (""matrix-vector"", (4, 5), (5,)),
+          (""vector-matrix"", (6,), (6, 4)),
+          (""matrix-matrix"", (3, 4), (4, 5)),
+          (""tensor-vector"", (4, 3, 2), (2,)),
+          (""vector-tensor"", (2,), (3, 2, 4)),
+          (""tensor-matrix"", (4, 3, 2), (2, 5)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""vector-vector"", (3,), (3,)),
+          (""matrix-vector"", (3, 3), (3,)),
+          (""vector-matrix"", (3,), (3, 3)),
+          (""matrix-matrix"", (3, 3), (3, 3)),
+          (""vector-tensor"", (3,), (5, 3, 2)),
+          (""tensor-vector"", (5, 3, 2), (2,)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-matrix"", (5, 2, 3), (3, 2)),
+          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
+          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
+                            check_dtypes=True)
+    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
+       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
+  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
+    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
+    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_decimals={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), decimals),
+       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for decimals in [0, 1, -2])
+  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
+    onp_fun = lambda x: onp.round(x, decimals=decimals)
+    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
+          axis, "","".join(str(d) for d in base_shape),
+          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
+       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
+       ""rng"": jtu.rand_default()}
+      for num_arrs in [3]
+      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for axis in range(-len(base_shape)+1, len(base_shape)))
+  def testConcatenate(self, axis, base_shape, dtypes, rng):
+    wrapped_axis = axis % len(base_shape)
+    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
+    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
+    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)
+
+    def args_maker():
+      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
+          ""_"".join(str(d) for d in shape),
+          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
+       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
+      for shape in all_shapes
+      for fill_value_dtype in default_dtypes
+      for out_dtype in default_dtypes)
+  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
+    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
+    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
+    args_maker = lambda: [rng((), fill_value_dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
+       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
+       ""dtype"": dtype, ""rng"": jtu.rand_default()}
+      for shape, axis, num_sections in [
+          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
+          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
+      for dtype in default_dtypes)
+  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
+    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
+    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          ((3, 4), 12),
+          ((3, 4), (12,)),
+          ((3, 4), -1),
+          ((2, 1, 4), (-1,)),
+          ((2, 2, 4), (2, 8))
+      ])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    onp_fun = lambda x: onp.reshape(x, out_shape)
+    lnp_fun = lambda x: lnp.reshape(x, out_shape)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
+       ""rng"": jtu.rand_default()}
+      for arg_shape in [(), (3,), (3, 4)]
+      for dtype in default_dtypes
+      for dim in range(-len(arg_shape)+1, len(arg_shape)))
+  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
+    onp_fun = lambda x: onp.expand_dims(x, dim)
+    lnp_fun = lambda x: lnp.expand_dims(x, dim)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax1, ax2 in [
+          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
+          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
+      for dtype in default_dtypes)
+  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
+    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
+    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax in [
+          ((3, 1), None),
+          ((3, 1), 1),
+          ((1, 3, 1), (0, 2)),
+          ((1, 4, 1), (0,))]
+      for dtype in default_dtypes)
+  def testSqueeze(self, arg_shape, dtype, ax, rng):
+    onp_fun = lambda x: onp.squeeze(x, ax)
+    lnp_fun = lambda x: lnp.squeeze(x, ax)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
+      for i, arg in enumerate([
+          [1, 2, 3], [1., 2., 3.],
+          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
+          [[3, onp.array(2), 1], onp.arange(3.)],
+      ]))
+  def testArray(self, arg):
+    args_maker = lambda: [arg]
+    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)
+
+  def testAllClose(self):
+    rng = onp.random.RandomState(0)
+    x = rng.randn(2, 2)
+    y = rng.randn(2)
+
+    def same(list1, list2):
+      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
+      elements_close = list(map(allclose, list1, list2))
+      return lnp.all(lnp.array(elements_close))
+
+    csame = api.jit(same)
+
+    a1 = same((x, y), (x, y))
+    a2 = csame((x, y), (x, y))
+    a3 = csame((x, y), (x, 2 * y))
+
+    self.assertTrue(a1)
+    self.assertTrue(a2)
+    self.assertFalse(a3)
+
+  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
+  def DISABLED_testOnesBroadcastingConstantHandler(self):
+    # TODO(mattjj): update this test for jax3
+
+    def fun(x):
+      ones = lnp.ones((3, 4))
+      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)
+
+      # To check that the constant handler generates a Broadcast for stride-zero
+      # arrays, we monkey-patch the client instance.
+      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
+      # we can check the HLO more directly.
+      c = x._node.c
+      Broadcast = c.Broadcast  # pylint: disable=invalid-name
+      was_called = []
+      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
+      out = x + ones  # the ndarray constant handler should call Broadcast here
+      assert was_called, ""Broadcast was not called.""
+
+      return out
+
+    fun = api.jit(fun)
+    out_val = fun(lnp.ones(4))
+    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)
+
+  def testZeroStridesConstantHandler(self):
+    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
+    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))
+
+    def fun(x):
+      return x * const
+
+    fun = api.jit(fun)
+    out_val = fun(3.)
+    self.assertAllClose(out_val, 3. * const, check_dtypes=False)
+
+  def testIsInstanceNdarrayDuringTracing(self):
+    arr = onp.ones(3)
+
+    @api.jit
+    def f(x):
+      self.assertIsInstance(x, lnp.ndarray)
+      return lnp.sum(x)
+
+    f(arr)
+
+
+  def testNonArrayErrorMessage(self):
+    x = [1., 2.]
+    y = onp.array([3., 4.])
+
+    def g(x, y):
+      return lnp.add(x, y)
+
+    def f(x, y):
+      return lnp.dot(x, y)
+
+    self.assertRaises(TypeError, lambda: g(x, y))
+    self.assertRaises(TypeError, lambda: f(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))
+
+  def testAbstractionErrorMessage(self):
+
+    @api.jit
+    def f(x, n):
+      for _ in range(n):
+        x = x * x
+      return x
+
+    self.assertRaises(TypeError, lambda: f(3., 3))
+
+    @api.jit
+    def g(x):
+      if x > 0.:
+        return x * 2
+      else:
+        return x + 2
+
+    self.assertRaises(TypeError, lambda: g(3.))
+
+  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
+    # TODO(mattjj): update this for jax3
+    foo = lnp._not_implemented(lambda x: x)
+
+    # No error if there's no tracing.
+    foo(onp.arange(3))
+
+    cfoo = api.jit(foo)
+    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))
+
+  # TODO(mattjj): test infix operator overrides
+
+  def DISABLED_testRavel(self):
+    # TODO(mattjj): support this method-based syntax?
+    rng = onp.random.RandomState(0)
+    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
+    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test other ndarray-like method overrides
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_scipy_test.py,tests/lax_scipy_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
new file mode 100644
index 000000000..21a5ac4b3
--- /dev/null
+++ b/tests/lax_scipy_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.misc as osp_misc
+import scipy.special as osp_special
+
+from jax import api
+from jax import test_util as jtu
+from jax.scipy import misc as lsp_misc
+from jax.scipy import special as lsp_special
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_SPECIAL_FUNCTION_RECORDS = [
+    op_record(""gammaln"", 1, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""digamma"", 1, float_dtypes, jtu.rand_positive(), []),
+    op_record(""erf"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfc"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfinv"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedScipyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Scipy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}_keepdims={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": jtu.rand_default(), ""shape"": shape, ""dtype"": dtype,
+       ""axis"": axis, ""keepdims"": keepdims}
+      for shape in all_shapes for dtype in float_dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    def scipy_fun(array_to_reduce):
+      return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    def lax_fun(array_to_reduce):
+      return lsp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.test_name, shapes, dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""modes"": rec.diff_modes,
+       ""scipy_op"": getattr(osp_special, rec.name),
+       ""lax_op"": getattr(lsp_special, rec.name)}
+      for rec in JAX_SPECIAL_FUNCTION_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
+    # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
+        and not shapes[0]):
+      return absltest.unittest.skip(""real() on scalar not supported on tpu"")
+
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    args = args_maker()
+    self.assertAllClose(scipy_op(*args), lax_op(*args), atol=1e-3, rtol=1e-3,
+                        check_dtypes=False)
+    self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
new file mode 100644
index 000000000..21a5ac4b3
--- /dev/null
+++ b/tests/lax_scipy_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.misc as osp_misc
+import scipy.special as osp_special
+
+from jax import api
+from jax import test_util as jtu
+from jax.scipy import misc as lsp_misc
+from jax.scipy import special as lsp_special
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_SPECIAL_FUNCTION_RECORDS = [
+    op_record(""gammaln"", 1, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""digamma"", 1, float_dtypes, jtu.rand_positive(), []),
+    op_record(""erf"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfc"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfinv"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedScipyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Scipy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}_keepdims={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": jtu.rand_default(), ""shape"": shape, ""dtype"": dtype,
+       ""axis"": axis, ""keepdims"": keepdims}
+      for shape in all_shapes for dtype in float_dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    def scipy_fun(array_to_reduce):
+      return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    def lax_fun(array_to_reduce):
+      return lsp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.test_name, shapes, dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""modes"": rec.diff_modes,
+       ""scipy_op"": getattr(osp_special, rec.name),
+       ""lax_op"": getattr(lsp_special, rec.name)}
+      for rec in JAX_SPECIAL_FUNCTION_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
+    # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
+        and not shapes[0]):
+      return absltest.unittest.skip(""real() on scalar not supported on tpu"")
+
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    args = args_maker()
+    self.assertAllClose(scipy_op(*args), lax_op(*args), atol=1e-3, rtol=1e-3,
+                        check_dtypes=False)
+    self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_test.py,tests/lax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_test.py b/tests/lax_test.py
new file mode 100644
index 000000000..b8c78524c
--- /dev/null
+++ b/tests/lax_test.py
@@ -0,0 +1,1981 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import numpy.random as npr
+
+from jax import api
+from jax import core
+from jax import lax
+from jax import test_util as jtu
+from jax import lax_reference
+from jax.interpreters import xla
+from jax.lib import xla_bridge
+
+FLAGS = flags.FLAGS
+
+
+def num_float_bits(dtype):
+  return onp.finfo(xla_bridge.canonicalize_dtype(dtype)).bits
+
+
+### lax tests
+
+# For standard unops and binops, we can generate a large number of tests on
+# arguments of appropriate shapes and dtypes using the following table.
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + complex_dtypes + int_dtypes + bool_dtypes
+
+compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]
+
+OpRecord = collections.namedtuple(""OpRecord"",
+                                  [""op"", ""nargs"", ""dtypes"", ""rng"", ""tol""])
+
+
+def op_record(op, nargs, dtypes, rng, tol=1e-5):
+  return OpRecord(op, nargs, dtypes, rng, tol)
+
+LAX_OPS = [
+    op_record(lax.neg, 1, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sign, 1, default_dtypes, jtu.rand_small()),
+    op_record(lax.floor, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.ceil, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.round, 1, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.is_finite, 1, float_dtypes, jtu.rand_small()),
+
+    op_record(lax.exp, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.expm1, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.log, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.log1p, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tanh, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sin, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cos, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.atan2, 2, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.sqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.rsqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.square, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.reciprocal, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tan, 1, float_dtypes, jtu.rand_default()),
+    op_record(lax.asin, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.acos, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.atan, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.sinh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cosh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.asinh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.acosh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.lgamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.digamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.erf, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erfc, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erf_inv, 1, float_dtypes, jtu.rand_small(), tol=1e-2),
+
+    op_record(lax.real, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.imag, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.complex, 2, [onp.float32], jtu.rand_default()),
+    op_record(lax.conj, 1, [onp.float32] + complex_dtypes, jtu.rand_default()),
+    op_record(lax.abs, 1, default_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.pow, 2, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.bitwise_and, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_not, 1, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_or, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_xor, 2, bool_dtypes, jtu.rand_small()),
+
+    op_record(lax.add, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sub, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.mul, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.div, 2, default_dtypes + complex_dtypes, jtu.rand_nonzero()),
+    op_record(lax.rem, 2, default_dtypes, jtu.rand_nonzero()),
+
+    op_record(lax.max, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.min, 2, default_dtypes, jtu.rand_small()),
+
+    op_record(lax.eq, 2, all_dtypes, jtu.rand_some_equal()),
+    op_record(lax.ne, 2, all_dtypes, jtu.rand_small()),
+    op_record(lax.ge, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.gt, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.le, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.lt, 2, default_dtypes, jtu.rand_small()),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxTest(jtu.JaxTestCase):
+  """"""Numerical tests for LAX operations.""""""
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOp(self, op, rng, shapes, dtype):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""tol"": rec.tol}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    numpy_op = getattr(lax_reference, op.__name__)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker, tol=tol)
+
+  # TODO test shift_left, shift_right_arithmetic, shift_right_logical
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
+                            rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    numpy_op = lambda *args: lax_reference.concatenate(args, dim)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv(lhs, rhs, strides, padding)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
+                           rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)
+    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dilation, rhs_dilation, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
+      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
+      rhs_dilation, rng):
+    # TODO(mattjj): make this test pass
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    def numpy_fun(lhs, rhs):
+      return lax_reference.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CheckAgainstNumpy(fun, numpy_fun, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}""
+       ""_dims={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation,
+           "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""dimension_numbers"": dim_nums,
+       ""perms"": perms, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (2, 1)]
+      for padding in [((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2)], repeat=2)
+      for rng in [jtu.rand_small()]
+      for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
+                              ([0, 1, 2, 3], [0, 1, 2, 3])),
+                             ((""NHWC"", ""HWIO"", ""NHWC""),
+                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
+                             padding, lhs_dilation, rhs_dilation,
+                             dimension_numbers, perms, rng):
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+
+    def args_maker():
+      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),
+              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]
+
+    def fun(lhs, rhs):
+      return lax.conv_general_dilated(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,
+          dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test conv_general_dilated against numpy
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDot(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               lhs_contracting, rhs_contracting),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""lhs_contracting"": lhs_contracting, ""rhs_contracting"": rhs_contracting,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [
+          # these all fail with ""RuntimeError: Unimplemented: Dot with
+          # non-standard contracting dimensions not implemented.""
+          # [(3, 5), (2, 5), [1], [1]],
+          # [(5, 3), (5, 2), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],
+          # [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],
+          [(3, 2), (2, 4), [1], [0]],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
+                                 lhs_contracting, rhs_contracting, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
+                                     dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
+                                 dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)
+    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
+                                     dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    numpy_op = lambda x: lax_reference.reshape(x, out_shape)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPad(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadAgainstNumpy(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
+    numpy_op = lambda x: lax_reference.pad(x, onp.array(0, dtype), pads)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testReverse(self):
+    rev = api.jit(lambda operand: lax.rev(operand, dimensions))
+
+    dimensions = [0]
+    self.assertAllClose(onp.array([3, 2, 1]), rev(onp.array([1, 2, 3])),
+                        check_dtypes=False)
+
+    dimensions = [0, 1]
+    self.assertAllClose(onp.array([[6, 5, 4], [3, 2, 1]]),
+                        rev(onp.array([[1, 2, 3], [4, 5, 6]])),
+                        check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSlice(self, shape, dtype, starts, limits, strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
+                            strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                   size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)
+    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
+                          check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CheckAgainstNumpy(lax.dynamic_update_slice,
+                            lax_reference.dynamic_update_slice, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTranspose(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    numpy_op = lambda x: lax_reference.transpose(x, perm)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, default_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.iinfo(onp.int32).min, lax.max, [onp.int32]),
+          (onp.iinfo(onp.int64).min, lax.max, [onp.int64]),
+          (onp.iinfo(onp.uint32).min, lax.max, [onp.uint32]),
+          (onp.iinfo(onp.uint64).min, lax.max, [onp.uint64]),
+          (onp.inf, lax.min, float_dtypes),
+          (onp.iinfo(onp.int32).max, lax.min, [onp.int32]),
+          (onp.iinfo(onp.int64).max, lax.min, [onp.int64]),
+          (onp.iinfo(onp.uint32).max, lax.min, [onp.uint32]),
+          (onp.iinfo(onp.uint64).max, lax.min, [onp.uint64]),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduce(self, op, init_val, shape, dtype, dims, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype), init_val]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    fun = lambda operand: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, [onp.float32]),
+          (-onp.inf, lax.max, [onp.float32]),
+          (onp.inf, lax.min, [onp.float32]),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testReduceWindow(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((4, 6), (2, 1), (1, 2))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],
+              [(2, 1), (1, 2)],
+              [(1, 1), (2, 1), (1, 2)]),
+          itertools.product(
+              [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],
+              [(1, 2, 2, 1), (1, 1, 1, 1)]))
+
+    def fun(operand, init_val):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype), init_val]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype)]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSort(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: lax.sort(x, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortAgainstNumpy(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.sort(x, axis)
+    numpy_op = lambda x: lax_reference.sort(x, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)
+    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testWhileWithTuple(self):
+    limit = 10
+
+    def loop_cond(state):
+      pos, _ = state
+      return lax.lt(pos, limit)
+
+    def loop_body(state):
+      pos, count = state
+      return (lax.add(pos, 1), lax.add(count, 1))
+
+    def loop(init):
+      result = lax._while_loop(loop_cond, loop_body, (init, 0))
+      _, count = result
+      return count
+
+    cloop = api.jit(loop)
+
+    self.assertEqual(loop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(3), limit - 3)
+
+  def testNestedWhile(self):
+
+    def outer_loop(num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        num, i, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        num, i, count = state
+        return (num, lax.add(i, 1), inner_loop(i, count))
+
+      init_val = (num, 0, 0)
+      _, i, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return (i, count)
+
+    def inner_loop(i, count):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        i, j, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, count = state
+        return (i, lax.add(j, 1), lax.add(count, 1))
+
+      init_val = (i, 0, count)
+      _, _, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return count
+
+    cloop = api.jit(outer_loop)
+
+    self.assertEqual(outer_loop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(2), (2, 3))
+    self.assertEqual(cloop(4), (4, 10))
+
+  def testNestedWhileWithDynamicUpdateSlice(self):
+    num = 5
+
+    def update_entry(arr, val, i, j):
+      val = lax.reshape(val, [1, 1])
+      return lax.dynamic_update_slice(arr, val, (i, j))
+
+    def outer_loop(arr):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, num, _, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        i, num, arr, out = state
+        return (lax.add(i, 1), num, arr, inner_loop(i, arr, out))
+
+      out = onp.zeros(arr.shape, dtype=arr.dtype)
+      init_val = (0, num, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    def inner_loop(i, arr, out):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, j, _, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, arr, out = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        arr_i_j = lax.dynamic_index_in_dim(arr_i, j, 0, False)
+        out = update_entry(out, arr_i_j, i, j)
+        return (i, lax.add(j, 1), arr, out)
+
+      init_val = (i, 0, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    cloop = api.jit(outer_loop)
+    arr = npr.RandomState(0).randn(5, 5)
+    self.assertAllClose(outer_loop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+
+  def testLoopWithConjunctionCondition(self):
+    def sum_first_n(arr, num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        arr, num, i, _ = state
+        return lax.bitwise_and(lax.lt(i, num), lax.lt(i, arr.shape[0]))
+
+      def body_fun(state):
+        arr, num, i, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, num, lax.add(i, 1), lax.add(total, arr_i))
+
+      init_val = (arr, num, 0, 0.)
+      _, _, _, total = lax._while_loop(cond_fun, body_fun, init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopBasic(self):
+    def count(num):
+      def body_fun(i, tot):
+        return lax.add(tot, i)
+      return lax.fori_loop(0, num, body_fun, 0)
+
+    cfun = api.jit(count)
+
+    self.assertEqual(count(2), 1)
+    self.assertEqual(count(2), cfun(2))
+    self.assertEqual(count(3), 3)
+    self.assertEqual(count(3), cfun(3))
+    self.assertEqual(count(4), 6)
+    self.assertEqual(count(4), cfun(4))
+
+  def testForiLoopTupleState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i))
+
+      init_val = (arr, 0.)
+      _, total = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun,
+                               init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopDictState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state['arr'], state['total']
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return {'arr': arr, 'total': lax.add(total, arr_i)}
+
+      init_val = {'arr': arr, 'total': 0.}
+      out_val = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return out_val['total']
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopEmptyTupleInState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total, _ = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i), ())
+
+      init_val = (arr, 0., ())
+      _, tot, _ = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return tot
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),
+                                   ((5, 3, 2), (5, 2, 4)),
+                                   ((1, 2, 2, 3), (1, 2, 3, 1))]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
+    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)
+
+  def testCollapse(self):
+
+    @api.jit
+    def collapse_first_two(x):
+      return lax.collapse(x, 0, 2)
+
+    self.assertEqual((6,), collapse_first_two(onp.zeros((2, 3))).shape)
+    self.assertEqual((6, 4), collapse_first_two(onp.zeros((2, 3, 4))).shape)
+    self.assertEqual((2, 3, 4),
+                     collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in all_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTake(self, shape, dtype, idxs, axes, rng):
+    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
+    fun = lambda src, idxs: lax.index_take(src, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+    ridxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(src_shape, dtype), rng(dst_shape, dtype), ridxs()]
+    fun = lambda src, dst, idxs: lax.index_untake(src, dst, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+
+GradTestSpec = collections.namedtuple(
+    ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])
+
+LAX_GRAD_OPS = [
+    GradTestSpec(lax.neg, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.floor, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.ceil, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.round, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.rem, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.exp, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.expm1, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log1p, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.tanh, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sin, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.cos, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+
+    GradTestSpec(lax.erf, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erfc, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erf_inv, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.lgamma, nargs=1, order=2, rng=jtu.rand_small(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.real, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    GradTestSpec(lax.imag, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    # GradTestSpec(lax.complex, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float32]),  # TODO(mattjj): enable
+    GradTestSpec(lax.conj, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float32, onp.complex64]),
+    GradTestSpec(lax.abs, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.pow, nargs=2, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.add, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sub, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.mul, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.div, nargs=2, order=1, rng=jtu.rand_not_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.max, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.min, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+]
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+def check_grads_bilinear(f, args, order, atol=None, rtol=None):
+  # Can use large eps to make up for numerical inaccuracies since the op is
+  # bilinear (relying on the fact that we only check one arg at a time)
+  lhs, rhs = args
+  check_grads(lambda lhs: f(lhs, rhs), (lhs,), order, atol, rtol, eps=1.)
+  check_grads(lambda rhs: f(lhs, rhs), (rhs,), order, atol, rtol, eps=1.)
+
+
+class LaxAutodiffTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""order"": rec.order}
+      for rec in LAX_GRAD_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes
+  )
+  def testOpGrad(self, op, rng, shapes, dtype, order):
+    if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
+      if dtype is onp.complex64:
+        return absltest.unittest.skip(""complex grads unimplemented on tpu"")
+      if op is lax.pow:
+        return absltest.unittest.skip(""pow grad imprecise on tpu"")
+    tol = 1e-1 if num_float_bits(dtype) == 32 else None
+    args = tuple(rng(shape, dtype) for shape in shapes)
+    check_grads(op, args, order, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.float64], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
+    args = (rng((2, 3), from_dtype),)
+    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
+    check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (), ()],
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [min_shape, operand_shape, max_shape]
+    min, operand, max = (rng(shape, dtype) for shape in shapes)
+    min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
+    check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in float_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    operands = tuple(rng(shape, dtype) for shape in shapes)
+    concatenate = lambda *args: lax.concatenate(args, dim)
+    check_grads(concatenate, operands, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""rng"": rng,}
+       for lhs_shape, rhs_shape, all_strides in itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)])])
+       for strides in all_strides
+       for dtype in [onp.float32]
+       for padding in [""VALID"", ""SAME""]
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv, window_strides=strides, padding=padding)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng}
+       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in
+       itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+             [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+             [(1, 1), (2, 1)], [(1, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)], [((1, 1),), ((0, 0),)],
+             [(1,), (2,)], [(1,), (2,)])])
+       for strides in all_strides
+       for rhs_dil in rhs_dils
+       for lhs_dil in lhs_dils
+       for dtype in [onp.float32]
+       for padding in all_pads
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                     padding, lhs_dil, rhs_dil, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv_with_general_padding, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}_dims={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil, "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
+       ""perms"": perms}
+      for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
+          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+          [(1, 1), (2, 1)], [(1, 1)])
+          for b, i, j in itertools.product([1, 2], repeat=3)]
+      for strides in all_strides
+      for rhs_dil in rhs_dils
+      for lhs_dil in lhs_dils
+      for dtype in [onp.float32]
+      for padding in all_pads
+      for rng in [jtu.rand_default()]
+      for dim_nums, perms in [
+          ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dil, rhs_dil, dimension_numbers,
+                                 perms, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-3
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+    lhs = onp.transpose(rng(lhs_shape, dtype), lhs_perm)
+    rhs = onp.transpose(rng(rhs_shape, dtype), rhs_perm)
+    conv = partial(lax.conv_general_dilated, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil,
+                   dimension_numbers=dimension_numbers)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
+      for dtype in float_dtypes)
+  def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
+    tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": jtu.rand_small()}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 2), (2, 4), (([1], [0]), ([], []))),
+          ((3, 5), (2, 5), (([1], [1]), ([], []))),
+          ((5, 3), (5, 2), (([0], [0]), ([], []))),
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+      ]
+      for dtype in float_dtypes)
+  @jtu.skip_on_devices(""tpu"")
+  def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
+                                          dimension_numbers, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-2
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
+    check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in float_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    args = (rng(shape, dtype),)
+    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
+    check_grads(broadcast, args, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(inshape, dtype)
+    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(arg_shape, dtype)
+    reshape = lambda x: lax.reshape(x, out_shape)
+    check_grads(reshape, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in float_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadGrad(self, shape, dtype, pads, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+
+    operand = rng(shape, dtype)
+    pad = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    check_grads(pad, (operand,), 2, tol, tol, tol)
+
+    operand = rng(shape, dtype)
+    padding_value = onp.array(0., dtype)
+    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)
+    check_grads(pad, (operand, padding_value), 2, tol, tol, tol)
+
+  def testReverseGrad(self):
+    rev = lambda operand: lax.rev(operand, dimensions)
+
+    dimensions = [0]
+    check_grads(rev, (onp.array([3., 2., 1.]),), 2)
+
+    dimensions = [0, 1]
+    check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    pred = rng(pred_shape, onp.bool_)
+    on_true = rng(arg_shape, dtype)
+    on_false = rng(arg_shape, dtype)
+    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
+    check_grads(select, (on_true, on_false), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    slice = lambda x: lax.slice(x, starts, limits, strides)
+    check_grads(slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
+                           rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
+    check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
+                                 update_shape, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    update = rng(update_shape, dtype)
+    start_indices = onp.array(start_indices)
+    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
+    check_grads(dus, (operand, update), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeGrad(self, shape, dtype, perm, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    transpose = lambda x: lax.transpose(x, perm)
+    check_grads(transpose, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, float_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.inf, lax.min, float_dtypes),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)],
+          [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)],
+          [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    init_val = onp.asarray(init_val, dtype=dtype)
+    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
+    check_grads(reduce, (operand,), 1, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes, rng in [
+          (0, lax.add, [onp.float32], jtu.rand_small()),
+          (-onp.inf, lax.max, [onp.float32], jtu.rand_default()),
+          (onp.inf, lax.min, [onp.float32], jtu.rand_default()),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_default()])
+  def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((6, 5, 4, 3), (2, 2, 1, 1), (1, 2, 1, 1))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],  # shapes
+              [(2, 1), (1, 2)],  # window_dimensions
+              [(1, 1), (2, 1), (1, 2)]  # strides
+          ),
+          itertools.product(
+              [(3, 2, 4, 6)],  # shapes
+              [(1, 1, 2, 1), (2, 1, 2, 1)],  # window_dimensions
+              [(1, 2, 2, 1), (1, 1, 1, 1)]),  # strides
+      )
+
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      operand = rng(shape, dtype)
+      if op is lax.add:
+        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
+      else:
+        # this test can fail if there are duplicates in operand
+        self.assertEqual(onp.unique(operand).size, operand.size,
+                         msg=""test requires operand elements to be unique."")
+        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
+                            1e-2, 1e-2, 1e-2)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32]
+      for shape in [(5,), (5, 7)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortGrad(self, shape, dtype, axis, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    sort = lambda x: lax.sort(x, axis)
+    check_grads(sort, (operand,), 2, tol, tol, tol)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32]
+      for val_dtype in [onp.float32]
+      for shape in [(3,), (5, 3)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+    keys, values = args_maker()
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in float_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(shape, dtype)
+    index_take = lambda src: lax.index_take(src, idxs, axes)
+    check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(src_shape, dtype)
+    dst = rng(dst_shape, dtype)
+    index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
+    check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/lax_test.py b/tests/lax_test.py
new file mode 100644
index 000000000..b8c78524c
--- /dev/null
+++ b/tests/lax_test.py
@@ -0,0 +1,1981 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import numpy.random as npr
+
+from jax import api
+from jax import core
+from jax import lax
+from jax import test_util as jtu
+from jax import lax_reference
+from jax.interpreters import xla
+from jax.lib import xla_bridge
+
+FLAGS = flags.FLAGS
+
+
+def num_float_bits(dtype):
+  return onp.finfo(xla_bridge.canonicalize_dtype(dtype)).bits
+
+
+### lax tests
+
+# For standard unops and binops, we can generate a large number of tests on
+# arguments of appropriate shapes and dtypes using the following table.
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + complex_dtypes + int_dtypes + bool_dtypes
+
+compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]
+
+OpRecord = collections.namedtuple(""OpRecord"",
+                                  [""op"", ""nargs"", ""dtypes"", ""rng"", ""tol""])
+
+
+def op_record(op, nargs, dtypes, rng, tol=1e-5):
+  return OpRecord(op, nargs, dtypes, rng, tol)
+
+LAX_OPS = [
+    op_record(lax.neg, 1, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sign, 1, default_dtypes, jtu.rand_small()),
+    op_record(lax.floor, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.ceil, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.round, 1, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.is_finite, 1, float_dtypes, jtu.rand_small()),
+
+    op_record(lax.exp, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.expm1, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.log, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.log1p, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tanh, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sin, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cos, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.atan2, 2, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.sqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.rsqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.square, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.reciprocal, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tan, 1, float_dtypes, jtu.rand_default()),
+    op_record(lax.asin, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.acos, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.atan, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.sinh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cosh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.asinh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.acosh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.lgamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.digamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.erf, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erfc, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erf_inv, 1, float_dtypes, jtu.rand_small(), tol=1e-2),
+
+    op_record(lax.real, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.imag, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.complex, 2, [onp.float32], jtu.rand_default()),
+    op_record(lax.conj, 1, [onp.float32] + complex_dtypes, jtu.rand_default()),
+    op_record(lax.abs, 1, default_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.pow, 2, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.bitwise_and, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_not, 1, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_or, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_xor, 2, bool_dtypes, jtu.rand_small()),
+
+    op_record(lax.add, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sub, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.mul, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.div, 2, default_dtypes + complex_dtypes, jtu.rand_nonzero()),
+    op_record(lax.rem, 2, default_dtypes, jtu.rand_nonzero()),
+
+    op_record(lax.max, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.min, 2, default_dtypes, jtu.rand_small()),
+
+    op_record(lax.eq, 2, all_dtypes, jtu.rand_some_equal()),
+    op_record(lax.ne, 2, all_dtypes, jtu.rand_small()),
+    op_record(lax.ge, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.gt, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.le, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.lt, 2, default_dtypes, jtu.rand_small()),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxTest(jtu.JaxTestCase):
+  """"""Numerical tests for LAX operations.""""""
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOp(self, op, rng, shapes, dtype):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""tol"": rec.tol}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    numpy_op = getattr(lax_reference, op.__name__)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker, tol=tol)
+
+  # TODO test shift_left, shift_right_arithmetic, shift_right_logical
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
+                            rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    numpy_op = lambda *args: lax_reference.concatenate(args, dim)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv(lhs, rhs, strides, padding)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
+                           rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)
+    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dilation, rhs_dilation, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
+      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
+      rhs_dilation, rng):
+    # TODO(mattjj): make this test pass
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    def numpy_fun(lhs, rhs):
+      return lax_reference.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CheckAgainstNumpy(fun, numpy_fun, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}""
+       ""_dims={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation,
+           "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""dimension_numbers"": dim_nums,
+       ""perms"": perms, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (2, 1)]
+      for padding in [((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2)], repeat=2)
+      for rng in [jtu.rand_small()]
+      for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
+                              ([0, 1, 2, 3], [0, 1, 2, 3])),
+                             ((""NHWC"", ""HWIO"", ""NHWC""),
+                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
+                             padding, lhs_dilation, rhs_dilation,
+                             dimension_numbers, perms, rng):
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+
+    def args_maker():
+      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),
+              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]
+
+    def fun(lhs, rhs):
+      return lax.conv_general_dilated(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,
+          dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test conv_general_dilated against numpy
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDot(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               lhs_contracting, rhs_contracting),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""lhs_contracting"": lhs_contracting, ""rhs_contracting"": rhs_contracting,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [
+          # these all fail with ""RuntimeError: Unimplemented: Dot with
+          # non-standard contracting dimensions not implemented.""
+          # [(3, 5), (2, 5), [1], [1]],
+          # [(5, 3), (5, 2), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],
+          # [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],
+          [(3, 2), (2, 4), [1], [0]],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
+                                 lhs_contracting, rhs_contracting, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
+                                     dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
+                                 dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)
+    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
+                                     dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    numpy_op = lambda x: lax_reference.reshape(x, out_shape)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPad(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadAgainstNumpy(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
+    numpy_op = lambda x: lax_reference.pad(x, onp.array(0, dtype), pads)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testReverse(self):
+    rev = api.jit(lambda operand: lax.rev(operand, dimensions))
+
+    dimensions = [0]
+    self.assertAllClose(onp.array([3, 2, 1]), rev(onp.array([1, 2, 3])),
+                        check_dtypes=False)
+
+    dimensions = [0, 1]
+    self.assertAllClose(onp.array([[6, 5, 4], [3, 2, 1]]),
+                        rev(onp.array([[1, 2, 3], [4, 5, 6]])),
+                        check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSlice(self, shape, dtype, starts, limits, strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
+                            strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                   size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)
+    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
+                          check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CheckAgainstNumpy(lax.dynamic_update_slice,
+                            lax_reference.dynamic_update_slice, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTranspose(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    numpy_op = lambda x: lax_reference.transpose(x, perm)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, default_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.iinfo(onp.int32).min, lax.max, [onp.int32]),
+          (onp.iinfo(onp.int64).min, lax.max, [onp.int64]),
+          (onp.iinfo(onp.uint32).min, lax.max, [onp.uint32]),
+          (onp.iinfo(onp.uint64).min, lax.max, [onp.uint64]),
+          (onp.inf, lax.min, float_dtypes),
+          (onp.iinfo(onp.int32).max, lax.min, [onp.int32]),
+          (onp.iinfo(onp.int64).max, lax.min, [onp.int64]),
+          (onp.iinfo(onp.uint32).max, lax.min, [onp.uint32]),
+          (onp.iinfo(onp.uint64).max, lax.min, [onp.uint64]),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduce(self, op, init_val, shape, dtype, dims, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype), init_val]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    fun = lambda operand: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, [onp.float32]),
+          (-onp.inf, lax.max, [onp.float32]),
+          (onp.inf, lax.min, [onp.float32]),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testReduceWindow(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((4, 6), (2, 1), (1, 2))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],
+              [(2, 1), (1, 2)],
+              [(1, 1), (2, 1), (1, 2)]),
+          itertools.product(
+              [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],
+              [(1, 2, 2, 1), (1, 1, 1, 1)]))
+
+    def fun(operand, init_val):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype), init_val]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype)]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSort(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: lax.sort(x, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortAgainstNumpy(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.sort(x, axis)
+    numpy_op = lambda x: lax_reference.sort(x, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)
+    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testWhileWithTuple(self):
+    limit = 10
+
+    def loop_cond(state):
+      pos, _ = state
+      return lax.lt(pos, limit)
+
+    def loop_body(state):
+      pos, count = state
+      return (lax.add(pos, 1), lax.add(count, 1))
+
+    def loop(init):
+      result = lax._while_loop(loop_cond, loop_body, (init, 0))
+      _, count = result
+      return count
+
+    cloop = api.jit(loop)
+
+    self.assertEqual(loop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(3), limit - 3)
+
+  def testNestedWhile(self):
+
+    def outer_loop(num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        num, i, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        num, i, count = state
+        return (num, lax.add(i, 1), inner_loop(i, count))
+
+      init_val = (num, 0, 0)
+      _, i, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return (i, count)
+
+    def inner_loop(i, count):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        i, j, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, count = state
+        return (i, lax.add(j, 1), lax.add(count, 1))
+
+      init_val = (i, 0, count)
+      _, _, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return count
+
+    cloop = api.jit(outer_loop)
+
+    self.assertEqual(outer_loop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(2), (2, 3))
+    self.assertEqual(cloop(4), (4, 10))
+
+  def testNestedWhileWithDynamicUpdateSlice(self):
+    num = 5
+
+    def update_entry(arr, val, i, j):
+      val = lax.reshape(val, [1, 1])
+      return lax.dynamic_update_slice(arr, val, (i, j))
+
+    def outer_loop(arr):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, num, _, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        i, num, arr, out = state
+        return (lax.add(i, 1), num, arr, inner_loop(i, arr, out))
+
+      out = onp.zeros(arr.shape, dtype=arr.dtype)
+      init_val = (0, num, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    def inner_loop(i, arr, out):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, j, _, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, arr, out = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        arr_i_j = lax.dynamic_index_in_dim(arr_i, j, 0, False)
+        out = update_entry(out, arr_i_j, i, j)
+        return (i, lax.add(j, 1), arr, out)
+
+      init_val = (i, 0, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    cloop = api.jit(outer_loop)
+    arr = npr.RandomState(0).randn(5, 5)
+    self.assertAllClose(outer_loop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+
+  def testLoopWithConjunctionCondition(self):
+    def sum_first_n(arr, num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        arr, num, i, _ = state
+        return lax.bitwise_and(lax.lt(i, num), lax.lt(i, arr.shape[0]))
+
+      def body_fun(state):
+        arr, num, i, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, num, lax.add(i, 1), lax.add(total, arr_i))
+
+      init_val = (arr, num, 0, 0.)
+      _, _, _, total = lax._while_loop(cond_fun, body_fun, init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopBasic(self):
+    def count(num):
+      def body_fun(i, tot):
+        return lax.add(tot, i)
+      return lax.fori_loop(0, num, body_fun, 0)
+
+    cfun = api.jit(count)
+
+    self.assertEqual(count(2), 1)
+    self.assertEqual(count(2), cfun(2))
+    self.assertEqual(count(3), 3)
+    self.assertEqual(count(3), cfun(3))
+    self.assertEqual(count(4), 6)
+    self.assertEqual(count(4), cfun(4))
+
+  def testForiLoopTupleState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i))
+
+      init_val = (arr, 0.)
+      _, total = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun,
+                               init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopDictState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state['arr'], state['total']
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return {'arr': arr, 'total': lax.add(total, arr_i)}
+
+      init_val = {'arr': arr, 'total': 0.}
+      out_val = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return out_val['total']
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopEmptyTupleInState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total, _ = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i), ())
+
+      init_val = (arr, 0., ())
+      _, tot, _ = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return tot
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),
+                                   ((5, 3, 2), (5, 2, 4)),
+                                   ((1, 2, 2, 3), (1, 2, 3, 1))]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
+    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)
+
+  def testCollapse(self):
+
+    @api.jit
+    def collapse_first_two(x):
+      return lax.collapse(x, 0, 2)
+
+    self.assertEqual((6,), collapse_first_two(onp.zeros((2, 3))).shape)
+    self.assertEqual((6, 4), collapse_first_two(onp.zeros((2, 3, 4))).shape)
+    self.assertEqual((2, 3, 4),
+                     collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in all_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTake(self, shape, dtype, idxs, axes, rng):
+    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
+    fun = lambda src, idxs: lax.index_take(src, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+    ridxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(src_shape, dtype), rng(dst_shape, dtype), ridxs()]
+    fun = lambda src, dst, idxs: lax.index_untake(src, dst, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+
+GradTestSpec = collections.namedtuple(
+    ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])
+
+LAX_GRAD_OPS = [
+    GradTestSpec(lax.neg, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.floor, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.ceil, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.round, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.rem, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.exp, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.expm1, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log1p, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.tanh, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sin, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.cos, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+
+    GradTestSpec(lax.erf, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erfc, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erf_inv, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.lgamma, nargs=1, order=2, rng=jtu.rand_small(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.real, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    GradTestSpec(lax.imag, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    # GradTestSpec(lax.complex, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float32]),  # TODO(mattjj): enable
+    GradTestSpec(lax.conj, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float32, onp.complex64]),
+    GradTestSpec(lax.abs, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.pow, nargs=2, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.add, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sub, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.mul, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.div, nargs=2, order=1, rng=jtu.rand_not_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.max, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.min, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+]
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+def check_grads_bilinear(f, args, order, atol=None, rtol=None):
+  # Can use large eps to make up for numerical inaccuracies since the op is
+  # bilinear (relying on the fact that we only check one arg at a time)
+  lhs, rhs = args
+  check_grads(lambda lhs: f(lhs, rhs), (lhs,), order, atol, rtol, eps=1.)
+  check_grads(lambda rhs: f(lhs, rhs), (rhs,), order, atol, rtol, eps=1.)
+
+
+class LaxAutodiffTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""order"": rec.order}
+      for rec in LAX_GRAD_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes
+  )
+  def testOpGrad(self, op, rng, shapes, dtype, order):
+    if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
+      if dtype is onp.complex64:
+        return absltest.unittest.skip(""complex grads unimplemented on tpu"")
+      if op is lax.pow:
+        return absltest.unittest.skip(""pow grad imprecise on tpu"")
+    tol = 1e-1 if num_float_bits(dtype) == 32 else None
+    args = tuple(rng(shape, dtype) for shape in shapes)
+    check_grads(op, args, order, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.float64], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
+    args = (rng((2, 3), from_dtype),)
+    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
+    check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (), ()],
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [min_shape, operand_shape, max_shape]
+    min, operand, max = (rng(shape, dtype) for shape in shapes)
+    min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
+    check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in float_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    operands = tuple(rng(shape, dtype) for shape in shapes)
+    concatenate = lambda *args: lax.concatenate(args, dim)
+    check_grads(concatenate, operands, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""rng"": rng,}
+       for lhs_shape, rhs_shape, all_strides in itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)])])
+       for strides in all_strides
+       for dtype in [onp.float32]
+       for padding in [""VALID"", ""SAME""]
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv, window_strides=strides, padding=padding)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng}
+       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in
+       itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+             [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+             [(1, 1), (2, 1)], [(1, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)], [((1, 1),), ((0, 0),)],
+             [(1,), (2,)], [(1,), (2,)])])
+       for strides in all_strides
+       for rhs_dil in rhs_dils
+       for lhs_dil in lhs_dils
+       for dtype in [onp.float32]
+       for padding in all_pads
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                     padding, lhs_dil, rhs_dil, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv_with_general_padding, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}_dims={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil, "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
+       ""perms"": perms}
+      for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
+          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+          [(1, 1), (2, 1)], [(1, 1)])
+          for b, i, j in itertools.product([1, 2], repeat=3)]
+      for strides in all_strides
+      for rhs_dil in rhs_dils
+      for lhs_dil in lhs_dils
+      for dtype in [onp.float32]
+      for padding in all_pads
+      for rng in [jtu.rand_default()]
+      for dim_nums, perms in [
+          ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dil, rhs_dil, dimension_numbers,
+                                 perms, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-3
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+    lhs = onp.transpose(rng(lhs_shape, dtype), lhs_perm)
+    rhs = onp.transpose(rng(rhs_shape, dtype), rhs_perm)
+    conv = partial(lax.conv_general_dilated, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil,
+                   dimension_numbers=dimension_numbers)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
+      for dtype in float_dtypes)
+  def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
+    tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": jtu.rand_small()}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 2), (2, 4), (([1], [0]), ([], []))),
+          ((3, 5), (2, 5), (([1], [1]), ([], []))),
+          ((5, 3), (5, 2), (([0], [0]), ([], []))),
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+      ]
+      for dtype in float_dtypes)
+  @jtu.skip_on_devices(""tpu"")
+  def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
+                                          dimension_numbers, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-2
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
+    check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in float_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    args = (rng(shape, dtype),)
+    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
+    check_grads(broadcast, args, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(inshape, dtype)
+    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(arg_shape, dtype)
+    reshape = lambda x: lax.reshape(x, out_shape)
+    check_grads(reshape, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in float_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadGrad(self, shape, dtype, pads, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+
+    operand = rng(shape, dtype)
+    pad = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    check_grads(pad, (operand,), 2, tol, tol, tol)
+
+    operand = rng(shape, dtype)
+    padding_value = onp.array(0., dtype)
+    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)
+    check_grads(pad, (operand, padding_value), 2, tol, tol, tol)
+
+  def testReverseGrad(self):
+    rev = lambda operand: lax.rev(operand, dimensions)
+
+    dimensions = [0]
+    check_grads(rev, (onp.array([3., 2., 1.]),), 2)
+
+    dimensions = [0, 1]
+    check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    pred = rng(pred_shape, onp.bool_)
+    on_true = rng(arg_shape, dtype)
+    on_false = rng(arg_shape, dtype)
+    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
+    check_grads(select, (on_true, on_false), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    slice = lambda x: lax.slice(x, starts, limits, strides)
+    check_grads(slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
+                           rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
+    check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
+                                 update_shape, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    update = rng(update_shape, dtype)
+    start_indices = onp.array(start_indices)
+    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
+    check_grads(dus, (operand, update), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeGrad(self, shape, dtype, perm, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    transpose = lambda x: lax.transpose(x, perm)
+    check_grads(transpose, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, float_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.inf, lax.min, float_dtypes),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)],
+          [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)],
+          [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    init_val = onp.asarray(init_val, dtype=dtype)
+    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
+    check_grads(reduce, (operand,), 1, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes, rng in [
+          (0, lax.add, [onp.float32], jtu.rand_small()),
+          (-onp.inf, lax.max, [onp.float32], jtu.rand_default()),
+          (onp.inf, lax.min, [onp.float32], jtu.rand_default()),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_default()])
+  def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((6, 5, 4, 3), (2, 2, 1, 1), (1, 2, 1, 1))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],  # shapes
+              [(2, 1), (1, 2)],  # window_dimensions
+              [(1, 1), (2, 1), (1, 2)]  # strides
+          ),
+          itertools.product(
+              [(3, 2, 4, 6)],  # shapes
+              [(1, 1, 2, 1), (2, 1, 2, 1)],  # window_dimensions
+              [(1, 2, 2, 1), (1, 1, 1, 1)]),  # strides
+      )
+
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      operand = rng(shape, dtype)
+      if op is lax.add:
+        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
+      else:
+        # this test can fail if there are duplicates in operand
+        self.assertEqual(onp.unique(operand).size, operand.size,
+                         msg=""test requires operand elements to be unique."")
+        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
+                            1e-2, 1e-2, 1e-2)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32]
+      for shape in [(5,), (5, 7)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortGrad(self, shape, dtype, axis, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    sort = lambda x: lax.sort(x, axis)
+    check_grads(sort, (operand,), 2, tol, tol, tol)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32]
+      for val_dtype in [onp.float32]
+      for shape in [(3,), (5, 3)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+    keys, values = args_maker()
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in float_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(shape, dtype)
+    index_take = lambda src: lax.index_take(src, idxs, axes)
+    check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(src_shape, dtype)
+    dst = rng(dst_shape, dtype)
+    index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
+    check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/minmax_test.py,tests/minmax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/minmax_test.py b/tests/minmax_test.py
new file mode 100644
index 000000000..63436c6fd
--- /dev/null
+++ b/tests/minmax_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the minmax optimizer module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+from absl.testing import absltest
+
+import jax.test_util as jtu
+import jax.numpy as np
+from jax.api import grad
+from jax.experimental import minmax
+from jax.lib import xla_bridge as xla
+
+
+class OptimizerTests(jtu.JaxTestCase):
+
+  def _CheckOptimizer(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    self._CheckFuns(optimizer, loss, x0, *args)
+    self._CheckRun(optimizer, loss, x0, num_steps, *args, **kwargs)
+
+  def _CheckFuns(self, optimizer, loss, x0, *args):
+    init_fun, update_fun = optimizer(*args)
+    opt_state = init_fun(x0)
+    update_fun(0, grad(loss)(x0, None), opt_state)  # doesn't crash
+
+  @jtu.skip_on_devices('gpu')
+  def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    return # TODO(mattjj): bring back fax!
+    num_repl = xla.get_replica_count()
+    infeeder = fax.make_infeed_from_sequence(
+        [np.ones(1, dtype='float32')] * num_steps * num_repl,
+        with_pyvals=True)
+
+    def op(infeed, x0):
+      opt_init, opt_update = optimizer(*args, **kwargs)
+      return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
+    cop = jit(op)
+
+    a1, _ = op(infeeder(), x0)
+    a2, _ = cop(infeeder(), x0)
+
+    assert loss(a1, None) < 1e-3
+    assert loss(a2, None) < 1e-3
+    self.assertAllClose(a1, a2, check_dtypes=False)
+
+  def testSgdScalar(self):
+    def loss(x, _): return x**2
+    x0 = 1.
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdNestedTuple(self):
+    def loss(xyz, _):
+      x, (y, z) = xyz
+      return sum(np.dot(a, a) for a in [x, y, z])
+    x0 = (np.ones(2), (np.ones(2), np.ones(2)))
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testMomentumVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    mass = 0.
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_size, mass)
+
+  def testRmspropVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_size)
+
+  @jtu.skip_on_devices('cpu')  # TODO(mattjj): investigate numerical failure
+  def testAdamVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_size)
+
+  def testSgdClosure(self):
+    def loss(y, x, _): return y**2 * x**2
+    x0 = 1.
+    y = 1.
+    num_iters = 20
+    step_size = 0.1
+    partial_loss = functools.partial(loss, y)
+    self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
new file mode 100644
index 000000000..63436c6fd
--- /dev/null
+++ b/tests/minmax_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the minmax optimizer module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+from absl.testing import absltest
+
+import jax.test_util as jtu
+import jax.numpy as np
+from jax.api import grad
+from jax.experimental import minmax
+from jax.lib import xla_bridge as xla
+
+
+class OptimizerTests(jtu.JaxTestCase):
+
+  def _CheckOptimizer(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    self._CheckFuns(optimizer, loss, x0, *args)
+    self._CheckRun(optimizer, loss, x0, num_steps, *args, **kwargs)
+
+  def _CheckFuns(self, optimizer, loss, x0, *args):
+    init_fun, update_fun = optimizer(*args)
+    opt_state = init_fun(x0)
+    update_fun(0, grad(loss)(x0, None), opt_state)  # doesn't crash
+
+  @jtu.skip_on_devices('gpu')
+  def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    return # TODO(mattjj): bring back fax!
+    num_repl = xla.get_replica_count()
+    infeeder = fax.make_infeed_from_sequence(
+        [np.ones(1, dtype='float32')] * num_steps * num_repl,
+        with_pyvals=True)
+
+    def op(infeed, x0):
+      opt_init, opt_update = optimizer(*args, **kwargs)
+      return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
+    cop = jit(op)
+
+    a1, _ = op(infeeder(), x0)
+    a2, _ = cop(infeeder(), x0)
+
+    assert loss(a1, None) < 1e-3
+    assert loss(a2, None) < 1e-3
+    self.assertAllClose(a1, a2, check_dtypes=False)
+
+  def testSgdScalar(self):
+    def loss(x, _): return x**2
+    x0 = 1.
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdNestedTuple(self):
+    def loss(xyz, _):
+      x, (y, z) = xyz
+      return sum(np.dot(a, a) for a in [x, y, z])
+    x0 = (np.ones(2), (np.ones(2), np.ones(2)))
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testMomentumVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    mass = 0.
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_size, mass)
+
+  def testRmspropVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_size)
+
+  @jtu.skip_on_devices('cpu')  # TODO(mattjj): investigate numerical failure
+  def testAdamVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_size)
+
+  def testSgdClosure(self):
+    def loss(y, x, _): return y**2 * x**2
+    x0 = 1.
+    y = 1.
+    num_iters = 20
+    step_size = 0.1
+    partial_loss = functools.partial(loss, y)
+    self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/random_test.py,tests/random_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/random_test.py b/tests/random_test.py
new file mode 100644
index 000000000..9d3ae7239
--- /dev/null
+++ b/tests/random_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.special
+import scipy.stats
+
+from jax import api
+from jax import lax
+from jax import random
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+
+class LaxRandomTest(jtu.JaxTestCase):
+
+  def _CheckCollisions(self, samples, nbits):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Chebyshev
+    nitems = len(samples)
+    nbins = 2 ** nbits
+    nexpected = nbins * (1 - ((nbins - 1) / nbins) ** nitems)
+    ncollisions = len(onp.unique(samples))
+    sq_percent_deviation = ((ncollisions - nexpected) / nexpected) ** 2
+    self.assertLess(sq_percent_deviation, 1 / onp.sqrt(nexpected * fail_prob))
+
+  def _CheckKolmogorovSmirnovCDF(self, samples, cdf):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Kolmo CDF
+    statistic = scipy.stats.kstest(samples, cdf).statistic
+    self.assertLess(1. - scipy.special.kolmogorov(statistic), fail_prob)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNumpyAndXLAAgreeOnFloatEndianness(self, dtype):
+    if not FLAGS.jax_enable_x64 and onp.issubdtype(dtype, onp.float64):
+      return absltest.unittest.skip(""can't test float64 agreement"")
+
+    bits_dtype = onp.uint32 if onp.finfo(dtype).bits == 32 else onp.uint64
+    numpy_bits = onp.array(1., dtype).view(bits_dtype)
+    xla_bits = api.jit(
+        lambda: lax.bitcast_convert_type(onp.array(1., dtype), bits_dtype))()
+    self.assertEqual(numpy_bits, xla_bits)
+
+  def testThreefry2x32(self):
+    # We test the hash by comparing to known values provided in the test code of
+    # the original reference implementation of Threefry. For the values, see
+    # https://github.com/DEShawResearch/Random123-Boost/blob/65e3d874b67aa7b3e02d5ad8306462f52d2079c0/libs/random/test/test_threefry.cpp#L30-L32
+    expected = (""0x6b200159L"", ""0x99ba4efeL"")
+    result = random.threefry_2x32(onp.uint32([0, 0]), onp.uint32([0, 0]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0x1cb996fcL"", ""0xbb002be7L"")
+    result = random.threefry_2x32(onp.uint32([-1, -1]), onp.uint32([-1, -1]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0xc4923a9cL"", ""0x483df7a0L"")
+    result = random.threefry_2x32(
+        onp.uint32([0x13198a2e, 0x03707344]),
+        onp.uint32([0x243f6a88, 0x85a308d3]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testRngUniform(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.uniform(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckCollisions(samples, onp.finfo(dtype).nmant)
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.uniform().cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.int32, onp.int64])
+  def testRngRandint(self, dtype):
+    lo = 5
+    hi = 10
+
+    key = random.PRNGKey(0)
+    rand = lambda key: random.randint(key, (10000,), lo, hi, dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self.assertTrue(onp.all(lo <= samples))
+      self.assertTrue(onp.all(samples < hi))
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNormal(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.normal(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
new file mode 100644
index 000000000..9d3ae7239
--- /dev/null
+++ b/tests/random_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.special
+import scipy.stats
+
+from jax import api
+from jax import lax
+from jax import random
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+
+class LaxRandomTest(jtu.JaxTestCase):
+
+  def _CheckCollisions(self, samples, nbits):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Chebyshev
+    nitems = len(samples)
+    nbins = 2 ** nbits
+    nexpected = nbins * (1 - ((nbins - 1) / nbins) ** nitems)
+    ncollisions = len(onp.unique(samples))
+    sq_percent_deviation = ((ncollisions - nexpected) / nexpected) ** 2
+    self.assertLess(sq_percent_deviation, 1 / onp.sqrt(nexpected * fail_prob))
+
+  def _CheckKolmogorovSmirnovCDF(self, samples, cdf):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Kolmo CDF
+    statistic = scipy.stats.kstest(samples, cdf).statistic
+    self.assertLess(1. - scipy.special.kolmogorov(statistic), fail_prob)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNumpyAndXLAAgreeOnFloatEndianness(self, dtype):
+    if not FLAGS.jax_enable_x64 and onp.issubdtype(dtype, onp.float64):
+      return absltest.unittest.skip(""can't test float64 agreement"")
+
+    bits_dtype = onp.uint32 if onp.finfo(dtype).bits == 32 else onp.uint64
+    numpy_bits = onp.array(1., dtype).view(bits_dtype)
+    xla_bits = api.jit(
+        lambda: lax.bitcast_convert_type(onp.array(1., dtype), bits_dtype))()
+    self.assertEqual(numpy_bits, xla_bits)
+
+  def testThreefry2x32(self):
+    # We test the hash by comparing to known values provided in the test code of
+    # the original reference implementation of Threefry. For the values, see
+    # https://github.com/DEShawResearch/Random123-Boost/blob/65e3d874b67aa7b3e02d5ad8306462f52d2079c0/libs/random/test/test_threefry.cpp#L30-L32
+    expected = (""0x6b200159L"", ""0x99ba4efeL"")
+    result = random.threefry_2x32(onp.uint32([0, 0]), onp.uint32([0, 0]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0x1cb996fcL"", ""0xbb002be7L"")
+    result = random.threefry_2x32(onp.uint32([-1, -1]), onp.uint32([-1, -1]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0xc4923a9cL"", ""0x483df7a0L"")
+    result = random.threefry_2x32(
+        onp.uint32([0x13198a2e, 0x03707344]),
+        onp.uint32([0x243f6a88, 0x85a308d3]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testRngUniform(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.uniform(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckCollisions(samples, onp.finfo(dtype).nmant)
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.uniform().cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.int32, onp.int64])
+  def testRngRandint(self, dtype):
+    lo = 5
+    hi = 10
+
+    key = random.PRNGKey(0)
+    rand = lambda key: random.randint(key, (10000,), lo, hi, dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self.assertTrue(onp.all(lo <= samples))
+      self.assertTrue(onp.all(samples < hi))
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNormal(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.normal(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/stax_test.py,tests/stax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/stax_test.py b/tests/stax_test.py
new file mode 100644
index 000000000..1689301f9
--- /dev/null
+++ b/tests/stax_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for Stax library.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import test_util as jtu
+from jax import random
+from jax.experimental import stax
+
+
+def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
+  result_shape, params = init_fun(input_shape)
+  inputs = onp.random.RandomState(0).randn(*input_shape).astype(""float32"")
+  rng_key = random.PRNGKey(0)
+  result = apply_fun(params, inputs, rng_key)
+  test_case.assertEqual(result.shape, result_shape)
+
+
+class StaxTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (5,)])
+  def testRandnInitShape(self, shape):
+    out = stax.randn()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (2, 3, 4)])
+  def testGlorotInitShape(self, shape):
+    out = stax.glorot()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_channels={}_filter_shape={}_padding={}_strides={}_input_shape={}""
+       .format(channels, filter_shape, padding, strides, input_shape),
+       ""channels"": channels, ""filter_shape"": filter_shape, ""padding"": padding,
+       ""strides"": strides, ""input_shape"": input_shape}
+      for channels in [2, 3]
+      for filter_shape in [(1, 1), (2, 3)]
+      for padding in [""SAME"", ""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 10, 11, 1)])
+  def testConvShape(self, channels, filter_shape, padding, strides,
+                    input_shape):
+    init_fun, apply_fun = stax.Conv(channels, filter_shape, strides=strides,
+                                    padding=padding)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_out_dim={}_input_shape={}""
+                        .format(out_dim, input_shape),
+       ""out_dim"": out_dim, ""input_shape"": input_shape}
+      for out_dim in [3, 4]
+      for input_shape in [(2, 3), (3, 4)])
+  def testDenseShape(self, out_dim, input_shape):
+    init_fun, apply_fun = stax.Dense(out_dim)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testReluShape(self, input_shape):
+    init_fun, apply_fun = stax.Relu
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
+                        .format(window_shape, padding, strides, input_shape),
+       ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
+       ""input_shape"": input_shape}
+      for window_shape in [(1, 1), (2, 3)]
+      for padding in [""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 5, 6, 1)])
+  def testPoolingShape(self, window_shape, padding, strides, input_shape):
+    init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
+                                       strides=strides)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testFlattenShape(self, input_shape):
+    init_fun, apply_fun = stax.Flatten
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}_spec={}"".format(input_shape, i),
+       ""input_shape"": input_shape, ""spec"": spec}
+      for input_shape in [(2, 5, 6, 1)]
+      for i, spec in enumerate([
+          [stax.Conv(3, (2, 2))],
+          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]]))
+  def testSerialComposeLayersShape(self, input_shape, spec):
+    init_fun, apply_fun = stax.serial(*spec)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(3, 4), (2, 5, 6, 1)])
+  def testDropoutShape(self, input_shape):
+    init_fun, apply_fun = stax.Dropout(0.9)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/stax_test.py b/tests/stax_test.py
new file mode 100644
index 000000000..1689301f9
--- /dev/null
+++ b/tests/stax_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for Stax library.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import test_util as jtu
+from jax import random
+from jax.experimental import stax
+
+
+def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
+  result_shape, params = init_fun(input_shape)
+  inputs = onp.random.RandomState(0).randn(*input_shape).astype(""float32"")
+  rng_key = random.PRNGKey(0)
+  result = apply_fun(params, inputs, rng_key)
+  test_case.assertEqual(result.shape, result_shape)
+
+
+class StaxTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (5,)])
+  def testRandnInitShape(self, shape):
+    out = stax.randn()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (2, 3, 4)])
+  def testGlorotInitShape(self, shape):
+    out = stax.glorot()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_channels={}_filter_shape={}_padding={}_strides={}_input_shape={}""
+       .format(channels, filter_shape, padding, strides, input_shape),
+       ""channels"": channels, ""filter_shape"": filter_shape, ""padding"": padding,
+       ""strides"": strides, ""input_shape"": input_shape}
+      for channels in [2, 3]
+      for filter_shape in [(1, 1), (2, 3)]
+      for padding in [""SAME"", ""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 10, 11, 1)])
+  def testConvShape(self, channels, filter_shape, padding, strides,
+                    input_shape):
+    init_fun, apply_fun = stax.Conv(channels, filter_shape, strides=strides,
+                                    padding=padding)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_out_dim={}_input_shape={}""
+                        .format(out_dim, input_shape),
+       ""out_dim"": out_dim, ""input_shape"": input_shape}
+      for out_dim in [3, 4]
+      for input_shape in [(2, 3), (3, 4)])
+  def testDenseShape(self, out_dim, input_shape):
+    init_fun, apply_fun = stax.Dense(out_dim)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testReluShape(self, input_shape):
+    init_fun, apply_fun = stax.Relu
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
+                        .format(window_shape, padding, strides, input_shape),
+       ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
+       ""input_shape"": input_shape}
+      for window_shape in [(1, 1), (2, 3)]
+      for padding in [""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 5, 6, 1)])
+  def testPoolingShape(self, window_shape, padding, strides, input_shape):
+    init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
+                                       strides=strides)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testFlattenShape(self, input_shape):
+    init_fun, apply_fun = stax.Flatten
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}_spec={}"".format(input_shape, i),
+       ""input_shape"": input_shape, ""spec"": spec}
+      for input_shape in [(2, 5, 6, 1)]
+      for i, spec in enumerate([
+          [stax.Conv(3, (2, 2))],
+          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]]))
+  def testSerialComposeLayersShape(self, input_shape, spec):
+    init_fun, apply_fun = stax.serial(*spec)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(3, 4), (2, 5, 6, 1)])
+  def testDropoutShape(self, input_shape):
+    init_fun, apply_fun = stax.Dropout(0.9)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
.gitignore,.gitignore,1b836515c2ce7f529e615b89302186018ad406c8,9584c4f55a5f40b470ed5505ef9b5b8da8ee3a12,update gitignore,"diff --git a/.gitignore b/.gitignore
index a1b5719c3..944358884 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,3 +3,4 @@
 /jax/lib/pywrap_xla.py
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
+jax.egg-info","diff --git a/.gitignore b/.gitignore
index a1b5719c3..944358884 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,3 +3,4 @@
 /jax/lib/pywrap_xla.py
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
+jax.egg-info",No
setup.py,setup.py,0dfa736ece742445721d3e031964ffb6695b933a,1b836515c2ce7f529e615b89302186018ad406c8,make setup.py copy over .so files,"diff --git a/setup.py b/setup.py
index 7fa4bdef2..54b9e9c46 100644
--- a/setup.py
+++ b/setup.py
@@ -1,4 +1,5 @@
 from setuptools import setup
+from glob import glob
 
 setup(
     name='jax',
@@ -11,4 +12,5 @@ setup(
     install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
+    package_data={'jax.lib': glob('jax/lib/*.so')},
 )","diff --git a/setup.py b/setup.py
index 7fa4bdef2..54b9e9c46 100644
--- a/setup.py
+++ b/setup.py
@@ -1,4 +1,5 @@
 from setuptools import setup
+from glob import glob
 
 setup(
     name='jax',
@@ -11,4 +12,5 @@ setup(
     install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
+    package_data={'jax.lib': glob('jax/lib/*.so')},
 )",No
jax/LICENSE,LICENSE,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/LICENSE b/LICENSE
new file mode 100644
index 000000000..d64569567
--- /dev/null
+++ b/LICENSE
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      ""License"" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      ""Licensor"" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      ""Legal Entity"" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      ""control"" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      ""You"" (or ""Your"") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      ""Source"" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      ""Object"" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      ""Work"" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      ""Derivative Works"" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      ""Contribution"" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, ""submitted""
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as ""Not a Contribution.""
+
+      ""Contributor"" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a ""NOTICE"" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an ""AS IS"" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets ""[]""
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same ""printed page"" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the ""License"");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an ""AS IS"" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.","diff --git a/LICENSE b/LICENSE
new file mode 100644
index 000000000..d64569567
--- /dev/null
+++ b/LICENSE
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      ""License"" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      ""Licensor"" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      ""Legal Entity"" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      ""control"" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      ""You"" (or ""Your"") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      ""Source"" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      ""Object"" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      ""Work"" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      ""Derivative Works"" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      ""Contribution"" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, ""submitted""
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as ""Not a Contribution.""
+
+      ""Contributor"" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a ""NOTICE"" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an ""AS IS"" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets ""[]""
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same ""printed page"" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the ""License"");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an ""AS IS"" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.",No
jax/LICENSE_SHORT,LICENSE_SHORT,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/LICENSE_SHORT b/LICENSE_SHORT
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/LICENSE_SHORT
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.","diff --git a/LICENSE_SHORT b/LICENSE_SHORT
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/LICENSE_SHORT
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.",No
jax/BUILD,jax/BUILD,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/jax/BUILD b/jax/BUILD
index 0b3567534..c5155e6e7 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,14 +1,4 @@
 # JAX is Autograd and XLA
-package(default_visibility = [""//visibility:public""])
-
-licenses([""notice""])  # Apache 2
-
-exports_files([""LICENSE""])
-
-load("":build_defs.bzl"", ""jax_test"")
-
-package_group(name = ""jax"")
-
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -29,128 +19,20 @@ py_library(
     ],
 )
 
-jax_test(
-    name = ""core_test"",
-    srcs = [""tests/core_test.py""],
-    shard_count = {
-        ""cpu"": 5,
-    },
-)
-
-jax_test(
-    name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""random_test"",
-    srcs = [""tests/random_test.py""],
-)
-
-jax_test(
-    name = ""api_test"",
-    srcs = [""tests/api_test.py""],
-)
-
-jax_test(
-    name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
-)
-
-py_binary(
-    name = ""interactive"",
-    srcs = [""examples/interactive.py""],
-    deps = ["":libjax""],
-)
-
 py_library(
     name = ""stax"",
     srcs = [""experimental/stax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
-)
-
 py_library(
     name = ""minmax"",
     srcs = [""experimental/minmax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
-)
-
 py_library(
     name = ""lapax"",
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
-
-jax_test(
-    name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
-)
-
-py_library(
-    name = ""datasets"",
-    srcs = [""examples/datasets.py""],
-)
-
-py_binary(
-    name = ""mnist_classifier"",
-    srcs = [""examples/mnist_classifier.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-    ],
-)
-
-py_binary(
-    name = ""mnist_vae"",
-    srcs = [""examples/mnist_vae.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-        "":minmax"",
-        "":stax"",
-    ],
-)","diff --git a/jax/BUILD b/jax/BUILD
index 0b3567534..c5155e6e7 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,14 +1,4 @@
 # JAX is Autograd and XLA
-package(default_visibility = [""//visibility:public""])
-
-licenses([""notice""])  # Apache 2
-
-exports_files([""LICENSE""])
-
-load("":build_defs.bzl"", ""jax_test"")
-
-package_group(name = ""jax"")
-
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -29,128 +19,20 @@ py_library(
     ],
 )
 
-jax_test(
-    name = ""core_test"",
-    srcs = [""tests/core_test.py""],
-    shard_count = {
-        ""cpu"": 5,
-    },
-)
-
-jax_test(
-    name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""random_test"",
-    srcs = [""tests/random_test.py""],
-)
-
-jax_test(
-    name = ""api_test"",
-    srcs = [""tests/api_test.py""],
-)
-
-jax_test(
-    name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
-)
-
-py_binary(
-    name = ""interactive"",
-    srcs = [""examples/interactive.py""],
-    deps = ["":libjax""],
-)
-
 py_library(
     name = ""stax"",
     srcs = [""experimental/stax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
-)
-
 py_library(
     name = ""minmax"",
     srcs = [""experimental/minmax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
-)
-
 py_library(
     name = ""lapax"",
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
-
-jax_test(
-    name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
-)
-
-py_library(
-    name = ""datasets"",
-    srcs = [""examples/datasets.py""],
-)
-
-py_binary(
-    name = ""mnist_classifier"",
-    srcs = [""examples/mnist_classifier.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-    ],
-)
-
-py_binary(
-    name = ""mnist_vae"",
-    srcs = [""examples/mnist_vae.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-        "":minmax"",
-        "":stax"",
-    ],
-)",No
jax/build_defs.bzl,tests/build_defs.bzl,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
new file mode 100644
index 000000000..1e1052687
--- /dev/null
+++ b/tests/build_defs.bzl
@@ -0,0 +1,77 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Helpers for defining multi-platform JAX binary/test targets.""""""
+
+def jax_test(
+        name,
+        srcs,
+        deps = [],
+        args = [],
+        disable = [],
+        data = [],
+        main = None,
+        shard_count = None):
+    """"""Defines a set of per-platform JAX test targets.""""""
+    if main == None:
+        if len(srcs) == 1:
+            main = srcs[0]
+        else:
+            fail(""Only one test source file is currently supported."")
+
+    # Deps that are linked into all test target variants.
+    all_test_deps = [
+        "":libjax"",
+    ]
+    disabled_tags = [""manual"", ""notap"", ""disabled""]
+    native.py_test(
+        name = name + ""_cpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_cpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_gpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )
+    native.py_test(
+        name = name + ""_gpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
new file mode 100644
index 000000000..1e1052687
--- /dev/null
+++ b/tests/build_defs.bzl
@@ -0,0 +1,77 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Helpers for defining multi-platform JAX binary/test targets.""""""
+
+def jax_test(
+        name,
+        srcs,
+        deps = [],
+        args = [],
+        disable = [],
+        data = [],
+        main = None,
+        shard_count = None):
+    """"""Defines a set of per-platform JAX test targets.""""""
+    if main == None:
+        if len(srcs) == 1:
+            main = srcs[0]
+        else:
+            fail(""Only one test source file is currently supported."")
+
+    # Deps that are linked into all test target variants.
+    all_test_deps = [
+        "":libjax"",
+    ]
+    disabled_tags = [""manual"", ""notap"", ""disabled""]
+    native.py_test(
+        name = name + ""_cpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_cpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_gpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )
+    native.py_test(
+        name = name + ""_gpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )",No
build/build_jax.sh,build/build_jax.sh,c03e5e80c5dab33907ef9594543810073f51f492,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,tweak build script,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index 708f78e16..776c972a1 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -65,14 +65,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 708f78e16..776c972a1 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -65,14 +65,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/",No
build/WORKSPACE,WORKSPACE,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,c03e5e80c5dab33907ef9594543810073f51f492,"add dummy binary build target, move WORKSPACE up","diff --git a/WORKSPACE b/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/WORKSPACE b/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
build/build_jax.sh,build/build_jax.sh,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,c03e5e80c5dab33907ef9594543810073f51f492,"add dummy binary build target, move WORKSPACE up","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 776c972a1..dbab5d69c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -8,8 +8,6 @@ then
   exit 1
 fi
 
-cp build/WORKSPACE .
-
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -28,7 +26,10 @@ export PATH=""${bazel_dir}/bin:$PATH""
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla with gpu support
-git clone https://github.com/tensorflow/tensorflow.git
+if [[ ! -d tensorflow ]]
+then
+  git clone https://github.com/tensorflow/tensorflow.git
+fi
 pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
@@ -65,14 +66,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
@@ -84,6 +85,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
-rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 776c972a1..dbab5d69c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -8,8 +8,6 @@ then
   exit 1
 fi
 
-cp build/WORKSPACE .
-
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -28,7 +26,10 @@ export PATH=""${bazel_dir}/bin:$PATH""
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla with gpu support
-git clone https://github.com/tensorflow/tensorflow.git
+if [[ ! -d tensorflow ]]
+then
+  git clone https://github.com/tensorflow/tensorflow.git
+fi
 pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
@@ -65,14 +66,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
@@ -84,6 +85,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
-rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}",No
jax/BUILD,jax/BUILD,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,c03e5e80c5dab33907ef9594543810073f51f492,"add dummy binary build target, move WORKSPACE up","diff --git a/jax/BUILD b/jax/BUILD
index c5155e6e7..310729498 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,6 @@
 # JAX is Autograd and XLA
+package(default_visibility = [""//visibility:public""])
+
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -36,3 +38,10 @@ py_library(
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
+
+# this is a dummy target for building purposes
+py_binary(
+    name = ""build_jax"",
+    srcs = [""core.py""],
+    deps = ["":libjax""],
+)","diff --git a/jax/BUILD b/jax/BUILD
index c5155e6e7..310729498 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,6 @@
 # JAX is Autograd and XLA
+package(default_visibility = [""//visibility:public""])
+
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -36,3 +38,10 @@ py_library(
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
+
+# this is a dummy target for building purposes
+py_binary(
+    name = ""build_jax"",
+    srcs = [""core.py""],
+    deps = ["":libjax""],
+)",No
jax/BUILD,jax/BUILD,ae641fdaeca7a12f98e9554faddd92580ed09a3e,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,add 'main' to build_jax dummy py_binary target,"diff --git a/jax/BUILD b/jax/BUILD
index 310729498..d03b01984 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -42,6 +42,7 @@ py_library(
 # this is a dummy target for building purposes
 py_binary(
     name = ""build_jax"",
-    srcs = [""core.py""],
+    srcs = [""__init__.py""],
+    main = ""__init__.py"",
     deps = ["":libjax""],
 )","diff --git a/jax/BUILD b/jax/BUILD
index 310729498..d03b01984 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -42,6 +42,7 @@ py_library(
 # this is a dummy target for building purposes
 py_binary(
     name = ""build_jax"",
-    srcs = [""core.py""],
+    srcs = [""__init__.py""],
+    main = ""__init__.py"",
     deps = ["":libjax""],
 )",No
examples/interactive.py,examples/interactive.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/examples/interactive.py b/examples/interactive.py
index e0c061aed..ca111b703 100644
--- a/examples/interactive.py
+++ b/examples/interactive.py
@@ -20,9 +20,11 @@ from absl import app
 import IPython
 import numpy as onp
 
+import jax
+import jax.numpy as np
 from jax import lax
-from jax import numpy as np
-from jax import jit, grad, vmap
+from jax import random
+from jax import jit, grad, vmap, jacfwd, jacrev, hessian
 
 
 def main(unused_argv):","diff --git a/examples/interactive.py b/examples/interactive.py
index e0c061aed..ca111b703 100644
--- a/examples/interactive.py
+++ b/examples/interactive.py
@@ -20,9 +20,11 @@ from absl import app
 import IPython
 import numpy as onp
 
+import jax
+import jax.numpy as np
 from jax import lax
-from jax import numpy as np
-from jax import jit, grad, vmap
+from jax import random
+from jax import jit, grad, vmap, jacfwd, jacrev, hessian
 
 
 def main(unused_argv):",No
examples/mnist_classifier.py,examples/mnist_classifier.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 58b660024..c97b18e15 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -12,9 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-""""""A basic MNIST example using Numpy and JAX.
-
-The primary aim here is simplicity and minimal dependencies.
+""""""A basic MNIST example using JAX together with the mini-libraries stax, for
+neural network building, and minmax, for first-order stochastic optimization.
 """"""
 
 from __future__ import absolute_import
@@ -22,26 +21,19 @@ from __future__ import division
 from __future__ import print_function
 
 import time
+import itertools
 
 from absl import app
 import numpy.random as npr
 
-from jax.api import jit, grad
-from jax.examples import datasets
-from jax.scipy.misc import logsumexp
 import jax.numpy as np
+from jax import jit, grad
+from jax.experimental import minmax
+from jax.examples import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
-def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
-  return [(scale * rng.randn(m, n), scale * rng.randn(n))
-          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
-
-def predict(params, inputs):
-  for w, b in params:
-    outputs = np.dot(inputs, w) + b
-    inputs = np.tanh(outputs)
-  return outputs - logsumexp(outputs, axis=1, keepdims=True)
-
 def loss(params, batch):
   inputs, targets = batch
   preds = predict(params, inputs)
@@ -53,13 +45,16 @@ def accuracy(params, batch):
   predicted_class = np.argmax(predict(params, inputs), axis=1)
   return np.mean(predicted_class == target_class)
 
+init_random_params, predict = stax.serial(
+    Dense(1024), Relu,
+    Dense(1024), Relu,
+    Dense(10), Softmax)
 
 def main(unused_argv):
-  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
-  param_scale = 0.1
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
+  momentum_mass = 0.9
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()
   num_train = train_images.shape[0]
@@ -75,19 +70,24 @@ def main(unused_argv):
         yield train_images[batch_idx], train_labels[batch_idx]
   batches = data_stream()
 
+  opt_init, opt_update = minmax.momentum(step_size, mass=momentum_mass)
+
   @jit
-  def update(params, batch):
-    grads = grad(loss)(params, batch)
-    return [(w - step_size * dw, b - step_size * db)
-            for (w, b), (dw, db) in zip(params, grads)]
+  def update(i, opt_state, batch):
+    params = minmax.get_params(opt_state)
+    return opt_update(i, grad(loss)(params, batch), opt_state)
+
+  _, init_params = init_random_params((-1, 28 * 28))
+  opt_state = opt_init(init_params)
+  itercount = itertools.count()
 
-  params = init_random_params(param_scale, layer_sizes)
   for epoch in range(num_epochs):
     start_time = time.time()
     for _ in range(num_batches):
-      params = update(params, next(batches))
+      opt_state = update(next(itercount), opt_state, next(batches))
     epoch_time = time.time() - start_time
 
+    params = minmax.get_params(opt_state)
     train_acc = accuracy(params, (train_images, train_labels))
     test_acc = accuracy(params, (test_images, test_labels))
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 58b660024..c97b18e15 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -12,9 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-""""""A basic MNIST example using Numpy and JAX.
-
-The primary aim here is simplicity and minimal dependencies.
+""""""A basic MNIST example using JAX together with the mini-libraries stax, for
+neural network building, and minmax, for first-order stochastic optimization.
 """"""
 
 from __future__ import absolute_import
@@ -22,26 +21,19 @@ from __future__ import division
 from __future__ import print_function
 
 import time
+import itertools
 
 from absl import app
 import numpy.random as npr
 
-from jax.api import jit, grad
-from jax.examples import datasets
-from jax.scipy.misc import logsumexp
 import jax.numpy as np
+from jax import jit, grad
+from jax.experimental import minmax
+from jax.examples import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
-def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
-  return [(scale * rng.randn(m, n), scale * rng.randn(n))
-          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
-
-def predict(params, inputs):
-  for w, b in params:
-    outputs = np.dot(inputs, w) + b
-    inputs = np.tanh(outputs)
-  return outputs - logsumexp(outputs, axis=1, keepdims=True)
-
 def loss(params, batch):
   inputs, targets = batch
   preds = predict(params, inputs)
@@ -53,13 +45,16 @@ def accuracy(params, batch):
   predicted_class = np.argmax(predict(params, inputs), axis=1)
   return np.mean(predicted_class == target_class)
 
+init_random_params, predict = stax.serial(
+    Dense(1024), Relu,
+    Dense(1024), Relu,
+    Dense(10), Softmax)
 
 def main(unused_argv):
-  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
-  param_scale = 0.1
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
+  momentum_mass = 0.9
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()
   num_train = train_images.shape[0]
@@ -75,19 +70,24 @@ def main(unused_argv):
         yield train_images[batch_idx], train_labels[batch_idx]
   batches = data_stream()
 
-  @jit
-  def update(params, batch):
-    grads = grad(loss)(params, batch)
-    return [(w - step_size * dw, b - step_size * db)
-            for (w, b), (dw, db) in zip(params, grads)]
+  opt_init, opt_update = minmax.momentum(step_size, mass=momentum_mass)
+
+  @jit
+  def update(i, opt_state, batch):
+    params = minmax.get_params(opt_state)
+    return opt_update(i, grad(loss)(params, batch), opt_state)
+
+  _, init_params = init_random_params((-1, 28 * 28))
+  opt_state = opt_init(init_params)
+  itercount = itertools.count()
 
-  params = init_random_params(param_scale, layer_sizes)
   for epoch in range(num_epochs):
     start_time = time.time()
     for _ in range(num_batches):
-      params = update(params, next(batches))
+      opt_state = update(next(itercount), opt_state, next(batches))
     epoch_time = time.time() - start_time
 
+    params = minmax.get_params(opt_state)
     train_acc = accuracy(params, (train_images, train_labels))
     test_acc = accuracy(params, (test_images, test_labels))
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))",Yes
examples/mnist_vae.py,examples/mnist_vae.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 163e59b38..7e9f719bb 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -28,13 +28,12 @@ import time
 from absl import app
 import matplotlib.pyplot as plt
 
-from jax import lax, random
-from jax.api import jit, grad
+import jax.numpy as np
+from jax import jit, grad, lax, random
 from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import jax.numpy as np
 
 
 def gaussian_kl(mu, sigmasq):","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 163e59b38..7e9f719bb 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -28,13 +28,12 @@ import time
 from absl import app
 import matplotlib.pyplot as plt
 
-from jax import lax, random
-from jax.api import jit, grad
+import jax.numpy as np
+from jax import jit, grad, lax, random
 from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import jax.numpy as np
 
 
 def gaussian_kl(mu, sigmasq):",No
jax/core.py,jax/core.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/core.py b/jax/core.py
index 9526bdd77..2d6081827 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -428,9 +428,9 @@ pytype_aval_mappings = {}
 # ------------------- Products -------------------
 
 class JaxTuple(tuple):
-  def __new__(self, xs):
-    assert all(map(valid_jaxtype, xs)), xs
-    return tuple.__new__(JaxTuple, xs)
+  def __new__(cls, xs):
+    assert skip_checks or all(map(valid_jaxtype, xs)), xs
+    return tuple.__new__(cls, xs)
 
   def __repr__(self):
     if self is unit:","diff --git a/jax/core.py b/jax/core.py
index 9526bdd77..2d6081827 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -428,9 +428,9 @@ pytype_aval_mappings = {}
 # ------------------- Products -------------------
 
 class JaxTuple(tuple):
-  def __new__(self, xs):
-    assert all(map(valid_jaxtype, xs)), xs
-    return tuple.__new__(JaxTuple, xs)
+  def __new__(cls, xs):
+    assert skip_checks or all(map(valid_jaxtype, xs)), xs
+    return tuple.__new__(cls, xs)
 
   def __repr__(self):
     if self is unit:",No
jax/experimental/minmax.py,jax/experimental/minmax.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index e24662325..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -23,21 +23,28 @@ from __future__ import division
 from __future__ import print_function
 
 import operator
+import functools
 
+import jax.numpy as np
 from jax.core import pack
 from jax.tree_util import tree_map, tree_multimap
-import jax.numpy as np
 
 
 def optimizer(opt_maker):
   """"""Decorator to make an optimizer map over tuple/list/dict containers.""""""
+  @functools.wraps(opt_maker)
   def tree_opt_maker(*args, **kwargs):
     init_fun, update_fun = opt_maker(*args, **kwargs)
+
+    @functools.wraps(init_fun)
     def fmapped_init_fun(x0_tree):
       return tree_map(lambda x0: pack(init_fun(x0)), x0_tree)
+
+    @functools.wraps(update_fun)
     def fmapped_update_fun(i, grad_tree, state_tree):
       update = lambda g, state: pack(update_fun(i, g, *state))
       return tree_multimap(update, grad_tree, state_tree)
+
     return fmapped_init_fun, fmapped_update_fun
   return tree_opt_maker
 
@@ -46,42 +53,86 @@ def iterate(state_tree):
   return tree_map(lambda state: tuple(state)[0], state_tree)
 get_params = iterate
 
+# optimizers
+
 @optimizer
 def sgd(step_size):
-  """"""Init and update step functions for stochastic gradient descent.""""""
+  """"""Construct init and update step functions for stochastic gradient descent.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     return (x0,)
   def update_fun(i, g, x):
-    return (x - step_size * g,)
+    return (x - step_size(i) * g,)
   return init_fun, update_fun
 
 @optimizer
 def momentum(step_size, mass):
-  """"""Init and update step functions for SGD with Nesterov momentum.""""""
+  """"""Construct init and update step functions for SGD with Nesterov momentum.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     v0 = np.zeros_like(x0)
     return x0, v0
   def update_fun(i, g, x, velocity):
     velocity = mass * velocity - (1. - mass) * g
-    x = x + step_size * velocity
+    x = x + step_size(i) * velocity
     return x, velocity
   return init_fun, update_fun
 
 @optimizer
 def rmsprop(step_size, gamma=0.9, eps=1e-8):
-  """"""Init and update step functions for RMSProp.""""""
+  """"""Construct init and update step functions for RMSProp.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     avg_sq_grad = np.ones_like(x0)
     return x0, avg_sq_grad
   def update_fun(i, g, x, avg_sq_grad):
     avg_sq_grad = avg_sq_grad * gamma + g**2 * (1. - gamma)
-    x = x - step_size * g / (np.sqrt(avg_sq_grad) + eps)
+    x = x - step_size(i) * g / (np.sqrt(avg_sq_grad) + eps)
     return x, avg_sq_grad
   return init_fun, update_fun
 
 @optimizer
 def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
-  """"""Init and update step functions for Adam.""""""
+  """"""Construct init and update step functions for Adam.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+    b1: optional, a positive scalar value for beta_1, the exponential decay rate
+      for the first moment estimates (default 0.9).
+    b2: optional, a positive scalar value for beta_2, the exponential decay rate
+      for the second moment estimates (default 0.999).
+    eps: optional, a positive scalar value for epsilon, a small constant for
+      numerical stability (default 1e-8).
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     m0 = np.zeros_like(x0)
     v0 = np.zeros_like(x0)
@@ -91,26 +142,47 @@ def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
     v = (1 - b2) * (g ** 2) + b2 * v  # Second moment estimate.
     mhat = m / (1 - b1 ** (i + 1))  # Bias correction.
     vhat = v / (1 - b2 ** (i + 1))
-    x = x - step_size*mhat / (np.sqrt(vhat) + eps)
+    x = x - step_size(i) * mhat / (np.sqrt(vhat) + eps)
     return x, m, v
   return init_fun, update_fun
 
-def run_optimizer(loss, infeed, update_fun, state):
-  """"""A convenience function for running optimizers with iterated map-reduce.
+# learning rate schedules
 
-  Args:
-    loss: a scalar-valued loss function taking two aguments, the current iterate
-      and a data value.
-    infeed: an infeed instance supplying the data stream.
-    update_fun: a function that has signature update_fun(i, grad, state) where
-      i is the integer iteration count, grad is the gradient of the loss at the
-      current iterate, and state is the current optimizer state.
-    state: the initial optimizer state.
+def constant(step_size):
+  def schedule(i):
+    return step_size
+  return schedule
 
-  Returns:
-    A pair (x, state) where is the final iterate and state represents the final
-    optimizer state.
-  """"""
-  map_fun = lambda _, state, batch: grad(loss)(iterate(state), batch)
-  state = fax.iterated_map_reduce(state, map_fun, update_fun, infeed)
-  return iterate(state), state
+def exponential_decay(step_size, decay_steps, decay_rate):
+  def schedule(i):
+    return step_size * decay_rate ** (i / decay_steps)
+  return schedule
+
+def inverse_time_decay(step_size, decay_steps, decay_rate, staircase=False):
+  if staircase:
+    def schedule(i):
+      return step_size / (1 + decay_rate * np.floor(i / decay_steps))
+  else:
+    def schedule(i):
+      return step_size / (1 + decay_rate * i / decay_steps)
+  return schedule
+
+def piecewise_constant(boundaries, values):
+  boundaries = np.array(boundaries)
+  values = np.array(values)
+  if not boundaries.ndim == values.ndim == 1:
+    raise ValueError(""boundaries and values must be sequences"")
+  if not boundaries.shape[0] == values.shape[0] - 1:
+    raise ValueError(""boundaries length must be one longer than values length"")
+
+  def schedule(i):
+    return values[np.sum(i > boundaries)]
+  return schedule
+
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
+  else:
+    raise TypeError, type(constant_scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index e24662325..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -23,21 +23,28 @@ from __future__ import division
 from __future__ import print_function
 
 import operator
+import functools
 
+import jax.numpy as np
 from jax.core import pack
 from jax.tree_util import tree_map, tree_multimap
-import jax.numpy as np
 
 
 def optimizer(opt_maker):
   """"""Decorator to make an optimizer map over tuple/list/dict containers.""""""
+  @functools.wraps(opt_maker)
   def tree_opt_maker(*args, **kwargs):
     init_fun, update_fun = opt_maker(*args, **kwargs)
+
+    @functools.wraps(init_fun)
     def fmapped_init_fun(x0_tree):
       return tree_map(lambda x0: pack(init_fun(x0)), x0_tree)
+
+    @functools.wraps(update_fun)
     def fmapped_update_fun(i, grad_tree, state_tree):
       update = lambda g, state: pack(update_fun(i, g, *state))
       return tree_multimap(update, grad_tree, state_tree)
+
     return fmapped_init_fun, fmapped_update_fun
   return tree_opt_maker
 
@@ -46,42 +53,86 @@ def iterate(state_tree):
   return tree_map(lambda state: tuple(state)[0], state_tree)
 get_params = iterate
 
+# optimizers
+
 @optimizer
 def sgd(step_size):
-  """"""Init and update step functions for stochastic gradient descent.""""""
+  """"""Construct init and update step functions for stochastic gradient descent.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     return (x0,)
   def update_fun(i, g, x):
-    return (x - step_size * g,)
+    return (x - step_size(i) * g,)
   return init_fun, update_fun
 
 @optimizer
 def momentum(step_size, mass):
-  """"""Init and update step functions for SGD with Nesterov momentum.""""""
+  """"""Construct init and update step functions for SGD with Nesterov momentum.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     v0 = np.zeros_like(x0)
     return x0, v0
   def update_fun(i, g, x, velocity):
     velocity = mass * velocity - (1. - mass) * g
-    x = x + step_size * velocity
+    x = x + step_size(i) * velocity
     return x, velocity
   return init_fun, update_fun
 
 @optimizer
 def rmsprop(step_size, gamma=0.9, eps=1e-8):
-  """"""Init and update step functions for RMSProp.""""""
+  """"""Construct init and update step functions for RMSProp.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     avg_sq_grad = np.ones_like(x0)
     return x0, avg_sq_grad
   def update_fun(i, g, x, avg_sq_grad):
     avg_sq_grad = avg_sq_grad * gamma + g**2 * (1. - gamma)
-    x = x - step_size * g / (np.sqrt(avg_sq_grad) + eps)
+    x = x - step_size(i) * g / (np.sqrt(avg_sq_grad) + eps)
     return x, avg_sq_grad
   return init_fun, update_fun
 
 @optimizer
 def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
-  """"""Init and update step functions for Adam.""""""
+  """"""Construct init and update step functions for Adam.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+    b1: optional, a positive scalar value for beta_1, the exponential decay rate
+      for the first moment estimates (default 0.9).
+    b2: optional, a positive scalar value for beta_2, the exponential decay rate
+      for the second moment estimates (default 0.999).
+    eps: optional, a positive scalar value for epsilon, a small constant for
+      numerical stability (default 1e-8).
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     m0 = np.zeros_like(x0)
     v0 = np.zeros_like(x0)
@@ -91,26 +142,47 @@ def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
     v = (1 - b2) * (g ** 2) + b2 * v  # Second moment estimate.
     mhat = m / (1 - b1 ** (i + 1))  # Bias correction.
     vhat = v / (1 - b2 ** (i + 1))
-    x = x - step_size*mhat / (np.sqrt(vhat) + eps)
+    x = x - step_size(i) * mhat / (np.sqrt(vhat) + eps)
     return x, m, v
   return init_fun, update_fun
 
-def run_optimizer(loss, infeed, update_fun, state):
-  """"""A convenience function for running optimizers with iterated map-reduce.
+# learning rate schedules
 
-  Args:
-    loss: a scalar-valued loss function taking two aguments, the current iterate
-      and a data value.
-    infeed: an infeed instance supplying the data stream.
-    update_fun: a function that has signature update_fun(i, grad, state) where
-      i is the integer iteration count, grad is the gradient of the loss at the
-      current iterate, and state is the current optimizer state.
-    state: the initial optimizer state.
+def constant(step_size):
+  def schedule(i):
+    return step_size
+  return schedule
 
-  Returns:
-    A pair (x, state) where is the final iterate and state represents the final
-    optimizer state.
-  """"""
-  map_fun = lambda _, state, batch: grad(loss)(iterate(state), batch)
-  state = fax.iterated_map_reduce(state, map_fun, update_fun, infeed)
-  return iterate(state), state
+def exponential_decay(step_size, decay_steps, decay_rate):
+  def schedule(i):
+    return step_size * decay_rate ** (i / decay_steps)
+  return schedule
+
+def inverse_time_decay(step_size, decay_steps, decay_rate, staircase=False):
+  if staircase:
+    def schedule(i):
+      return step_size / (1 + decay_rate * np.floor(i / decay_steps))
+  else:
+    def schedule(i):
+      return step_size / (1 + decay_rate * i / decay_steps)
+  return schedule
+
+def piecewise_constant(boundaries, values):
+  boundaries = np.array(boundaries)
+  values = np.array(values)
+  if not boundaries.ndim == values.ndim == 1:
+    raise ValueError(""boundaries and values must be sequences"")
+  if not boundaries.shape[0] == values.shape[0] - 1:
+    raise ValueError(""boundaries length must be one longer than values length"")
+
+  def schedule(i):
+    return values[np.sum(i > boundaries)]
+  return schedule
+
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
+  else:
+    raise TypeError, type(constant_scalar_or_schedule_fun)",No
jax/interpreters/partial_eval.py,jax/interpreters/partial_eval.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index eb311fd6f..74ab93d60 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -173,12 +173,12 @@ class JaxprTracerTuple(tuple): pass
 Destructuring = namedtuple('Destructuring', ['i', 'eqn', 'key'])
 
 class PartialVal(tuple):
-  def __init__(self, xs):
+  def __new__(cls, xs):
     assert core.skip_checks or (
         isinstance(xs[0], valid_pv_types)
         and isinstance(xs[1], core.Tracer) or core.valid_jaxtype(xs[1])
     ), xs
-    super(PartialVal, self).__init__(xs)
+    return tuple.__new__(cls, xs)
 
 valid_pv_types = (AbstractValue, JaxprTracerTuple, type(None))
 ","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index eb311fd6f..74ab93d60 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -173,12 +173,12 @@ class JaxprTracerTuple(tuple): pass
 Destructuring = namedtuple('Destructuring', ['i', 'eqn', 'key'])
 
 class PartialVal(tuple):
-  def __init__(self, xs):
+  def __new__(cls, xs):
     assert core.skip_checks or (
         isinstance(xs[0], valid_pv_types)
         and isinstance(xs[1], core.Tracer) or core.valid_jaxtype(xs[1])
     ), xs
-    super(PartialVal, self).__init__(xs)
+    return tuple.__new__(cls, xs)
 
 valid_pv_types = (AbstractValue, JaxprTracerTuple, type(None))
 ",No
jax/lax.py,jax/lax.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/lax.py b/jax/lax.py
index 5aea18e03..367555659 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1566,9 +1566,30 @@ def slice_transpose_rule(t, start_indices, limit_indices, strides,
   assert result.shape == operand_shape
   return [result]
 
+def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
+                        strides, **unused_kwargs):
+  operand, = batched_args
+  bdim, = batch_dims
+
+  new_start_indices = list(start_indices)
+  new_start_indices.insert(bdim, 0)
+
+  new_limit_indices = list(limit_indices)
+  new_limit_indices.insert(bdim, operand.shape[bdim])
+
+  if strides is None:
+    new_strides = None
+  else:
+    new_strides = list(strides)
+    new_strides.insert(bdim, 1)
+
+  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
+  return out, bdim
+
 slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                              slice_translation_rule)
 ad.deflinear(slice_p, slice_transpose_rule)
+batching.primitive_batchers[slice_p] = slice_batching_rule
 
 
 def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,","diff --git a/jax/lax.py b/jax/lax.py
index 5aea18e03..367555659 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1566,9 +1566,30 @@ def slice_transpose_rule(t, start_indices, limit_indices, strides,
   assert result.shape == operand_shape
   return [result]
 
+def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
+                        strides, **unused_kwargs):
+  operand, = batched_args
+  bdim, = batch_dims
+
+  new_start_indices = list(start_indices)
+  new_start_indices.insert(bdim, 0)
+
+  new_limit_indices = list(limit_indices)
+  new_limit_indices.insert(bdim, operand.shape[bdim])
+
+  if strides is None:
+    new_strides = None
+  else:
+    new_strides = list(strides)
+    new_strides.insert(bdim, 1)
+
+  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
+  return out, bdim
+
 slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                              slice_translation_rule)
 ad.deflinear(slice_p, slice_transpose_rule)
+batching.primitive_batchers[slice_p] = slice_batching_rule
 
 
 def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 21717ade1..71019b420 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -171,7 +171,7 @@ def _constant_like(x, const):
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Numpy docstring below.
+  LAX-backed implementation of {fun}. Original docstring below.
 
   {np_doc}
   """""".format(fun=fun.__name__, np_doc=fun.__doc__)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 21717ade1..71019b420 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -171,7 +171,7 @@ def _constant_like(x, const):
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Numpy docstring below.
+  LAX-backed implementation of {fun}. Original docstring below.
 
   {np_doc}
   """""".format(fun=fun.__name__, np_doc=fun.__doc__)",No
jax/random.py,jax/random.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/random.py b/jax/random.py
index 69fdea8da..c626664c9 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -309,7 +309,7 @@ def shuffle(key, x, axis=0):
   for _ in range(num_rounds):
     key, subkey = split(key)
     sort_keys = _random_bits(subkey, 32, x.shape)
-    _, x = lax.sort_keyval(sort_keys, x, axis)
+    _, x = lax.sort_key_val(sort_keys, x, axis)
 
   return x
 ","diff --git a/jax/random.py b/jax/random.py
index 69fdea8da..c626664c9 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -309,7 +309,7 @@ def shuffle(key, x, axis=0):
   for _ in range(num_rounds):
     key, subkey = split(key)
     sort_keys = _random_bits(subkey, 32, x.shape)
-    _, x = lax.sort_keyval(sort_keys, x, axis)
+    _, x = lax.sort_key_val(sort_keys, x, axis)
 
   return x
 ",No
jax/scipy/misc.py,jax/scipy/misc.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/scipy/misc.py b/jax/scipy/misc.py
index f8f9591a6..57c237c4c 100644
--- a/jax/scipy/misc.py
+++ b/jax/scipy/misc.py
@@ -20,22 +20,7 @@ import numpy as onp
 import scipy.misc as osp_misc
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps, _reduction_dims, _constant_like
 
 
 @_wraps(osp_misc.logsumexp)
@@ -50,19 +35,3 @@ def logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False):
   out = lax.add(lax.log(lax.reduce(lax.exp(lax.sub(a, amax_singletons)),
                                    _constant_like(a, 0), lax.add, dims)), amax)
   return dimadd(out) if keepdims else out
-
-
-# TODO(mattjj): this is duplicated from lax_numpy.py
-def _reduction_dims(a, axis):
-  if axis is None:
-    return onp.arange(onp.ndim(a))
-  elif isinstance(axis, (onp.ndarray, tuple, list)):
-    return onp.mod(onp.asarray(axis), onp.ndim(a))
-  elif isinstance(axis, int):
-    return onp.mod([axis], onp.ndim(a))
-  else:
-    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))
-
-
-def _constant_like(x, const):
-  return onp.array(const, dtype=lax._dtype(x))","diff --git a/jax/scipy/misc.py b/jax/scipy/misc.py
index f8f9591a6..57c237c4c 100644
--- a/jax/scipy/misc.py
+++ b/jax/scipy/misc.py
@@ -20,22 +20,7 @@ import numpy as onp
 import scipy.misc as osp_misc
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps, _reduction_dims, _constant_like
 
 
 @_wraps(osp_misc.logsumexp)
@@ -50,19 +35,3 @@ def logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False):
   out = lax.add(lax.log(lax.reduce(lax.exp(lax.sub(a, amax_singletons)),
                                    _constant_like(a, 0), lax.add, dims)), amax)
   return dimadd(out) if keepdims else out
-
-
-# TODO(mattjj): this is duplicated from lax_numpy.py
-def _reduction_dims(a, axis):
-  if axis is None:
-    return onp.arange(onp.ndim(a))
-  elif isinstance(axis, (onp.ndarray, tuple, list)):
-    return onp.mod(onp.asarray(axis), onp.ndim(a))
-  elif isinstance(axis, int):
-    return onp.mod([axis], onp.ndim(a))
-  else:
-    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))
-
-
-def _constant_like(x, const):
-  return onp.array(const, dtype=lax._dtype(x))",No
jax/scipy/special.py,jax/scipy/special.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/scipy/special.py b/jax/scipy/special.py
index b158276de..66cd9defa 100644
--- a/jax/scipy/special.py
+++ b/jax/scipy/special.py
@@ -19,22 +19,7 @@ from __future__ import print_function
 import scipy.special as osp_special
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps
 
 
 gammaln = _wraps(osp_special.gammaln)(lax.lgamma)","diff --git a/jax/scipy/special.py b/jax/scipy/special.py
index b158276de..66cd9defa 100644
--- a/jax/scipy/special.py
+++ b/jax/scipy/special.py
@@ -19,22 +19,7 @@ from __future__ import print_function
 import scipy.special as osp_special
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps
 
 
 gammaln = _wraps(osp_special.gammaln)(lax.lgamma)",No
tests/batching_test.py,tests/batching_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 6020079ef..edc3c2672 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -21,6 +21,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
+from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
 from jax.api import vmap
 from jax.core import unit
@@ -135,6 +136,24 @@ class BatchingTest(jtu.JaxTestCase):
                         check_dtypes=False)
     self.assertEqual(len(side), 1)
 
+  def testSliceLax(self):
+    fun = lambda x: lax.slice(x, (2,), (4,))
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, 2:4]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+  def testSliceNumpy(self):
+    fun = lambda x: x[:, 2]
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, :, 2]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 6020079ef..edc3c2672 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -21,6 +21,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
+from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
 from jax.api import vmap
 from jax.core import unit
@@ -135,6 +136,24 @@ class BatchingTest(jtu.JaxTestCase):
                         check_dtypes=False)
     self.assertEqual(len(side), 1)
 
+  def testSliceLax(self):
+    fun = lambda x: lax.slice(x, (2,), (4,))
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, 2:4]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+  def testSliceNumpy(self):
+    fun = lambda x: x[:, 2]
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, :, 2]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 21a5ac4b3..4b7c95ed6 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -27,15 +27,17 @@ from absl.testing import parameterized
 import numpy as onp
 import scipy.misc as osp_misc
 import scipy.special as osp_special
+import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
+from jax.scipy import stats as lsp_stats
 
 FLAGS = flags.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -79,6 +81,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True])
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    # TODO(mattjj): test autodiff
     def scipy_fun(array_to_reduce):
       return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
 
@@ -101,6 +104,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
   def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
     # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    # TODO(mattjj): test autodiff
     if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
         and not shapes[0]):
       return absltest.unittest.skip(""real() on scalar not supported on tpu"")
@@ -111,7 +115,40 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 3)
+      for dtypes in CombosWithReplacement(default_dtypes, 3)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scipy_fun = osp_stats.norm.logpdf
+    lax_fun = lsp_stats.norm.logpdf
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = 0.5 + onp.abs(scale)
+      return [x, loc, scale]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 2)
+      for dtypes in CombosWithReplacement(default_dtypes, 2)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scale = 0.5
+    scipy_fun = functools.partial(osp_stats.norm.logpdf, scale=scale)
+    lax_fun = functools.partial(lsp_stats.norm.logpdf, scale=scale)
+    def args_maker():
+      return list(map(rng, shapes, dtypes))
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
 
 if __name__ == ""__main__"":","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 21a5ac4b3..4b7c95ed6 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -27,15 +27,17 @@ from absl.testing import parameterized
 import numpy as onp
 import scipy.misc as osp_misc
 import scipy.special as osp_special
+import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
+from jax.scipy import stats as lsp_stats
 
 FLAGS = flags.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -79,6 +81,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True])
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    # TODO(mattjj): test autodiff
     def scipy_fun(array_to_reduce):
       return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
 
@@ -101,6 +104,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
   def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
     # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    # TODO(mattjj): test autodiff
     if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
         and not shapes[0]):
       return absltest.unittest.skip(""real() on scalar not supported on tpu"")
@@ -111,7 +115,40 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 3)
+      for dtypes in CombosWithReplacement(default_dtypes, 3)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scipy_fun = osp_stats.norm.logpdf
+    lax_fun = lsp_stats.norm.logpdf
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = 0.5 + onp.abs(scale)
+      return [x, loc, scale]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 2)
+      for dtypes in CombosWithReplacement(default_dtypes, 2)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scale = 0.5
+    scipy_fun = functools.partial(osp_stats.norm.logpdf, scale=scale)
+    lax_fun = functools.partial(lsp_stats.norm.logpdf, scale=scale)
+    def args_maker():
+      return list(map(rng, shapes, dtypes))
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
 
 if __name__ == ""__main__"":",No
tests/minmax_test.py,tests/minmax_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 63436c6fd..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -114,5 +114,41 @@ class OptimizerTests(jtu.JaxTestCase):
     partial_loss = functools.partial(loss, y)
     self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
 
+  def testSgdVectorExponentialDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.exponential_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testSgdVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testAdamVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_schedule)
+
+  def testMomentumVectorInverseTimeDecayStaircaseSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_sched = minmax.inverse_time_decay(0.1, 3, 2., staircase=True)
+    mass = 0.9
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_sched, mass)
+
+  def testRmspropVectorPiecewiseConstantSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 63436c6fd..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -114,5 +114,41 @@ class OptimizerTests(jtu.JaxTestCase):
     partial_loss = functools.partial(loss, y)
     self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
 
+  def testSgdVectorExponentialDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.exponential_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testSgdVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testAdamVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_schedule)
+
+  def testMomentumVectorInverseTimeDecayStaircaseSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_sched = minmax.inverse_time_decay(0.1, 3, 2., staircase=True)
+    mass = 0.9
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_sched, mass)
+
+  def testRmspropVectorPiecewiseConstantSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
+
 if __name__ == '__main__':
   absltest.main()",No
tests/random_test.py,tests/random_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/random_test.py b/tests/random_test.py
index 9d3ae7239..128b599b4 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -127,6 +127,23 @@ class LaxRandomTest(jtu.JaxTestCase):
     for samples in [uncompiled_samples, compiled_samples]:
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64])
+  def testShuffle(self, dtype):
+    key = random.PRNGKey(0)
+    x = onp.arange(100).astype(dtype)
+    rand = lambda key: random.shuffle(key, x)
+    crand = api.jit(rand)
+
+    perm1 = rand(key)
+    perm2 = crand(key)
+
+    self.assertTrue(onp.all(perm1 == perm2))
+    self.assertTrue(onp.all(perm1.dtype == perm2.dtype))
+    self.assertFalse(onp.all(perm1 == x))  # seems unlikely!
+    self.assertTrue(onp.all(onp.sort(perm1) == x))
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
index 9d3ae7239..128b599b4 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -127,6 +127,23 @@ class LaxRandomTest(jtu.JaxTestCase):
     for samples in [uncompiled_samples, compiled_samples]:
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64])
+  def testShuffle(self, dtype):
+    key = random.PRNGKey(0)
+    x = onp.arange(100).astype(dtype)
+    rand = lambda key: random.shuffle(key, x)
+    crand = api.jit(rand)
+
+    perm1 = rand(key)
+    perm2 = crand(key)
+
+    self.assertTrue(onp.all(perm1 == perm2))
+    self.assertTrue(onp.all(perm1.dtype == perm2.dtype))
+    self.assertFalse(onp.all(perm1 == x))  # seems unlikely!
+    self.assertTrue(onp.all(onp.sort(perm1) == x))
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
build/build_jax.sh,build/build_jax.sh,77731679954a4ae24d5c3b8250b5cf536e9bb604,46c6a9170f5829e9ff52a970b7d8933a99d2216d,add CPU-only build option,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index dbab5d69c..be093a636 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,17 @@
 #!/bin/bash
 set -exv
 
+# For a build with CUDA, from the repo root run:
+#   bash build/build_jax.sh
+# For building without CUDA (CPU-only), instead run:
+#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
+# To clean intermediate results, run
+#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
+# To clean everything, run
+#   rm -rf /tmp/jax-build
+
+JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}
+
 init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
 if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
 then
@@ -25,7 +36,7 @@ export PATH=""${bazel_dir}/bin:$PATH""
 # BUG: https://github.com/bazelbuild/bazel/issues/6665
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
-## get and configure tensorflow for building xla with gpu support
+## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
 then
   git clone https://github.com/tensorflow/tensorflow.git
@@ -34,12 +45,17 @@ pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
 export USE_DEFAULT_PYTHON_LIB_PATH=1
-export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
-export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
-export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
-export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
-export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-export TF_NEED_CUDA=1
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
+  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
+  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
+  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
+  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NEED_CUDA=1
+else
+  export TF_NEED_CUDA=0
+fi
 export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
 export TF_ENABLE_XLA=1
 export TF_NEED_MKL=0
@@ -62,8 +78,14 @@ mkdir -p ${PYTHON_LIB_PATH}
 bazel_output_user_root=${tmp}/jax-bazel-output-user-root
 bazel_output_base=${bazel_output_user_root}/output-base
 bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
-bazel_build_opt=""-c opt --config=cuda""
-if [ -n $handle_temporary_bazel_0_19_1_bug ]
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  bazel_build_opt=""-c opt --config=cuda""
+else
+  bazel_build_opt=""-c opt""
+fi
+# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
+if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
 then
   set +e
   bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
@@ -74,9 +96,9 @@ bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
 runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
-cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
 
 ## rewrite some imports
 sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
@@ -86,4 +108,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 ## clean up
 rm -f bazel-*  # symlinks
 rm -rf tensorflow
-# rm -rf ${tmp}
+rm -rf ${bazel_output_user_root}  # clean build results
+# rm -rf ${tmp}  # clean everything, including the bazel binary","diff --git a/build/build_jax.sh b/build/build_jax.sh
index dbab5d69c..be093a636 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,17 @@
 #!/bin/bash
 set -exv
 
+# For a build with CUDA, from the repo root run:
+#   bash build/build_jax.sh
+# For building without CUDA (CPU-only), instead run:
+#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
+# To clean intermediate results, run
+#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
+# To clean everything, run
+#   rm -rf /tmp/jax-build
+
+JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}
+
 init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
 if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
 then
@@ -25,7 +36,7 @@ export PATH=""${bazel_dir}/bin:$PATH""
 # BUG: https://github.com/bazelbuild/bazel/issues/6665
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
-## get and configure tensorflow for building xla with gpu support
+## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
 then
   git clone https://github.com/tensorflow/tensorflow.git
@@ -34,12 +45,17 @@ pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
 export USE_DEFAULT_PYTHON_LIB_PATH=1
-export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
-export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
-export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
-export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
-export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-export TF_NEED_CUDA=1
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
+  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
+  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
+  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
+  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NEED_CUDA=1
+else
+  export TF_NEED_CUDA=0
+fi
 export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
 export TF_ENABLE_XLA=1
 export TF_NEED_MKL=0
@@ -62,8 +78,14 @@ mkdir -p ${PYTHON_LIB_PATH}
 bazel_output_user_root=${tmp}/jax-bazel-output-user-root
 bazel_output_base=${bazel_output_user_root}/output-base
 bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
-bazel_build_opt=""-c opt --config=cuda""
-if [ -n $handle_temporary_bazel_0_19_1_bug ]
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  bazel_build_opt=""-c opt --config=cuda""
+else
+  bazel_build_opt=""-c opt""
+fi
+# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
+if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
 then
   set +e
   bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
@@ -74,9 +96,9 @@ bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
 runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
-cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
 
 ## rewrite some imports
 sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
@@ -86,4 +108,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 ## clean up
 rm -f bazel-*  # symlinks
 rm -rf tensorflow
-# rm -rf ${tmp}
+rm -rf ${bazel_output_user_root}  # clean build results
+# rm -rf ${tmp}  # clean everything, including the bazel binary",No
build/build_jax.sh,build/build_jax.sh,af62fdc17dd49279e3bfb26ff066785954716099,77731679954a4ae24d5c3b8250b5cf536e9bb604,update build script: bazel-0.19.2 and darwin,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index be093a636..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -27,14 +27,17 @@ bazel_dir=${tmp}/jax-bazel
 if [ ! -d ${bazel_dir}/bin ]
 then
     mkdir -p ${bazel_dir}
-    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.1/bazel-0.19.1-installer-linux-x86_64.sh
-    chmod +x bazel-0.19.1-installer-linux-x86_64.sh
-    ./bazel-0.19.1-installer-linux-x86_64.sh --prefix=${bazel_dir}
-    rm bazel-0.19.1-installer-linux-x86_64.sh
+    case ""$(uname -s)"" in
+      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
+      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
+      *) exit 1;;
+    esac
+    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
+    chmod +x ${installer}
+    bash ${installer} --prefix=${bazel_dir}
+    rm ${installer}
 fi
 export PATH=""${bazel_dir}/bin:$PATH""
-# BUG: https://github.com/bazelbuild/bazel/issues/6665
-handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
@@ -84,14 +87,6 @@ then
 else
   bazel_build_opt=""-c opt""
 fi
-# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
-if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
-then
-  set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
-  sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
-  set -e
-fi
 bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need","diff --git a/build/build_jax.sh b/build/build_jax.sh
index be093a636..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -27,14 +27,17 @@ bazel_dir=${tmp}/jax-bazel
 if [ ! -d ${bazel_dir}/bin ]
 then
     mkdir -p ${bazel_dir}
-    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.1/bazel-0.19.1-installer-linux-x86_64.sh
-    chmod +x bazel-0.19.1-installer-linux-x86_64.sh
-    ./bazel-0.19.1-installer-linux-x86_64.sh --prefix=${bazel_dir}
-    rm bazel-0.19.1-installer-linux-x86_64.sh
+    case ""$(uname -s)"" in
+      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
+      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
+      *) exit 1;;
+    esac
+    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
+    chmod +x ${installer}
+    bash ${installer} --prefix=${bazel_dir}
+    rm ${installer}
 fi
 export PATH=""${bazel_dir}/bin:$PATH""
-# BUG: https://github.com/bazelbuild/bazel/issues/6665
-handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
@@ -84,14 +87,6 @@ then
 else
   bazel_build_opt=""-c opt""
 fi
-# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
-if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
-then
-  set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
-  sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
-  set -e
-fi
 bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need",No
examples/BUILD,examples/BUILD,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/examples/BUILD b/examples/BUILD
index 227c1bce7..714100038 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],","diff --git a/examples/BUILD b/examples/BUILD
index 227c1bce7..714100038 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],",No
jax/BUILD,jax/BUILD,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index d03b01984..ba7c51848 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,19 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # JAX is Autograd and XLA
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index d03b01984..ba7c51848 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,19 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # JAX is Autograd and XLA
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(",No
setup.py,setup.py,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/setup.py b/setup.py
index 54b9e9c46..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from setuptools import setup
 from glob import glob
 ","diff --git a/setup.py b/setup.py
index 54b9e9c46..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from setuptools import setup
 from glob import glob
 ",No
tests/BUILD,tests/BUILD,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/tests/BUILD b/tests/BUILD
index 3d4a13e3b..f343cb634 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(","diff --git a/tests/BUILD b/tests/BUILD
index 3d4a13e3b..f343cb634 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(",No
jax/BUILD,jax/BUILD,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,da2d53ad33dd5cb988e457ad49da422e440ed663,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index ba7c51848..a9132e364 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -31,9 +31,7 @@ py_library(
             ""**/*_test.py"",
         ],
     ),
-    deps = [
-        ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
-    ],
+    deps = [""@org_tensorflow//tensorflow/compiler/xla/python:xla_client""],
 )
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index ba7c51848..a9132e364 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -31,9 +31,7 @@ py_library(
             ""**/*_test.py"",
         ],
     ),
-    deps = [
-        ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
-    ],
+    deps = [""@org_tensorflow//tensorflow/compiler/xla/python:xla_client""],
 )
 
 py_library(",No
examples/BUILD,examples/BUILD,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,source sync,"diff --git a/examples/BUILD b/examples/BUILD
index 714100038..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],","diff --git a/examples/BUILD b/examples/BUILD
index 714100038..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],",No
jax/BUILD,jax/BUILD,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index a9132e364..281b87482 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -14,6 +14,8 @@
 
 # JAX is Autograd and XLA
 
+licenses([""notice""])  # Apache 2
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index a9132e364..281b87482 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -14,6 +14,8 @@
 
 # JAX is Autograd and XLA
 
+licenses([""notice""])  # Apache 2
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(",No
tests/BUILD,tests/BUILD,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,source sync,"diff --git a/tests/BUILD b/tests/BUILD
index f343cb634..310d71c90 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(","diff --git a/tests/BUILD b/tests/BUILD
index f343cb634..310d71c90 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(",No
jax/BUILD,jax/BUILD,3731ca2299d7a1f55bd43a21cdec0c7249bacb32,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index 281b87482..a339ebb8e 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,6 +16,8 @@
 
 licenses([""notice""])  # Apache 2
 
+# top-level EF placeholder
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index 281b87482..a339ebb8e 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,6 +16,8 @@
 
 licenses([""notice""])  # Apache 2
 
+# top-level EF placeholder
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(",No
jax/BUILD,jax/BUILD,fe11b19e46c8d92817dd7b8c710b644b98845a5a,3731ca2299d7a1f55bd43a21cdec0c7249bacb32,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index a339ebb8e..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,10 +16,10 @@
 
 licenses([""notice""])  # Apache 2
 
-# top-level EF placeholder
-
 package(default_visibility = [""//visibility:public""])
 
+# top-level EF placeholder
+
 py_library(
     name = ""libjax"",
     srcs = glob(","diff --git a/jax/BUILD b/jax/BUILD
index a339ebb8e..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,10 +16,10 @@
 
 licenses([""notice""])  # Apache 2
 
-# top-level EF placeholder
-
 package(default_visibility = [""//visibility:public""])
 
+# top-level EF placeholder
+
 py_library(
     name = ""libjax"",
     srcs = glob(",No
build/build_jax.sh,build/build_jax.sh,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ",No
examples/BUILD,examples/BUILD,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )",No
jax/BUILD,jax/BUILD,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",",No
setup.py,setup.py,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},","diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},",No
tests/BUILD,tests/BUILD,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/tests/BUILD b/tests/BUILD
index 310d71c90..95d6ee29e 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index 310d71c90..95d6ee29e 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )",No
jax/experimental/minmax.py,jax/experimental/minmax.py,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,50038c07c815b82d412af43996a122e33eecd385,source sync,"diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)",No
jax/experimental/stax.py,jax/experimental/stax.py,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,50038c07c815b82d412af43996a122e33eecd385,source sync,"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ",No
examples/mnist_classifier.py,examples/mnist_classifier.py,5e9fcbb6e8426a20a24bdba2a78eb3900d87d593,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,source sync,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c97b18e15..f88624f97 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 from jax.examples import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 def main(unused_argv):
   step_size = 0.001","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c97b18e15..f88624f97 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 from jax.examples import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 def main(unused_argv):
   step_size = 0.001",No
examples/resnet50.py,examples/resnet50.py,5e9fcbb6e8426a20a24bdba2a78eb3900d87d593,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,source sync,"diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):",No
tests/minmax_test.py,tests/minmax_test.py,5e9fcbb6e8426a20a24bdba2a78eb3900d87d593,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,source sync,"diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()",No
README.md,README.md,b58ccb430aeec58e3597077a25fb22d525a57ef3,76346a87b5d43d0c1f499fe78e8e654ae9d18d83,update readme with 'not a Google product',"diff --git a/README.md b/README.md
index 2c4adae2f..d05bf570b 100644
--- a/README.md
+++ b/README.md
@@ -1 +1,3 @@
 JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+
+This is not an official Google product.","diff --git a/README.md b/README.md
index 2c4adae2f..d05bf570b 100644
--- a/README.md
+++ b/README.md
@@ -1 +1,3 @@
 JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+
+This is not an official Google product.",No
README.md,README.md,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,b58ccb430aeec58e3597077a25fb22d525a57ef3,Add two-pager link to README.,"diff --git a/README.md b/README.md
index d05bf570b..22e31d080 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,5 @@
-JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd).
+Here's a [two-page abstract](https://www.sysml.cc/doc/146.pdf) about an early version.
+Watch this space for updates!
 
 This is not an official Google product.","diff --git a/README.md b/README.md
index d05bf570b..22e31d080 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,5 @@
-JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd).
+Here's a [two-page abstract](https://www.sysml.cc/doc/146.pdf) about an early version.
+Watch this space for updates!
 
 This is not an official Google product.",No
examples/mnist_classifier.py,examples/mnist_classifier.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index f88624f97..7eba6b960 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,15 +23,14 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.examples import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):
@@ -50,7 +49,7 @@ init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(10), LogSoftmax)
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index f88624f97..7eba6b960 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,15 +23,14 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.examples import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):
@@ -50,7 +49,7 @@ init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(10), LogSoftmax)
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 58b660024..2910b1c3b 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,13 +23,12 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-from jax.examples import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -54,7 +53,7 @@ def accuracy(params, batch):
   return np.mean(predicted_class == target_class)
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
   param_scale = 0.1
   step_size = 0.001
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 58b660024..2910b1c3b 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,13 +23,12 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-from jax.examples import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -54,7 +53,7 @@ def accuracy(params, batch):
   return np.mean(predicted_class == target_class)
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
   param_scale = 0.1
   step_size = 0.001
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 7e9f719bb..e97c181e4 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,15 +25,14 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -84,7 +83,7 @@ decoder_init, decode = stax.serial(
 )
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 100
   batch_size = 32
@@ -138,7 +137,3 @@ def main(unused_argv):
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 7e9f719bb..e97c181e4 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,15 +25,14 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -84,7 +83,7 @@ decoder_init, decode = stax.serial(
 )
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 100
   batch_size = 32
@@ -138,7 +137,3 @@ def main(unused_argv):
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..d44f40e56 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..d44f40e56 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)",No
build/build_jax.sh,build/build_jax.sh,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 0bf8ae06c..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,7 +55,6 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -73,6 +72,7 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
+export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 0bf8ae06c..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,7 +55,6 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -73,6 +72,7 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
+export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ",No
examples/BUILD,examples/BUILD,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
+        "":minmax"",
+        "":stax"",
         ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
+        "":minmax"",
+        "":stax"",
         ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )",No
examples/mnist_classifier.py,examples/mnist_classifier.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 7eba6b960..c17f5fc1f 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,14 +23,15 @@ from __future__ import print_function
 import time
 import itertools
 
+from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, LogSoftmax
 import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
 def loss(params, batch):
@@ -47,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), LogSoftmax)
+    Dense(10), Softmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 7eba6b960..c17f5fc1f 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,14 +23,15 @@ from __future__ import print_function
 import time
 import itertools
 
+from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, LogSoftmax
 import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
 def loss(params, batch):
@@ -47,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), LogSoftmax)
+    Dense(10), Softmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 2910b1c3b..03d21538a 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,12 +23,13 @@ from __future__ import print_function
 
 import time
 
+from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
+import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
-import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 2910b1c3b..03d21538a 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,12 +23,13 @@ from __future__ import print_function
 
 import time
 
+from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
+import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
-import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index e97c181e4..f15e3dd30 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,14 +25,15 @@ from __future__ import print_function
 import os
 import time
 
+from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
+import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -137,3 +138,7 @@ if __name__ == ""__main__"":
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index e97c181e4..f15e3dd30 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,14 +25,15 @@ from __future__ import print_function
 import os
 import time
 
+from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
+import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -137,3 +138,7 @@ if __name__ == ""__main__"":
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..9c958c131 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import app
+
 import numpy.random as npr
 
 import jax.numpy as np
@@ -29,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, LogSoftmax)
+                                   MaxPool, Relu, Softmax)
 
 
 # ResNet blocks compose other layers
@@ -80,10 +82,12 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+
 
+def main(argv):
+  del argv  # Unused.
 
-if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -124,3 +128,7 @@ if __name__ == ""__main__"":
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
+
+
+if __name__ == '__main__':
+  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..9c958c131 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import app
+
 import numpy.random as npr
 
 import jax.numpy as np
@@ -29,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, LogSoftmax)
+                                   MaxPool, Relu, Softmax)
 
 
 # ResNet blocks compose other layers
@@ -80,10 +82,12 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
 
 
-if __name__ == ""__main__"":
+def main(argv):
+  del argv  # Unused.
+
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -124,3 +128,7 @@ if __name__ == ""__main__"":
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
+
+
+if __name__ == '__main__':
+  app.run(main)",Yes
jax/BUILD,jax/BUILD,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/jax/BUILD b/jax/BUILD
index c9754372f..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,7 +29,6 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
-            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",","diff --git a/jax/BUILD b/jax/BUILD
index c9754372f..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,7 +29,6 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
-            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",",No
jax/experimental/minmax.py,jax/experimental/minmax.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index 92caefdeb..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(scalar_or_schedule_fun):
-  if callable(scalar_or_schedule_fun):
-    return scalar_or_schedule_fun
-  elif np.ndim(scalar_or_schedule_fun) == 0:
-    return constant(scalar_or_schedule_fun)
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
   else:
-    raise TypeError, type(scalar_or_schedule_fun)
+    raise TypeError, type(constant_scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index 92caefdeb..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(scalar_or_schedule_fun):
-  if callable(scalar_or_schedule_fun):
-    return scalar_or_schedule_fun
-  elif np.ndim(scalar_or_schedule_fun) == 0:
-    return constant(scalar_or_schedule_fun)
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
   else:
-    raise TypeError, type(scalar_or_schedule_fun)
+    raise TypeError, type(constant_scalar_or_schedule_fun)",No
jax/experimental/stax.py,jax/experimental/stax.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 6db50cc48..874aaa735 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,8 +42,9 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-def logsoftmax(x, axis=-1):
-  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
+# TODO(mattjj): change this name to better fit convention
+def softmax(x, axis=-1):
+  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -145,7 +146,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
+Softmax = _elemwise_no_params(softmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 6db50cc48..874aaa735 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,8 +42,9 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-def logsoftmax(x, axis=-1):
-  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
+# TODO(mattjj): change this name to better fit convention
+def softmax(x, axis=-1):
+  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -145,7 +146,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
+Softmax = _elemwise_no_params(softmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ",No
setup.py,setup.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/setup.py b/setup.py
index b634177d3..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},","diff --git a/setup.py b/setup.py
index b634177d3..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},",No
tests/BUILD,tests/BUILD,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/tests/BUILD b/tests/BUILD
index 95d6ee29e..ec4e62f73 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -14,11 +14,11 @@
 
 licenses([""notice""])  # Apache 2
 
-load("":build_defs.bzl"", ""jax_test"")
+load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""core_test.py""],
+    srcs = [""tests/core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""lax_test.py""],
+    srcs = [""tests/lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""lax_numpy_test.py""],
+    srcs = [""tests/lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""lax_numpy_indexing_test.py""],
+    srcs = [""tests/lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""lax_scipy_test.py""],
+    srcs = [""tests/lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""random_test.py""],
+    srcs = [""tests/random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""api_test.py""],
+    srcs = [""tests/api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""batching_test.py""],
+    srcs = [""tests/batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""stax_test.py""],
-    deps = [""//jax:stax""],
+    srcs = [""tests/stax_test.py""],
+    deps = ["":stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""minmax_test.py""],
-    deps = [""//jax:minmax""],
+    srcs = [""tests/minmax_test.py""],
+    deps = ["":minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""lapax_test.py""],
-    deps = [""//jax:lapax""],
+    srcs = [""tests/lapax_test.py""],
+    deps = ["":lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index 95d6ee29e..ec4e62f73 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -14,11 +14,11 @@
 
 licenses([""notice""])  # Apache 2
 
-load("":build_defs.bzl"", ""jax_test"")
+load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""core_test.py""],
+    srcs = [""tests/core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""lax_test.py""],
+    srcs = [""tests/lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""lax_numpy_test.py""],
+    srcs = [""tests/lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""lax_numpy_indexing_test.py""],
+    srcs = [""tests/lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""lax_scipy_test.py""],
+    srcs = [""tests/lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""random_test.py""],
+    srcs = [""tests/random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""api_test.py""],
+    srcs = [""tests/api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""batching_test.py""],
+    srcs = [""tests/batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""stax_test.py""],
-    deps = [""//jax:stax""],
+    srcs = [""tests/stax_test.py""],
+    deps = ["":stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""minmax_test.py""],
-    deps = [""//jax:minmax""],
+    srcs = [""tests/minmax_test.py""],
+    deps = ["":minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""lapax_test.py""],
-    deps = [""//jax:lapax""],
+    srcs = [""tests/lapax_test.py""],
+    deps = ["":lapax""],
 )",No
tests/minmax_test.py,tests/minmax_test.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 5eee658ce..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.numpy as np
 import jax.test_util as jtu
-from jax import jit, grad
+import jax.numpy as np
+from jax.api import grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,24 +150,5 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
-  def testTracedStepSize(self):
-    def loss(x, _): return np.dot(x, x)
-    x0 = np.ones(2)
-    num_iters = 100
-    step_size = 0.1
-
-    init_fun, _ = minmax.sgd(step_size)
-    opt_state = init_fun(x0)
-
-    @jit
-    def update(opt_state, step_size):
-      _, update_fun = minmax.sgd(step_size)
-      x = minmax.get_params(opt_state)
-      g = grad(loss)(x, None)
-      return update_fun(0, g, opt_state)
-
-    update(opt_state, 0.9)  # doesn't crash
-
-
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 5eee658ce..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.numpy as np
 import jax.test_util as jtu
-from jax import jit, grad
+import jax.numpy as np
+from jax.api import grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,24 +150,5 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
-  def testTracedStepSize(self):
-    def loss(x, _): return np.dot(x, x)
-    x0 = np.ones(2)
-    num_iters = 100
-    step_size = 0.1
-
-    init_fun, _ = minmax.sgd(step_size)
-    opt_state = init_fun(x0)
-
-    @jit
-    def update(opt_state, step_size):
-      _, update_fun = minmax.sgd(step_size)
-      x = minmax.get_params(opt_state)
-      g = grad(loss)(x, None)
-      return update_fun(0, g, opt_state)
-
-    update(opt_state, 0.9)  # doesn't crash
-
-
 if __name__ == '__main__':
   absltest.main()",No
examples/BUILD,examples/BUILD,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )",No
examples/mnist_classifier.py,examples/mnist_classifier.py,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c17f5fc1f..52f1e3df2 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c17f5fc1f..52f1e3df2 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001",No
examples/resnet50.py,examples/resnet50.py,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):",No
jax/BUILD,jax/BUILD,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",",No
jax/experimental/stax.py,jax/experimental/stax.py,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ",No
tests/BUILD,tests/BUILD,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/tests/BUILD b/tests/BUILD
index ec4e62f73..3317f5017 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index ec4e62f73..3317f5017 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )",No
jax/experimental/minmax.py,jax/experimental/minmax.py,7f546b8c02553f392e423e06bde5991bfa066b18,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,"source sync

PiperOrigin-RevId: 222175432","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)",No
tests/minmax_test.py,tests/minmax_test.py,7f546b8c02553f392e423e06bde5991bfa066b18,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,"source sync

PiperOrigin-RevId: 222175432","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()",No
build/build_jax.sh,build/build_jax.sh,a954389d06a3268919254093b192e159e16f7ccf,7f546b8c02553f392e423e06bde5991bfa066b18,"source sync

PiperOrigin-RevId: 222189611","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ",No
setup.py,setup.py,a954389d06a3268919254093b192e159e16f7ccf,7f546b8c02553f392e423e06bde5991bfa066b18,"source sync

PiperOrigin-RevId: 222189611","diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},","diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},",No
jax/random.py,jax/random.py,065bb0baa2a397c6bac0eb844e7f9c20aa6f0da7,a954389d06a3268919254093b192e159e16f7ccf,"source sync

PiperOrigin-RevId: 222291726","diff --git a/jax/random.py b/jax/random.py
index c626664c9..e458516de 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -325,7 +325,7 @@ def normal(key, shape, dtype=onp.float32):
   Returns:
     A random array with the specified shape and dtype.
   """"""
-  lo = onp.nextafter(onp.array(-1., dtype), 0.)
+  lo = onp.nextafter(onp.array(-1., dtype), 0., dtype=dtype)
   hi = onp.array(1., dtype)
   u = uniform(key, shape, dtype, lo, hi)
   return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)","diff --git a/jax/random.py b/jax/random.py
index c626664c9..e458516de 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -325,7 +325,7 @@ def normal(key, shape, dtype=onp.float32):
   Returns:
     A random array with the specified shape and dtype.
   """"""
-  lo = onp.nextafter(onp.array(-1., dtype), 0.)
+  lo = onp.nextafter(onp.array(-1., dtype), 0., dtype=dtype)
   hi = onp.array(1., dtype)
   u = uniform(key, shape, dtype, lo, hi)
   return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)",No
jax/lax.py,jax/lax.py,e5b76f4fdea1205aa78690894f68be3ce57717bc,065bb0baa2a397c6bac0eb844e7f9c20aa6f0da7,"source sync

PiperOrigin-RevId: 222340967","diff --git a/jax/lax.py b/jax/lax.py
index 367555659..28b580018 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1347,7 +1347,7 @@ def concatenate_transpose_rule(t, *operands, **kwargs):
   operand_shapes = kwargs.pop('operand_shapes')
   limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim))
+  starts = onp.zeros((len(operands), t.ndim), dtype=int)
   starts[1:, dimension] = limit_points[:-1]
   limits = onp.tile(t.shape, (len(operands), 1))
   limits[:, dimension] = limit_points","diff --git a/jax/lax.py b/jax/lax.py
index 367555659..28b580018 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1347,7 +1347,7 @@ def concatenate_transpose_rule(t, *operands, **kwargs):
   operand_shapes = kwargs.pop('operand_shapes')
   limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim))
+  starts = onp.zeros((len(operands), t.ndim), dtype=int)
   starts[1:, dimension] = limit_points[:-1]
   limits = onp.tile(t.shape, (len(operands), 1))
   limits[:, dimension] = limit_points",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,e5b76f4fdea1205aa78690894f68be3ce57717bc,065bb0baa2a397c6bac0eb844e7f9c20aa6f0da7,"source sync

PiperOrigin-RevId: 222340967","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 7b0c918fd..70280c0c6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -288,6 +288,23 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(
+          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
+       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
+      for dtypes in [
+        [onp.float32],
+        [onp.float32, onp.float32],
+        [onp.float32, onp.int32, onp.float32],
+        [onp.float32, onp.int64, onp.float32],
+        [onp.float32, onp.int32, onp.float64],
+      ]
+      for shape in [(), (2,), (3, 4), (1, 100)]
+      for rng in [jtu.rand_default()])
+  def testStack(self, shape, dtypes, rng):
+    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
+    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
           ""_"".join(str(d) for d in shape),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 7b0c918fd..70280c0c6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -288,6 +288,23 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(
+          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
+       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
+      for dtypes in [
+        [onp.float32],
+        [onp.float32, onp.float32],
+        [onp.float32, onp.int32, onp.float32],
+        [onp.float32, onp.int64, onp.float32],
+        [onp.float32, onp.int32, onp.float64],
+      ]
+      for shape in [(), (2,), (3, 4), (1, 100)]
+      for rng in [jtu.rand_default()])
+  def testStack(self, shape, dtypes, rng):
+    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
+    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
           ""_"".join(str(d) for d in shape),",No
examples/BUILD,examples/BUILD,323be694a7cd5e6497b27769f8a302e37fa9a514,e5b76f4fdea1205aa78690894f68be3ce57717bc,"source sync

PiperOrigin-RevId: 222448341","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..63c44f98f 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -25,12 +25,23 @@ py_library(
     srcs = [""datasets.py""],
 )
 
+py_binary(
+    name = ""mnist_classifier_fromscratch"",
+    srcs = [""mnist_classifier_fromscratch.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+    ],
+)
+
 py_binary(
     name = ""mnist_classifier"",
     srcs = [""mnist_classifier.py""],
     deps = [
         "":datasets"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )
 
@@ -44,3 +55,14 @@ py_binary(
         ""//jax:stax"",
     ],
 )
+
+py_binary(
+    name = ""resnet50"",
+    srcs = [""resnet50.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
+    ],
+)","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..63c44f98f 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -25,12 +25,23 @@ py_library(
     srcs = [""datasets.py""],
 )
 
+py_binary(
+    name = ""mnist_classifier_fromscratch"",
+    srcs = [""mnist_classifier_fromscratch.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+    ],
+)
+
 py_binary(
     name = ""mnist_classifier"",
     srcs = [""mnist_classifier.py""],
     deps = [
         "":datasets"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )
 
@@ -44,3 +55,14 @@ py_binary(
         ""//jax:stax"",
     ],
 )
+
+py_binary(
+    name = ""resnet50"",
+    srcs = [""resnet50.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
+    ],
+)",No
examples/mnist_classifier.py,examples/mnist_classifier.py,323be694a7cd5e6497b27769f8a302e37fa9a514,e5b76f4fdea1205aa78690894f68be3ce57717bc,"source sync

PiperOrigin-RevId: 222448341","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 52f1e3df2..544d58b4d 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -29,9 +29,9 @@ import numpy.random as npr
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 52f1e3df2..544d58b4d 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -29,9 +29,9 @@ import numpy.random as npr
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,323be694a7cd5e6497b27769f8a302e37fa9a514,e5b76f4fdea1205aa78690894f68be3ce57717bc,"source sync

PiperOrigin-RevId: 222448341","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 03d21538a..6aa3a6cd9 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -27,9 +27,9 @@ from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 03d21538a..6aa3a6cd9 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -27,9 +27,9 @@ from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):",No
examples/mnist_vae.py,examples/mnist_vae.py,323be694a7cd5e6497b27769f8a302e37fa9a514,e5b76f4fdea1205aa78690894f68be3ce57717bc,"source sync

PiperOrigin-RevId: 222448341","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index f15e3dd30..d3fc4680b 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -30,10 +30,10 @@ import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index f15e3dd30..d3fc4680b 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -30,10 +30,10 @@ import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):",No
jax/util.py,jax/util.py,fe4edf2839b7ec650b1aac289db69669537a9540,323be694a7cd5e6497b27769f8a302e37fa9a514,"source sync

PiperOrigin-RevId: 222449830","diff --git a/jax/util.py b/jax/util.py
index 7eebdd72d..c7e7235e4 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -16,7 +16,7 @@ from __future__ import absolute_import
 
 import functools
 import itertools as it
-import weakref
+
 
 allow_memoize_hash_failures = False
 ","diff --git a/jax/util.py b/jax/util.py
index 7eebdd72d..c7e7235e4 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -16,7 +16,7 @@ from __future__ import absolute_import
 
 import functools
 import itertools as it
-import weakref
+
 
 allow_memoize_hash_failures = False
 ",No
jax/__init__.py,jax/__init__.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/__init__.py b/jax/__init__.py
index 91f75dcca..46a817b68 100644
--- a/jax/__init__.py
+++ b/jax/__init__.py
@@ -12,4 +12,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from api import *
+from jax.api import *","diff --git a/jax/__init__.py b/jax/__init__.py
index 91f75dcca..46a817b68 100644
--- a/jax/__init__.py
+++ b/jax/__init__.py
@@ -12,4 +12,4 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from api import *
+from jax.api import *",No
jax/abstract_arrays.py,jax/abstract_arrays.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 4efd370e6..d37aac013 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -15,6 +15,7 @@
 from __future__ import absolute_import
 
 import numpy as onp
+import six
 
 from . import core
 from . import ad_util
@@ -52,7 +53,8 @@ class UnshapedArray(core.AbstractValue):
   _bool = _nonzero = concretization_function_error(bool)
   _float   = concretization_function_error(float)
   _int     = concretization_function_error(int)
-  _long    = concretization_function_error(long)
+  if six.PY2:
+    _long    = concretization_function_error(long)
   _complex = concretization_function_error(complex)
   _hex     = concretization_function_error(hex)
   _oct     = concretization_function_error(oct)
@@ -94,7 +96,7 @@ class ShapedArray(UnshapedArray):
     elif self.dtype == other.dtype:
       return UnshapedArray(self.dtype)
     else:
-      raise TypeError, other
+      raise TypeError(other)
 
   def str_short(self):
     dtypestr = onp.dtype(self.dtype).name
@@ -137,7 +139,7 @@ class ConcreteArray(ShapedArray):
     elif self.dtype == other.dtype:
       return UnshapedArray(self.dtype)
     else:
-      raise TypeError, other
+      raise TypeError(other)
 
   def str_short(self):
     return str(self.val)","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 4efd370e6..d37aac013 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -15,6 +15,7 @@
 from __future__ import absolute_import
 
 import numpy as onp
+import six
 
 from . import core
 from . import ad_util
@@ -52,7 +53,8 @@ class UnshapedArray(core.AbstractValue):
   _bool = _nonzero = concretization_function_error(bool)
   _float   = concretization_function_error(float)
   _int     = concretization_function_error(int)
-  _long    = concretization_function_error(long)
+  if six.PY2:
+    _long    = concretization_function_error(long)
   _complex = concretization_function_error(complex)
   _hex     = concretization_function_error(hex)
   _oct     = concretization_function_error(oct)
@@ -94,7 +96,7 @@ class ShapedArray(UnshapedArray):
     elif self.dtype == other.dtype:
       return UnshapedArray(self.dtype)
     else:
-      raise TypeError, other
+      raise TypeError(other)
 
   def str_short(self):
     dtypestr = onp.dtype(self.dtype).name
@@ -137,7 +139,7 @@ class ConcreteArray(ShapedArray):
     elif self.dtype == other.dtype:
       return UnshapedArray(self.dtype)
     else:
-      raise TypeError, other
+      raise TypeError(other)
 
   def str_short(self):
     return str(self.val)",No
jax/ad_util.py,jax/ad_util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/ad_util.py b/jax/ad_util.py
index 9288e9fca..806eefc49 100644
--- a/jax/ad_util.py
+++ b/jax/ad_util.py
@@ -17,6 +17,9 @@ from __future__ import absolute_import
 from .core import JaxTuple, lattice_join
 from .interpreters.partial_eval import Primitive
 from .tree_util import register_pytree_node
+from .util import safe_map
+
+map = safe_map
 
 jaxval_adders = {}
 ","diff --git a/jax/ad_util.py b/jax/ad_util.py
index 9288e9fca..806eefc49 100644
--- a/jax/ad_util.py
+++ b/jax/ad_util.py
@@ -17,6 +17,9 @@ from __future__ import absolute_import
 from .core import JaxTuple, lattice_join
 from .interpreters.partial_eval import Primitive
 from .tree_util import register_pytree_node
+from .util import safe_map
+
+map = safe_map
 
 jaxval_adders = {}
 ",No
jax/api_util.py,jax/api_util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/api_util.py b/jax/api_util.py
index ff061723d..c2ee774d8 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -17,8 +17,9 @@ from __future__ import absolute_import
 from .core import pack
 from .tree_util import build_tree, process_pytree
 from .linear_util import transformation_with_aux
-from .util import unzip2, partial
+from .util import safe_map, unzip2, partial
 
+map = safe_map
 
 @transformation_with_aux
 def flatten_fun(in_trees, *args, **kwargs):","diff --git a/jax/api_util.py b/jax/api_util.py
index ff061723d..c2ee774d8 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -17,8 +17,9 @@ from __future__ import absolute_import
 from .core import pack
 from .tree_util import build_tree, process_pytree
 from .linear_util import transformation_with_aux
-from .util import unzip2, partial
+from .util import safe_map, unzip2, partial
 
+map = safe_map
 
 @transformation_with_aux
 def flatten_fun(in_trees, *args, **kwargs):",No
jax/core.py,jax/core.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/core.py b/jax/core.py
index 2d6081827..4d92578d5 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -18,6 +18,7 @@ from operator import attrgetter
 from contextlib import contextmanager
 from collections import namedtuple, Counter, defaultdict
 from weakref import ref
+import six
 import types
 
 from . import linear_util as lu
@@ -248,7 +249,6 @@ class Tracer(object):
   def __oct__(self): return self.aval._oct(self)
 
 
-
   def __getattr__(self, name):
     # if the aval property raises an AttributeError, gets caught here
     assert skip_checks or name != ""aval""
@@ -263,7 +263,10 @@ class Tracer(object):
       if t is aval_property:
         return attr.fget(self)
       elif t is aval_method:
-        return types.MethodType(attr.fun, self, None)
+        if six.PY3:
+          return types.MethodType(attr.fun, self)
+        else:
+          return types.MethodType(attr.fun, self, None)
       else:
         return attr
 
@@ -345,7 +348,7 @@ def new_master(trace_type, bottom=False):
     t = ref(master)
     del master
     if t() is not None:
-      print trace_stack
+      print(trace_stack)
       raise Exception('Leaked trace {}'.format(t()))
 
 ","diff --git a/jax/core.py b/jax/core.py
index 2d6081827..4d92578d5 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -18,6 +18,7 @@ from operator import attrgetter
 from contextlib import contextmanager
 from collections import namedtuple, Counter, defaultdict
 from weakref import ref
+import six
 import types
 
 from . import linear_util as lu
@@ -248,7 +249,6 @@ class Tracer(object):
   def __oct__(self): return self.aval._oct(self)
 
 
-
   def __getattr__(self, name):
     # if the aval property raises an AttributeError, gets caught here
     assert skip_checks or name != ""aval""
@@ -263,7 +263,10 @@ class Tracer(object):
       if t is aval_property:
         return attr.fget(self)
       elif t is aval_method:
-        return types.MethodType(attr.fun, self, None)
+        if six.PY3:
+          return types.MethodType(attr.fun, self)
+        else:
+          return types.MethodType(attr.fun, self, None)
       else:
         return attr
 
@@ -345,7 +348,7 @@ def new_master(trace_type, bottom=False):
     t = ref(master)
     del master
     if t() is not None:
-      print trace_stack
+      print(trace_stack)
       raise Exception('Leaked trace {}'.format(t()))
 
 ",No
jax/experimental/lapax.py,jax/experimental/lapax.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/experimental/lapax.py b/jax/experimental/lapax.py
index 0dcb0570e..3a3ab09d8 100644
--- a/jax/experimental/lapax.py
+++ b/jax/experimental/lapax.py
@@ -182,6 +182,7 @@ class LapaxMatrix(object):
   __sub__ = _make_infix_op(lax.sub)
   __mul__ = _make_infix_op(lax.batch_matmul)
   __div__ = _make_infix_op(lax.div)
+  __truediv__ = _make_infix_op(lax.div)
   T = property(_make_infix_op(_matrix_transpose))
 
 ","diff --git a/jax/experimental/lapax.py b/jax/experimental/lapax.py
index 0dcb0570e..3a3ab09d8 100644
--- a/jax/experimental/lapax.py
+++ b/jax/experimental/lapax.py
@@ -182,6 +182,7 @@ class LapaxMatrix(object):
   __sub__ = _make_infix_op(lax.sub)
   __mul__ = _make_infix_op(lax.batch_matmul)
   __div__ = _make_infix_op(lax.div)
+  __truediv__ = _make_infix_op(lax.div)
   T = property(_make_infix_op(_matrix_transpose))
 
 ",No
jax/experimental/minmax.py,jax/experimental/minmax.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index 92caefdeb..5e810accc 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -185,4 +185,4 @@ def make_schedule(scalar_or_schedule_fun):
   elif np.ndim(scalar_or_schedule_fun) == 0:
     return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(scalar_or_schedule_fun)
+    raise TypeError(type(scalar_or_schedule_fun))","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index 92caefdeb..5e810accc 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -185,4 +185,4 @@ def make_schedule(scalar_or_schedule_fun):
   elif np.ndim(scalar_or_schedule_fun) == 0:
     return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(scalar_or_schedule_fun)
+    raise TypeError(type(scalar_or_schedule_fun))",No
jax/experimental/stax.py,jax/experimental/stax.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 6db50cc48..99546da16 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -27,6 +27,7 @@ import operator as op
 
 import numpy as onp
 import numpy.random as npr
+from six.moves import reduce
 
 from jax import lax
 from jax import random","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 6db50cc48..99546da16 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -27,6 +27,7 @@ import operator as op
 
 import numpy as onp
 import numpy.random as npr
+from six.moves import reduce
 
 from jax import lax
 from jax import random",No
jax/interpreters/ad.py,jax/interpreters/ad.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 5900955c0..0f8c512a5 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -20,11 +20,14 @@ from .. import core as core
 from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
 from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                        zeros_like_p, zero, Zero)
-from ..util import unzip2, unzip3, safe_zip, partial
+from ..util import unzip2, unzip3, safe_map, safe_zip, partial
 from ..tree_util import process_pytree, build_tree, register_pytree_node
 from ..linear_util import thunk, staged, transformation, transformation_with_aux, wrap_init
 
+from six.moves import builtins, reduce
+
 zip = safe_zip
+map = safe_map
 
 def jvp(fun):
   return jvpfun(jvp_subtrace(fun))
@@ -102,13 +105,18 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
           for subjaxpr, const_vars, bound_vars in eqn.bound_subjaxprs])
       cts_out, ct_free_vars_out = get_primitive_transpose(eqn.primitive)(
           eqn.params, subjaxprs, sub_consts, sub_freevar_vals, invals, ct_in)
+      # TODO(dougalm): support cases != 1
+      assert(len(eqn.bound_subjaxprs) == 1)
+      _, _, bound_vars = eqn.bound_subjaxprs[0]
       map(write_cotangent, bound_vars, ct_free_vars_out)
     else:
       cts_out = get_primitive_transpose(eqn.primitive)(ct_in, *invals, **eqn.params)
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    map(write_cotangent, eqn.invars, cts_out)
+    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
+    for var, ct in builtins.zip(eqn.invars, cts_out):
+      write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)
   freevar_cts = map(read_cotangent, jaxpr.freevars)
@@ -195,7 +203,7 @@ class JVPTrace(Trace):
     elif xt is zero and yt is zero:
       return xt, yt
     else:
-      raise TypeError, (xt, yt)
+      raise TypeError((xt, yt))
 
   def pack(self, tracers):
     primals = pack(t.primal for t in tracers)
@@ -344,7 +352,10 @@ def transposed_fun(jaxpr, in_tree_def, args):
   out_jtuple, tree_def = tree_to_jaxtuples((cotangents_out, freevar_cts))
   yield out_jtuple, tree_def
 
-def call_transpose(primitive, params, (jaxpr,), (consts,), (freevar_vals,), args, ct):
+def call_transpose(primitive, params, jaxpr, consts, freevar_vals, args, ct):
+  jaxpr, = jaxpr
+  consts, = consts
+  freevar_vals, = freevar_vals
   assert isinstance(jaxpr, core.Jaxpr)
   assert all(a is None for a in args), ""TODO(dougalm): handle non-tangent primal args""
   (ct, freevar_vals), in_tree_def = tree_to_jaxtuples((ct, freevar_vals))","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 5900955c0..0f8c512a5 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -20,11 +20,14 @@ from .. import core as core
 from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
 from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                        zeros_like_p, zero, Zero)
-from ..util import unzip2, unzip3, safe_zip, partial
+from ..util import unzip2, unzip3, safe_map, safe_zip, partial
 from ..tree_util import process_pytree, build_tree, register_pytree_node
 from ..linear_util import thunk, staged, transformation, transformation_with_aux, wrap_init
 
+from six.moves import builtins, reduce
+
 zip = safe_zip
+map = safe_map
 
 def jvp(fun):
   return jvpfun(jvp_subtrace(fun))
@@ -102,13 +105,18 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
           for subjaxpr, const_vars, bound_vars in eqn.bound_subjaxprs])
       cts_out, ct_free_vars_out = get_primitive_transpose(eqn.primitive)(
           eqn.params, subjaxprs, sub_consts, sub_freevar_vals, invals, ct_in)
+      # TODO(dougalm): support cases != 1
+      assert(len(eqn.bound_subjaxprs) == 1)
+      _, _, bound_vars = eqn.bound_subjaxprs[0]
       map(write_cotangent, bound_vars, ct_free_vars_out)
     else:
       cts_out = get_primitive_transpose(eqn.primitive)(ct_in, *invals, **eqn.params)
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    map(write_cotangent, eqn.invars, cts_out)
+    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
+    for var, ct in builtins.zip(eqn.invars, cts_out):
+      write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)
   freevar_cts = map(read_cotangent, jaxpr.freevars)
@@ -195,7 +203,7 @@ class JVPTrace(Trace):
     elif xt is zero and yt is zero:
       return xt, yt
     else:
-      raise TypeError, (xt, yt)
+      raise TypeError((xt, yt))
 
   def pack(self, tracers):
     primals = pack(t.primal for t in tracers)
@@ -344,7 +352,10 @@ def transposed_fun(jaxpr, in_tree_def, args):
   out_jtuple, tree_def = tree_to_jaxtuples((cotangents_out, freevar_cts))
   yield out_jtuple, tree_def
 
-def call_transpose(primitive, params, (jaxpr,), (consts,), (freevar_vals,), args, ct):
+def call_transpose(primitive, params, jaxpr, consts, freevar_vals, args, ct):
+  jaxpr, = jaxpr
+  consts, = consts
+  freevar_vals, = freevar_vals
   assert isinstance(jaxpr, core.Jaxpr)
   assert all(a is None for a in args), ""TODO(dougalm): handle non-tangent primal args""
   (ct, freevar_vals), in_tree_def = tree_to_jaxtuples((ct, freevar_vals))",No
jax/interpreters/batching.py,jax/interpreters/batching.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 9c5987cf1..ab567dde1 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -18,6 +18,8 @@ import itertools as it
 
 import numpy as onp
 
+from six.moves import reduce
+
 from .. import core
 from ..core import Trace, Tracer, new_master, pack, AbstractTuple, JaxTuple
 from ..abstract_arrays import ShapedArray, make_shaped_array, array_types
@@ -83,7 +85,7 @@ class BatchTracer(Tracer):
     elif t is int:
       batch_dims = [self.batch_dim] * len(self.val)
     else:
-      raise TypeError, t
+      raise TypeError(t)
     return map(partial(BatchTracer, self.trace), self.val, batch_dims)
 
   def full_lower(self):
@@ -150,7 +152,7 @@ def raise_to_shaped(aval):
   elif isinstance(aval, ShapedArray):
     return ShapedArray(aval.shape, aval.dtype)
   else:
-    raise TypeError, type(aval)
+    raise TypeError(type(aval))
 
 def remove_batch_dim_from_aval(bdim, aval):
   t = type(aval)
@@ -167,7 +169,7 @@ def remove_batch_dim_from_aval(bdim, aval):
       unbatched_shape = tuple(onp.delete(aval.shape, bdim))
       return ShapedArray(unbatched_shape, aval.dtype)
   else:
-    raise TypeError, t
+    raise TypeError(t)
 
 pytype_aval_mappings = {}
 
@@ -259,7 +261,7 @@ def dimsize(dim, x):
   elif dim is None:
     return set()
   else:
-    raise TypeError, type(dim)
+    raise TypeError(type(dim))
 
 def moveaxis(sz, dst, src, x):
   aval = get_aval(x)
@@ -284,7 +286,7 @@ def moveaxis(sz, dst, src, x):
         perm.insert(dst, src)
         return x.transpose(perm)
   else:
-    raise TypeError, type(aval)
+    raise TypeError(type(aval))
 
 def broadcast(x, sz):
   try:","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 9c5987cf1..ab567dde1 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -18,6 +18,8 @@ import itertools as it
 
 import numpy as onp
 
+from six.moves import reduce
+
 from .. import core
 from ..core import Trace, Tracer, new_master, pack, AbstractTuple, JaxTuple
 from ..abstract_arrays import ShapedArray, make_shaped_array, array_types
@@ -83,7 +85,7 @@ class BatchTracer(Tracer):
     elif t is int:
       batch_dims = [self.batch_dim] * len(self.val)
     else:
-      raise TypeError, t
+      raise TypeError(t)
     return map(partial(BatchTracer, self.trace), self.val, batch_dims)
 
   def full_lower(self):
@@ -150,7 +152,7 @@ def raise_to_shaped(aval):
   elif isinstance(aval, ShapedArray):
     return ShapedArray(aval.shape, aval.dtype)
   else:
-    raise TypeError, type(aval)
+    raise TypeError(type(aval))
 
 def remove_batch_dim_from_aval(bdim, aval):
   t = type(aval)
@@ -167,7 +169,7 @@ def remove_batch_dim_from_aval(bdim, aval):
       unbatched_shape = tuple(onp.delete(aval.shape, bdim))
       return ShapedArray(unbatched_shape, aval.dtype)
   else:
-    raise TypeError, t
+    raise TypeError(t)
 
 pytype_aval_mappings = {}
 
@@ -259,7 +261,7 @@ def dimsize(dim, x):
   elif dim is None:
     return set()
   else:
-    raise TypeError, type(dim)
+    raise TypeError(type(dim))
 
 def moveaxis(sz, dst, src, x):
   aval = get_aval(x)
@@ -284,7 +286,7 @@ def moveaxis(sz, dst, src, x):
         perm.insert(dst, src)
         return x.transpose(perm)
   else:
-    raise TypeError, type(aval)
+    raise TypeError(type(aval))
 
 def broadcast(x, sz):
   try:",No
jax/interpreters/partial_eval.py,jax/interpreters/partial_eval.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index 74ab93d60..f152e863d 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -25,6 +25,8 @@ from ..core import (Trace, Tracer, new_master, Jaxpr, JaxprEqn, get_aval, pack,
                     AbstractValue, AbstractTuple, unit, unitvar, Primitive,
                     call_p)
 
+map = safe_map
+zip = safe_zip
 
 class JaxprTrace(Trace):
   def pure(self, val):
@@ -212,7 +214,7 @@ def as_abstract_val(pv):
   elif isinstance(pv, JaxprTracerTuple):
     return AbstractTuple(map(as_abstract_val, pv))
   elif pv is None:
-    raise TypeError, ""{} is not abstract"".format(pv)
+    raise TypeError(""{} is not abstract"".format(pv))
 
 
 def partial_val_aval(pv, const):
@@ -283,7 +285,7 @@ def eqn_tracer_to_var(var, outvars, eqn):
 def tracers_to_jaxpr(in_tracers, out_tracer):
   newvar = gensym('')
   t_to_var = defaultdict(newvar)
-  var = lambda t: t_to_var[t]
+  var = lambda t: t_to_var[id(t)]
   sorted_tracers = toposort(out_tracer)
   invars = map(var, in_tracers)
   eqns = []
@@ -308,11 +310,11 @@ def tracers_to_jaxpr(in_tracers, out_tracer):
         destructuring_vars[key] = outvars
       else:
         outvars = destructuring_vars[key]
-      t_to_var[t] = outvars[i]
+      t_to_var[id(t)] = outvars[i]
     elif recipe is unit:
-      t_to_var[t] = unitvar
+      t_to_var[id(t)] = unitvar
     else:
-      raise TypeError, recipe
+      raise TypeError(recipe)
 
   env_vars, env_vals = unzip2(env.items())
   const_vars, const_vals = unzip2(consts.items())
@@ -323,7 +325,7 @@ def tracers_to_jaxpr(in_tracers, out_tracer):
 
 def gensym(suffix):
   counter = it.count()
-  return lambda: Var(counter.next(), suffix)
+  return lambda: Var(next(counter), suffix)
 
 class Var(object):
   def __init__(self, count, suffix):
@@ -334,7 +336,7 @@ class Var(object):
     rem = self.count
     s = ''
     while True:
-      rem, i = rem / 26, rem % 26
+      rem, i = rem // 26, rem % 26
       s = chr(97 + i % 26) + s
       if not rem:
         break","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index 74ab93d60..f152e863d 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -25,6 +25,8 @@ from ..core import (Trace, Tracer, new_master, Jaxpr, JaxprEqn, get_aval, pack,
                     AbstractValue, AbstractTuple, unit, unitvar, Primitive,
                     call_p)
 
+map = safe_map
+zip = safe_zip
 
 class JaxprTrace(Trace):
   def pure(self, val):
@@ -212,7 +214,7 @@ def as_abstract_val(pv):
   elif isinstance(pv, JaxprTracerTuple):
     return AbstractTuple(map(as_abstract_val, pv))
   elif pv is None:
-    raise TypeError, ""{} is not abstract"".format(pv)
+    raise TypeError(""{} is not abstract"".format(pv))
 
 
 def partial_val_aval(pv, const):
@@ -283,7 +285,7 @@ def eqn_tracer_to_var(var, outvars, eqn):
 def tracers_to_jaxpr(in_tracers, out_tracer):
   newvar = gensym('')
   t_to_var = defaultdict(newvar)
-  var = lambda t: t_to_var[t]
+  var = lambda t: t_to_var[id(t)]
   sorted_tracers = toposort(out_tracer)
   invars = map(var, in_tracers)
   eqns = []
@@ -308,11 +310,11 @@ def tracers_to_jaxpr(in_tracers, out_tracer):
         destructuring_vars[key] = outvars
       else:
         outvars = destructuring_vars[key]
-      t_to_var[t] = outvars[i]
+      t_to_var[id(t)] = outvars[i]
     elif recipe is unit:
-      t_to_var[t] = unitvar
+      t_to_var[id(t)] = unitvar
     else:
-      raise TypeError, recipe
+      raise TypeError(recipe)
 
   env_vars, env_vals = unzip2(env.items())
   const_vars, const_vals = unzip2(consts.items())
@@ -323,7 +325,7 @@ def tracers_to_jaxpr(in_tracers, out_tracer):
 
 def gensym(suffix):
   counter = it.count()
-  return lambda: Var(counter.next(), suffix)
+  return lambda: Var(next(counter), suffix)
 
 class Var(object):
   def __init__(self, count, suffix):
@@ -334,7 +336,7 @@ class Var(object):
     rem = self.count
     s = ''
     while True:
-      rem, i = rem / 26, rem % 26
+      rem, i = rem // 26, rem % 26
       s = chr(97 + i % 26) + s
       if not rem:
         break",No
jax/interpreters/xla.py,jax/interpreters/xla.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 95bda5440..6e5e2bd4d 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -18,13 +18,15 @@ from collections import namedtuple
 import itertools as it
 import numpy as onp
 import operator as op
+import six
+from six.moves import xrange
 
 from absl import flags
 from .. import core
 from .. import ad_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types
 from ..core import AbstractTuple, JaxTuple, pack, valid_jaxtype
-from ..util import partial, partialmethod, memoize, unzip2, concatenate
+from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map
 from ..linear_util import transformation_with_aux, memoize as linear_memoize
 from ..lib import xla_bridge as xb
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
@@ -32,6 +34,7 @@ from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 FLAGS = flags.FLAGS
 flags.DEFINE_bool('jax_device_values', True, 'Enable device-persistent values.')
 
+map = safe_map
 
 def apply_primitive(prim, *args, **kwargs):
   abstract_args = map(abstractify, args)
@@ -123,7 +126,8 @@ translations = {}
 
 translations[core.pack_p] = lambda c, *xs: c.Tuple(*xs)
 translations[ad_util.add_jaxvals_p] = lambda c, x, y: c.Add(x, y)
-translations[core.call_p] = lambda c, (subc, a1), *a2: c.Call(subc, a1 + a2)
+translations[core.call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
+                                                           subc_a1[1] + a2)
 translations[core.identity_p] = lambda c, x: x
 
 
@@ -242,7 +246,8 @@ class DeviceArray(DeviceValue):
   __bool__ = __nonzero__ = partialmethod(forward_to_value, bool)
   __float__ = partialmethod(forward_to_value, float)
   __int__ = partialmethod(forward_to_value, int)
-  __long__ = partialmethod(forward_to_value, long)
+  if six.PY2:
+    __long__ = partialmethod(forward_to_value, long)
   __complex__ = partialmethod(forward_to_value, complex)
   __hex__ = partialmethod(forward_to_value, hex)
   __oct__ = partialmethod(forward_to_value, oct)
@@ -253,6 +258,14 @@ class DeviceArray(DeviceValue):
   # clobbered when jax.numpy is imported, but useful in tests
   def __eq__(self, other): return self._value == other
 
+  def __hash__(self):
+    # TODO(mattjj): this is not semantically correct because it is possible
+    # __eq__ is true for values with unequal __hash__ values. However, the
+    # main use case at the moment is memoization for which false negatives are
+    # fine.
+    return id(self)
+
+
 core.pytype_aval_mappings[DeviceArray] = ConcreteArray
 pytype_aval_mappings[DeviceArray] = make_shaped_array
 canonicalize_dtype_handlers[DeviceArray] = identity
@@ -267,7 +280,7 @@ def xla_shape(x):
     if type(x) in (core.AbstractTuple, core.JaxTuple):
       return xb.Shape.tuple_shape(tuple(map(xla_shape, x)))
     else:
-      raise TypeError, type(x)
+      raise TypeError(type(x))
 
 
 # For callable XLA Computations (as opposed to, e.g., Computations used in the
@@ -298,7 +311,7 @@ def tree_flatten(maybe_tree):
   elif core.skip_checks or valid_jaxtype(maybe_tree):
     return [maybe_tree], leaf
   else:
-    raise TypeError, type(maybe_tree)
+    raise TypeError(type(maybe_tree))
 
 JTupleTreeDef = namedtuple(""JTupleTreeDef"", [""child_specs""])
 
@@ -313,7 +326,7 @@ def build_tree(xs, tree_spec):
   elif type(tree_spec) is JTupleTreeDef:
     return pack(map(partial(build_tree, xs), tree_spec.child_specs))
   else:
-    raise TypeError, type(tree_spec)
+    raise TypeError(type(tree_spec))
 
 
 def device_put(x):
@@ -364,4 +377,5 @@ xla_call = partial(core.call_bind, xla_call_p)
 xla_call_p.def_custom_bind(xla_call)
 xla_call_p.def_impl(xla_call_impl)
 
-translations[xla_call_p] = lambda c, (subc, a1), *a2: c.Call(subc, a1 + a2)
+translations[xla_call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
+                                                          subc_a1[1] + a2)","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 95bda5440..6e5e2bd4d 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -18,13 +18,15 @@ from collections import namedtuple
 import itertools as it
 import numpy as onp
 import operator as op
+import six
+from six.moves import xrange
 
 from absl import flags
 from .. import core
 from .. import ad_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types
 from ..core import AbstractTuple, JaxTuple, pack, valid_jaxtype
-from ..util import partial, partialmethod, memoize, unzip2, concatenate
+from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map
 from ..linear_util import transformation_with_aux, memoize as linear_memoize
 from ..lib import xla_bridge as xb
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
@@ -32,6 +34,7 @@ from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 FLAGS = flags.FLAGS
 flags.DEFINE_bool('jax_device_values', True, 'Enable device-persistent values.')
 
+map = safe_map
 
 def apply_primitive(prim, *args, **kwargs):
   abstract_args = map(abstractify, args)
@@ -123,7 +126,8 @@ translations = {}
 
 translations[core.pack_p] = lambda c, *xs: c.Tuple(*xs)
 translations[ad_util.add_jaxvals_p] = lambda c, x, y: c.Add(x, y)
-translations[core.call_p] = lambda c, (subc, a1), *a2: c.Call(subc, a1 + a2)
+translations[core.call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
+                                                           subc_a1[1] + a2)
 translations[core.identity_p] = lambda c, x: x
 
 
@@ -242,7 +246,8 @@ class DeviceArray(DeviceValue):
   __bool__ = __nonzero__ = partialmethod(forward_to_value, bool)
   __float__ = partialmethod(forward_to_value, float)
   __int__ = partialmethod(forward_to_value, int)
-  __long__ = partialmethod(forward_to_value, long)
+  if six.PY2:
+    __long__ = partialmethod(forward_to_value, long)
   __complex__ = partialmethod(forward_to_value, complex)
   __hex__ = partialmethod(forward_to_value, hex)
   __oct__ = partialmethod(forward_to_value, oct)
@@ -253,6 +258,14 @@ class DeviceArray(DeviceValue):
   # clobbered when jax.numpy is imported, but useful in tests
   def __eq__(self, other): return self._value == other
 
+  def __hash__(self):
+    # TODO(mattjj): this is not semantically correct because it is possible
+    # __eq__ is true for values with unequal __hash__ values. However, the
+    # main use case at the moment is memoization for which false negatives are
+    # fine.
+    return id(self)
+
+
 core.pytype_aval_mappings[DeviceArray] = ConcreteArray
 pytype_aval_mappings[DeviceArray] = make_shaped_array
 canonicalize_dtype_handlers[DeviceArray] = identity
@@ -267,7 +280,7 @@ def xla_shape(x):
     if type(x) in (core.AbstractTuple, core.JaxTuple):
       return xb.Shape.tuple_shape(tuple(map(xla_shape, x)))
     else:
-      raise TypeError, type(x)
+      raise TypeError(type(x))
 
 
 # For callable XLA Computations (as opposed to, e.g., Computations used in the
@@ -298,7 +311,7 @@ def tree_flatten(maybe_tree):
   elif core.skip_checks or valid_jaxtype(maybe_tree):
     return [maybe_tree], leaf
   else:
-    raise TypeError, type(maybe_tree)
+    raise TypeError(type(maybe_tree))
 
 JTupleTreeDef = namedtuple(""JTupleTreeDef"", [""child_specs""])
 
@@ -313,7 +326,7 @@ def build_tree(xs, tree_spec):
   elif type(tree_spec) is JTupleTreeDef:
     return pack(map(partial(build_tree, xs), tree_spec.child_specs))
   else:
-    raise TypeError, type(tree_spec)
+    raise TypeError(type(tree_spec))
 
 
 def device_put(x):
@@ -364,4 +377,5 @@ xla_call = partial(core.call_bind, xla_call_p)
 xla_call_p.def_custom_bind(xla_call)
 xla_call_p.def_impl(xla_call_impl)
 
-translations[xla_call_p] = lambda c, (subc, a1), *a2: c.Call(subc, a1 + a2)
+translations[xla_call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
+                                                          subc_a1[1] + a2)",No
jax/lax.py,jax/lax.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/lax.py b/jax/lax.py
index 28b580018..90cb2e707 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -15,12 +15,13 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import __builtin__
 
 import collections
 from .util import partial
 import itertools
 import operator
+import six
+from six.moves import builtins, xrange
 import string
 
 import numpy as onp
@@ -40,9 +41,14 @@ from .util import curry, safe_zip, unzip2
 from .tree_util import build_tree
 from .lib import xla_bridge
 
-_max = __builtin__.max
-_min = __builtin__.max
+_max = builtins.max
+_min = builtins.max
 
+if six.PY3:
+  def maketrans(s1, s2):
+    return s1.maketrans(s1, s2)
+else:
+  maketrans = string.maketrans
 
 ### traceables
 
@@ -383,7 +389,7 @@ def _while_loop(cond_fun, body_fun, init_val):
   params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
   out_flat = while_p.bind(init_val_flat, opaque_params=params)
   if out_tree() != in_tree:
-    raise TypeError, ""body_fun input and output must have identical structure""
+    raise TypeError(""body_fun input and output must have identical structure"")
   return build_tree(out_tree(), out_flat)
 
 class OpaqueParam(object):
@@ -500,8 +506,9 @@ def fori_loop(lower, upper, body_fun, init_val):
   # state: (upper limit, index, loop value)
   # The `lt` and `add` functions are added to the namespace programmatically.
   _, _, result = _while_loop(
-      lambda (upper, i, _): lt(i, upper),
-      lambda (upper, i, x): (upper, add(i, 1), body_fun(i, x)),
+      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
+      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
+                         body_fun(upper_i_x[1], upper_i_x[2])),
       (upper, lower, init_val))
   return result
 
@@ -519,7 +526,7 @@ def foreach_loop(sequence, body_fun, init_val):
   """"""
   _, result = fori_loop(
       0, len(sequence),
-      lambda i, (seq, val): body_fun(seq[i], val),
+      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
       (sequence, init_val))
   return result
 
@@ -708,7 +715,7 @@ standard_binop = partial(binop, _input_dtype)
 def _brcast(x, *others):
   # used in jvprules to make binop broadcasting explicit for transposability.
   # requires shape info during jvp tracing, which isn't strictly necessary.
-  shapes = filter(None, map(onp.shape, (x,) + others))
+  shapes = list(filter(None, map(onp.shape, (x,) + others)))
   shape = tuple(shapes and onp.max(shapes, axis=0))
   if onp.shape(x) != shape:
     return _brcast_to(x, shape)
@@ -1017,8 +1024,8 @@ def conv_general_dilated_transpose_rhs(
     lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
     lhs_spec, rhs_spec, out_spec = dimension_numbers
     trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
-                              out_spec.translate(string.maketrans(""NC"", ""IO"")),
-                              rhs_spec.translate(string.maketrans(""IO"", ""NC"")))
+                               out_spec.translate(maketrans(""NC"", ""IO"")),
+                               rhs_spec.translate(maketrans(""IO"", ""NC"")))
 
   padding = _conv_general_vjp_rhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
@@ -1543,9 +1550,8 @@ def slice_shape_rule(operand, start_indices, limit_indices, strides,
       msg = ""slice strides must be positive, got {}""
       raise TypeError(msg.format(strides))
 
-  result_shape = onp.divide(onp.add(onp.subtract(limit_indices, start_indices),
-                                    strides) - 1,
-                            strides)
+  result_shape = onp.floor_divide(
+      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
   return tuple(result_shape)
 
 def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
@@ -1911,7 +1917,8 @@ def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                               padding):
   pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
   operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
-  t = onp.divide(onp.subtract(operand_padded, window_dimensions), window_strides) + 1
+  t = onp.floor_divide(
+      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
   return tuple(t)
 
 
@@ -2127,7 +2134,7 @@ def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
   """"""Check that dtypes agree, possibly ignoring float precision.""""""
   # the `ignore_fp_precision` flag exists because the XLA shape inference logic
   # allows mixed floating point precision, but the HLO verifier often rejects it
-  dtypes = map(onp.dtype, dtypes)  # canonicalize
+  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
   if ignore_fp_precision:
     dtypes = [
         onp.floating if onp.issubdtype(dtype, onp.floating)
@@ -2172,7 +2179,8 @@ def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
     raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))
 
   lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
-  out_space = onp.divide(onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
+  out_space = onp.floor_divide(
+      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
   out_space = onp.maximum(0, out_space)
   out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
   return tuple(out_shape)
@@ -2268,7 +2276,7 @@ def remaining(original, *removed_lists):
 
 
 def _charswap(a, b, s):
-  return s.translate(string.maketrans(a+b, b+a))
+  return s.translate(maketrans(a + b, b + a))
 
 
 def _get_sdims(dimension_numbers):
@@ -2339,13 +2347,13 @@ def _eq_meet(a, b):
 
 def maybe_tracer_tuple_to_abstract_tuple(tup):
   if isinstance(tup, pe.JaxprTracerTuple):
-    return core.AbstractTuple(map(maybe_tracer_tuple_to_abstract_tuple, tup))
+    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
   elif isinstance(tup, core.AbstractValue):
     return tup
   elif tup is None:
     return core.AbstractTuple(())  # TODO(dougalm): check this
   else:
-    raise TypeError, tup
+    raise TypeError(tup)
 
 
 def subvals(lst, replace):","diff --git a/jax/lax.py b/jax/lax.py
index 28b580018..90cb2e707 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -15,12 +15,13 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import __builtin__
 
 import collections
 from .util import partial
 import itertools
 import operator
+import six
+from six.moves import builtins, xrange
 import string
 
 import numpy as onp
@@ -40,9 +41,14 @@ from .util import curry, safe_zip, unzip2
 from .tree_util import build_tree
 from .lib import xla_bridge
 
-_max = __builtin__.max
-_min = __builtin__.max
+_max = builtins.max
+_min = builtins.max
 
+if six.PY3:
+  def maketrans(s1, s2):
+    return s1.maketrans(s1, s2)
+else:
+  maketrans = string.maketrans
 
 ### traceables
 
@@ -383,7 +389,7 @@ def _while_loop(cond_fun, body_fun, init_val):
   params = OpaqueParam((abs_out, cond_jaxpr, cond_consts, body_jaxpr, body_consts))
   out_flat = while_p.bind(init_val_flat, opaque_params=params)
   if out_tree() != in_tree:
-    raise TypeError, ""body_fun input and output must have identical structure""
+    raise TypeError(""body_fun input and output must have identical structure"")
   return build_tree(out_tree(), out_flat)
 
 class OpaqueParam(object):
@@ -500,8 +506,9 @@ def fori_loop(lower, upper, body_fun, init_val):
   # state: (upper limit, index, loop value)
   # The `lt` and `add` functions are added to the namespace programmatically.
   _, _, result = _while_loop(
-      lambda (upper, i, _): lt(i, upper),
-      lambda (upper, i, x): (upper, add(i, 1), body_fun(i, x)),
+      lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
+      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
+                         body_fun(upper_i_x[1], upper_i_x[2])),
       (upper, lower, init_val))
   return result
 
@@ -519,7 +526,7 @@ def foreach_loop(sequence, body_fun, init_val):
   """"""
   _, result = fori_loop(
       0, len(sequence),
-      lambda i, (seq, val): body_fun(seq[i], val),
+      lambda i, seq_val: body_fun(seq_val[0][i], seq_val[1]),
       (sequence, init_val))
   return result
 
@@ -708,7 +715,7 @@ standard_binop = partial(binop, _input_dtype)
 def _brcast(x, *others):
   # used in jvprules to make binop broadcasting explicit for transposability.
   # requires shape info during jvp tracing, which isn't strictly necessary.
-  shapes = filter(None, map(onp.shape, (x,) + others))
+  shapes = list(filter(None, map(onp.shape, (x,) + others)))
   shape = tuple(shapes and onp.max(shapes, axis=0))
   if onp.shape(x) != shape:
     return _brcast_to(x, shape)
@@ -1017,8 +1024,8 @@ def conv_general_dilated_transpose_rhs(
     lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
     lhs_spec, rhs_spec, out_spec = dimension_numbers
     trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
-                              out_spec.translate(string.maketrans(""NC"", ""IO"")),
-                              rhs_spec.translate(string.maketrans(""IO"", ""NC"")))
+                               out_spec.translate(maketrans(""NC"", ""IO"")),
+                               rhs_spec.translate(maketrans(""IO"", ""NC"")))
 
   padding = _conv_general_vjp_rhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
@@ -1543,9 +1550,8 @@ def slice_shape_rule(operand, start_indices, limit_indices, strides,
       msg = ""slice strides must be positive, got {}""
       raise TypeError(msg.format(strides))
 
-  result_shape = onp.divide(onp.add(onp.subtract(limit_indices, start_indices),
-                                    strides) - 1,
-                            strides)
+  result_shape = onp.floor_divide(
+      onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
   return tuple(result_shape)
 
 def slice_translation_rule(c, operand, start_indices, limit_indices, strides,
@@ -1911,7 +1917,8 @@ def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
                               padding):
   pads = padtype_to_pads(operand_shape, window_dimensions, window_strides, padding)
   operand_padded = onp.add(operand_shape, onp.add(*zip(*pads)))
-  t = onp.divide(onp.subtract(operand_padded, window_dimensions), window_strides) + 1
+  t = onp.floor_divide(
+      onp.subtract(operand_padded, window_dimensions), window_strides) + 1
   return tuple(t)
 
 
@@ -2127,7 +2134,7 @@ def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
   """"""Check that dtypes agree, possibly ignoring float precision.""""""
   # the `ignore_fp_precision` flag exists because the XLA shape inference logic
   # allows mixed floating point precision, but the HLO verifier often rejects it
-  dtypes = map(onp.dtype, dtypes)  # canonicalize
+  dtypes = list(map(onp.dtype, dtypes))  # canonicalize
   if ignore_fp_precision:
     dtypes = [
         onp.floating if onp.issubdtype(dtype, onp.floating)
@@ -2172,7 +2179,8 @@ def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):
     raise TypeError(msg.format(len(lhs_shape) - 2, len(pads)))
 
   lhs_padded = onp.add(lhs_shape[2:], onp.add(*zip(*pads)))
-  out_space = onp.divide(onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
+  out_space = onp.floor_divide(
+      onp.subtract(lhs_padded, rhs_shape[2:]), strides) + 1
   out_space = onp.maximum(0, out_space)
   out_shape = (lhs_shape[0], rhs_shape[0]) + tuple(out_space)
   return tuple(out_shape)
@@ -2268,7 +2276,7 @@ def remaining(original, *removed_lists):
 
 
 def _charswap(a, b, s):
-  return s.translate(string.maketrans(a+b, b+a))
+  return s.translate(maketrans(a + b, b + a))
 
 
 def _get_sdims(dimension_numbers):
@@ -2339,13 +2347,13 @@ def _eq_meet(a, b):
 
 def maybe_tracer_tuple_to_abstract_tuple(tup):
   if isinstance(tup, pe.JaxprTracerTuple):
-    return core.AbstractTuple(map(maybe_tracer_tuple_to_abstract_tuple, tup))
+    return core.AbstractTuple(list(map(maybe_tracer_tuple_to_abstract_tuple, tup)))
   elif isinstance(tup, core.AbstractValue):
     return tup
   elif tup is None:
     return core.AbstractTuple(())  # TODO(dougalm): check this
   else:
-    raise TypeError, tup
+    raise TypeError(tup)
 
 
 def subvals(lst, replace):",No
jax/lax_reference.py,jax/lax_reference.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/lax_reference.py b/jax/lax_reference.py
index 4c1cb9e5d..6533f3cd9 100644
--- a/jax/lax_reference.py
+++ b/jax/lax_reference.py
@@ -15,7 +15,6 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import __builtin__
 
 import collections
 import itertools
@@ -24,10 +23,12 @@ import numpy as onp
 import opt_einsum
 import scipy.special
 
-_slice = __builtin__.slice
-_max = __builtin__.max
-_min = __builtin__.min
-_map = __builtin__.map
+from six.moves import builtins
+
+_slice = builtins.slice
+_max = builtins.max
+_min = builtins.min
+_map = builtins.map
 
 neg = onp.negative
 sign = onp.sign
@@ -87,13 +88,13 @@ sub = onp.subtract
 mul = onp.multiply
 
 def div(lhs, rhs):
-  quotient = onp.divide(lhs, rhs)
   if onp.issubdtype(onp.result_type(lhs), onp.integer):
+    quotient = onp.floor_divide(lhs, rhs)
     select = onp.logical_and(onp.sign(lhs) != onp.sign(rhs),
                              onp.remainder(lhs, rhs) != 0)
     return onp.where(select, quotient + 1, quotient)
   else:
-    return quotient
+    return onp.divide(lhs, rhs)
 
 def rem(lhs, rhs):
   return onp.sign(lhs) * onp.remainder(onp.abs(lhs), onp.abs(rhs))
@@ -150,14 +151,14 @@ dot = onp.dot
 
 def dot_general(lhs, rhs, dimension_numbers):
   (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
-  new_id = itertools.count().next
-  lhs_axis_ids = [new_id() for _ in lhs.shape]
-  rhs_axis_ids = [new_id() for _ in rhs.shape]
+  new_id = itertools.count()
+  lhs_axis_ids = [next(new_id) for _ in lhs.shape]
+  rhs_axis_ids = [next(new_id) for _ in rhs.shape]
   lhs_out_axis_ids = lhs_axis_ids[:]
   rhs_out_axis_ids = rhs_axis_ids[:]
 
   for lhs_axis, rhs_axis in zip(lhs_contracting, rhs_contracting):
-    shared_id = new_id()
+    shared_id = next(new_id)
     lhs_axis_ids[lhs_axis] = shared_id
     rhs_axis_ids[rhs_axis] = shared_id
     lhs_out_axis_ids[lhs_axis] = None
@@ -165,7 +166,7 @@ def dot_general(lhs, rhs, dimension_numbers):
 
   batch_ids = []
   for lhs_axis, rhs_axis in zip(lhs_batch, rhs_batch):
-    shared_id = new_id()
+    shared_id = next(new_id)
     lhs_axis_ids[lhs_axis] = shared_id
     rhs_axis_ids[rhs_axis] = shared_id
     lhs_out_axis_ids[lhs_axis] = None
@@ -215,7 +216,7 @@ select = onp.where
 def slice(operand, start_indices, limit_indices, strides=None):  # pylint: disable=redefined-builtin
   if strides is None:
     strides = onp.ones(len(start_indices)).astype(int)
-  slices = _map(_slice, start_indices, limit_indices, strides)
+  slices = tuple(_map(_slice, start_indices, limit_indices, strides))
   return operand[slices]
 
 def dynamic_slice(operand, start_indices, slice_sizes):
@@ -227,7 +228,7 @@ def dynamic_slice(operand, start_indices, slice_sizes):
   return out
 
 def dynamic_update_slice(operand, update, start_indices):
-  slices = _map(_slice, start_indices, onp.add(start_indices, update.shape))
+  slices = tuple(_map(_slice, start_indices, onp.add(start_indices, update.shape)))
   updated_operand = onp.copy(operand)
   updated_operand[slices] = update
   return updated_operand
@@ -295,8 +296,8 @@ def _conv_view(lhs, rhs_shape, window_strides, pads, pad_value):
   out_strides = onp.multiply(window_strides, lhs.strides[2:])
   view_strides = lhs.strides[:1] + tuple(out_strides) + lhs.strides[1:]
 
-  out_shape = onp.divide(onp.subtract(in_shape, filter_shape),
-                         window_strides) + 1
+  out_shape = onp.floor_divide(
+      onp.subtract(in_shape, filter_shape), window_strides) + 1
   view_shape = lhs.shape[:1] + tuple(out_shape) + rhs_shape[1:]
 
   view = onp.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)","diff --git a/jax/lax_reference.py b/jax/lax_reference.py
index 4c1cb9e5d..6533f3cd9 100644
--- a/jax/lax_reference.py
+++ b/jax/lax_reference.py
@@ -15,7 +15,6 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import __builtin__
 
 import collections
 import itertools
@@ -24,10 +23,12 @@ import numpy as onp
 import opt_einsum
 import scipy.special
 
-_slice = __builtin__.slice
-_max = __builtin__.max
-_min = __builtin__.min
-_map = __builtin__.map
+from six.moves import builtins
+
+_slice = builtins.slice
+_max = builtins.max
+_min = builtins.min
+_map = builtins.map
 
 neg = onp.negative
 sign = onp.sign
@@ -87,13 +88,13 @@ sub = onp.subtract
 mul = onp.multiply
 
 def div(lhs, rhs):
-  quotient = onp.divide(lhs, rhs)
   if onp.issubdtype(onp.result_type(lhs), onp.integer):
+    quotient = onp.floor_divide(lhs, rhs)
     select = onp.logical_and(onp.sign(lhs) != onp.sign(rhs),
                              onp.remainder(lhs, rhs) != 0)
     return onp.where(select, quotient + 1, quotient)
   else:
-    return quotient
+    return onp.divide(lhs, rhs)
 
 def rem(lhs, rhs):
   return onp.sign(lhs) * onp.remainder(onp.abs(lhs), onp.abs(rhs))
@@ -150,14 +151,14 @@ dot = onp.dot
 
 def dot_general(lhs, rhs, dimension_numbers):
   (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers
-  new_id = itertools.count().next
-  lhs_axis_ids = [new_id() for _ in lhs.shape]
-  rhs_axis_ids = [new_id() for _ in rhs.shape]
+  new_id = itertools.count()
+  lhs_axis_ids = [next(new_id) for _ in lhs.shape]
+  rhs_axis_ids = [next(new_id) for _ in rhs.shape]
   lhs_out_axis_ids = lhs_axis_ids[:]
   rhs_out_axis_ids = rhs_axis_ids[:]
 
   for lhs_axis, rhs_axis in zip(lhs_contracting, rhs_contracting):
-    shared_id = new_id()
+    shared_id = next(new_id)
     lhs_axis_ids[lhs_axis] = shared_id
     rhs_axis_ids[rhs_axis] = shared_id
     lhs_out_axis_ids[lhs_axis] = None
@@ -165,7 +166,7 @@ def dot_general(lhs, rhs, dimension_numbers):
 
   batch_ids = []
   for lhs_axis, rhs_axis in zip(lhs_batch, rhs_batch):
-    shared_id = new_id()
+    shared_id = next(new_id)
     lhs_axis_ids[lhs_axis] = shared_id
     rhs_axis_ids[rhs_axis] = shared_id
     lhs_out_axis_ids[lhs_axis] = None
@@ -215,7 +216,7 @@ select = onp.where
 def slice(operand, start_indices, limit_indices, strides=None):  # pylint: disable=redefined-builtin
   if strides is None:
     strides = onp.ones(len(start_indices)).astype(int)
-  slices = _map(_slice, start_indices, limit_indices, strides)
+  slices = tuple(_map(_slice, start_indices, limit_indices, strides))
   return operand[slices]
 
 def dynamic_slice(operand, start_indices, slice_sizes):
@@ -227,7 +228,7 @@ def dynamic_slice(operand, start_indices, slice_sizes):
   return out
 
 def dynamic_update_slice(operand, update, start_indices):
-  slices = _map(_slice, start_indices, onp.add(start_indices, update.shape))
+  slices = tuple(_map(_slice, start_indices, onp.add(start_indices, update.shape)))
   updated_operand = onp.copy(operand)
   updated_operand[slices] = update
   return updated_operand
@@ -295,8 +296,8 @@ def _conv_view(lhs, rhs_shape, window_strides, pads, pad_value):
   out_strides = onp.multiply(window_strides, lhs.strides[2:])
   view_strides = lhs.strides[:1] + tuple(out_strides) + lhs.strides[1:]
 
-  out_shape = onp.divide(onp.subtract(in_shape, filter_shape),
-                         window_strides) + 1
+  out_shape = onp.floor_divide(
+      onp.subtract(in_shape, filter_shape), window_strides) + 1
   view_shape = lhs.shape[:1] + tuple(out_shape) + rhs_shape[1:]
 
   view = onp.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 6ec5e8fb0..68a244206 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -112,10 +112,10 @@ def get_xla_client():
     xla_client.initialize_platform_name(FLAGS.jax_platform_name)
   else:
     try:
-      xla_client.initialize_platform_name('CUDA')
+      xla_client.initialize_platform_name(b'CUDA')
     except RuntimeError:
       warnings.warn('No GPU found, falling back to CPU.')
-      xla_client.initialize_platform_name('Host')
+      xla_client.initialize_platform_name(b'Host')
   return xla_client
 
 ","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 6ec5e8fb0..68a244206 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -112,10 +112,10 @@ def get_xla_client():
     xla_client.initialize_platform_name(FLAGS.jax_platform_name)
   else:
     try:
-      xla_client.initialize_platform_name('CUDA')
+      xla_client.initialize_platform_name(b'CUDA')
     except RuntimeError:
       warnings.warn('No GPU found, falling back to CPU.')
-      xla_client.initialize_platform_name('Host')
+      xla_client.initialize_platform_name(b'Host')
   return xla_client
 
 ",No
jax/linear_util.py,jax/linear_util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/linear_util.py b/jax/linear_util.py
index 3d6e11780..e5f655c8b 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -42,6 +42,8 @@ class Store(object):
   def __nonzero__(self):
     return hasattr(self, '_val')
 
+  __bool__ = __nonzero__
+
 
 @curry
 def staged(f, *init_args):
@@ -74,7 +76,7 @@ class WrappedFun(object):
     stack = []
     for gen, gen_args, out_store in self.transforms:
       gen = gen(*(gen_args + tuple(args)))
-      args = gen.next()
+      args = next(gen)
       stack.append((gen, out_store))
 
     del gen","diff --git a/jax/linear_util.py b/jax/linear_util.py
index 3d6e11780..e5f655c8b 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -42,6 +42,8 @@ class Store(object):
   def __nonzero__(self):
     return hasattr(self, '_val')
 
+  __bool__ = __nonzero__
+
 
 @curry
 def staged(f, *init_args):
@@ -74,7 +76,7 @@ class WrappedFun(object):
     stack = []
     for gen, gen_args, out_store in self.transforms:
       gen = gen(*(gen_args + tuple(args)))
-      args = gen.next()
+      args = next(gen)
       stack.append((gen, out_store))
 
     del gen",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 71019b420..044f7be12 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -15,7 +15,8 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import __builtin__
+
+from six.moves import builtins
 
 import six
 import numpy as onp
@@ -38,11 +39,11 @@ import jax.lax as lax
 
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
-_all = __builtin__.all
-_any = __builtin__.any
-_max = __builtin__.max
-_min = __builtin__.min
-_sum = __builtin__.sum
+_all = builtins.all
+_any = builtins.any
+_max = builtins.max
+_min = builtins.min
+_sum = builtins.sum
 
 # We need some numpy scalars
 # TODO(mattjj): handle constants in an indirected, less explicit way?
@@ -985,7 +986,7 @@ def _canonicalize_tuple_index(arr, idx):
   if ellipsis_index is not None:
     if next(ellipses, None) is not None:
       msg = ""Multiple ellipses (...) not supported: {}.""
-      raise IndexError(msg.format(map(type, idx)))
+      raise IndexError(msg.format(list(map(type, idx))))
     colons = (slice(None),) * (arr.ndim - len_without_none)
     idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
   elif len_without_none < arr.ndim:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 71019b420..044f7be12 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -15,7 +15,8 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import __builtin__
+
+from six.moves import builtins
 
 import six
 import numpy as onp
@@ -38,11 +39,11 @@ import jax.lax as lax
 
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
-_all = __builtin__.all
-_any = __builtin__.any
-_max = __builtin__.max
-_min = __builtin__.min
-_sum = __builtin__.sum
+_all = builtins.all
+_any = builtins.any
+_max = builtins.max
+_min = builtins.min
+_sum = builtins.sum
 
 # We need some numpy scalars
 # TODO(mattjj): handle constants in an indirected, less explicit way?
@@ -985,7 +986,7 @@ def _canonicalize_tuple_index(arr, idx):
   if ellipsis_index is not None:
     if next(ellipses, None) is not None:
       msg = ""Multiple ellipses (...) not supported: {}.""
-      raise IndexError(msg.format(map(type, idx)))
+      raise IndexError(msg.format(list(map(type, idx))))
     colons = (slice(None),) * (arr.ndim - len_without_none)
     idx = idx[:ellipsis_index] + colons + idx[ellipsis_index + 1:]
   elif len_without_none < arr.ndim:",No
jax/pprint_util.py,jax/pprint_util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/pprint_util.py b/jax/pprint_util.py
index 7a90057a0..328552519 100644
--- a/jax/pprint_util.py
+++ b/jax/pprint_util.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from six.moves import reduce
+
 
 class PrettyPrint(object):
   """"""Crude Hughes-inspired pretty printer.""""""","diff --git a/jax/pprint_util.py b/jax/pprint_util.py
index 7a90057a0..328552519 100644
--- a/jax/pprint_util.py
+++ b/jax/pprint_util.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from six.moves import reduce
+
 
 class PrettyPrint(object):
   """"""Crude Hughes-inspired pretty printer.""""""",No
jax/random.py,jax/random.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/random.py b/jax/random.py
index e458516de..0bed84801 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -58,7 +58,7 @@ class PRNGKey(object):
   def from_keypair(cls, keypair):
     """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
     new = cls.__new__(cls)
-    new.keypair = keypair
+    new.keypair = tuple(keypair)
     return new
 
 
@@ -83,7 +83,7 @@ def _make_rotate_left(dtype):
 
 def _bit_stats(bits):
   """"""This is a debugging function to compute the statistics of bit fields.""""""
-  return onp.array([map(int, onp.binary_repr(x, 64)) for x in bits]).mean(0)
+  return onp.array([list(map(int, onp.binary_repr(x, 64))) for x in bits]).mean(0)
 
 
 ### hash function and split","diff --git a/jax/random.py b/jax/random.py
index e458516de..0bed84801 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -58,7 +58,7 @@ class PRNGKey(object):
   def from_keypair(cls, keypair):
     """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
     new = cls.__new__(cls)
-    new.keypair = keypair
+    new.keypair = tuple(keypair)
     return new
 
 
@@ -83,7 +83,7 @@ def _make_rotate_left(dtype):
 
 def _bit_stats(bits):
   """"""This is a debugging function to compute the statistics of bit fields.""""""
-  return onp.array([map(int, onp.binary_repr(x, 64)) for x in bits]).mean(0)
+  return onp.array([list(map(int, onp.binary_repr(x, 64))) for x in bits]).mean(0)
 
 
 ### hash function and split",No
jax/test_util.py,jax/test_util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/test_util.py b/jax/test_util.py
index fb30ea49c..b3de8cd5f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -15,6 +15,7 @@
 from __future__ import absolute_import
 
 import functools
+import re
 
 from absl import flags
 from absl.testing import absltest
@@ -277,6 +278,13 @@ def check_raises(thunk, err_type, msg):
   except err_type as e:
     assert str(e) == msg, ""{}\n\n{}\n"".format(e, msg)
 
+def check_raises_regexp(thunk, err_type, pattern):
+  try:
+    thunk()
+    assert False
+  except err_type as e:
+    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
+
 
 class JaxTestCase(parameterized.TestCase):
   """"""Base class for JAX tests including numerical checks and boilerplate.""""""","diff --git a/jax/test_util.py b/jax/test_util.py
index fb30ea49c..b3de8cd5f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -15,6 +15,7 @@
 from __future__ import absolute_import
 
 import functools
+import re
 
 from absl import flags
 from absl.testing import absltest
@@ -277,6 +278,13 @@ def check_raises(thunk, err_type, msg):
   except err_type as e:
     assert str(e) == msg, ""{}\n\n{}\n"".format(e, msg)
 
+def check_raises_regexp(thunk, err_type, pattern):
+  try:
+    thunk()
+    assert False
+  except err_type as e:
+    assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
+
 
 class JaxTestCase(parameterized.TestCase):
   """"""Base class for JAX tests including numerical checks and boilerplate.""""""",No
jax/tree_util.py,jax/tree_util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/tree_util.py b/jax/tree_util.py
index e5227c58d..3f2a0a081 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -16,8 +16,11 @@ from __future__ import absolute_import
 
 from collections import namedtuple
 import itertools as it
+from six.moves import reduce
 
-from .util import unzip2, concatenate, partial
+from .util import unzip2, concatenate, partial, safe_map
+
+map = safe_map
 
 
 def tree_map(f, tree):","diff --git a/jax/tree_util.py b/jax/tree_util.py
index e5227c58d..3f2a0a081 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -16,8 +16,11 @@ from __future__ import absolute_import
 
 from collections import namedtuple
 import itertools as it
+from six.moves import reduce
 
-from .util import unzip2, concatenate, partial
+from .util import unzip2, concatenate, partial, safe_map
+
+map = safe_map
 
 
 def tree_map(f, tree):",No
jax/util.py,jax/util.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/jax/util.py b/jax/util.py
index c7e7235e4..3b19a8cb6 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -24,16 +24,16 @@ allow_memoize_hash_failures = False
 def safe_zip(*args):
   n = len(args[0])
   for arg in args[1:]:
-    assert len(arg) == n, 'length mismatch: {}'.format(map(len, args))
-  return zip(*args)
+    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
+  return list(zip(*args))
 
 
 def safe_map(f, *args):
-  args = map(list, args)
+  args = list(map(list, args))
   n = len(args[0])
   for arg in args[1:]:
-    assert len(arg) == n, 'length mismatch: {}'.format(map(len, args))
-  return map(f, *args)
+    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
+  return list(map(f, *args))
 
 
 def unzip2(xys):
@@ -82,10 +82,10 @@ def toposort(end_node):
   stack = [end_node]
   while stack:
     node = stack.pop()
-    if node in child_counts:
-      child_counts[node] += 1
+    if id(node) in child_counts:
+      child_counts[id(node)] += 1
     else:
-      child_counts[node] = 1
+      child_counts[id(node)] = 1
       stack.extend(node.parents)
 
   sorted_nodes = []
@@ -94,16 +94,16 @@ def toposort(end_node):
     node = childless_nodes.pop()
     sorted_nodes.append(node)
     for parent in node.parents:
-      if child_counts[parent] == 1:
+      if child_counts[id(parent)] == 1:
         childless_nodes.append(parent)
       else:
-        child_counts[parent] -= 1
+        child_counts[id(parent)] -= 1
 
   return sorted_nodes[::-1]
 
 
 def split_merge(predicate, xs):
-  sides = map(predicate, xs)
+  sides = list(map(predicate, xs))
   lhs = [x for x, s in zip(xs, sides) if s]
   rhs = [x for x, s in zip(xs, sides) if not s]
   def merge(new_lhs, new_rhs):","diff --git a/jax/util.py b/jax/util.py
index c7e7235e4..3b19a8cb6 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -24,16 +24,16 @@ allow_memoize_hash_failures = False
 def safe_zip(*args):
   n = len(args[0])
   for arg in args[1:]:
-    assert len(arg) == n, 'length mismatch: {}'.format(map(len, args))
-  return zip(*args)
+    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
+  return list(zip(*args))
 
 
 def safe_map(f, *args):
-  args = map(list, args)
+  args = list(map(list, args))
   n = len(args[0])
   for arg in args[1:]:
-    assert len(arg) == n, 'length mismatch: {}'.format(map(len, args))
-  return map(f, *args)
+    assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args)))
+  return list(map(f, *args))
 
 
 def unzip2(xys):
@@ -82,10 +82,10 @@ def toposort(end_node):
   stack = [end_node]
   while stack:
     node = stack.pop()
-    if node in child_counts:
-      child_counts[node] += 1
+    if id(node) in child_counts:
+      child_counts[id(node)] += 1
     else:
-      child_counts[node] = 1
+      child_counts[id(node)] = 1
       stack.extend(node.parents)
 
   sorted_nodes = []
@@ -94,16 +94,16 @@ def toposort(end_node):
     node = childless_nodes.pop()
     sorted_nodes.append(node)
     for parent in node.parents:
-      if child_counts[parent] == 1:
+      if child_counts[id(parent)] == 1:
         childless_nodes.append(parent)
       else:
-        child_counts[parent] -= 1
+        child_counts[id(parent)] -= 1
 
   return sorted_nodes[::-1]
 
 
 def split_merge(predicate, xs):
-  sides = map(predicate, xs)
+  sides = list(map(predicate, xs))
   lhs = [x for x, s in zip(xs, sides) if s]
   rhs = [x for x, s in zip(xs, sides) if not s]
   def merge(new_lhs, new_rhs):",No
tests/api_test.py,tests/api_test.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/tests/api_test.py b/tests/api_test.py
index 00adfbae2..c3d98166e 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -16,6 +16,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import six
+
 import numpy as onp
 from absl.testing import absltest
 from jax import test_util as jtu
@@ -102,11 +104,11 @@ class APITest(jtu.JaxTestCase):
     def f(x):
       return x
 
-    jtu.check_raises(lambda: grad(f)(""foo""), TypeError,
-                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+    jtu.check_raises_regexp(lambda: grad(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <.*'str'> is not a valid JAX type"")
 
-    jtu.check_raises(lambda: jit(f)(""foo""), TypeError,
-                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+    jtu.check_raises_regexp(lambda: jit(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <.*'str'> is not a valid JAX type"")
 
   # TODO(dougalm): enable when we remove 'None' from pytree nodes
   # def test_bad_output(self):
@@ -176,12 +178,18 @@ class APITest(jtu.JaxTestCase):
       return x
 
     assert jit(f, static_argnums=(1,))(0, 5) == 10
-    jtu.check_raises(lambda: jit(f)(0, 5), TypeError, concretization_err_msg(int))
+    jtu.check_raises_regexp(
+        lambda: jit(f)(0, 5), TypeError,
+        ""('JaxprTracer' object cannot be interpreted as an integer""
+        ""|Abstract value passed to function.*)"")
 
   def test_casts(self):
-    for castfun in [float, int, long, complex, hex, oct]:
+    for castfun in [float, complex, hex, oct] + list(six.integer_types):
       f = lambda x: castfun(x)
-      jtu.check_raises(lambda: jit(f)(0), TypeError, concretization_err_msg(castfun))
+      jtu.check_raises_regexp(
+          lambda: jit(f)(0), TypeError,
+          ""('JaxprTracer' object cannot be interpreted as an integer""
+          ""|Abstract value passed to function.*)"")
 
   def test_unimplemented_interpreter_rules(self):
     foo_p = Primitive('foo')","diff --git a/tests/api_test.py b/tests/api_test.py
index 00adfbae2..c3d98166e 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -16,6 +16,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import six
+
 import numpy as onp
 from absl.testing import absltest
 from jax import test_util as jtu
@@ -102,11 +104,11 @@ class APITest(jtu.JaxTestCase):
     def f(x):
       return x
 
-    jtu.check_raises(lambda: grad(f)(""foo""), TypeError,
-                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+    jtu.check_raises_regexp(lambda: grad(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <.*'str'> is not a valid JAX type"")
 
-    jtu.check_raises(lambda: jit(f)(""foo""), TypeError,
-                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+    jtu.check_raises_regexp(lambda: jit(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <.*'str'> is not a valid JAX type"")
 
   # TODO(dougalm): enable when we remove 'None' from pytree nodes
   # def test_bad_output(self):
@@ -176,12 +178,18 @@ class APITest(jtu.JaxTestCase):
       return x
 
     assert jit(f, static_argnums=(1,))(0, 5) == 10
-    jtu.check_raises(lambda: jit(f)(0, 5), TypeError, concretization_err_msg(int))
+    jtu.check_raises_regexp(
+        lambda: jit(f)(0, 5), TypeError,
+        ""('JaxprTracer' object cannot be interpreted as an integer""
+        ""|Abstract value passed to function.*)"")
 
   def test_casts(self):
-    for castfun in [float, int, long, complex, hex, oct]:
+    for castfun in [float, complex, hex, oct] + list(six.integer_types):
       f = lambda x: castfun(x)
-      jtu.check_raises(lambda: jit(f)(0), TypeError, concretization_err_msg(castfun))
+      jtu.check_raises_regexp(
+          lambda: jit(f)(0), TypeError,
+          ""('JaxprTracer' object cannot be interpreted as an integer""
+          ""|Abstract value passed to function.*)"")
 
   def test_unimplemented_interpreter_rules(self):
     foo_p = Primitive('foo')",No
tests/random_test.py,tests/random_test.py,e180f081131cdf821247f4213170c6c52ad5b8b9,fe4edf2839b7ec650b1aac289db69669537a9540,"source sync

PiperOrigin-RevId: 222451919","diff --git a/tests/random_test.py b/tests/random_test.py
index 128b599b4..c5ac91106 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -65,19 +65,23 @@ class LaxRandomTest(jtu.JaxTestCase):
     # We test the hash by comparing to known values provided in the test code of
     # the original reference implementation of Threefry. For the values, see
     # https://github.com/DEShawResearch/Random123-Boost/blob/65e3d874b67aa7b3e02d5ad8306462f52d2079c0/libs/random/test/test_threefry.cpp#L30-L32
-    expected = (""0x6b200159L"", ""0x99ba4efeL"")
+    def result_to_hex(result):
+      return tuple([hex(x.copy()).rstrip(""L"") for x in result])
+
+    expected = (""0x6b200159"", ""0x99ba4efe"")
     result = random.threefry_2x32(onp.uint32([0, 0]), onp.uint32([0, 0]))
-    self.assertEqual(expected, tuple(map(hex, result)))
 
-    expected = (""0x1cb996fcL"", ""0xbb002be7L"")
+    self.assertEqual(expected, result_to_hex(result))
+
+    expected = (""0x1cb996fc"", ""0xbb002be7"")
     result = random.threefry_2x32(onp.uint32([-1, -1]), onp.uint32([-1, -1]))
-    self.assertEqual(expected, tuple(map(hex, result)))
+    self.assertEqual(expected, result_to_hex(result))
 
-    expected = (""0xc4923a9cL"", ""0x483df7a0L"")
+    expected = (""0xc4923a9c"", ""0x483df7a0"")
     result = random.threefry_2x32(
         onp.uint32([0x13198a2e, 0x03707344]),
         onp.uint32([0x243f6a88, 0x85a308d3]))
-    self.assertEqual(expected, tuple(map(hex, result)))
+    self.assertEqual(expected, result_to_hex(result))
 
   @parameterized.named_parameters(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}","diff --git a/tests/random_test.py b/tests/random_test.py
index 128b599b4..c5ac91106 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -65,19 +65,23 @@ class LaxRandomTest(jtu.JaxTestCase):
     # We test the hash by comparing to known values provided in the test code of
     # the original reference implementation of Threefry. For the values, see
     # https://github.com/DEShawResearch/Random123-Boost/blob/65e3d874b67aa7b3e02d5ad8306462f52d2079c0/libs/random/test/test_threefry.cpp#L30-L32
-    expected = (""0x6b200159L"", ""0x99ba4efeL"")
+    def result_to_hex(result):
+      return tuple([hex(x.copy()).rstrip(""L"") for x in result])
+
+    expected = (""0x6b200159"", ""0x99ba4efe"")
     result = random.threefry_2x32(onp.uint32([0, 0]), onp.uint32([0, 0]))
-    self.assertEqual(expected, tuple(map(hex, result)))
 
-    expected = (""0x1cb996fcL"", ""0xbb002be7L"")
+    self.assertEqual(expected, result_to_hex(result))
+
+    expected = (""0x1cb996fc"", ""0xbb002be7"")
     result = random.threefry_2x32(onp.uint32([-1, -1]), onp.uint32([-1, -1]))
-    self.assertEqual(expected, tuple(map(hex, result)))
+    self.assertEqual(expected, result_to_hex(result))
 
-    expected = (""0xc4923a9cL"", ""0x483df7a0L"")
+    expected = (""0xc4923a9c"", ""0x483df7a0"")
     result = random.threefry_2x32(
         onp.uint32([0x13198a2e, 0x03707344]),
         onp.uint32([0x243f6a88, 0x85a308d3]))
-    self.assertEqual(expected, tuple(map(hex, result)))
+    self.assertEqual(expected, result_to_hex(result))
 
   @parameterized.named_parameters(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}",No
jax/abstract_arrays.py,jax/abstract_arrays.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index d37aac013..ff9944122 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import numpy as onp
 import six","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index d37aac013..ff9944122 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import numpy as onp
 import six",No
jax/ad_util.py,jax/ad_util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/ad_util.py b/jax/ad_util.py
index 806eefc49..f5c4f7a98 100644
--- a/jax/ad_util.py
+++ b/jax/ad_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from .core import JaxTuple, lattice_join
 from .interpreters.partial_eval import Primitive","diff --git a/jax/ad_util.py b/jax/ad_util.py
index 806eefc49..f5c4f7a98 100644
--- a/jax/ad_util.py
+++ b/jax/ad_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from .core import JaxTuple, lattice_join
 from .interpreters.partial_eval import Primitive",No
jax/api.py,jax/api.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/api.py b/jax/api.py
index f0270d9f2..8297d77d4 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import itertools
 ","diff --git a/jax/api.py b/jax/api.py
index f0270d9f2..8297d77d4 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import itertools
 ",No
jax/api_util.py,jax/api_util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/api_util.py b/jax/api_util.py
index c2ee774d8..742910697 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from .core import pack
 from .tree_util import build_tree, process_pytree","diff --git a/jax/api_util.py b/jax/api_util.py
index c2ee774d8..742910697 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from .core import pack
 from .tree_util import build_tree, process_pytree",No
jax/core.py,jax/core.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/core.py b/jax/core.py
index 4d92578d5..1a984affc 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from operator import attrgetter
 from contextlib import contextmanager","diff --git a/jax/core.py b/jax/core.py
index 4d92578d5..1a984affc 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from operator import attrgetter
 from contextlib import contextmanager",No
jax/experimental/lapax.py,jax/experimental/lapax.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/experimental/lapax.py b/jax/experimental/lapax.py
index 3a3ab09d8..bd1857b72 100644
--- a/jax/experimental/lapax.py
+++ b/jax/experimental/lapax.py
@@ -13,7 +13,9 @@
 # limitations under the License.
 
 """"""A linear algebra library for use with JAX.""""""
+
 from __future__ import absolute_import
+from __future__ import division
 from __future__ import print_function
 
 import numpy as onp","diff --git a/jax/experimental/lapax.py b/jax/experimental/lapax.py
index 3a3ab09d8..bd1857b72 100644
--- a/jax/experimental/lapax.py
+++ b/jax/experimental/lapax.py
@@ -13,7 +13,9 @@
 # limitations under the License.
 
 """"""A linear algebra library for use with JAX.""""""
+
 from __future__ import absolute_import
+from __future__ import division
 from __future__ import print_function
 
 import numpy as onp",No
jax/interpreters/ad.py,jax/interpreters/ad.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 0f8c512a5..10c8af15f 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from . import partial_eval as pe
 from . import xla","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 0f8c512a5..10c8af15f 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from . import partial_eval as pe
 from . import xla",No
jax/interpreters/batching.py,jax/interpreters/batching.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index ab567dde1..e9645f281 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from collections import namedtuple
 
 import itertools as it","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index ab567dde1..e9645f281 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from collections import namedtuple
 
 import itertools as it",No
jax/interpreters/partial_eval.py,jax/interpreters/partial_eval.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index f152e863d..ad4834f07 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import itertools as it
 from collections import namedtuple, Counter, defaultdict","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index f152e863d..ad4834f07 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import itertools as it
 from collections import namedtuple, Counter, defaultdict",No
jax/interpreters/xla.py,jax/interpreters/xla.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 6e5e2bd4d..cb11313be 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from collections import namedtuple
 import itertools as it","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 6e5e2bd4d..cb11313be 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from collections import namedtuple
 import itertools as it",No
jax/linear_util.py,jax/linear_util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/linear_util.py b/jax/linear_util.py
index e5f655c8b..857f3bce6 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from .util import curry, partial
 ","diff --git a/jax/linear_util.py b/jax/linear_util.py
index e5f655c8b..857f3bce6 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from .util import curry, partial
 ",No
jax/pprint_util.py,jax/pprint_util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/pprint_util.py b/jax/pprint_util.py
index 328552519..62e2afb53 100644
--- a/jax/pprint_util.py
+++ b/jax/pprint_util.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from six.moves import reduce
 
 ","diff --git a/jax/pprint_util.py b/jax/pprint_util.py
index 328552519..62e2afb53 100644
--- a/jax/pprint_util.py
+++ b/jax/pprint_util.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from six.moves import reduce
 
 ",No
jax/test_util.py,jax/test_util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/test_util.py b/jax/test_util.py
index b3de8cd5f..790230c04 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import functools
 import re","diff --git a/jax/test_util.py b/jax/test_util.py
index b3de8cd5f..790230c04 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import functools
 import re",No
jax/tree_util.py,jax/tree_util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/tree_util.py b/jax/tree_util.py
index 3f2a0a081..5c210e0b4 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from collections import namedtuple
 import itertools as it","diff --git a/jax/tree_util.py b/jax/tree_util.py
index 3f2a0a081..5c210e0b4 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 from collections import namedtuple
 import itertools as it",No
jax/util.py,jax/util.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/jax/util.py b/jax/util.py
index 3b19a8cb6..4ea75bbf9 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import functools
 import itertools as it","diff --git a/jax/util.py b/jax/util.py
index 3b19a8cb6..4ea75bbf9 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import functools
 import itertools as it",No
tests/batching_test.py,tests/batching_test.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/tests/batching_test.py b/tests/batching_test.py
index edc3c2672..18034718b 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import numpy as onp
 from absl.testing import absltest","diff --git a/tests/batching_test.py b/tests/batching_test.py
index edc3c2672..18034718b 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -13,6 +13,8 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
 import numpy as onp
 from absl.testing import absltest",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,e180f081131cdf821247f4213170c6c52ad5b8b9,"source sync

PiperOrigin-RevId: 222452709","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index eb3329dd9..6b4db39dd 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import collections
 from functools import partial
 import itertools","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index eb3329dd9..6b4db39dd 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import collections
 from functools import partial
 import itertools",No
tests/BUILD,tests/BUILD,6361b784a83e8c47fd77358fca04787178b38a40,5e60639bc58eaa4a9ea2baa9f9e1fd74d355a4ac,"source sync

PiperOrigin-RevId: 222456068","diff --git a/tests/BUILD b/tests/BUILD
index 3317f5017..95d6ee29e 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -14,7 +14,7 @@
 
 licenses([""notice""])  # Apache 2
 
-load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
+load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",","diff --git a/tests/BUILD b/tests/BUILD
index 3317f5017..95d6ee29e 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -14,7 +14,7 @@
 
 licenses([""notice""])  # Apache 2
 
-load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
+load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",",No
jax/core.py,jax/core.py,2ae9a2bc352b708468d0bf89894db2256adc3d93,6361b784a83e8c47fd77358fca04787178b38a40,"source sync

PiperOrigin-RevId: 222461242","diff --git a/jax/core.py b/jax/core.py
index 1a984affc..9564985f6 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -221,6 +221,7 @@ class Tracer(object):
   def __div__(self, other): return self.aval._div(self, other)
   def __rdiv__(self, other): return self.aval._rdiv(self, other)
   def __truediv__(self, other): return self.aval._truediv(self, other)
+  def __rtruediv__(self, other): return self.aval._rtruediv(self, other)
   def __floordiv__(self, other): return self.aval._floordiv(self, other)
   def __rfloordiv__(self, other): return self.aval._rfloordiv(self, other)
   def __divmod__(self, other): return self.aval._divmod(self, other)","diff --git a/jax/core.py b/jax/core.py
index 1a984affc..9564985f6 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -221,6 +221,7 @@ class Tracer(object):
   def __div__(self, other): return self.aval._div(self, other)
   def __rdiv__(self, other): return self.aval._rdiv(self, other)
   def __truediv__(self, other): return self.aval._truediv(self, other)
+  def __rtruediv__(self, other): return self.aval._rtruediv(self, other)
   def __floordiv__(self, other): return self.aval._floordiv(self, other)
   def __rfloordiv__(self, other): return self.aval._rfloordiv(self, other)
   def __divmod__(self, other): return self.aval._divmod(self, other)",No
jax/random.py,jax/random.py,2ae9a2bc352b708468d0bf89894db2256adc3d93,6361b784a83e8c47fd77358fca04787178b38a40,"source sync

PiperOrigin-RevId: 222461242","diff --git a/jax/random.py b/jax/random.py
index 0bed84801..e07b89513 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -227,7 +227,9 @@ def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
       lax.shift_right_logical(bits, onp.array(nbits - nmant, lax._dtype(bits))),
       onp.array(1., dtype).view(onp.uint32 if nbits == 32 else onp.uint64))
   floats = lax.bitcast_convert_type(float_bits, dtype) - onp.array(1., dtype)
-  return lax.reshape(floats * (maxval - minval) + minval, shape)
+  return lax.max(
+      minval,
+      lax.reshape(floats * (maxval - minval) + minval, shape))
 
 
 def randint(key, shape, minval, maxval, dtype=onp.int32):","diff --git a/jax/random.py b/jax/random.py
index 0bed84801..e07b89513 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -227,7 +227,9 @@ def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
       lax.shift_right_logical(bits, onp.array(nbits - nmant, lax._dtype(bits))),
       onp.array(1., dtype).view(onp.uint32 if nbits == 32 else onp.uint64))
   floats = lax.bitcast_convert_type(float_bits, dtype) - onp.array(1., dtype)
-  return lax.reshape(floats * (maxval - minval) + minval, shape)
+  return lax.max(
+      minval,
+      lax.reshape(floats * (maxval - minval) + minval, shape))
 
 
 def randint(key, shape, minval, maxval, dtype=onp.int32):",No
jax/tree_util.py,jax/tree_util.py,ab53373665cd84296c375cd4403dadb23a4c5398,2ae9a2bc352b708468d0bf89894db2256adc3d93,"source sync

PiperOrigin-RevId: 222470141","diff --git a/jax/tree_util.py b/jax/tree_util.py
index 5c210e0b4..ba4e2e9f4 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -90,6 +90,14 @@ def build_tree(treedef, xs):
 
 tree_flatten = partial(walk_pytree, concatenate, lambda x: [x])
 
+def tree_unflatten(xs, treedef):
+  xs = iter(xs)
+  if treedef is leaf:
+    return next(xs)
+  else:
+    children = map(partial(tree_unflatten, xs), treedef.children)
+    return treedef.node_type.from_iterable(treedef.node_data, children)
+
 
 def tree_structure(tree):
   spec, _ = process_pytree(tree, lambda _: None)","diff --git a/jax/tree_util.py b/jax/tree_util.py
index 5c210e0b4..ba4e2e9f4 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -90,6 +90,14 @@ def build_tree(treedef, xs):
 
 tree_flatten = partial(walk_pytree, concatenate, lambda x: [x])
 
+def tree_unflatten(xs, treedef):
+  xs = iter(xs)
+  if treedef is leaf:
+    return next(xs)
+  else:
+    children = map(partial(tree_unflatten, xs), treedef.children)
+    return treedef.node_type.from_iterable(treedef.node_data, children)
+
 
 def tree_structure(tree):
   spec, _ = process_pytree(tree, lambda _: None)",No
tests/core_test.py,tests/core_test.py,ab53373665cd84296c375cd4403dadb23a4c5398,2ae9a2bc352b708468d0bf89894db2256adc3d93,"source sync

PiperOrigin-RevId: 222470141","diff --git a/tests/core_test.py b/tests/core_test.py
index a69bd4bd3..52383a6ac 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -16,6 +16,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import operator
 from collections import namedtuple
 
 import numpy as onp
@@ -28,7 +29,7 @@ from jax import numpy as np
 from jax import test_util as jtu
 from jax.api import jvp, linearize, vjp, jit
 from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
-from jax.tree_util import tree_flatten, tree_multimap
+from jax.tree_util import tree_flatten, tree_unflatten, tree_multimap, tree_reduce
 from jax.util import partial
 from jax.interpreters import partial_eval as pe
 from jax.interpreters import xla
@@ -189,6 +190,14 @@ class CoreTest(jtu.JaxTestCase):
     flat, _ = tree_flatten(({'a': 1}, [2, 3], 4))
     assert flat == [1, 2, 3, 4]
 
+  def test_tree_unflatten(self):
+    tree = [(1, 2), {""roy"": (3, [4, 5])}]
+    flat, treedef = tree_flatten(tree)
+    assert flat == [1, 2, 3, 4, 5]
+    tree2 = tree_unflatten(flat, treedef)
+    nodes_equal = tree_multimap(operator.eq, tree, tree2)
+    assert tree_reduce(operator.and_, nodes_equal)
+
   @parameterized.parameters(test_specs)
   def test_jit(self, f, args):
     jtu.check_eq(jit(f)(*args), f(*args))","diff --git a/tests/core_test.py b/tests/core_test.py
index a69bd4bd3..52383a6ac 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -16,6 +16,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import operator
 from collections import namedtuple
 
 import numpy as onp
@@ -28,7 +29,7 @@ from jax import numpy as np
 from jax import test_util as jtu
 from jax.api import jvp, linearize, vjp, jit
 from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
-from jax.tree_util import tree_flatten, tree_multimap
+from jax.tree_util import tree_flatten, tree_unflatten, tree_multimap, tree_reduce
 from jax.util import partial
 from jax.interpreters import partial_eval as pe
 from jax.interpreters import xla
@@ -189,6 +190,14 @@ class CoreTest(jtu.JaxTestCase):
     flat, _ = tree_flatten(({'a': 1}, [2, 3], 4))
     assert flat == [1, 2, 3, 4]
 
+  def test_tree_unflatten(self):
+    tree = [(1, 2), {""roy"": (3, [4, 5])}]
+    flat, treedef = tree_flatten(tree)
+    assert flat == [1, 2, 3, 4, 5]
+    tree2 = tree_unflatten(flat, treedef)
+    nodes_equal = tree_multimap(operator.eq, tree, tree2)
+    assert tree_reduce(operator.and_, nodes_equal)
+
   @parameterized.parameters(test_specs)
   def test_jit(self, f, args):
     jtu.check_eq(jit(f)(*args), f(*args))",No
jax/api.py,jax/api.py,3b3490f406c523268ec78a0ab5534cd60a36598d,ab53373665cd84296c375cd4403dadb23a4c5398,"source sync

PiperOrigin-RevId: 222483357","diff --git a/jax/api.py b/jax/api.py
index 8297d77d4..7ef6b59b1 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -24,7 +24,8 @@ from . import core
 from . import linear_util as lu
 from .core import pack, eval_jaxpr
 from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
-from .tree_util import process_pytree, node_types, build_tree, PyTreeDef, leaf
+from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
+                        tree_map)
 from .util import unzip2, unzip3, curry, partial, safe_map
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
@@ -75,9 +76,8 @@ def jacrev(fun, x):
   jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
-@curry
-def hessian(fun, x):
-  return jacfwd(jacrev(fun))(x)
+def hessian(fun):
+  return jacfwd(jacrev(fun))
 
 def vmap(fun, *args, **kwargs):
   in_bdims = kwargs.pop(""in_bdims"", 0)
@@ -159,6 +159,16 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 
+@jit
+def device_put(x):
+  return x
+
+def device_get(x):
+  def get(x):
+    return x.copy() if type(x) is xla.DeviceArray else x
+  return tree_map(get, x)
+
+
 @lu.transformation_with_aux
 def flatten_fun(in_trees, *args, **kwargs):
   py_args = map(build_tree, in_trees, args)","diff --git a/jax/api.py b/jax/api.py
index 8297d77d4..7ef6b59b1 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -24,7 +24,8 @@ from . import core
 from . import linear_util as lu
 from .core import pack, eval_jaxpr
 from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
-from .tree_util import process_pytree, node_types, build_tree, PyTreeDef, leaf
+from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
+                        tree_map)
 from .util import unzip2, unzip3, curry, partial, safe_map
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
@@ -75,9 +76,8 @@ def jacrev(fun, x):
   jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
-@curry
-def hessian(fun, x):
-  return jacfwd(jacrev(fun))(x)
+def hessian(fun):
+  return jacfwd(jacrev(fun))
 
 def vmap(fun, *args, **kwargs):
   in_bdims = kwargs.pop(""in_bdims"", 0)
@@ -159,6 +159,16 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 
+@jit
+def device_put(x):
+  return x
+
+def device_get(x):
+  def get(x):
+    return x.copy() if type(x) is xla.DeviceArray else x
+  return tree_map(get, x)
+
+
 @lu.transformation_with_aux
 def flatten_fun(in_trees, *args, **kwargs):
   py_args = map(build_tree, in_trees, args)",No
tests/api_test.py,tests/api_test.py,3b3490f406c523268ec78a0ab5534cd60a36598d,ab53373665cd84296c375cd4403dadb23a4c5398,"source sync

PiperOrigin-RevId: 222483357","diff --git a/tests/api_test.py b/tests/api_test.py
index c3d98166e..188fff8e9 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,10 +23,11 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
-from jax import jit, grad
+from jax import jit, grad, device_get, device_put
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
 from jax.interpreters.ad import defjvp
+from jax.interpreters.xla import DeviceArray
 from jax.abstract_arrays import concretization_err_msg
 
 class APITest(jtu.JaxTestCase):
@@ -216,6 +217,26 @@ class APITest(jtu.JaxTestCase):
     jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
                      ""Reverse-mode differentiation rule for 'foo' not implemented"")
 
+  def test_device_put_and_get(self):
+    x = onp.arange(12.).reshape((3, 4)).astype(""float32"")
+    dx = device_put(x)
+    assert isinstance(dx, DeviceArray)
+    x2 = device_get(dx)
+    assert isinstance(x2, onp.ndarray)
+    assert onp.all(x == x2)
+
+    y = [x, (2 * x, 3 * x)]
+    dy = device_put(y)
+    y2 = device_get(dy)
+    assert isinstance(y2, list)
+    assert isinstance(y2[0], onp.ndarray)
+    assert onp.all(y2[0] == x)
+    assert isinstance(y2[1], tuple)
+    assert isinstance(y2[1][0], onp.ndarray)
+    assert onp.all(y2[1][0] == 2 * x)
+    assert isinstance(y2[1][1], onp.ndarray)
+    assert onp.all(y2[1][1] == 3 * x)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
index c3d98166e..188fff8e9 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,10 +23,11 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
-from jax import jit, grad
+from jax import jit, grad, device_get, device_put
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
 from jax.interpreters.ad import defjvp
+from jax.interpreters.xla import DeviceArray
 from jax.abstract_arrays import concretization_err_msg
 
 class APITest(jtu.JaxTestCase):
@@ -216,6 +217,26 @@ class APITest(jtu.JaxTestCase):
     jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
                      ""Reverse-mode differentiation rule for 'foo' not implemented"")
 
+  def test_device_put_and_get(self):
+    x = onp.arange(12.).reshape((3, 4)).astype(""float32"")
+    dx = device_put(x)
+    assert isinstance(dx, DeviceArray)
+    x2 = device_get(dx)
+    assert isinstance(x2, onp.ndarray)
+    assert onp.all(x == x2)
+
+    y = [x, (2 * x, 3 * x)]
+    dy = device_put(y)
+    y2 = device_get(dy)
+    assert isinstance(y2, list)
+    assert isinstance(y2[0], onp.ndarray)
+    assert onp.all(y2[0] == x)
+    assert isinstance(y2[1], tuple)
+    assert isinstance(y2[1][0], onp.ndarray)
+    assert onp.all(y2[1][0] == 2 * x)
+    assert isinstance(y2[1][1], onp.ndarray)
+    assert onp.all(y2[1][1] == 3 * x)
+
 
 if __name__ == '__main__':
   absltest.main()",No
examples/mnist_vae.py,examples/mnist_vae.py,8317cc3618ec8bbcf7006dfe766e8c26d40a8e97,3b3490f406c523268ec78a0ab5534cd60a36598d,"source sync

PiperOrigin-RevId: 222484671","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index d3fc4680b..662616a8f 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -98,9 +98,6 @@ if __name__ == ""__main__"":
   num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
   num_batches = num_complete_batches + bool(leftover)
 
-  # TODO(mattjj): automatically keep large closed-over consts device-persistent
-  train_images = jit(lambda x: x)(train_images)  # dataset on device
-
   _, init_encoder_params = encoder_init((batch_size, 28 * 28))
   _, init_decoder_params = decoder_init((batch_size, 10))
   init_params = init_encoder_params, init_decoder_params
@@ -113,14 +110,16 @@ if __name__ == ""__main__"":
     return random.bernoulli(rng, batch)
 
   @jit
-  def run_epoch(rng, opt_state, images):
+  def run_epoch(rng, opt_state):
     def body_fun(i, (rng, opt_state, images)):
       rng, elbo_rng, data_rng = random.split(rng, 3)
       batch = binarize_batch(data_rng, i, images)
       loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
       g = grad(loss)(minmax.get_params(opt_state))
       return rng, opt_update(i, g, opt_state), images
-    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))
+    init_val = rng, opt_state, train_images
+    _, opt_state, _ =  lax.fori_loop(0, num_batches, body_fun, init_val)
+    return opt_state
 
   @jit
   def evaluate(opt_state, images):
@@ -134,10 +133,11 @@ if __name__ == ""__main__"":
   opt_state = opt_init(init_params)
   for epoch in range(num_epochs):
     tic = time.time()
-    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
-    test_elbo, images = evaluate(opt_state, test_images)
+    rng, epoch_rng = random.split(rng)
+    opt_state = run_epoch(epoch_rng, opt_state)
+    test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
-    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+    plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
 
 
 if __name__ == ""__main__"":","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index d3fc4680b..662616a8f 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -98,9 +98,6 @@ if __name__ == ""__main__"":
   num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
   num_batches = num_complete_batches + bool(leftover)
 
-  # TODO(mattjj): automatically keep large closed-over consts device-persistent
-  train_images = jit(lambda x: x)(train_images)  # dataset on device
-
   _, init_encoder_params = encoder_init((batch_size, 28 * 28))
   _, init_decoder_params = decoder_init((batch_size, 10))
   init_params = init_encoder_params, init_decoder_params
@@ -113,14 +110,16 @@ if __name__ == ""__main__"":
     return random.bernoulli(rng, batch)
 
   @jit
-  def run_epoch(rng, opt_state, images):
+  def run_epoch(rng, opt_state):
     def body_fun(i, (rng, opt_state, images)):
       rng, elbo_rng, data_rng = random.split(rng, 3)
       batch = binarize_batch(data_rng, i, images)
       loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
       g = grad(loss)(minmax.get_params(opt_state))
       return rng, opt_update(i, g, opt_state), images
-    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))
+    init_val = rng, opt_state, train_images
+    _, opt_state, _ =  lax.fori_loop(0, num_batches, body_fun, init_val)
+    return opt_state
 
   @jit
   def evaluate(opt_state, images):
@@ -134,10 +133,11 @@ if __name__ == ""__main__"":
   opt_state = opt_init(init_params)
   for epoch in range(num_epochs):
     tic = time.time()
-    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
-    test_elbo, images = evaluate(opt_state, test_images)
+    rng, epoch_rng = random.split(rng)
+    opt_state = run_epoch(epoch_rng, opt_state)
+    test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
-    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+    plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
 
 
 if __name__ == ""__main__"":",No
jax/random.py,jax/random.py,8317cc3618ec8bbcf7006dfe766e8c26d40a8e97,3b3490f406c523268ec78a0ab5534cd60a36598d,"source sync

PiperOrigin-RevId: 222484671","diff --git a/jax/random.py b/jax/random.py
index e07b89513..2805aff94 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -18,12 +18,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from functools import partial
+
 import numpy as onp
 
 from . import lax
 from . import numpy as np
 from . import tree_util
 from .lib import xla_bridge
+from .api import jit
 
 
 # TODO(mattjj): add api.jit decorators to the user-facing functions
@@ -156,6 +159,7 @@ def threefry_2x32(keypair, count):
   return lax.reshape(out[:-1] if odd_size else out, count.shape)
 
 
+@partial(jit, static_argnums=(1,))
 def split(key, num=2):
   """"""Splits a PRNG key pair of 32bit unsigned integers into `num` new key pairs.
 
@@ -192,6 +196,7 @@ def _random_bits(key, bit_width, shape):
 ### random samplers
 
 
+@partial(jit, static_argnums=(1, 2))
 def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
   """"""Sample uniform random values in [minval, maxval) with given shape/dtype.
 
@@ -232,6 +237,7 @@ def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
       lax.reshape(floats * (maxval - minval) + minval, shape))
 
 
+@partial(jit, static_argnums=(1, 4))
 def randint(key, shape, minval, maxval, dtype=onp.int32):
   """"""Sample uniform random values in [minval, maxval) with given shape/dtype.
 
@@ -280,6 +286,7 @@ def randint(key, shape, minval, maxval, dtype=onp.int32):
   return lax.add(minval, lax.convert_element_type(random_offset, dtype))
 
 
+@partial(jit, static_argnums=(2,))
 def shuffle(key, x, axis=0):
   """"""Shuffle the elements of an array uniformly at random along an axis.
 
@@ -316,6 +323,7 @@ def shuffle(key, x, axis=0):
   return x
 
 
+@partial(jit, static_argnums=(1, 2))
 def normal(key, shape, dtype=onp.float32):
   """"""Sample standard normal random values with given shape and float dtype.
 
@@ -333,6 +341,7 @@ def normal(key, shape, dtype=onp.float32):
   return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)
 
 
+@partial(jit, static_argnums=(2,))
 def bernoulli(key, mean=onp.float32(0.5), shape=()):
   """"""Sample Bernoulli random values with given shape and mean.
 ","diff --git a/jax/random.py b/jax/random.py
index e07b89513..2805aff94 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -18,12 +18,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from functools import partial
+
 import numpy as onp
 
 from . import lax
 from . import numpy as np
 from . import tree_util
 from .lib import xla_bridge
+from .api import jit
 
 
 # TODO(mattjj): add api.jit decorators to the user-facing functions
@@ -156,6 +159,7 @@ def threefry_2x32(keypair, count):
   return lax.reshape(out[:-1] if odd_size else out, count.shape)
 
 
+@partial(jit, static_argnums=(1,))
 def split(key, num=2):
   """"""Splits a PRNG key pair of 32bit unsigned integers into `num` new key pairs.
 
@@ -192,6 +196,7 @@ def _random_bits(key, bit_width, shape):
 ### random samplers
 
 
+@partial(jit, static_argnums=(1, 2))
 def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
   """"""Sample uniform random values in [minval, maxval) with given shape/dtype.
 
@@ -232,6 +237,7 @@ def uniform(key, shape, dtype=onp.float32, minval=0., maxval=1.):
       lax.reshape(floats * (maxval - minval) + minval, shape))
 
 
+@partial(jit, static_argnums=(1, 4))
 def randint(key, shape, minval, maxval, dtype=onp.int32):
   """"""Sample uniform random values in [minval, maxval) with given shape/dtype.
 
@@ -280,6 +286,7 @@ def randint(key, shape, minval, maxval, dtype=onp.int32):
   return lax.add(minval, lax.convert_element_type(random_offset, dtype))
 
 
+@partial(jit, static_argnums=(2,))
 def shuffle(key, x, axis=0):
   """"""Shuffle the elements of an array uniformly at random along an axis.
 
@@ -316,6 +323,7 @@ def shuffle(key, x, axis=0):
   return x
 
 
+@partial(jit, static_argnums=(1, 2))
 def normal(key, shape, dtype=onp.float32):
   """"""Sample standard normal random values with given shape and float dtype.
 
@@ -333,6 +341,7 @@ def normal(key, shape, dtype=onp.float32):
   return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)
 
 
+@partial(jit, static_argnums=(2,))
 def bernoulli(key, mean=onp.float32(0.5), shape=()):
   """"""Sample Bernoulli random values with given shape and mean.
 ",No
examples/BUILD,examples/BUILD,98dd8775d78f73f4a342e9161d5eb82b5a38fb6f,8317cc3618ec8bbcf7006dfe766e8c26d40a8e97,"source sync

PiperOrigin-RevId: 222484959","diff --git a/examples/BUILD b/examples/BUILD
index 63c44f98f..e8d0bf985 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -14,12 +14,6 @@
 
 licenses([""notice""])  # Apache 2
 
-py_binary(
-    name = ""interactive"",
-    srcs = [""interactive.py""],
-    deps = [""//jax:libjax""],
-)
-
 py_library(
     name = ""datasets"",
     srcs = [""datasets.py""],","diff --git a/examples/BUILD b/examples/BUILD
index 63c44f98f..e8d0bf985 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -14,12 +14,6 @@
 
 licenses([""notice""])  # Apache 2
 
-py_binary(
-    name = ""interactive"",
-    srcs = [""interactive.py""],
-    deps = [""//jax:libjax""],
-)
-
 py_library(
     name = ""datasets"",
     srcs = [""datasets.py""],",No
jax/api.py,jax/api.py,9101d66b4e22dfbdf756ad0bcc51017e64c8da14,98dd8775d78f73f4a342e9161d5eb82b5a38fb6f,"source sync

PiperOrigin-RevId: 222487460","diff --git a/jax/api.py b/jax/api.py
index 7ef6b59b1..70e290003 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -159,14 +159,9 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 
-@jit
-def device_put(x):
-  return x
-
-def device_get(x):
-  def get(x):
-    return x.copy() if type(x) is xla.DeviceArray else x
-  return tree_map(get, x)
+device_put = jit(lambda x: x)
+device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
+device_get = partial(tree_map, device_get_array)
 
 
 @lu.transformation_with_aux","diff --git a/jax/api.py b/jax/api.py
index 7ef6b59b1..70e290003 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -159,14 +159,9 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 
-@jit
-def device_put(x):
-  return x
-
-def device_get(x):
-  def get(x):
-    return x.copy() if type(x) is xla.DeviceArray else x
-  return tree_map(get, x)
+device_put = jit(lambda x: x)
+device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
+device_get = partial(tree_map, device_get_array)
 
 
 @lu.transformation_with_aux",No
examples/mnist_classifier.py,examples/mnist_classifier.py,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,f1bb77dafbbb1f6294645b0e1f43fb397b36526f,source sync fixups,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 544d58b4d..7eba6b960 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
@@ -93,7 +92,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 544d58b4d..7eba6b960 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
@@ -93,7 +92,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,f1bb77dafbbb1f6294645b0e1f43fb397b36526f,source sync fixups,"diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 6aa3a6cd9..2910b1c3b 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
@@ -93,7 +92,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 6aa3a6cd9..2910b1c3b 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
@@ -93,7 +92,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,f1bb77dafbbb1f6294645b0e1f43fb397b36526f,source sync fixups,"diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 662616a8f..80214b406 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,7 +25,6 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
@@ -138,7 +137,3 @@ if __name__ == ""__main__"":
     test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 662616a8f..80214b406 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,7 +25,6 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
@@ -138,7 +137,3 @@ if __name__ == ""__main__"":
     test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,f1bb77dafbbb1f6294645b0e1f43fb397b36526f,source sync fixups,"diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..d44f40e56 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..d44f40e56 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,2b995217b07dd1798ce1147949c07a1940f3d162,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"Explicit tuples are not valid function parameters in Python 3

[flake8](http://flake8.pycqa.org) testing of https://github.com/google/jax on Python 3.7.1

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./examples/mnist_vae.py:113:21: E999 SyntaxError: invalid syntax
    def body_fun(i, (rng, opt_state, images)):
                    ^
1     E999 SyntaxError: invalid syntax
1
```","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 80214b406..b86e76791 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -110,7 +110,8 @@ if __name__ == ""__main__"":
 
   @jit
   def run_epoch(rng, opt_state):
-    def body_fun(i, (rng, opt_state, images)):
+    def body_fun(i, rng__opt_state__images):
+      (rng, opt_state, images) = rng__opt_state__images
       rng, elbo_rng, data_rng = random.split(rng, 3)
       batch = binarize_batch(data_rng, i, images)
       loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 80214b406..b86e76791 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -110,7 +110,8 @@ if __name__ == ""__main__"":
 
   @jit
   def run_epoch(rng, opt_state):
-    def body_fun(i, (rng, opt_state, images)):
+    def body_fun(i, rng__opt_state__images):
+      (rng, opt_state, images) = rng__opt_state__images
       rng, elbo_rng, data_rng = random.split(rng, 3)
       batch = binarize_batch(data_rng, i, images)
       loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size",No
jax/interpreters/ad.py,jax/interpreters/ad.py,33292ff9621ee3023507eaf8faf6d6d8f8d3abf3,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"Undefined name: from ..core import JaxTuple

[flake8](http://flake8.pycqa.org) testing of https://github.com/google/jax on Python 3.7.1

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./jax/interpreters/ad.py:189:20: F821 undefined name 'JaxTuple'
        return xt, JaxTuple(map(zeros_like_jaxval, xt))
                   ^
./jax/interpreters/ad.py:196:16: F821 undefined name 'JaxTuple'
        return JaxTuple(map(zeros_like_jaxval, yt)), yt
               ^
2    F821 undefined name 'JaxTuple'
2
```","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f..ab08cf094 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -19,7 +19,7 @@ from __future__ import print_function
 from . import partial_eval as pe
 from . import xla
 from .. import core as core
-from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
+from ..core import JaxTuple, Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
 from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                        zeros_like_p, zero, Zero)
 from ..util import unzip2, unzip3, safe_map, safe_zip, partial","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f..ab08cf094 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -19,7 +19,7 @@ from __future__ import print_function
 from . import partial_eval as pe
 from . import xla
 from .. import core as core
-from ..core import Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
+from ..core import JaxTuple, Trace, Tracer, new_master, get_aval, pack, call_p, Primitive
 from ..ad_util import (add_jaxvals, add_jaxvals_p, zeros_like_jaxval,
                        zeros_like_p, zero, Zero)
 from ..util import unzip2, unzip3, safe_map, safe_zip, partial",No
examples/resnet50.py,examples/resnet50.py,c3c64138a4044fbe4293806874bf59162ec3f361,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"Undefined name: from six.moves import xrange

[flake8](http://flake8.pycqa.org) testing of https://github.com/google/jax on Python 3.7.1

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./examples/resnet50.py:124:12: F821 undefined name 'xrange'
  for i in xrange(num_steps):
           ^
1    F821 undefined name 'xrange'
1
```","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..7e3c23b78 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -23,6 +23,8 @@ from __future__ import print_function
 
 import numpy.random as npr
 
+from six.moves import xrange
+
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..7e3c23b78 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -23,6 +23,8 @@ from __future__ import print_function
 
 import numpy.random as npr
 
+from six.moves import xrange
+
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax",No
examples/mnist_classifier.py,examples/mnist_classifier.py,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"source sync

PiperOrigin-RevId: 222500675","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 7eba6b960..544d58b4d 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,6 +23,7 @@ from __future__ import print_function
 import time
 import itertools
 
+from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 7eba6b960..544d58b4d 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,6 +23,7 @@ from __future__ import print_function
 import time
 import itertools
 
+from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"source sync

PiperOrigin-RevId: 222500675","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 2910b1c3b..6aa3a6cd9 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,6 +23,7 @@ from __future__ import print_function
 
 import time
 
+from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 2910b1c3b..6aa3a6cd9 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,6 +23,7 @@ from __future__ import print_function
 
 import time
 
+from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"source sync

PiperOrigin-RevId: 222500675","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 80214b406..662616a8f 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,6 +25,7 @@ from __future__ import print_function
 import os
 import time
 
+from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
@@ -137,3 +138,7 @@ if __name__ == ""__main__"":
     test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 80214b406..662616a8f 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,6 +25,7 @@ from __future__ import print_function
 import os
 import time
 
+from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
@@ -137,3 +138,7 @@ if __name__ == ""__main__"":
     test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"source sync

PiperOrigin-RevId: 222500675","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import app
+
 import numpy.random as npr
 
 import jax.numpy as np
@@ -83,7 +85,9 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-if __name__ == ""__main__"":
+def main(argv):
+  del argv  # Unused.
+
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -124,3 +128,7 @@ if __name__ == ""__main__"":
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
+
+
+if __name__ == '__main__':
+  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import app
+
 import numpy.random as npr
 
 import jax.numpy as np
@@ -83,7 +85,9 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-if __name__ == ""__main__"":
+def main(argv):
+  del argv  # Unused.
+
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -124,3 +128,7 @@ if __name__ == ""__main__"":
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
+
+
+if __name__ == '__main__':
+  app.run(main)",No
jax/interpreters/batching.py,jax/interpreters/batching.py,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"source sync

PiperOrigin-RevId: 222500675","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index e9645f281..4611f0daa 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -88,6 +88,8 @@ class BatchTracer(Tracer):
       batch_dims = self.batch_dim
     elif t is int:
       batch_dims = [self.batch_dim] * len(self.val)
+    elif t is type(None):
+      return tuple(self.val)
     else:
       raise TypeError(t)
     return map(partial(BatchTracer, self.trace), self.val, batch_dims)","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index e9645f281..4611f0daa 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -88,6 +88,8 @@ class BatchTracer(Tracer):
       batch_dims = self.batch_dim
     elif t is int:
       batch_dims = [self.batch_dim] * len(self.val)
+    elif t is type(None):
+      return tuple(self.val)
     else:
       raise TypeError(t)
     return map(partial(BatchTracer, self.trace), self.val, batch_dims)",No
build/build_jax.sh,jax/oss/build/build_jax.sh,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,7a313bf6229e0f0949c1ddb0ceaf0b1aa1d67519,"source sync

PiperOrigin-RevId: 222500675","diff --git a/jax/oss/build/build_jax.sh b/jax/oss/build/build_jax.sh
new file mode 100755
index 000000000..0bf8ae06c
--- /dev/null
+++ b/jax/oss/build/build_jax.sh
@@ -0,0 +1,107 @@
+#!/bin/bash
+set -exv
+
+# For a build with CUDA, from the repo root run:
+#   bash build/build_jax.sh
+# For building without CUDA (CPU-only), instead run:
+#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
+# To clean intermediate results, run
+#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
+# To clean everything, run
+#   rm -rf /tmp/jax-build
+
+JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}
+
+init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
+if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
+then
+  (>&2 echo ""must be executed from jax repo root"")
+  exit 1
+fi
+
+tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
+mkdir -p ${tmp}
+
+## get bazel
+bazel_dir=${tmp}/jax-bazel
+if [ ! -d ${bazel_dir}/bin ]
+then
+    mkdir -p ${bazel_dir}
+    case ""$(uname -s)"" in
+      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
+      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
+      *) exit 1;;
+    esac
+    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
+    chmod +x ${installer}
+    bash ${installer} --prefix=${bazel_dir}
+    rm ${installer}
+fi
+export PATH=""${bazel_dir}/bin:$PATH""
+
+## get and configure tensorflow for building xla
+if [[ ! -d tensorflow ]]
+then
+  git clone https://github.com/tensorflow/tensorflow.git
+fi
+pushd tensorflow
+export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
+export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
+export USE_DEFAULT_PYTHON_LIB_PATH=1
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
+  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
+  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
+  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
+  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
+  export TF_NEED_CUDA=1
+else
+  export TF_NEED_CUDA=0
+fi
+export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
+export TF_ENABLE_XLA=1
+export TF_NEED_MKL=0
+export CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
+export TF_NEED_IGNITE=1
+export TF_NEED_OPENCL=0
+export TF_NEED_OPENCL_SYCL=0
+export TF_NEED_ROCM=0
+export TF_NEED_MPI=0
+export TF_DOWNLOAD_CLANG=0
+export TF_SET_ANDROID_WORKSPACE=0
+export TF_CUDA_CLANG=0
+export TF_NEED_TENSORRT=0
+./configure
+popd
+
+## build xla inside tensorflow
+mkdir -p ${PYTHON_LIB_PATH}
+bazel_output_user_root=${tmp}/jax-bazel-output-user-root
+bazel_output_base=${bazel_output_user_root}/output-base
+bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  bazel_build_opt=""-c opt --config=cuda""
+else
+  bazel_build_opt=""-c opt""
+fi
+bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
+
+## extract the pieces we need
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
+cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
+
+## rewrite some imports
+sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
+sed -i 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' jax/lib/xla_client.py
+sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_client.py
+
+## clean up
+rm -f bazel-*  # symlinks
+rm -rf tensorflow
+rm -rf ${bazel_output_user_root}  # clean build results
+# rm -rf ${tmp}  # clean everything, including the bazel binary","diff --git a/jax/oss/build/build_jax.sh b/jax/oss/build/build_jax.sh
new file mode 100755
index 000000000..0bf8ae06c
--- /dev/null
+++ b/jax/oss/build/build_jax.sh
@@ -0,0 +1,107 @@
+#!/bin/bash
+set -exv
+
+# For a build with CUDA, from the repo root run:
+#   bash build/build_jax.sh
+# For building without CUDA (CPU-only), instead run:
+#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
+# To clean intermediate results, run
+#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
+# To clean everything, run
+#   rm -rf /tmp/jax-build
+
+JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}
+
+init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
+if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
+then
+  (>&2 echo ""must be executed from jax repo root"")
+  exit 1
+fi
+
+tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
+mkdir -p ${tmp}
+
+## get bazel
+bazel_dir=${tmp}/jax-bazel
+if [ ! -d ${bazel_dir}/bin ]
+then
+    mkdir -p ${bazel_dir}
+    case ""$(uname -s)"" in
+      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
+      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
+      *) exit 1;;
+    esac
+    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
+    chmod +x ${installer}
+    bash ${installer} --prefix=${bazel_dir}
+    rm ${installer}
+fi
+export PATH=""${bazel_dir}/bin:$PATH""
+
+## get and configure tensorflow for building xla
+if [[ ! -d tensorflow ]]
+then
+  git clone https://github.com/tensorflow/tensorflow.git
+fi
+pushd tensorflow
+export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
+export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
+export USE_DEFAULT_PYTHON_LIB_PATH=1
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
+  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
+  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
+  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
+  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
+  export TF_NEED_CUDA=1
+else
+  export TF_NEED_CUDA=0
+fi
+export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
+export TF_ENABLE_XLA=1
+export TF_NEED_MKL=0
+export CC_OPT_FLAGS=""-march=native -Wno-sign-compare""
+export TF_NEED_IGNITE=1
+export TF_NEED_OPENCL=0
+export TF_NEED_OPENCL_SYCL=0
+export TF_NEED_ROCM=0
+export TF_NEED_MPI=0
+export TF_DOWNLOAD_CLANG=0
+export TF_SET_ANDROID_WORKSPACE=0
+export TF_CUDA_CLANG=0
+export TF_NEED_TENSORRT=0
+./configure
+popd
+
+## build xla inside tensorflow
+mkdir -p ${PYTHON_LIB_PATH}
+bazel_output_user_root=${tmp}/jax-bazel-output-user-root
+bazel_output_base=${bazel_output_user_root}/output-base
+bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  bazel_build_opt=""-c opt --config=cuda""
+else
+  bazel_build_opt=""-c opt""
+fi
+bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
+
+## extract the pieces we need
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
+cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
+
+## rewrite some imports
+sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
+sed -i 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' jax/lib/xla_client.py
+sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_client.py
+
+## clean up
+rm -f bazel-*  # symlinks
+rm -rf tensorflow
+rm -rf ${bazel_output_user_root}  # clean build results
+# rm -rf ${tmp}  # clean everything, including the bazel binary",No
jax/interpreters/xla.py,jax/interpreters/xla.py,642046e2c0dad02abf2d2accb9c401d23e0d3a20,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,"Bridge to the XRT backend.

PiperOrigin-RevId: 222763810","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index cb11313be..8d7539ccb 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -335,7 +335,7 @@ def device_put(x):
   if type(x) is DeviceArray:
     return x.device_buffer
   else:
-    return xb.get_xla_client().LocalBuffer.from_pyval(x)
+    return xb.device_put(x)
 
 def handle_result(device_buffer):
   if device_buffer.shape().is_tuple():","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index cb11313be..8d7539ccb 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -335,7 +335,7 @@ def device_put(x):
   if type(x) is DeviceArray:
     return x.device_buffer
   else:
-    return xb.get_xla_client().LocalBuffer.from_pyval(x)
+    return xb.device_put(x)
 
 def handle_result(device_buffer):
   if device_buffer.shape().is_tuple():",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,642046e2c0dad02abf2d2accb9c401d23e0d3a20,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,"Bridge to the XRT backend.

PiperOrigin-RevId: 222763810","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 68a244206..f5b9cf153 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -44,15 +44,18 @@ flags.DEFINE_string('jax_dump_hlo_optimized', None,
 flags.DEFINE_string('jax_dump_hlo_per_pass', None,
                     'Dirpath for per-pass HLO dump.')
 flags.DEFINE_integer('jax_replica_count', 1, 'Replica count for computations.')
+flags.DEFINE_enum(
+    'jax_xla_backend', 'xla', ['xla', 'xrt'],
+    'Either ""xla"" for the XLA service directly, or ""xrt"" for an XRT backend.')
+flags.DEFINE_string(
+    'jax_backend_target', 'local',
+    'Either ""local"" or ""rpc:address"" to connect to a remote service target.')
 flags.DEFINE_string(
     'jax_platform_name', '',
     'Platform name for XLA. The default is to attempt to use a '
     'GPU if available, but fall back to CPU otherwise. To set '
     'the platform manually, pass ""Host"" for CPU or ""CUDA"" for '
     'GPU.')
-flags.DEFINE_string(
-    'jax_xla_client_mode', 'local',
-    'Either ""local"", ""rpc:address"" or ""rpc:binpath"" to spawn the service.')
 
 # Prefix for HLO-dump flags indicating output should go into Sponge's
 # visible output files.
@@ -107,15 +110,33 @@ def memoize_thunk(func):
 
 @memoize_thunk
 def get_xla_client():
-  xla_client.initialize_replica_count(FLAGS.jax_replica_count)
-  if FLAGS.jax_platform_name:
-    xla_client.initialize_platform_name(FLAGS.jax_platform_name)
-  else:
-    try:
-      xla_client.initialize_platform_name(b'CUDA')
-    except RuntimeError:
-      warnings.warn('No GPU found, falling back to CPU.')
-      xla_client.initialize_platform_name(b'Host')
+  return _get_xla_client(FLAGS.jax_xla_backend,
+                         FLAGS.jax_platform_name,
+                         FLAGS.jax_replica_count)
+
+
+def _get_xla_client(backend_name, platform_name, replica_count):
+  """"""Configures and returns a handle to the XLA client.
+
+  Args:
+    backend_name: backend name, 'xla' or 'xrt'
+    platform_name: platform name for XLA backend
+    replica_count: number of computation replicas with which to configure the
+      backend library.
+
+  Returns:
+    A client library module, or an object that behaves identically to one.
+  """"""
+  xla_client.initialize_replica_count(replica_count)
+  if backend_name == 'xla':
+    if platform_name:
+      xla_client.initialize_platform_name(platform_name)
+    else:
+      try:
+        xla_client.initialize_platform_name(b'CUDA')
+      except RuntimeError:
+        warnings.warn('No GPU found, falling back to CPU.')
+        xla_client.initialize_platform_name(b'Host')
   return xla_client
 
 
@@ -123,6 +144,24 @@ def get_replica_count():
   return get_xla_client().get_replica_count()
 
 
+_backend_flag_to_type = {
+    'xla': xla_client.BackendType.XLA_LOCAL,
+    'xrt': xla_client.BackendType.XRT,
+}
+
+
+@memoize_thunk
+def _get_backend():
+  return xla_client.BackendSpec(_backend_flag_to_type[FLAGS.jax_xla_backend],
+                                FLAGS.jax_backend_target)
+
+
+def device_put(pyval):
+  # TODO(frostig): Accept a replica id for placement. For now, this places on
+  # the first replica only.
+  return get_xla_client().LocalBuffer.from_pyval(pyval, backend=_get_backend())
+
+
 Shape = xla_client.Shape        # pylint: disable=invalid-name
 
 
@@ -257,6 +296,10 @@ class _JaxComputationBuilderBase(object):
   # Method name case follows that of the XLA ComputationBuilder
   # pylint: disable=invalid-name
 
+  def Build(self, *args, **kwargs):
+    return super(_JaxComputationBuilderBase, self).Build(
+        *args, backend=_get_backend(), **kwargs)
+
   def Parameter(self, value, name=None, parameter_num=None):
     return super(_JaxComputationBuilderBase, self).ParameterWithShape(
         shape_of(value), name=name, parameter_num=parameter_num)","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 68a244206..f5b9cf153 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -44,15 +44,18 @@ flags.DEFINE_string('jax_dump_hlo_optimized', None,
 flags.DEFINE_string('jax_dump_hlo_per_pass', None,
                     'Dirpath for per-pass HLO dump.')
 flags.DEFINE_integer('jax_replica_count', 1, 'Replica count for computations.')
+flags.DEFINE_enum(
+    'jax_xla_backend', 'xla', ['xla', 'xrt'],
+    'Either ""xla"" for the XLA service directly, or ""xrt"" for an XRT backend.')
+flags.DEFINE_string(
+    'jax_backend_target', 'local',
+    'Either ""local"" or ""rpc:address"" to connect to a remote service target.')
 flags.DEFINE_string(
     'jax_platform_name', '',
     'Platform name for XLA. The default is to attempt to use a '
     'GPU if available, but fall back to CPU otherwise. To set '
     'the platform manually, pass ""Host"" for CPU or ""CUDA"" for '
     'GPU.')
-flags.DEFINE_string(
-    'jax_xla_client_mode', 'local',
-    'Either ""local"", ""rpc:address"" or ""rpc:binpath"" to spawn the service.')
 
 # Prefix for HLO-dump flags indicating output should go into Sponge's
 # visible output files.
@@ -107,15 +110,33 @@ def memoize_thunk(func):
 
 @memoize_thunk
 def get_xla_client():
-  xla_client.initialize_replica_count(FLAGS.jax_replica_count)
-  if FLAGS.jax_platform_name:
-    xla_client.initialize_platform_name(FLAGS.jax_platform_name)
-  else:
-    try:
-      xla_client.initialize_platform_name(b'CUDA')
-    except RuntimeError:
-      warnings.warn('No GPU found, falling back to CPU.')
-      xla_client.initialize_platform_name(b'Host')
+  return _get_xla_client(FLAGS.jax_xla_backend,
+                         FLAGS.jax_platform_name,
+                         FLAGS.jax_replica_count)
+
+
+def _get_xla_client(backend_name, platform_name, replica_count):
+  """"""Configures and returns a handle to the XLA client.
+
+  Args:
+    backend_name: backend name, 'xla' or 'xrt'
+    platform_name: platform name for XLA backend
+    replica_count: number of computation replicas with which to configure the
+      backend library.
+
+  Returns:
+    A client library module, or an object that behaves identically to one.
+  """"""
+  xla_client.initialize_replica_count(replica_count)
+  if backend_name == 'xla':
+    if platform_name:
+      xla_client.initialize_platform_name(platform_name)
+    else:
+      try:
+        xla_client.initialize_platform_name(b'CUDA')
+      except RuntimeError:
+        warnings.warn('No GPU found, falling back to CPU.')
+        xla_client.initialize_platform_name(b'Host')
   return xla_client
 
 
@@ -123,6 +144,24 @@ def get_replica_count():
   return get_xla_client().get_replica_count()
 
 
+_backend_flag_to_type = {
+    'xla': xla_client.BackendType.XLA_LOCAL,
+    'xrt': xla_client.BackendType.XRT,
+}
+
+
+@memoize_thunk
+def _get_backend():
+  return xla_client.BackendSpec(_backend_flag_to_type[FLAGS.jax_xla_backend],
+                                FLAGS.jax_backend_target)
+
+
+def device_put(pyval):
+  # TODO(frostig): Accept a replica id for placement. For now, this places on
+  # the first replica only.
+  return get_xla_client().LocalBuffer.from_pyval(pyval, backend=_get_backend())
+
+
 Shape = xla_client.Shape        # pylint: disable=invalid-name
 
 
@@ -257,6 +296,10 @@ class _JaxComputationBuilderBase(object):
   # Method name case follows that of the XLA ComputationBuilder
   # pylint: disable=invalid-name
 
+  def Build(self, *args, **kwargs):
+    return super(_JaxComputationBuilderBase, self).Build(
+        *args, backend=_get_backend(), **kwargs)
+
   def Parameter(self, value, name=None, parameter_num=None):
     return super(_JaxComputationBuilderBase, self).ParameterWithShape(
         shape_of(value), name=name, parameter_num=parameter_num)",No
jax/test_util.py,jax/test_util.py,642046e2c0dad02abf2d2accb9c401d23e0d3a20,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,"Bridge to the XRT backend.

PiperOrigin-RevId: 222763810","diff --git a/jax/test_util.py b/jax/test_util.py
index 790230c04..4c01b612a 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -145,6 +145,22 @@ def skip_on_devices(*disabled_devices):
   return skip
 
 
+def skip_on_flag(flag_name, skip_value):
+  """"""A decorator for test methods to skip the test when flags are set.""""""
+  def skip(test_method):        # pylint: disable=missing-docstring
+    @functools.wraps(test_method)
+    def test_method_wrapper(self, *args, **kwargs):
+      flag_value = getattr(FLAGS, flag_name)
+      if flag_value == skip_value:
+        test_name = getattr(test_method, '__name__', '[unknown test]')
+        return absltest.unittest.skip(
+            '{} not supported when FLAGS.{} is {}'.format(
+                test_name, flag_name, flag_value))
+      return test_method(self, *args, **kwargs)
+    return test_method_wrapper
+  return skip
+
+
 def format_test_name_suffix(opname, shapes, dtypes):
   arg_descriptions = (format_shape_dtype_string(shape, dtype)
                       for shape, dtype in zip(shapes, dtypes))
@@ -369,4 +385,3 @@ class JaxTestCase(parameterized.TestCase):
     numpy_ans = numpy_reference_op(*args)
     self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                         atol=tol, rtol=tol)
-","diff --git a/jax/test_util.py b/jax/test_util.py
index 790230c04..4c01b612a 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -145,6 +145,22 @@ def skip_on_devices(*disabled_devices):
   return skip
 
 
+def skip_on_flag(flag_name, skip_value):
+  """"""A decorator for test methods to skip the test when flags are set.""""""
+  def skip(test_method):        # pylint: disable=missing-docstring
+    @functools.wraps(test_method)
+    def test_method_wrapper(self, *args, **kwargs):
+      flag_value = getattr(FLAGS, flag_name)
+      if flag_value == skip_value:
+        test_name = getattr(test_method, '__name__', '[unknown test]')
+        return absltest.unittest.skip(
+            '{} not supported when FLAGS.{} is {}'.format(
+                test_name, flag_name, flag_value))
+      return test_method(self, *args, **kwargs)
+    return test_method_wrapper
+  return skip
+
+
 def format_test_name_suffix(opname, shapes, dtypes):
   arg_descriptions = (format_shape_dtype_string(shape, dtype)
                       for shape, dtype in zip(shapes, dtypes))
@@ -369,4 +385,3 @@ class JaxTestCase(parameterized.TestCase):
     numpy_ans = numpy_reference_op(*args)
     self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                         atol=tol, rtol=tol)
-",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,642046e2c0dad02abf2d2accb9c401d23e0d3a20,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,"Bridge to the XRT backend.

PiperOrigin-RevId: 222763810","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 4b7c95ed6..bd587d98c 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -80,6 +80,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for shape in all_shapes for dtype in float_dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True])
+  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
     # TODO(mattjj): test autodiff
     def scipy_fun(array_to_reduce):
@@ -122,6 +123,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for shapes in CombosWithReplacement(all_shapes, 3)
       for dtypes in CombosWithReplacement(default_dtypes, 3)
       for rng in [jtu.rand_default()])
+  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
     # TODO(mattjj): test autodiff
     scipy_fun = osp_stats.norm.logpdf","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 4b7c95ed6..bd587d98c 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -80,6 +80,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for shape in all_shapes for dtype in float_dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True])
+  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
     # TODO(mattjj): test autodiff
     def scipy_fun(array_to_reduce):
@@ -122,6 +123,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for shapes in CombosWithReplacement(all_shapes, 3)
       for dtypes in CombosWithReplacement(default_dtypes, 3)
       for rng in [jtu.rand_default()])
+  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
     # TODO(mattjj): test autodiff
     scipy_fun = osp_stats.norm.logpdf",No
tests/lax_test.py,tests/lax_test.py,642046e2c0dad02abf2d2accb9c401d23e0d3a20,dbf3c606f5b0db4d0bd22d9da38d63ed0e194e14,"Bridge to the XRT backend.

PiperOrigin-RevId: 222763810","diff --git a/tests/lax_test.py b/tests/lax_test.py
index b8c78524c..136d641d9 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1603,6 +1603,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rng"": jtu.rand_default()}
       for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
       for dtype in float_dtypes)
+  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
     tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
     lhs = rng(lhs_shape, dtype)","diff --git a/tests/lax_test.py b/tests/lax_test.py
index b8c78524c..136d641d9 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1603,6 +1603,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rng"": jtu.rand_default()}
       for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
       for dtype in float_dtypes)
+  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
     tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
     lhs = rng(lhs_shape, dtype)",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,6001fd219d04608d832d286887318e1ff296c1b5,642046e2c0dad02abf2d2accb9c401d23e0d3a20,"[JAX] Python 3 fix: xla_client.initialize_platform_name() accepts a string, not bytes, so we shouldn't pass it bytes. (The previous Python 3 change was a bit overzealous and made both changes; apparently this path isn't tested by the tests because they pass an explicit --jax_xla_backend flag..)

PiperOrigin-RevId: 222810811","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index f5b9cf153..442a1f935 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -133,10 +133,10 @@ def _get_xla_client(backend_name, platform_name, replica_count):
       xla_client.initialize_platform_name(platform_name)
     else:
       try:
-        xla_client.initialize_platform_name(b'CUDA')
+        xla_client.initialize_platform_name('CUDA')
       except RuntimeError:
         warnings.warn('No GPU found, falling back to CPU.')
-        xla_client.initialize_platform_name(b'Host')
+        xla_client.initialize_platform_name('Host')
   return xla_client
 
 ","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index f5b9cf153..442a1f935 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -133,10 +133,10 @@ def _get_xla_client(backend_name, platform_name, replica_count):
       xla_client.initialize_platform_name(platform_name)
     else:
       try:
-        xla_client.initialize_platform_name(b'CUDA')
+        xla_client.initialize_platform_name('CUDA')
       except RuntimeError:
         warnings.warn('No GPU found, falling back to CPU.')
-        xla_client.initialize_platform_name(b'Host')
+        xla_client.initialize_platform_name('Host')
   return xla_client
 
 ",No
examples/mnist_classifier.py,examples/mnist_classifier.py,d318c68827a60add968d0c2fcf8835a35eed480a,6001fd219d04608d832d286887318e1ff296c1b5,"source sync

PiperOrigin-RevId: 222822439","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 544d58b4d..0eac70541 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
@@ -94,6 +93,3 @@ if __name__ == ""__main__"":
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
 
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 544d58b4d..0eac70541 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
@@ -94,6 +93,3 @@ if __name__ == ""__main__"":
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
 
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,d318c68827a60add968d0c2fcf8835a35eed480a,6001fd219d04608d832d286887318e1ff296c1b5,"source sync

PiperOrigin-RevId: 222822439","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 6aa3a6cd9..1f675a68e 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
@@ -94,6 +93,3 @@ if __name__ == ""__main__"":
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
 
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 6aa3a6cd9..1f675a68e 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,7 +23,6 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
@@ -94,6 +93,3 @@ if __name__ == ""__main__"":
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
 
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,d318c68827a60add968d0c2fcf8835a35eed480a,6001fd219d04608d832d286887318e1ff296c1b5,"source sync

PiperOrigin-RevId: 222822439","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 662616a8f..da7b4c717 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,7 +25,6 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
@@ -139,6 +138,3 @@ if __name__ == ""__main__"":
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
 
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 662616a8f..da7b4c717 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,7 +25,6 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
@@ -139,6 +138,3 @@ if __name__ == ""__main__"":
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
 
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,d318c68827a60add968d0c2fcf8835a35eed480a,6001fd219d04608d832d286887318e1ff296c1b5,"source sync

PiperOrigin-RevId: 222822439","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..164bf88bc 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -131,4 +129,5 @@ def main(argv):
 
 
 if __name__ == '__main__':
+  from absl import app
   app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..164bf88bc 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -131,4 +129,5 @@ def main(argv):
 
 
 if __name__ == '__main__':
+  from absl import app
   app.run(main)",No
jax/random.py,jax/random.py,c293f7c875932eb53bc4f20fd1cbaf08d50d9c78,d318c68827a60add968d0c2fcf8835a35eed480a,"minor: add @jit to threefry hash function in random.py

PiperOrigin-RevId: 222841601","diff --git a/jax/random.py b/jax/random.py
index 2805aff94..0edd3f1fb 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -29,9 +29,6 @@ from .lib import xla_bridge
 from .api import jit
 
 
-# TODO(mattjj): add api.jit decorators to the user-facing functions
-
-
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
   __slots__ = [""keypair""]
@@ -92,6 +89,7 @@ def _bit_stats(bits):
 ### hash function and split
 
 
+@jit
 def threefry_2x32(keypair, count):
   """"""Apply the Threefry 2x32 hash.
 ","diff --git a/jax/random.py b/jax/random.py
index 2805aff94..0edd3f1fb 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -29,9 +29,6 @@ from .lib import xla_bridge
 from .api import jit
 
 
-# TODO(mattjj): add api.jit decorators to the user-facing functions
-
-
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
   __slots__ = [""keypair""]
@@ -92,6 +89,7 @@ def _bit_stats(bits):
 ### hash function and split
 
 
+@jit
 def threefry_2x32(keypair, count):
   """"""Apply the Threefry 2x32 hash.
 ",No
jax/lax.py,jax/lax.py,326773808b8cd2de83b37d44a4ce1a533e2efa91,c293f7c875932eb53bc4f20fd1cbaf08d50d9c78,"source sync

PiperOrigin-RevId: 222867729","diff --git a/jax/lax.py b/jax/lax.py
index 90cb2e707..70e8fc1a2 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1506,13 +1506,26 @@ def select_transpose_rule(t, pred, on_true, on_false):
           select(pred, t, _zeros(on_false)) if on_true is None else None,
           select(pred, _zeros(on_true), t) if on_false is None else None]
 
+def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
+  oprand, on_true, on_false, = batched_args
+  pred_bdim, ot_bdim, of_bdim = batch_dims
+
+  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
+    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.
+
+  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
+  ot = _ones(oprand) * on_true
+  of = _ones(oprand) * on_false
+
+  return select(oprand, ot, of), pred_bdim
+
 select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
 ad.defjvp(select_p,
           None,
           lambda g, b, x, y: select(b, g, _zeros(g)),
           lambda g, b, x, y: select(b, _zeros(g), g))
 ad.primitive_transposes[select_p] = select_transpose_rule
-
+batching.primitive_batchers[select_p] = select_batch_rule
 
 def slice_shape_rule(operand, start_indices, limit_indices, strides,
                      operand_shape):","diff --git a/jax/lax.py b/jax/lax.py
index 90cb2e707..70e8fc1a2 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1506,13 +1506,26 @@ def select_transpose_rule(t, pred, on_true, on_false):
           select(pred, t, _zeros(on_false)) if on_true is None else None,
           select(pred, _zeros(on_true), t) if on_false is None else None]
 
+def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
+  oprand, on_true, on_false, = batched_args
+  pred_bdim, ot_bdim, of_bdim = batch_dims
+
+  if (ot_bdim not in {None, pred_bdim}) or (of_bdim not in {None, pred_bdim}):
+    raise NotImplementedError  # TODO(schsam, mattjj): Handle more cases.
+
+  # TODO(schsam, mattjj): Switch to using broadcast_in_dim.
+  ot = _ones(oprand) * on_true
+  of = _ones(oprand) * on_false
+
+  return select(oprand, ot, of), pred_bdim
+
 select_p = standard_primitive(select_shape_rule, select_dtype_rule, 'select')
 ad.defjvp(select_p,
           None,
           lambda g, b, x, y: select(b, g, _zeros(g)),
           lambda g, b, x, y: select(b, _zeros(g), g))
 ad.primitive_transposes[select_p] = select_transpose_rule
-
+batching.primitive_batchers[select_p] = select_batch_rule
 
 def slice_shape_rule(operand, start_indices, limit_indices, strides,
                      operand_shape):",No
tests/batching_test.py,tests/batching_test.py,326773808b8cd2de83b37d44a4ce1a533e2efa91,c293f7c875932eb53bc4f20fd1cbaf08d50d9c78,"source sync

PiperOrigin-RevId: 222867729","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 18034718b..d42012f51 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -30,6 +30,7 @@ from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial
 
+import functools as fn
 
 class BatchingTest(jtu.JaxTestCase):
 
@@ -156,6 +157,42 @@ class BatchingTest(jtu.JaxTestCase):
     expected_ans = x[:, :, 2]
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
+  def testNpMaximum(self):
+    fun = lambda x: np.maximum(x, 0.0)
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(fun, x)
+    expected_ans = onp.maximum(x, 0.0)
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+  def testNpGtrThan(self):
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(lambda x: x > 1.0, x)
+    expected_ans = x > 1.0
+    self.assertAllClose(ans, expected_ans, check_dtypes=True)
+
+  def testNpMaximumPerExampleGrad(self):
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5)
+    W = R(5, 5)
+
+    fun = lambda W, x: np.sum(np.maximum(np.dot(x, W), 0.0) ** 2)
+
+    ans = vmap(fn.partial(grad(fun), W), x)
+
+    W_t = np.transpose(W)
+    for i in range(10):
+      x_ex = x[i:i + 1]
+
+      expected_ans = 2.0 * np.dot(
+          np.maximum(np.dot(W_t, np.transpose(x_ex)), 0.0), x_ex)
+      expected_ans = np.transpose(expected_ans)
+
+      self.assertAllClose(ans[i], expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 18034718b..d42012f51 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -30,6 +30,7 @@ from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial
 
+import functools as fn
 
 class BatchingTest(jtu.JaxTestCase):
 
@@ -156,6 +157,42 @@ class BatchingTest(jtu.JaxTestCase):
     expected_ans = x[:, :, 2]
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
+  def testNpMaximum(self):
+    fun = lambda x: np.maximum(x, 0.0)
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(fun, x)
+    expected_ans = onp.maximum(x, 0.0)
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+  def testNpGtrThan(self):
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(lambda x: x > 1.0, x)
+    expected_ans = x > 1.0
+    self.assertAllClose(ans, expected_ans, check_dtypes=True)
+
+  def testNpMaximumPerExampleGrad(self):
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5)
+    W = R(5, 5)
+
+    fun = lambda W, x: np.sum(np.maximum(np.dot(x, W), 0.0) ** 2)
+
+    ans = vmap(fn.partial(grad(fun), W), x)
+
+    W_t = np.transpose(W)
+    for i in range(10):
+      x_ex = x[i:i + 1]
+
+      expected_ans = 2.0 * np.dot(
+          np.maximum(np.dot(W_t, np.transpose(x_ex)), 0.0), x_ex)
+      expected_ans = np.transpose(expected_ans)
+
+      self.assertAllClose(ans[i], expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()",No
WORKSPACE,WORKSPACE,f3513a7bfbf6ccca3c030cfe548ba280d7a04de7,326773808b8cd2de83b37d44a4ce1a533e2efa91,"[JAX] Rewrite OSS build script.

Significant changes:
* Mac OS X support.
* build script is in Python, not shell.
* build configuration is passed via flags, not environment variables.
* build script configures TF itself, and does not require explicitly checking out the TF git repository and running its configure script. Changes the TF dependency in the Bazel workspace to be an http_archive(), rather than a local checkout of TF.
* rather than trying to guess the path for Bazel-generated XLA artifacts, use a sh_binary() to perform installation of the built artifacts in to the JAX source tree. Bazel's runfiles mechanism is the supported route to find build artifacts.
* downloads Bazel in Python and checks its SHA256 before running it, rather than running an untrusted binary from the internet.
* intentionally does not delete the Bazel cache or Bazel after building.

Example of new build interaction:

Building without CUDA on Mac or Linux:
$ cd jax
$ python3 build.py   (or python2 build.py if you want a Python 2 build)

     _   _    __  __
    | | / \   \ \/ /
 _  | |/ _ \   \  /
| |_| / ___ \  /  \
 \___/_/   \_\/_/\_\

Starting local Bazel server and connecting to it...
Bazel binary path: /Users/xyz/bin/bazel
Python binary path: /Library/Frameworks/Python.framework/Versions/3.7/bin/python3
CUDA enabled: no

Building XLA and installing it in the JAX source tree...
...

Example of building with CUDA enabled on Linux:
$ python3 build.py --enable_cuda --cudnn_path=/usr/lib/x86_64-linux-gnu/
... as before, except ...
CUDA enabled: yes
CUDA toolkit path: /usr/local/cuda
CUDNN library path: /usr/lib/x86_64-linux-gnu/
...

PiperOrigin-RevId: 222868835","diff --git a/WORKSPACE b/WORKSPACE
index f49ede9a4..d5f0d96e5 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -1,8 +1,3 @@
-local_repository(
-    name = ""org_tensorflow"",
-    path = ""tensorflow"",
-)
-
 http_archive(
     name = ""io_bazel_rules_closure"",
     sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
@@ -13,6 +8,28 @@ http_archive(
     ],
 )
 
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""599e9aad221a27882fce98ff472372030a2ebfe63cfc643d3470691f34bb68d6"",
+    strip_prefix=""tensorflow-64e084b8cb27e8c53b15468c21f1b3471b4b9659"",
+    urls = [
+      ""https://github.com/tensorflow/tensorflow/archive/64e084b8cb27e8c53b15468c21f1b3471b4b9659.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+
 load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
 
 tf_workspace(","diff --git a/WORKSPACE b/WORKSPACE
index f49ede9a4..d5f0d96e5 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -1,8 +1,3 @@
-local_repository(
-    name = ""org_tensorflow"",
-    path = ""tensorflow"",
-)
-
 http_archive(
     name = ""io_bazel_rules_closure"",
     sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
@@ -13,6 +8,28 @@ http_archive(
     ],
 )
 
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""599e9aad221a27882fce98ff472372030a2ebfe63cfc643d3470691f34bb68d6"",
+    strip_prefix=""tensorflow-64e084b8cb27e8c53b15468c21f1b3471b4b9659"",
+    urls = [
+      ""https://github.com/tensorflow/tensorflow/archive/64e084b8cb27e8c53b15468c21f1b3471b4b9659.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+
 load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
 
 tf_workspace(",No
build/install_xla_in_source_tree.sh,build/install_xla_in_source_tree.sh,599ea381753a00d88bf53ab2e4ab5a162d601949,f3513a7bfbf6ccca3c030cfe548ba280d7a04de7,"[JAX] Explicitly use /bin/bash in install_xla_in_source_tree.sh.

It turns out the script uses some bash-isms not supported by the /bin/sh (dash) shell on a Debian machine.

PiperOrigin-RevId: 222884583","diff --git a/build/install_xla_in_source_tree.sh b/build/install_xla_in_source_tree.sh
index a399e8ebb..55e8ff8bf 100755
--- a/build/install_xla_in_source_tree.sh
+++ b/build/install_xla_in_source_tree.sh
@@ -1,4 +1,4 @@
-#!/bin/sh
+#!/bin/bash
 #
 # Copyright 2018 Google LLC
 #","diff --git a/build/install_xla_in_source_tree.sh b/build/install_xla_in_source_tree.sh
index a399e8ebb..55e8ff8bf 100755
--- a/build/install_xla_in_source_tree.sh
+++ b/build/install_xla_in_source_tree.sh
@@ -1,4 +1,4 @@
-#!/bin/sh
+#!/bin/bash
 #
 # Copyright 2018 Google LLC
 #",No
jax/abstract_arrays.py,jax/abstract_arrays.py,ca2634ea5dea19354d7ee2322dcbb2e15d206d98,599ea381753a00d88bf53ab2e4ab5a162d601949,"source sync

PiperOrigin-RevId: 222923229","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index ff9944122..61fd8f7cb 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -21,6 +21,7 @@ import six
 
 from . import core
 from . import ad_util
+from . util import prod
 from .lib import xla_bridge
 
 
@@ -80,7 +81,7 @@ class ShapedArray(UnshapedArray):
     self.shape = shape
 
   ndim = property(lambda self: len(self.shape))
-  size = property(lambda self: int(onp.prod(self.shape)))
+  size = property(lambda self: prod(self.shape))
 
   def __eq__(self, other):
     return (type(self) is type(other)","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index ff9944122..61fd8f7cb 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -21,6 +21,7 @@ import six
 
 from . import core
 from . import ad_util
+from . util import prod
 from .lib import xla_bridge
 
 
@@ -80,7 +81,7 @@ class ShapedArray(UnshapedArray):
     self.shape = shape
 
   ndim = property(lambda self: len(self.shape))
-  size = property(lambda self: int(onp.prod(self.shape)))
+  size = property(lambda self: prod(self.shape))
 
   def __eq__(self, other):
     return (type(self) is type(other)",No
jax/interpreters/xla.py,jax/interpreters/xla.py,ca2634ea5dea19354d7ee2322dcbb2e15d206d98,599ea381753a00d88bf53ab2e4ab5a162d601949,"source sync

PiperOrigin-RevId: 222923229","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 8d7539ccb..2e559060f 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -28,7 +28,7 @@ from .. import core
 from .. import ad_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types
 from ..core import AbstractTuple, JaxTuple, pack, valid_jaxtype
-from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map
+from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map, prod
 from ..linear_util import transformation_with_aux, memoize as linear_memoize
 from ..lib import xla_bridge as xb
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
@@ -190,7 +190,7 @@ class DeviceArray(DeviceValue):
     self.shape = xla_shape.dimensions()
     self.dtype = xla_shape.element_type()
     self.ndim = len(self.shape)
-    size = int(onp.prod(self.shape))
+    size = prod(self.shape)
     self._npy_value = None
 
   @property","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 8d7539ccb..2e559060f 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -28,7 +28,7 @@ from .. import core
 from .. import ad_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types
 from ..core import AbstractTuple, JaxTuple, pack, valid_jaxtype
-from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map
+from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map, prod
 from ..linear_util import transformation_with_aux, memoize as linear_memoize
 from ..lib import xla_bridge as xb
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
@@ -190,7 +190,7 @@ class DeviceArray(DeviceValue):
     self.shape = xla_shape.dimensions()
     self.dtype = xla_shape.element_type()
     self.ndim = len(self.shape)
-    size = int(onp.prod(self.shape))
+    size = prod(self.shape)
     self._npy_value = None
 
   @property",No
jax/lax.py,jax/lax.py,ca2634ea5dea19354d7ee2322dcbb2e15d206d98,599ea381753a00d88bf53ab2e4ab5a162d601949,"source sync

PiperOrigin-RevId: 222923229","diff --git a/jax/lax.py b/jax/lax.py
index 70e8fc1a2..122fd6cbf 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -37,7 +37,7 @@ from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
 from .interpreters import batching
-from .util import curry, safe_zip, unzip2
+from .util import curry, safe_zip, unzip2, prod
 from .tree_util import build_tree
 from .lib import xla_bridge
 
@@ -424,7 +424,7 @@ def full_like(x, fill_value, dtype=None, shape=None):
 
 def collapse(operand, start_dimension, stop_dimension):
   lo, hi = start_dimension, stop_dimension
-  size = onp.product(operand.shape[lo:hi])
+  size = prod(operand.shape[lo:hi])
   new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
   return reshape(operand, new_shape)
 
@@ -1407,7 +1407,7 @@ def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
   if not onp.all(onp.greater_equal(new_sizes, 0)):
     msg = 'reshape new_sizes must all be positive, got {}.'
     raise TypeError(msg.format(new_sizes))
-  if onp.prod(onp.shape(operand)) != onp.prod(new_sizes):
+  if prod(onp.shape(operand)) != prod(new_sizes):
     msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
     raise TypeError(msg.format(new_sizes, onp.shape(operand)))
   if dimensions is not None:","diff --git a/jax/lax.py b/jax/lax.py
index 70e8fc1a2..122fd6cbf 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -37,7 +37,7 @@ from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
 from .interpreters import batching
-from .util import curry, safe_zip, unzip2
+from .util import curry, safe_zip, unzip2, prod
 from .tree_util import build_tree
 from .lib import xla_bridge
 
@@ -424,7 +424,7 @@ def full_like(x, fill_value, dtype=None, shape=None):
 
 def collapse(operand, start_dimension, stop_dimension):
   lo, hi = start_dimension, stop_dimension
-  size = onp.product(operand.shape[lo:hi])
+  size = prod(operand.shape[lo:hi])
   new_shape = operand.shape[:lo] + (size,) + operand.shape[hi:]
   return reshape(operand, new_shape)
 
@@ -1407,7 +1407,7 @@ def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):
   if not onp.all(onp.greater_equal(new_sizes, 0)):
     msg = 'reshape new_sizes must all be positive, got {}.'
     raise TypeError(msg.format(new_sizes))
-  if onp.prod(onp.shape(operand)) != onp.prod(new_sizes):
+  if prod(onp.shape(operand)) != prod(new_sizes):
     msg = 'reshape total size must be unchanged, got new_sizes {} for shape {}.'
     raise TypeError(msg.format(new_sizes, onp.shape(operand)))
   if dimensions is not None:",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,ca2634ea5dea19354d7ee2322dcbb2e15d206d98,599ea381753a00d88bf53ab2e4ab5a162d601949,"source sync

PiperOrigin-RevId: 222923229","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 044f7be12..d0a43546a 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -26,6 +26,7 @@ from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from ..lib import xla_bridge
 import jax.lax as lax
+from ..util import memoize
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
 # and also use some common names (like 'shape' and 'dtype') at the top-level.
@@ -105,6 +106,7 @@ def _promote_shapes(*args):
             if len(shp) != nd else arg for arg, shp in zip(args, shapes)]
 
 
+@memoize
 def _broadcast_shapes(*shapes):
   """"""Apply Numpy broadcasting rules to the given shapes.""""""
   if len(shapes) == 1:
@@ -120,6 +122,7 @@ def _broadcast_shapes(*shapes):
 
 def _promote_dtypes(*args):
   """"""Convenience function to apply Numpy argument dtype promotion.""""""
+  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
   if len(args) < 2:
     return args
   else:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 044f7be12..d0a43546a 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -26,6 +26,7 @@ from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from ..lib import xla_bridge
 import jax.lax as lax
+from ..util import memoize
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
 # and also use some common names (like 'shape' and 'dtype') at the top-level.
@@ -105,6 +106,7 @@ def _promote_shapes(*args):
             if len(shp) != nd else arg for arg, shp in zip(args, shapes)]
 
 
+@memoize
 def _broadcast_shapes(*shapes):
   """"""Apply Numpy broadcasting rules to the given shapes.""""""
   if len(shapes) == 1:
@@ -120,6 +122,7 @@ def _broadcast_shapes(*shapes):
 
 def _promote_dtypes(*args):
   """"""Convenience function to apply Numpy argument dtype promotion.""""""
+  # TODO(dougalm,mattjj): This is a performance bottleneck. Consider memoizing.
   if len(args) < 2:
     return args
   else:",No
jax/util.py,jax/util.py,ca2634ea5dea19354d7ee2322dcbb2e15d206d98,599ea381753a00d88bf53ab2e4ab5a162d601949,"source sync

PiperOrigin-RevId: 222923229","diff --git a/jax/util.py b/jax/util.py
index 4ea75bbf9..62587fe2d 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -18,7 +18,7 @@ from __future__ import print_function
 
 import functools
 import itertools as it
-
+from operator import mul
 
 allow_memoize_hash_failures = False
 
@@ -139,3 +139,7 @@ def memoize(fun):
       else:
         raise
   return memoized_fun
+
+
+def prod(xs):
+  return functools.reduce(mul, xs, 1)","diff --git a/jax/util.py b/jax/util.py
index 4ea75bbf9..62587fe2d 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -18,7 +18,7 @@ from __future__ import print_function
 
 import functools
 import itertools as it
-
+from operator import mul
 
 allow_memoize_hash_failures = False
 
@@ -139,3 +139,7 @@ def memoize(fun):
       else:
         raise
   return memoized_fun
+
+
+def prod(xs):
+  return functools.reduce(mul, xs, 1)",No
WORKSPACE,WORKSPACE,93ac03ea083e7a913927daf35d7b0d39ad5d2eca,ca2634ea5dea19354d7ee2322dcbb2e15d206d98,"[JAX] Update TensorFlow release to https://github.com/tensorflow/tensorflow/commit/0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b to pick up fixes to the XLA Mac OS X build.

PiperOrigin-RevId: 223048563","diff --git a/WORKSPACE b/WORKSPACE
index d5f0d96e5..c2770f000 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,10 +16,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""599e9aad221a27882fce98ff472372030a2ebfe63cfc643d3470691f34bb68d6"",
-    strip_prefix=""tensorflow-64e084b8cb27e8c53b15468c21f1b3471b4b9659"",
+    sha256 = ""9e5dbd0186eebb35577d6c71337ad79774732f52b7e89f7bd1d974153bfe7a5e"",
+    strip_prefix=""tensorflow-0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/64e084b8cb27e8c53b15468c21f1b3471b4b9659.tar.gz"",
+      ""https://github.com/tensorflow/tensorflow/archive/0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b.tar.gz"",
     ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index d5f0d96e5..c2770f000 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,10 +16,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""599e9aad221a27882fce98ff472372030a2ebfe63cfc643d3470691f34bb68d6"",
-    strip_prefix=""tensorflow-64e084b8cb27e8c53b15468c21f1b3471b4b9659"",
+    sha256 = ""9e5dbd0186eebb35577d6c71337ad79774732f52b7e89f7bd1d974153bfe7a5e"",
+    strip_prefix=""tensorflow-0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/64e084b8cb27e8c53b15468c21f1b3471b4b9659.tar.gz"",
+      ""https://github.com/tensorflow/tensorflow/archive/0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b.tar.gz"",
     ],
 )
 ",No
examples/resnet50.py,examples/resnet50.py,0ea98501aa00d361ae5584be2fefd660e43e028d,93ac03ea083e7a913927daf35d7b0d39ad5d2eca,"source sync

PiperOrigin-RevId: 223080639","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 164bf88bc..5cd858505 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -83,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 164bf88bc..5cd858505 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -83,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)",No
examples/resnet50.py,examples/resnet50.py,14844224fccef62a81ef6b39a89de1793303c7ce,0ea98501aa00d361ae5584be2fefd660e43e028d,"source sync

PiperOrigin-RevId: 223081582","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 5cd858505..f384c2f30 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -125,7 +125,3 @@ if __name__ == ""__main__"":
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
 
-
-if __name__ == '__main__':
-  from absl import app
-  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 5cd858505..f384c2f30 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -125,7 +125,3 @@ if __name__ == ""__main__"":
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
 
-
-if __name__ == '__main__':
-  from absl import app
-  app.run(main)",No
jax/lax.py,jax/lax.py,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,aefd8f9eb5071cdc2edac59b0ce21161a618c5c2,"fix handling of symbolic zeros for a few special primitives

PiperOrigin-RevId: 223264329","diff --git a/jax/lax.py b/jax/lax.py
index 122fd6cbf..5666cf212 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1678,7 +1678,12 @@ def dynamic_update_slice_jvp(primals, tangents, update_shape):
   g_operand, g_update, g_start_indices = tangents
   assert g_start_indices is ad_util.zero
   val_out = dynamic_update_slice(operand, update, start_indices)
-  tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
+  if g_operand is ad_util.zero and g_update is ad_util.zero:
+    tangent_out = ad_util.zero
+  else:
+    g_operand = ad.instantiate_zeros(operand, g_operand)
+    g_update = ad.instantiate_zeros(update, g_update)
+    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
   return val_out, tangent_out
 
 def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
@@ -1717,7 +1722,7 @@ def index_take_translation_rule(c, src, *idxs, **kwargs):
 def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
   src = primals[0]
   idxs = tuple(primals[1:])
-  g =tangents[0]
+  g = ad.instantiate_zeros(src, tangents[0])
   return index_take(src, idxs, axes), index_take(g, idxs, axes)
 
 def index_take_transpose_rule(t, src, *idxs, **kwargs):
@@ -1747,6 +1752,8 @@ def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
   src, dst = primals[0], primals[1]
   idxs = tuple(primals[2:])
   g_src, g_dst = tangents[0], tangents[1]
+  g_src = ad.instantiate_zeros(src, g_src)
+  g_dst = ad.instantiate_zeros(dst, g_dst)
   val_out = index_untake(src, dst, idxs, axes)
   tangent_out = index_untake(g_src, g_dst, idxs, axes)
   return val_out, tangent_out
@@ -2064,10 +2071,17 @@ def sort_key_val_jvp(primals, tangents, dimension):
 
   val_out = sort_key_val(keys, values, dimension)
 
-  keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
-  values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
-  tangents_out = keys_tangents_out, values_tangents_out
+  if keys_tangents is ad_util.zero:
+    keys_tangents_out = ad_util.zero
+  else:
+    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
 
+  if values_tangents is ad_util.zero:
+    values_tangents_out = ad_util.zero
+  else:
+    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
+
+  tangents_out = keys_tangents_out, values_tangents_out
   return core.pack(val_out), core.pack(tangents_out)
 
 def sort_key_val_transpose_rule(t, keys, values, dimension):","diff --git a/jax/lax.py b/jax/lax.py
index 122fd6cbf..5666cf212 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1678,7 +1678,12 @@ def dynamic_update_slice_jvp(primals, tangents, update_shape):
   g_operand, g_update, g_start_indices = tangents
   assert g_start_indices is ad_util.zero
   val_out = dynamic_update_slice(operand, update, start_indices)
-  tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
+  if g_operand is ad_util.zero and g_update is ad_util.zero:
+    tangent_out = ad_util.zero
+  else:
+    g_operand = ad.instantiate_zeros(operand, g_operand)
+    g_update = ad.instantiate_zeros(update, g_update)
+    tangent_out = dynamic_update_slice(g_operand, g_update, start_indices)
   return val_out, tangent_out
 
 def dynamic_update_slice_transpose_rule(t, operand, update, start_indices,
@@ -1717,7 +1722,7 @@ def index_take_translation_rule(c, src, *idxs, **kwargs):
 def index_take_jvp(primals, tangents, axes, input_shape, jaxpr, consts):
   src = primals[0]
   idxs = tuple(primals[1:])
-  g =tangents[0]
+  g = ad.instantiate_zeros(src, tangents[0])
   return index_take(src, idxs, axes), index_take(g, idxs, axes)
 
 def index_take_transpose_rule(t, src, *idxs, **kwargs):
@@ -1747,6 +1752,8 @@ def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
   src, dst = primals[0], primals[1]
   idxs = tuple(primals[2:])
   g_src, g_dst = tangents[0], tangents[1]
+  g_src = ad.instantiate_zeros(src, g_src)
+  g_dst = ad.instantiate_zeros(dst, g_dst)
   val_out = index_untake(src, dst, idxs, axes)
   tangent_out = index_untake(g_src, g_dst, idxs, axes)
   return val_out, tangent_out
@@ -2064,10 +2071,17 @@ def sort_key_val_jvp(primals, tangents, dimension):
 
   val_out = sort_key_val(keys, values, dimension)
 
-  keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
-  values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
-  tangents_out = keys_tangents_out, values_tangents_out
+  if keys_tangents is ad_util.zero:
+    keys_tangents_out = ad_util.zero
+  else:
+    keys_tangents_out = sort_jvp_rule(keys_tangents, keys, dimension)
 
+  if values_tangents is ad_util.zero:
+    values_tangents_out = ad_util.zero
+  else:
+    values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
+
+  tangents_out = keys_tangents_out, values_tangents_out
   return core.pack(val_out), core.pack(tangents_out)
 
 def sort_key_val_transpose_rule(t, keys, values, dimension):",No
tests/lax_test.py,tests/lax_test.py,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,aefd8f9eb5071cdc2edac59b0ce21161a618c5c2,"fix handling of symbolic zeros for a few special primitives

PiperOrigin-RevId: 223264329","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 136d641d9..800709fef 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -839,8 +839,8 @@ class LaxTest(jtu.JaxTestCase):
       ]
       for dtype in default_dtypes
       for rng in [jtu.rand_default()])
-  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
-                             rng):
+  def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                         update_shape, rng):
 
     def args_maker():
       return [rng(shape, dtype), rng(update_shape, dtype),
@@ -1797,9 +1797,16 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     operand = rng(shape, dtype)
     update = rng(update_shape, dtype)
     start_indices = onp.array(start_indices)
+
     dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
     check_grads(dus, (operand, update), 2, tol, tol, tol)
 
+    dus = lambda x: lax.dynamic_update_slice(x, update, start_indices)
+    check_grads(dus, (operand,), 2, tol, tol, tol)
+
+    dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)
+    check_grads(dus, (update,), 2, tol, tol, tol)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 136d641d9..800709fef 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -839,8 +839,8 @@ class LaxTest(jtu.JaxTestCase):
       ]
       for dtype in default_dtypes
       for rng in [jtu.rand_default()])
-  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
-                             rng):
+  def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                         update_shape, rng):
 
     def args_maker():
       return [rng(shape, dtype), rng(update_shape, dtype),
@@ -1797,9 +1797,16 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     operand = rng(shape, dtype)
     update = rng(update_shape, dtype)
     start_indices = onp.array(start_indices)
+
     dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
     check_grads(dus, (operand, update), 2, tol, tol, tol)
 
+    dus = lambda x: lax.dynamic_update_slice(x, update, start_indices)
+    check_grads(dus, (operand,), 2, tol, tol, tol)
+
+    dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)
+    check_grads(dus, (update,), 2, tol, tol, tol)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),",No
examples/mnist_classifier.py,examples/mnist_classifier.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 0eac70541..6336ac675 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -26,6 +26,7 @@ import itertools
 import numpy.random as npr
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad
 from jax.experimental import minmax
 from jax.experimental import stax
@@ -80,6 +81,7 @@ if __name__ == ""__main__"":
   opt_state = opt_init(init_params)
   itercount = itertools.count()
 
+  print(""\nStarting training..."")
   for epoch in range(num_epochs):
     start_time = time.time()
     for _ in range(num_batches):","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 0eac70541..6336ac675 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -26,6 +26,7 @@ import itertools
 import numpy.random as npr
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad
 from jax.experimental import minmax
 from jax.experimental import stax
@@ -80,6 +81,7 @@ if __name__ == ""__main__"":
   opt_state = opt_init(init_params)
   itercount = itertools.count()
 
+  print(""\nStarting training..."")
   for epoch in range(num_epochs):
     start_time = time.time()
     for _ in range(num_batches):",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 1f675a68e..b0dfbb5cd 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -26,6 +26,7 @@ import time
 import numpy.random as npr
 
 from jax.api import jit, grad
+from jax.config import config
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
 import datasets","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 1f675a68e..b0dfbb5cd 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -26,6 +26,7 @@ import time
 import numpy.random as npr
 
 from jax.api import jit, grad
+from jax.config import config
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
 import datasets",No
examples/mnist_vae.py,examples/mnist_vae.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index da7b4c717..aa2fd1725 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -28,6 +28,7 @@ import time
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad, lax, random
 from jax.experimental import minmax
 from jax.experimental import stax","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index da7b4c717..aa2fd1725 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -28,6 +28,7 @@ import time
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad, lax, random
 from jax.experimental import minmax
 from jax.experimental import stax",No
examples/resnet50.py,examples/resnet50.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/examples/resnet50.py b/examples/resnet50.py
index f384c2f30..2f92272e0 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -24,6 +24,7 @@ from __future__ import print_function
 import numpy.random as npr
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad
 from jax.experimental import minmax
 from jax.experimental import stax","diff --git a/examples/resnet50.py b/examples/resnet50.py
index f384c2f30..2f92272e0 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -24,6 +24,7 @@ from __future__ import print_function
 import numpy.random as npr
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad
 from jax.experimental import minmax
 from jax.experimental import stax",No
jax/interpreters/xla.py,jax/interpreters/xla.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 2e559060f..8f99a7cc0 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -23,7 +23,7 @@ import operator as op
 import six
 from six.moves import xrange
 
-from absl import flags
+from ..config import flags
 from .. import core
 from .. import ad_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 2e559060f..8f99a7cc0 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -23,7 +23,7 @@ import operator as op
 import six
 from six.moves import xrange
 
-from absl import flags
+from ..config import flags
 from .. import core
 from .. import ad_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 442a1f935..053c6bbe9 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -26,7 +26,7 @@ from __future__ import print_function
 import os
 import warnings
 
-from absl import flags
+from ..config import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
 from . import xla_data_pb2
@@ -206,21 +206,8 @@ _dtype_to_32bit_dtype = {
 }
 
 
-def canonicalize_dtype(dtype):
-  """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
-  # This function is a thin wrapper around the memoized _canonicalize_dtype to
-  # handle the case where FLAGS haven't been parsed yet, for example because
-  # this function is called at module loading time. This situation can't obtain
-  # during tracing and instead can arise when there are module-level constants
-  # computed using lax or lax_numpy.
-  if FLAGS.is_parsed():
-    return _canonicalize_dtype(dtype)
-  else:
-    return dtype
-
-
 @memoize
-def _canonicalize_dtype(dtype):
+def canonicalize_dtype(dtype):
   """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
   dtype = onp.dtype(dtype)
   if FLAGS.jax_enable_x64:","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 442a1f935..053c6bbe9 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -26,7 +26,7 @@ from __future__ import print_function
 import os
 import warnings
 
-from absl import flags
+from ..config import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
 from . import xla_data_pb2
@@ -206,21 +206,8 @@ _dtype_to_32bit_dtype = {
 }
 
 
-def canonicalize_dtype(dtype):
-  """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
-  # This function is a thin wrapper around the memoized _canonicalize_dtype to
-  # handle the case where FLAGS haven't been parsed yet, for example because
-  # this function is called at module loading time. This situation can't obtain
-  # during tracing and instead can arise when there are module-level constants
-  # computed using lax or lax_numpy.
-  if FLAGS.is_parsed():
-    return _canonicalize_dtype(dtype)
-  else:
-    return dtype
-
-
 @memoize
-def _canonicalize_dtype(dtype):
+def canonicalize_dtype(dtype):
   """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
   dtype = onp.dtype(dtype)
   if FLAGS.jax_enable_x64:",No
jax/test_util.py,jax/test_util.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/jax/test_util.py b/jax/test_util.py
index 4c01b612a..2c05a0b65 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -19,7 +19,6 @@ from __future__ import print_function
 import functools
 import re
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -27,6 +26,7 @@ import numpy as onp
 import numpy.random as npr
 
 from . import api
+from .config import flags
 from .util import partial
 from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce
 ","diff --git a/jax/test_util.py b/jax/test_util.py
index 4c01b612a..2c05a0b65 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -19,7 +19,6 @@ from __future__ import print_function
 import functools
 import re
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -27,6 +26,7 @@ import numpy as onp
 import numpy.random as npr
 
 from . import api
+from .config import flags
 from .util import partial
 from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce
 ",No
tests/api_test.py,tests/api_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/api_test.py b/tests/api_test.py
index 188fff8e9..39eb326b7 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,6 +23,7 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad, device_get, device_put
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
@@ -239,4 +240,5 @@ class APITest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
index 188fff8e9..39eb326b7 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,6 +23,7 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
+from jax.config import config
 from jax import jit, grad, device_get, device_put
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
@@ -239,4 +240,5 @@ class APITest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()",No
tests/batching_test.py,tests/batching_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/batching_test.py b/tests/batching_test.py
index d42012f51..fd372bc47 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -26,6 +26,7 @@ from jax.abstract_arrays import ShapedArray
 from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
 from jax.api import vmap
+from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial
@@ -195,4 +196,5 @@ class BatchingTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index d42012f51..fd372bc47 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -26,6 +26,7 @@ from jax.abstract_arrays import ShapedArray
 from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
 from jax.api import vmap
+from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial
@@ -195,4 +196,5 @@ class BatchingTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()",No
tests/core_test.py,tests/core_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/core_test.py b/tests/core_test.py
index 52383a6ac..6091d92ef 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -28,6 +28,7 @@ from jax import core
 from jax import numpy as np
 from jax import test_util as jtu
 from jax.api import jvp, linearize, vjp, jit
+from jax.config import config
 from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
 from jax.tree_util import tree_flatten, tree_unflatten, tree_multimap, tree_reduce
 from jax.util import partial
@@ -331,4 +332,5 @@ class CoreTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/core_test.py b/tests/core_test.py
index 52383a6ac..6091d92ef 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -28,6 +28,7 @@ from jax import core
 from jax import numpy as np
 from jax import test_util as jtu
 from jax.api import jvp, linearize, vjp, jit
+from jax.config import config
 from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
 from jax.tree_util import tree_flatten, tree_unflatten, tree_multimap, tree_reduce
 from jax.util import partial
@@ -331,4 +332,5 @@ class CoreTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()",No
tests/lapax_test.py,tests/lapax_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
index 07ef0ad20..444156171 100644
--- a/tests/lapax_test.py
+++ b/tests/lapax_test.py
@@ -25,6 +25,7 @@ from absl.testing import parameterized
 
 from jax import jit
 from jax import test_util as jtu
+from jax.config import config
 from jax.experimental import lapax
 
 
@@ -202,4 +203,5 @@ class LapaxTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
index 07ef0ad20..444156171 100644
--- a/tests/lapax_test.py
+++ b/tests/lapax_test.py
@@ -25,6 +25,7 @@ from absl.testing import parameterized
 
 from jax import jit
 from jax import test_util as jtu
+from jax.config import config
 from jax.experimental import lapax
 
 
@@ -202,4 +203,5 @@ class LapaxTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 6b4db39dd..024c3e991 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -20,7 +20,6 @@ import collections
 from functools import partial
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -30,8 +29,9 @@ from jax import api
 from jax import lax
 from jax import numpy as lnp
 from jax import test_util as jtu
+from jax.config import config
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 # We disable the whitespace continuation check in this file because otherwise it
 # makes the test name formatting unwieldy.
@@ -587,4 +587,5 @@ class IndexingTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 6b4db39dd..024c3e991 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -20,7 +20,6 @@ import collections
 from functools import partial
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -30,8 +29,9 @@ from jax import api
 from jax import lax
 from jax import numpy as lnp
 from jax import test_util as jtu
+from jax.config import config
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 # We disable the whitespace continuation check in this file because otherwise it
 # makes the test name formatting unwieldy.
@@ -587,4 +587,5 @@ class IndexingTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 70280c0c6..3881dc06b 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -20,7 +20,6 @@ import collections
 import functools
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -29,8 +28,9 @@ import numpy as onp
 from jax import api
 from jax import numpy as lnp
 from jax import test_util as jtu
+from jax.config import config
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
 
@@ -542,4 +542,5 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 70280c0c6..3881dc06b 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -20,7 +20,6 @@ import collections
 import functools
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -29,8 +28,9 @@ import numpy as onp
 from jax import api
 from jax import numpy as lnp
 from jax import test_util as jtu
+from jax.config import config
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
 
@@ -542,4 +542,5 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index bd587d98c..53c1e688f 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -20,7 +20,6 @@ import collections
 import functools
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -31,11 +30,12 @@ import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
+from jax.config import config
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
 from jax.scipy import stats as lsp_stats
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
@@ -154,4 +154,5 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index bd587d98c..53c1e688f 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -20,7 +20,6 @@ import collections
 import functools
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -31,11 +30,12 @@ import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
+from jax.config import config
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
 from jax.scipy import stats as lsp_stats
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
@@ -154,4 +154,5 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()",No
tests/lax_test.py,tests/lax_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 800709fef..f1c3d8580 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -21,7 +21,6 @@ import functools
 from functools import partial
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -33,10 +32,11 @@ from jax import core
 from jax import lax
 from jax import test_util as jtu
 from jax import lax_reference
+from jax.config import config
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 
 def num_float_bits(dtype):
@@ -1986,4 +1986,5 @@ class LaxAutodiffTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 800709fef..f1c3d8580 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -21,7 +21,6 @@ import functools
 from functools import partial
 import itertools
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -33,10 +32,11 @@ from jax import core
 from jax import lax
 from jax import test_util as jtu
 from jax import lax_reference
+from jax.config import config
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 
 def num_float_bits(dtype):
@@ -1986,4 +1986,5 @@ class LaxAutodiffTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()",No
tests/minmax_test.py,tests/minmax_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 5eee658ce..a548d1362 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 import functools
 
 from absl.testing import absltest
-
+from jax.config import config
 import jax.numpy as np
 import jax.test_util as jtu
 from jax import jit, grad
@@ -170,4 +170,5 @@ class OptimizerTests(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 5eee658ce..a548d1362 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 import functools
 
 from absl.testing import absltest
-
+from jax.config import config
 import jax.numpy as np
 import jax.test_util as jtu
 from jax import jit, grad
@@ -170,4 +170,5 @@ class OptimizerTests(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
+  config.config_with_absl()
   absltest.main()",No
tests/random_test.py,tests/random_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/random_test.py b/tests/random_test.py
index c5ac91106..e90582b92 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -16,7 +16,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -28,8 +27,9 @@ from jax import api
 from jax import lax
 from jax import random
 from jax import test_util as jtu
+from jax.config import config
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 
 class LaxRandomTest(jtu.JaxTestCase):
@@ -150,4 +150,5 @@ class LaxRandomTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
index c5ac91106..e90582b92 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -16,7 +16,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 
@@ -28,8 +27,9 @@ from jax import api
 from jax import lax
 from jax import random
 from jax import test_util as jtu
+from jax.config import config
 
-FLAGS = flags.FLAGS
+FLAGS = config.FLAGS
 
 
 class LaxRandomTest(jtu.JaxTestCase):
@@ -150,4 +150,5 @@ class LaxRandomTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()",No
tests/stax_test.py,tests/stax_test.py,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,1d2aaad6fe30fef5e7e15a2898358f9120a3686d,"Made a shim to handle configuration without having absl parse command-line flags.

PiperOrigin-RevId: 223391288","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 1689301f9..3b48dadfb 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -24,6 +24,7 @@ import numpy as onp
 
 from jax import test_util as jtu
 from jax import random
+from jax.config import config
 from jax.experimental import stax
 
 
@@ -129,4 +130,5 @@ class StaxTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 1689301f9..3b48dadfb 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -24,6 +24,7 @@ import numpy as onp
 
 from jax import test_util as jtu
 from jax import random
+from jax.config import config
 from jax.experimental import stax
 
 
@@ -129,4 +130,5 @@ class StaxTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
+  config.config_with_absl()
   absltest.main()",No
jax/api.py,jax/api.py,0411beb357274410ccaf694c36b7d902244a5825,aab9faa41d08a300c2f1a647f39ec35c6185f105,Wrapped static args to jit to be hashed on id. This is conservative but simple and cheap.,"diff --git a/jax/api.py b/jax/api.py
index 70e290003..4bd4005ea 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -26,7 +26,7 @@ from .core import pack, eval_jaxpr
 from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                         tree_map)
-from .util import unzip2, unzip3, curry, partial, safe_map
+from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
@@ -191,17 +191,16 @@ def argnums_partial(f, dyn_argnums, args):
     dyn_argnums = (dyn_argnums,)
   else:
     dyn_argnums = tuple(dyn_argnums)
-  fixed_args = tuple([None if i in dyn_argnums else arg for i, arg in enumerate(args)])
+  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
+                      for i, arg in enumerate(args)])
   dyn_args = [args[i] for i in dyn_argnums]
   return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args
 
 @lu.transformation
 def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
-  args = list(fixed_args)
+  args = [None if arg is None else arg.val for arg in fixed_args]
   for i, arg in zip(dyn_argnums, dyn_args):
     args[i] = arg
-
-
   ans = yield args
   yield ans
 ","diff --git a/jax/api.py b/jax/api.py
index 70e290003..4bd4005ea 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -26,7 +26,7 @@ from .core import pack, eval_jaxpr
 from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                         tree_map)
-from .util import unzip2, unzip3, curry, partial, safe_map
+from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
@@ -191,17 +191,16 @@ def argnums_partial(f, dyn_argnums, args):
     dyn_argnums = (dyn_argnums,)
   else:
     dyn_argnums = tuple(dyn_argnums)
-  fixed_args = tuple([None if i in dyn_argnums else arg for i, arg in enumerate(args)])
+  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
+                      for i, arg in enumerate(args)])
   dyn_args = [args[i] for i in dyn_argnums]
   return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args
 
 @lu.transformation
 def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
-  args = list(fixed_args)
+  args = [None if arg is None else arg.val for arg in fixed_args]
   for i, arg in zip(dyn_argnums, dyn_args):
     args[i] = arg
-
-
   ans = yield args
   yield ans
 ",No
jax/util.py,jax/util.py,0411beb357274410ccaf694c36b7d902244a5825,aab9faa41d08a300c2f1a647f39ec35c6185f105,Wrapped static args to jit to be hashed on id. This is conservative but simple and cheap.,"diff --git a/jax/util.py b/jax/util.py
index 62587fe2d..38d22f00f 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -143,3 +143,15 @@ def memoize(fun):
 
 def prod(xs):
   return functools.reduce(mul, xs, 1)
+
+
+class WrapHashably(object):
+  def __init__(self, val):
+    self.val = val
+
+  def __hash__(self):
+    return id(self.val)
+
+  def __eq__(self, other):
+    return self.val == other.val
+","diff --git a/jax/util.py b/jax/util.py
index 62587fe2d..38d22f00f 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -143,3 +143,15 @@ def memoize(fun):
 
 def prod(xs):
   return functools.reduce(mul, xs, 1)
+
+
+class WrapHashably(object):
+  def __init__(self, val):
+    self.val = val
+
+  def __hash__(self):
+    return id(self.val)
+
+  def __eq__(self, other):
+    return self.val == other.val
+",No
jax/__init__.py,jax/__init__.py,5d5a6bc7ced30564d67eb675599a9ece4fd71cc8,0411beb357274410ccaf694c36b7d902244a5825,"Set default TF log level to ""1"" to avoid reporting things like CPU frequency at import time. Also import jax.numpy in __init__.py because it has side effects that set up the infix operator overloading.","diff --git a/jax/__init__.py b/jax/__init__.py
index 46a817b68..8ea62cb7e 100644
--- a/jax/__init__.py
+++ b/jax/__init__.py
@@ -12,4 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
+os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '1')
 from jax.api import *
+import jax.numpy as np  # side-effecting import sets up operator overloads","diff --git a/jax/__init__.py b/jax/__init__.py
index 46a817b68..8ea62cb7e 100644
--- a/jax/__init__.py
+++ b/jax/__init__.py
@@ -12,4 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
+os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '1')
 from jax.api import *
+import jax.numpy as np  # side-effecting import sets up operator overloads",No
tests/quickercheck.py,tests/quickercheck.py,4715ce52ec482c0a078f98923001a45d0a9d53f9,5d5a6bc7ced30564d67eb675599a9ece4fd71cc8,Added forward derivative checks,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index c74f50988..145ff8a95 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,8 +2,9 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit
+from jax import jit, jvp
 import itertools as it
+import sys
 
 npr.seed(0)
 
@@ -151,26 +152,61 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
+def gen_inputs(fun):
+  return [gen_array_val(v.vartype) for v in fun.in_vars]
+
+def jvp_fd(fun, args, directions):
+  EPS = 1e-4
+  def eval_eps(eps):
+    return fun(*[x if d is None else x + eps * d
+                 for x, d in zip(args, directions)])
+
+  ys_neg = eval_eps(-EPS)
+  ys_pos = eval_eps(EPS)
+  ys = eval_eps(0.0)
+  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
+  return ys, deriv
+
+def check_all_close(xs, ys, tol=1e-3):
+  for x, y in zip(xs, ys):
+    assert x.shape == y.shape
+    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+    # assert x.dtype == y.dtype
+    assert np.allclose(x, y, rtol=tol, atol=tol), \
+       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 def jit_is_identity(fun):
-  vals = map(gen_array_val, [v.vartype for v in fun.in_vars])
+  vals = gen_inputs(fun)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
-  ans_jitted = jit(fun)(*vals)
-  for i, (x, x_jit) in enumerate(zip(ans, ans_jitted)):
-    assert x.shape == x_jit.shape
-    # assert x.dtype == x_jit.dtype, 'dtype mismatch: {} != {}'.format(x.dtype, x_jit.dtype)
-    assert np.allclose(x, x_jit)
+  static_argnums = thin(range(len(vals)), 0.5)
+  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
+  check_all_close(ans, ans_jitted)
+
+def jvp_matches_fd(fun):
+  vals = gen_inputs(fun)
+  directions = gen_inputs(fun)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, deriv1 = jvp_fd(fun, vals, directions)
+  ans2, deriv2 = jvp(fun, vals, directions)
+  check_all_close(ans1, ans2)
+  check_all_close(deriv1, deriv2)
 
 properties = [
-  jit_is_identity ]
-  # jvp_matches_fd,
-  # vjp_matches_fd,
-  # vmap_matches_map ]
+  jit_is_identity,
+  jvp_matches_fd,
+]
+# vjp_matches_fd,
+# vmap_matches_map ]
 
 def run_tests():
-  sizes = [3, 10, 30]
-  num_examples = 30
-  for size, _, check_prop in it.product(sizes, range(num_examples), properties):
+  sizes = [3, 10]
+  num_examples = 50
+  cases = it.product(sizes, range(num_examples), properties)
+  for i, (size, _, check_prop) in enumerate(cases):
+    sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
 
 if __name__ == ""__main__"":","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index c74f50988..145ff8a95 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,8 +2,9 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit
+from jax import jit, jvp
 import itertools as it
+import sys
 
 npr.seed(0)
 
@@ -151,26 +152,61 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
+def gen_inputs(fun):
+  return [gen_array_val(v.vartype) for v in fun.in_vars]
+
+def jvp_fd(fun, args, directions):
+  EPS = 1e-4
+  def eval_eps(eps):
+    return fun(*[x if d is None else x + eps * d
+                 for x, d in zip(args, directions)])
+
+  ys_neg = eval_eps(-EPS)
+  ys_pos = eval_eps(EPS)
+  ys = eval_eps(0.0)
+  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
+  return ys, deriv
+
+def check_all_close(xs, ys, tol=1e-3):
+  for x, y in zip(xs, ys):
+    assert x.shape == y.shape
+    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+    # assert x.dtype == y.dtype
+    assert np.allclose(x, y, rtol=tol, atol=tol), \
+       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 def jit_is_identity(fun):
-  vals = map(gen_array_val, [v.vartype for v in fun.in_vars])
+  vals = gen_inputs(fun)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
-  ans_jitted = jit(fun)(*vals)
-  for i, (x, x_jit) in enumerate(zip(ans, ans_jitted)):
-    assert x.shape == x_jit.shape
-    # assert x.dtype == x_jit.dtype, 'dtype mismatch: {} != {}'.format(x.dtype, x_jit.dtype)
-    assert np.allclose(x, x_jit)
+  static_argnums = thin(range(len(vals)), 0.5)
+  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
+  check_all_close(ans, ans_jitted)
+
+def jvp_matches_fd(fun):
+  vals = gen_inputs(fun)
+  directions = gen_inputs(fun)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, deriv1 = jvp_fd(fun, vals, directions)
+  ans2, deriv2 = jvp(fun, vals, directions)
+  check_all_close(ans1, ans2)
+  check_all_close(deriv1, deriv2)
 
 properties = [
-  jit_is_identity ]
-  # jvp_matches_fd,
-  # vjp_matches_fd,
-  # vmap_matches_map ]
+  jit_is_identity,
+  jvp_matches_fd,
+]
+# vjp_matches_fd,
+# vmap_matches_map ]
 
 def run_tests():
-  sizes = [3, 10, 30]
-  num_examples = 30
-  for size, _, check_prop in it.product(sizes, range(num_examples), properties):
+  sizes = [3, 10]
+  num_examples = 50
+  cases = it.product(sizes, range(num_examples), properties)
+  for i, (size, _, check_prop) in enumerate(cases):
+    sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
 
 if __name__ == ""__main__"":",No
tests/quickercheck.py,tests/quickercheck.py,a5df01abfdcd27dd4ef95171dca687347a251fde,4715ce52ec482c0a078f98923001a45d0a9d53f9,Added reverse-mode checks,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 145ff8a95..8191b8182 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,7 +2,7 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit, jvp
+from jax import jit, jvp, vjp
 import itertools as it
 import sys
 
@@ -152,14 +152,19 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
-def gen_inputs(fun):
-  return [gen_array_val(v.vartype) for v in fun.in_vars]
+def gen_vals(vs):
+  return [gen_array_val(v.vartype) for v in vs]
 
-def jvp_fd(fun, args, directions):
+def inner_prod(xs, ys):
+  xys = zip(xs, ys)
+  assert all(x.shape == y.shape for x, y in xys)
+  return sum(np.sum(x * y) for x, y in xys)
+
+def jvp_fd(fun, args, tangents):
   EPS = 1e-4
   def eval_eps(eps):
-    return fun(*[x if d is None else x + eps * d
-                 for x, d in zip(args, directions)])
+    return fun(*[x if t is None else x + eps * t
+                 for x, t in zip(args, tangents)])
 
   ys_neg = eval_eps(-EPS)
   ys_pos = eval_eps(EPS)
@@ -169,14 +174,18 @@ def jvp_fd(fun, args, directions):
 
 def check_all_close(xs, ys, tol=1e-3):
   for x, y in zip(xs, ys):
-    assert x.shape == y.shape
-    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
-    # assert x.dtype == y.dtype
-    assert np.allclose(x, y, rtol=tol, atol=tol), \
-       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+    check_close(x, y, tol)
+
+def check_close(x, y, tol=1e-3):
+  assert np.shape(x) == np.shape(y)
+  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+  # assert x.dtype == y.dtype
+  assert np.allclose(x, y, rtol=tol, atol=tol), \
+     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 
 def jit_is_identity(fun):
-  vals = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
   static_argnums = thin(range(len(vals)), 0.5)
@@ -184,21 +193,39 @@ def jit_is_identity(fun):
   check_all_close(ans, ans_jitted)
 
 def jvp_matches_fd(fun):
-  vals = gen_inputs(fun)
-  directions = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
+  tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
 
   # TODO: differentiate wrt some inputs only
-  ans1, deriv1 = jvp_fd(fun, vals, directions)
-  ans2, deriv2 = jvp(fun, vals, directions)
+  ans1, deriv1 = jvp_fd(fun, vals, tangents)
+  ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
   check_all_close(deriv1, deriv2)
 
+
+def vjp_matches_fd(fun):
+  # print fun
+  vals = gen_vals(fun.in_vars)
+  in_tangents = gen_vals(fun.in_vars)
+  in_cotangents = gen_vals(fun.out_vars)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+  ans2, vjpfun = vjp(fun, *vals)
+  out_cotangents = vjpfun(in_cotangents)
+  check_all_close(ans1, ans2)
+  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+  check_close(inner_prod_fd, inner_prod_ad)
+
+
 properties = [
   jit_is_identity,
   jvp_matches_fd,
+  vjp_matches_fd,
 ]
-# vjp_matches_fd,
 # vmap_matches_map ]
 
 def run_tests():","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 145ff8a95..8191b8182 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,7 +2,7 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit, jvp
+from jax import jit, jvp, vjp
 import itertools as it
 import sys
 
@@ -152,14 +152,19 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
-def gen_inputs(fun):
-  return [gen_array_val(v.vartype) for v in fun.in_vars]
+def gen_vals(vs):
+  return [gen_array_val(v.vartype) for v in vs]
 
-def jvp_fd(fun, args, directions):
+def inner_prod(xs, ys):
+  xys = zip(xs, ys)
+  assert all(x.shape == y.shape for x, y in xys)
+  return sum(np.sum(x * y) for x, y in xys)
+
+def jvp_fd(fun, args, tangents):
   EPS = 1e-4
   def eval_eps(eps):
-    return fun(*[x if d is None else x + eps * d
-                 for x, d in zip(args, directions)])
+    return fun(*[x if t is None else x + eps * t
+                 for x, t in zip(args, tangents)])
 
   ys_neg = eval_eps(-EPS)
   ys_pos = eval_eps(EPS)
@@ -169,14 +174,18 @@ def jvp_fd(fun, args, directions):
 
 def check_all_close(xs, ys, tol=1e-3):
   for x, y in zip(xs, ys):
-    assert x.shape == y.shape
-    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
-    # assert x.dtype == y.dtype
-    assert np.allclose(x, y, rtol=tol, atol=tol), \
-       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+    check_close(x, y, tol)
+
+def check_close(x, y, tol=1e-3):
+  assert np.shape(x) == np.shape(y)
+  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+  # assert x.dtype == y.dtype
+  assert np.allclose(x, y, rtol=tol, atol=tol), \
+     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 
 def jit_is_identity(fun):
-  vals = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
   static_argnums = thin(range(len(vals)), 0.5)
@@ -184,21 +193,39 @@ def jit_is_identity(fun):
   check_all_close(ans, ans_jitted)
 
 def jvp_matches_fd(fun):
-  vals = gen_inputs(fun)
-  directions = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
+  tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
 
   # TODO: differentiate wrt some inputs only
-  ans1, deriv1 = jvp_fd(fun, vals, directions)
-  ans2, deriv2 = jvp(fun, vals, directions)
+  ans1, deriv1 = jvp_fd(fun, vals, tangents)
+  ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
   check_all_close(deriv1, deriv2)
 
+
+def vjp_matches_fd(fun):
+  # print fun
+  vals = gen_vals(fun.in_vars)
+  in_tangents = gen_vals(fun.in_vars)
+  in_cotangents = gen_vals(fun.out_vars)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+  ans2, vjpfun = vjp(fun, *vals)
+  out_cotangents = vjpfun(in_cotangents)
+  check_all_close(ans1, ans2)
+  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+  check_close(inner_prod_fd, inner_prod_ad)
+
+
 properties = [
   jit_is_identity,
   jvp_matches_fd,
+  vjp_matches_fd,
 ]
-# vjp_matches_fd,
 # vmap_matches_map ]
 
 def run_tests():",No
tests/BUILD,tests/BUILD,20878c76f449bbd28dd143b6227485e2cb638a30,2df36f7510e5cd3c7a3be639c7827d2ff3ae0c01,"source sync

PiperOrigin-RevId: 223530503","diff --git a/tests/BUILD b/tests/BUILD
index 95d6ee29e..141aa38dd 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -27,6 +27,7 @@ jax_test(
 jax_test(
     name = ""lax_test"",
     srcs = [""lax_test.py""],
+    disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -36,6 +37,7 @@ jax_test(
 jax_test(
     name = ""lax_numpy_test"",
     srcs = [""lax_numpy_test.py""],
+    disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,","diff --git a/tests/BUILD b/tests/BUILD
index 95d6ee29e..141aa38dd 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -27,6 +27,7 @@ jax_test(
 jax_test(
     name = ""lax_test"",
     srcs = [""lax_test.py""],
+    disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -36,6 +37,7 @@ jax_test(
 jax_test(
     name = ""lax_numpy_test"",
     srcs = [""lax_numpy_test.py""],
+    disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,",No
tests/BUILD,tests/BUILD,f5b051b431f505ea410a0c198dd84e67b8584bb8,20878c76f449bbd28dd143b6227485e2cb638a30,"Double gpu test shards

PiperOrigin-RevId: 223647959","diff --git a/tests/BUILD b/tests/BUILD
index 141aa38dd..0669300c8 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -30,7 +30,7 @@ jax_test(
     disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
-        ""gpu"": 20,
+        ""gpu"": 40,
     },
 )
 
@@ -40,7 +40,7 @@ jax_test(
     disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
-        ""gpu"": 20,
+        ""gpu"": 40,
     },
 )
 
@@ -49,7 +49,7 @@ jax_test(
     srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
-        ""gpu"": 2,
+        ""gpu"": 4,
     },
 )
 
@@ -58,7 +58,7 @@ jax_test(
     srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
-        ""gpu"": 2,
+        ""gpu"": 4,
     },
 )
 ","diff --git a/tests/BUILD b/tests/BUILD
index 141aa38dd..0669300c8 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -30,7 +30,7 @@ jax_test(
     disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
-        ""gpu"": 20,
+        ""gpu"": 40,
     },
 )
 
@@ -40,7 +40,7 @@ jax_test(
     disable = [""tpu""],
     shard_count = {
         ""cpu"": 40,
-        ""gpu"": 20,
+        ""gpu"": 40,
     },
 )
 
@@ -49,7 +49,7 @@ jax_test(
     srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
-        ""gpu"": 2,
+        ""gpu"": 4,
     },
 )
 
@@ -58,7 +58,7 @@ jax_test(
     srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
-        ""gpu"": 2,
+        ""gpu"": 4,
     },
 )
 ",No
tests/quickercheck.py,tests/quickercheck.py,99023a24fa076b103a168f05efa69f97b0f6ba88,a5df01abfdcd27dd4ef95171dca687347a251fde,Made some subset of vjp/jvp inputs static in quickercheck. Exposing bugs.,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 8191b8182..34162c5a2 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -183,6 +183,16 @@ def check_close(x, y, tol=1e-3):
   assert np.allclose(x, y, rtol=tol, atol=tol), \
      ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
 
+def partial_argnums(f, args, dyn_argnums):
+  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
+  def f_(*dyn_args):
+    args = fixed_args[:]
+    for i, arg in zip(dyn_argnums, dyn_args):
+      args[i] = arg
+    return f(*args)
+
+  dyn_args = [args[i] for i in dyn_argnums]
+  return f_, dyn_args
 
 def jit_is_identity(fun):
   vals = gen_vals(fun.in_vars)
@@ -196,8 +206,9 @@ def jvp_matches_fd(fun):
   vals = gen_vals(fun.in_vars)
   tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  tangents = [tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, deriv1 = jvp_fd(fun, vals, tangents)
   ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
@@ -210,8 +221,9 @@ def vjp_matches_fd(fun):
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  in_tangents = [in_tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
   ans2, vjpfun = vjp(fun, *vals)
   out_cotangents = vjpfun(in_cotangents)
@@ -235,6 +247,7 @@ def run_tests():
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
+  print ""\nok""
 
 if __name__ == ""__main__"":
   run_tests()","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 8191b8182..34162c5a2 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -183,6 +183,16 @@ def check_close(x, y, tol=1e-3):
   assert np.allclose(x, y, rtol=tol, atol=tol), \
      ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
 
+def partial_argnums(f, args, dyn_argnums):
+  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
+  def f_(*dyn_args):
+    args = fixed_args[:]
+    for i, arg in zip(dyn_argnums, dyn_args):
+      args[i] = arg
+    return f(*args)
+
+  dyn_args = [args[i] for i in dyn_argnums]
+  return f_, dyn_args
 
 def jit_is_identity(fun):
   vals = gen_vals(fun.in_vars)
@@ -196,8 +206,9 @@ def jvp_matches_fd(fun):
   vals = gen_vals(fun.in_vars)
   tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  tangents = [tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, deriv1 = jvp_fd(fun, vals, tangents)
   ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
@@ -210,8 +221,9 @@ def vjp_matches_fd(fun):
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  in_tangents = [in_tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
   ans2, vjpfun = vjp(fun, *vals)
   out_cotangents = vjpfun(in_cotangents)
@@ -235,6 +247,7 @@ def run_tests():
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
+  print ""\nok""
 
 if __name__ == ""__main__"":
   run_tests()",No
jax/api.py,jax/api.py,f5232aaeea52c794569d83a5578ff54d82902c52,99023a24fa076b103a168f05efa69f97b0f6ba88,Fixed bug in vjp with constant-zero tangent outputs,"diff --git a/jax/api.py b/jax/api.py
index 4bd4005ea..9a2240d58 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -123,8 +123,8 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
   def fun(*args):
     primals = pack(args) # doesn't matter what these are-they'll be ignored
     tangents = pack(args)
-    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
-    return list(pe.merge_pvals(ans, out_pval))[1]
+    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
+    return pe.merge_pvals(ans, out_pval)
 
   return unflatten_fun(fun, io_tree, *py_args)
 ","diff --git a/jax/api.py b/jax/api.py
index 4bd4005ea..9a2240d58 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -123,8 +123,8 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
   def fun(*args):
     primals = pack(args) # doesn't matter what these are-they'll be ignored
     tangents = pack(args)
-    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
-    return list(pe.merge_pvals(ans, out_pval))[1]
+    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
+    return pe.merge_pvals(ans, out_pval)
 
   return unflatten_fun(fun, io_tree, *py_args)
 ",No
jax/interpreters/ad.py,jax/interpreters/ad.py,f5232aaeea52c794569d83a5578ff54d82902c52,99023a24fa076b103a168f05efa69f97b0f6ba88,Fixed bug in vjp with constant-zero tangent outputs,"diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f..3f205b1db 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -64,21 +64,40 @@ def linearize(traceable, *primals):
   in_pvals = (pe.PartialVal((None, pack(primals))),
               pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
   jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
-  out_pv, out_const = out_pval
-  assert out_pv is None or out_pv[0] is None
-  primal_out = tuple(out_const)[0]
-  return primal_out, out_pval, jaxpr, consts
-
+  pval_primal, pval_tangent = unpair_pval(out_pval)
+  aval_primal, const_primal = pval_primal
+  assert aval_primal is None
+  return const_primal, pval_tangent, jaxpr, consts
 
 def vjp(traceable, primals):
-  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
+  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
   def vjp_(ct):
+    ct = ignore_consts(ct, pval)
     dummy_primal_and_ct = pack((core.unit, ct))
     _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
     return instantiate_zeros(pack(primals), arg_cts[1])
 
   return out_primal, vjp_
 
+def ignore_consts(ct, pval):
+  aval, const = pval
+  if isinstance(aval, core.AbstractValue):
+    return ct
+  elif isinstance(aval, pe.JaxprTracerTuple):
+    return pack(map(ignore_consts, ct, zip(aval, const)))
+  elif aval is None:
+    return core.unit
+  else:
+    raise TypeError(aval)
+
+def unpair_pval(pval):
+  aval, const = pval
+  const_1, const_2 = const
+  if aval is None:
+    return (None, const_1), (None, const_2)
+  else:
+    aval_1, aval_2 = aval
+    return (aval_1, const_1), (aval_2, const_2)
 
 def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
   def write_cotangent(v, ct):
@@ -116,8 +135,8 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
-    for var, ct in builtins.zip(eqn.invars, cts_out):
+
+    for var, ct in zip(eqn.invars, cts_out):
       write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f..3f205b1db 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -64,21 +64,40 @@ def linearize(traceable, *primals):
   in_pvals = (pe.PartialVal((None, pack(primals))),
               pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
   jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
-  out_pv, out_const = out_pval
-  assert out_pv is None or out_pv[0] is None
-  primal_out = tuple(out_const)[0]
-  return primal_out, out_pval, jaxpr, consts
-
+  pval_primal, pval_tangent = unpair_pval(out_pval)
+  aval_primal, const_primal = pval_primal
+  assert aval_primal is None
+  return const_primal, pval_tangent, jaxpr, consts
 
 def vjp(traceable, primals):
-  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
+  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
   def vjp_(ct):
+    ct = ignore_consts(ct, pval)
     dummy_primal_and_ct = pack((core.unit, ct))
     _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
     return instantiate_zeros(pack(primals), arg_cts[1])
 
   return out_primal, vjp_
 
+def ignore_consts(ct, pval):
+  aval, const = pval
+  if isinstance(aval, core.AbstractValue):
+    return ct
+  elif isinstance(aval, pe.JaxprTracerTuple):
+    return pack(map(ignore_consts, ct, zip(aval, const)))
+  elif aval is None:
+    return core.unit
+  else:
+    raise TypeError(aval)
+
+def unpair_pval(pval):
+  aval, const = pval
+  const_1, const_2 = const
+  if aval is None:
+    return (None, const_1), (None, const_2)
+  else:
+    aval_1, aval_2 = aval
+    return (aval_1, const_1), (aval_2, const_2)
 
 def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
   def write_cotangent(v, ct):
@@ -116,8 +135,8 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
-    for var, ct in builtins.zip(eqn.invars, cts_out):
+
+    for var, ct in zip(eqn.invars, cts_out):
       write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)",No
tests/quickercheck.py,tests/quickercheck.py,f5232aaeea52c794569d83a5578ff54d82902c52,99023a24fa076b103a168f05efa69f97b0f6ba88,Fixed bug in vjp with constant-zero tangent outputs,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 34162c5a2..828e12c13 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -216,7 +216,6 @@ def jvp_matches_fd(fun):
 
 
 def vjp_matches_fd(fun):
-  # print fun
   vals = gen_vals(fun.in_vars)
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
@@ -232,7 +231,6 @@ def vjp_matches_fd(fun):
   inner_prod_ad = inner_prod(in_tangents, out_cotangents)
   check_close(inner_prod_fd, inner_prod_ad)
 
-
 properties = [
   jit_is_identity,
   jvp_matches_fd,
@@ -246,7 +244,13 @@ def run_tests():
   cases = it.product(sizes, range(num_examples), properties)
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
-    check_prop(gen_fun_and_types(size))
+    try:
+      fun = gen_fun_and_types(size)
+      check_prop(fun)
+    except:
+      print fun
+      raise
+
   print ""\nok""
 
 if __name__ == ""__main__"":","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 34162c5a2..828e12c13 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -216,7 +216,6 @@ def jvp_matches_fd(fun):
 
 
 def vjp_matches_fd(fun):
-  # print fun
   vals = gen_vals(fun.in_vars)
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
@@ -232,7 +231,6 @@ def vjp_matches_fd(fun):
   inner_prod_ad = inner_prod(in_tangents, out_cotangents)
   check_close(inner_prod_fd, inner_prod_ad)
 
-
 properties = [
   jit_is_identity,
   jvp_matches_fd,
@@ -246,7 +244,13 @@ def run_tests():
   cases = it.product(sizes, range(num_examples), properties)
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
-    check_prop(gen_fun_and_types(size))
+    try:
+      fun = gen_fun_and_types(size)
+      check_prop(fun)
+    except:
+      print fun
+      raise
+
   print ""\nok""
 
 if __name__ == ""__main__"":",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,585f011d63e77bfafc200fff50d13f0af73e17f7,f5232aaeea52c794569d83a5578ff54d82902c52,Error message for unimplemented numpy functions,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d0a43546a..82b078bd9 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -820,9 +820,10 @@ def _argminmax(op, a, axis):
   return min(mask_idxs, axis)
 
 
-# TODO plan how to handle unsupported ops
 def _not_implemented(fun):
-  return None
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
 
 argpartition = _not_implemented(onp.argpartition)
 argsort = _not_implemented(onp.argsort)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d0a43546a..82b078bd9 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -820,9 +820,10 @@ def _argminmax(op, a, axis):
   return min(mask_idxs, axis)
 
 
-# TODO plan how to handle unsupported ops
 def _not_implemented(fun):
-  return None
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
 
 argpartition = _not_implemented(onp.argpartition)
 argsort = _not_implemented(onp.argsort)",No
tests/quickercheck.py,tests/quickercheck.py,ac6cee215710f33c5164632201174e2fa90b7606,585f011d63e77bfafc200fff50d13f0af73e17f7,Added license header,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 828e12c13..ab5579575 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from collections import namedtuple
 from functools import partial
 import numpy.random as npr","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 828e12c13..ab5579575 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from collections import namedtuple
 from functools import partial
 import numpy.random as npr",No
WORKSPACE,WORKSPACE,77db9bd556a753c348868a7587f7a7d73eb4a5ff,f5b051b431f505ea410a0c198dd84e67b8584bb8,"[JAX] Update XLA in JAX workspace to include optimized computation launch implementation.

PiperOrigin-RevId: 223863753","diff --git a/WORKSPACE b/WORKSPACE
index c2770f000..a3e97ebfa 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,10 +16,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""9e5dbd0186eebb35577d6c71337ad79774732f52b7e89f7bd1d974153bfe7a5e"",
-    strip_prefix=""tensorflow-0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b"",
+    sha256 = ""1a685ee839eb52e5fd26c0c9fbb0cddbf39a49cc5edaa6506604a3ba0fecfa2e"",
+    strip_prefix=""tensorflow-47fe933d89146a0488563bdfde2c0675a750ed16"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b.tar.gz"",
+      ""https://github.com/tensorflow/tensorflow/archive/47fe933d89146a0488563bdfde2c0675a750ed16.tar.gz"",
     ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index c2770f000..a3e97ebfa 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,10 +16,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""9e5dbd0186eebb35577d6c71337ad79774732f52b7e89f7bd1d974153bfe7a5e"",
-    strip_prefix=""tensorflow-0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b"",
+    sha256 = ""1a685ee839eb52e5fd26c0c9fbb0cddbf39a49cc5edaa6506604a3ba0fecfa2e"",
+    strip_prefix=""tensorflow-47fe933d89146a0488563bdfde2c0675a750ed16"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/0b6ed4887ab1a65d2a0500ec8ebb03e221d8681b.tar.gz"",
+      ""https://github.com/tensorflow/tensorflow/archive/47fe933d89146a0488563bdfde2c0675a750ed16.tar.gz"",
     ],
 )
 ",No
jax/interpreters/xla.py,jax/interpreters/xla.py,3b049e385388df1ef24009c3ee972a5d5643c424,77db9bd556a753c348868a7587f7a7d73eb4a5ff,"[XLA] separate out an Execute from ExecutePerReplica
[JAX] reduce the creation of XLA Shape protos on every call (which is slow)

PiperOrigin-RevId: 223915944","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 8f99a7cc0..ea91e6b74 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -47,8 +47,10 @@ def apply_primitive(prim, *args, **kwargs):
 def xla_primitive_callable(prim, *abstract_args, **kwargs):
   shapes = map(xla_shape, abstract_args)
   built_c = primitive_computation(prim, *shapes, **kwargs)
+  result_shape = xla_shape_to_result_shape(built_c.GetReturnValueShape())
+  handle_result = result_handler(result_shape)
   compiled = built_c.Compile(shapes, xb.get_compile_options())
-  return partial(execute_compiled_primitive, compiled)
+  return partial(execute_compiled_primitive, compiled, handle_result)
 
 @memoize
 def primitive_computation(prim, *shapes, **kwargs):
@@ -64,15 +66,48 @@ def primitive_computation(prim, *shapes, **kwargs):
 def aval_from_xla_shape(shape):
   return ShapedArray(shape.dimensions(), shape.element_type())
 
-def execute_compiled_primitive(compiled, *args):
+def execute_compiled_primitive(compiled, result_handler, *args):
   input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
-  return handle_result(compiled.Execute(input_bufs))
+  return result_handler(compiled.Execute(input_bufs, not core.skip_checks))
+
+def device_put(x):
+  if type(x) is DeviceArray:
+    return x.device_buffer
+  else:
+    return xb.device_put(x)  # can round-trip elements of tuples here
+
+def result_handler(result_shape):
+  if type(result_shape) is ResultArray and FLAGS.jax_device_values:
+    def handle_result(device_buffer):
+      return DeviceArray(device_buffer, *result_shape)
+  elif type(result_shape) is ResultArray:
+    def handle_result(device_buffer):
+      return onp.asarray(DeviceArray(device_buffer, *result_shape))
+  elif type(result_shape) is ResultTuple:
+    handlers = list(map(result_handler, result_shape))
+    def handle_result(device_buffer):
+      bufs = device_buffer.destructure()
+      return JaxTuple(handler(buf) for handler, buf in zip(handlers, bufs))
+  else:
+    raise TypeError(type(result_shape))
+  return handle_result
+
+def xla_shape_to_result_shape(xla_shape):
+  if xla_shape.is_tuple():
+    return ResultTuple(map(xla_shape_to_result_shape, xla_shape.tuple_shapes()))
+  else:
+    shape, dtype = xla_shape.dimensions(), xla_shape.element_type()
+    ndim, size = len(shape), prod(shape)
+    return ResultArray((shape, dtype, ndim, size))
+class ResultTuple(tuple): pass
+class ResultArray(tuple): pass
 
 
 def compile_jaxpr(jaxpr, const_vals, *abstract_args):
-  arg_shapes = map(xla_shape, abstract_args)
-  built = jaxpr_computation(jaxpr, const_vals, (), *arg_shapes)
-  return built.Compile(arg_shapes, xb.get_compile_options())
+  arg_shapes = list(map(xla_shape, abstract_args))
+  built_c = jaxpr_computation(jaxpr, const_vals, (), *arg_shapes)
+  result_shape = xla_shape_to_result_shape(built_c.GetReturnValueShape())
+  return built_c.Compile(arg_shapes, xb.get_compile_options()), result_shape
 
 def jaxpr_computation(jaxpr, const_vals, freevar_shapes, *arg_shapes):
   c = xb.make_computation_builder(""jaxpr_computation"")
@@ -184,13 +219,12 @@ class DeviceArray(DeviceValue):
   __slots__ = [""shape"", ""dtype"", ""ndim"", ""size"", ""_npy_value""]
   __array_priority__ = 100.
 
-  def __init__(self, device_buffer):
+  def __init__(self, device_buffer, shape, dtype, ndim, size):
     self.device_buffer = device_buffer
-    xla_shape = device_buffer.shape()
-    self.shape = xla_shape.dimensions()
-    self.dtype = xla_shape.element_type()
-    self.ndim = len(self.shape)
-    size = prod(self.shape)
+    self.shape = shape
+    self.dtype = dtype
+    self.ndim = ndim
+    self.size = size
     self._npy_value = None
 
   @property
@@ -331,20 +365,6 @@ def build_tree(xs, tree_spec):
     raise TypeError(type(tree_spec))
 
 
-def device_put(x):
-  if type(x) is DeviceArray:
-    return x.device_buffer
-  else:
-    return xb.device_put(x)
-
-def handle_result(device_buffer):
-  if device_buffer.shape().is_tuple():
-    return JaxTuple(map(handle_result, device_buffer.destructure()))
-  else:
-    dval = DeviceArray(device_buffer)
-    return dval if FLAGS.jax_device_values else onp.asarray(dval)
-
-
 def xla_call_impl(fun, *args):
   flat_args, in_trees = unzip2(map(tree_flatten, args))
   flat_args = concatenate(flat_args)
@@ -364,14 +384,15 @@ def xla_callable(fun, *abstract_args):
     pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
     assert not env  # no subtraces here (though cond might eventually need them)
-    compiled = compile_jaxpr(jaxpr, consts, *abstract_args)
+    compiled, result_shape = compile_jaxpr(jaxpr, consts, *abstract_args)
     del master, pvals, consts, jaxpr, env
-    return partial(execute_compiled, compiled, pval)
+    handle_result = result_handler(result_shape)
+    return partial(execute_compiled, compiled, pval, handle_result)
 
-def execute_compiled(compiled, pval, *args):
+def execute_compiled(compiled, pval, handle_result, *args):
   input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
-  ans = handle_result(compiled.Execute(input_bufs))
-  return merge_pvals(ans, pval)
+  out_buf = compiled.Execute(input_bufs, not core.skip_checks)
+  return merge_pvals(handle_result(out_buf), pval)
 
 
 xla_call_p = core.Primitive('xla_call')","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 8f99a7cc0..ea91e6b74 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -47,8 +47,10 @@ def apply_primitive(prim, *args, **kwargs):
 def xla_primitive_callable(prim, *abstract_args, **kwargs):
   shapes = map(xla_shape, abstract_args)
   built_c = primitive_computation(prim, *shapes, **kwargs)
+  result_shape = xla_shape_to_result_shape(built_c.GetReturnValueShape())
+  handle_result = result_handler(result_shape)
   compiled = built_c.Compile(shapes, xb.get_compile_options())
-  return partial(execute_compiled_primitive, compiled)
+  return partial(execute_compiled_primitive, compiled, handle_result)
 
 @memoize
 def primitive_computation(prim, *shapes, **kwargs):
@@ -64,15 +66,48 @@ def primitive_computation(prim, *shapes, **kwargs):
 def aval_from_xla_shape(shape):
   return ShapedArray(shape.dimensions(), shape.element_type())
 
-def execute_compiled_primitive(compiled, *args):
+def execute_compiled_primitive(compiled, result_handler, *args):
   input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
-  return handle_result(compiled.Execute(input_bufs))
+  return result_handler(compiled.Execute(input_bufs, not core.skip_checks))
+
+def device_put(x):
+  if type(x) is DeviceArray:
+    return x.device_buffer
+  else:
+    return xb.device_put(x)  # can round-trip elements of tuples here
+
+def result_handler(result_shape):
+  if type(result_shape) is ResultArray and FLAGS.jax_device_values:
+    def handle_result(device_buffer):
+      return DeviceArray(device_buffer, *result_shape)
+  elif type(result_shape) is ResultArray:
+    def handle_result(device_buffer):
+      return onp.asarray(DeviceArray(device_buffer, *result_shape))
+  elif type(result_shape) is ResultTuple:
+    handlers = list(map(result_handler, result_shape))
+    def handle_result(device_buffer):
+      bufs = device_buffer.destructure()
+      return JaxTuple(handler(buf) for handler, buf in zip(handlers, bufs))
+  else:
+    raise TypeError(type(result_shape))
+  return handle_result
+
+def xla_shape_to_result_shape(xla_shape):
+  if xla_shape.is_tuple():
+    return ResultTuple(map(xla_shape_to_result_shape, xla_shape.tuple_shapes()))
+  else:
+    shape, dtype = xla_shape.dimensions(), xla_shape.element_type()
+    ndim, size = len(shape), prod(shape)
+    return ResultArray((shape, dtype, ndim, size))
+class ResultTuple(tuple): pass
+class ResultArray(tuple): pass
 
 
 def compile_jaxpr(jaxpr, const_vals, *abstract_args):
-  arg_shapes = map(xla_shape, abstract_args)
-  built = jaxpr_computation(jaxpr, const_vals, (), *arg_shapes)
-  return built.Compile(arg_shapes, xb.get_compile_options())
+  arg_shapes = list(map(xla_shape, abstract_args))
+  built_c = jaxpr_computation(jaxpr, const_vals, (), *arg_shapes)
+  result_shape = xla_shape_to_result_shape(built_c.GetReturnValueShape())
+  return built_c.Compile(arg_shapes, xb.get_compile_options()), result_shape
 
 def jaxpr_computation(jaxpr, const_vals, freevar_shapes, *arg_shapes):
   c = xb.make_computation_builder(""jaxpr_computation"")
@@ -184,13 +219,12 @@ class DeviceArray(DeviceValue):
   __slots__ = [""shape"", ""dtype"", ""ndim"", ""size"", ""_npy_value""]
   __array_priority__ = 100.
 
-  def __init__(self, device_buffer):
+  def __init__(self, device_buffer, shape, dtype, ndim, size):
     self.device_buffer = device_buffer
-    xla_shape = device_buffer.shape()
-    self.shape = xla_shape.dimensions()
-    self.dtype = xla_shape.element_type()
-    self.ndim = len(self.shape)
-    size = prod(self.shape)
+    self.shape = shape
+    self.dtype = dtype
+    self.ndim = ndim
+    self.size = size
     self._npy_value = None
 
   @property
@@ -331,20 +365,6 @@ def build_tree(xs, tree_spec):
     raise TypeError(type(tree_spec))
 
 
-def device_put(x):
-  if type(x) is DeviceArray:
-    return x.device_buffer
-  else:
-    return xb.device_put(x)
-
-def handle_result(device_buffer):
-  if device_buffer.shape().is_tuple():
-    return JaxTuple(map(handle_result, device_buffer.destructure()))
-  else:
-    dval = DeviceArray(device_buffer)
-    return dval if FLAGS.jax_device_values else onp.asarray(dval)
-
-
 def xla_call_impl(fun, *args):
   flat_args, in_trees = unzip2(map(tree_flatten, args))
   flat_args = concatenate(flat_args)
@@ -364,14 +384,15 @@ def xla_callable(fun, *abstract_args):
     pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
     assert not env  # no subtraces here (though cond might eventually need them)
-    compiled = compile_jaxpr(jaxpr, consts, *abstract_args)
+    compiled, result_shape = compile_jaxpr(jaxpr, consts, *abstract_args)
     del master, pvals, consts, jaxpr, env
-    return partial(execute_compiled, compiled, pval)
+    handle_result = result_handler(result_shape)
+    return partial(execute_compiled, compiled, pval, handle_result)
 
-def execute_compiled(compiled, pval, *args):
+def execute_compiled(compiled, pval, handle_result, *args):
   input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
-  ans = handle_result(compiled.Execute(input_bufs))
-  return merge_pvals(ans, pval)
+  out_buf = compiled.Execute(input_bufs, not core.skip_checks)
+  return merge_pvals(handle_result(out_buf), pval)
 
 
 xla_call_p = core.Primitive('xla_call')",No
build.py,build.py,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,3b049e385388df1ef24009c3ee972a5d5643c424,"source sync

PiperOrigin-RevId: 224157599","diff --git a/build.py b/build.py
old mode 100644
new mode 100755
index c66dcb8ba..65f727108
--- a/build.py
+++ b/build.py
@@ -176,11 +176,11 @@ def write_bazelrc(**kwargs):
 
 
 BANNER = r""""""
-     _   _    __  __
-    | | / \   \ \/ /
- _  | |/ _ \   \  / 
-| |_| / ___ \  /  \ 
- \___/_/   \_\/_/\_\ 
+     _   _  __  __
+    | | / \ \ \/ /
+ _  | |/ _ \ \  /
+| |_| / ___ \/  \
+ \___/_/   \/_/\_\
 
 """"""
 ","diff --git a/build.py b/build.py
old mode 100644
new mode 100755
index c66dcb8ba..65f727108
--- a/build.py
+++ b/build.py
@@ -176,11 +176,11 @@ def write_bazelrc(**kwargs):
 
 
 BANNER = r""""""
-     _   _    __  __
-    | | / \   \ \/ /
- _  | |/ _ \   \  / 
-| |_| / ___ \  /  \ 
- \___/_/   \_\/_/\_\ 
+     _   _  __  __
+    | | / \ \ \/ /
+ _  | |/ _ \ \  /
+| |_| / ___ \/  \
+ \___/_/   \/_/\_\
 
 """"""
 ",No
jax/interpreters/xla.py,jax/interpreters/xla.py,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,3b049e385388df1ef24009c3ee972a5d5643c424,"source sync

PiperOrigin-RevId: 224157599","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index ea91e6b74..5f09a2859 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -60,7 +60,8 @@ def primitive_computation(prim, *shapes, **kwargs):
   try:
     return c.Build()
   except RuntimeError as e:
-    prim.abstract_eval(*map(aval_from_xla_shape, shapes)) # try for better error
+    # try for a better error message by using the abstract_eval checks
+    prim.abstract_eval(*map(aval_from_xla_shape, shapes), **kwargs)
     raise e
 
 def aval_from_xla_shape(shape):","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index ea91e6b74..5f09a2859 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -60,7 +60,8 @@ def primitive_computation(prim, *shapes, **kwargs):
   try:
     return c.Build()
   except RuntimeError as e:
-    prim.abstract_eval(*map(aval_from_xla_shape, shapes)) # try for better error
+    # try for a better error message by using the abstract_eval checks
+    prim.abstract_eval(*map(aval_from_xla_shape, shapes), **kwargs)
     raise e
 
 def aval_from_xla_shape(shape):",No
jax/lax.py,jax/lax.py,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,3b049e385388df1ef24009c3ee972a5d5643c424,"source sync

PiperOrigin-RevId: 224157599","diff --git a/jax/lax.py b/jax/lax.py
index 5666cf212..131a59c4e 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2176,25 +2176,25 @@ def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
     raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))
 
 
-def _check_conv_shapes(fun_name, lhs_shape, rhs_shape, window_strides):
+def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
   """"""Check that conv shapes are valid and are consistent with window_strides.""""""
   if len(lhs_shape) != len(rhs_shape):
     msg = ""Arguments to {} must have same rank, got {} and {}.""
     raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
   if len(lhs_shape) < 2:
     msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
-    raise TypeError(msg.format(fun_name, len(lhs_shape), len(rhs_shape)))
+    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
   if lhs_shape[1] != rhs_shape[1]:
     msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
-    raise TypeError(msg.format(fun_name, lhs_shape[1], rhs_shape[1]))
-  _check_shapelike(fun_name, ""window_strides"", window_strides)
+    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
+  _check_shapelike(name, ""window_strides"", window_strides)
   if not onp.all(onp.greater(window_strides, 0)):
     msg = ""All elements of window_strides must be positive, got {}.""
     raise TypeError(msg.format(window_strides))
   if len(window_strides) != len(lhs_shape) - 2:
     msg = ""{} window_strides has wrong length: expected {}, got {}.""
     expected_length = len(lhs_shape) - 2
-    raise TypeError(msg.format(fun_name, expected_length, len(window_strides)))
+    raise TypeError(msg.format(name, expected_length, len(window_strides)))
 
 
 def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):","diff --git a/jax/lax.py b/jax/lax.py
index 5666cf212..131a59c4e 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2176,25 +2176,25 @@ def _check_same_dtypes(name, ignore_fp_precision, *dtypes):
     raise TypeError(msg.format(name, "", "".join(map(str, dtypes))))
 
 
-def _check_conv_shapes(fun_name, lhs_shape, rhs_shape, window_strides):
+def _check_conv_shapes(name, lhs_shape, rhs_shape, window_strides):
   """"""Check that conv shapes are valid and are consistent with window_strides.""""""
   if len(lhs_shape) != len(rhs_shape):
     msg = ""Arguments to {} must have same rank, got {} and {}.""
     raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
   if len(lhs_shape) < 2:
     msg = ""Arguments to {} must have rank at least 2, got {} and {}.""
-    raise TypeError(msg.format(fun_name, len(lhs_shape), len(rhs_shape)))
+    raise TypeError(msg.format(name, len(lhs_shape), len(rhs_shape)))
   if lhs_shape[1] != rhs_shape[1]:
     msg = ""Arguments to {} must agree on input feature size, got {} and {}.""
-    raise TypeError(msg.format(fun_name, lhs_shape[1], rhs_shape[1]))
-  _check_shapelike(fun_name, ""window_strides"", window_strides)
+    raise TypeError(msg.format(name, lhs_shape[1], rhs_shape[1]))
+  _check_shapelike(name, ""window_strides"", window_strides)
   if not onp.all(onp.greater(window_strides, 0)):
     msg = ""All elements of window_strides must be positive, got {}.""
     raise TypeError(msg.format(window_strides))
   if len(window_strides) != len(lhs_shape) - 2:
     msg = ""{} window_strides has wrong length: expected {}, got {}.""
     expected_length = len(lhs_shape) - 2
-    raise TypeError(msg.format(fun_name, expected_length, len(window_strides)))
+    raise TypeError(msg.format(name, expected_length, len(window_strides)))
 
 
 def conv_shape_tuple(lhs_shape, rhs_shape, strides, pads):",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,3b049e385388df1ef24009c3ee972a5d5643c424,"source sync

PiperOrigin-RevId: 224157599","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d0a43546a..99bd78dca 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -686,7 +686,9 @@ def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
   if ndmin != 0 or order != ""K"":
     raise NotImplementedError(""Only implemented for order='K', ndmin=0."")
 
-  if isinstance(object, ndarray):
+  if hasattr(object, '__asarray__'):
+    return object.__asarray__(dtype)
+  elif isinstance(object, ndarray):
     if dtype and _dtype(object) != dtype:
       return lax.convert_element_type(object, dtype)
     else:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d0a43546a..99bd78dca 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -686,7 +686,9 @@ def array(object, dtype=None, copy=True, order=""K"", ndmin=0):
   if ndmin != 0 or order != ""K"":
     raise NotImplementedError(""Only implemented for order='K', ndmin=0."")
 
-  if isinstance(object, ndarray):
+  if hasattr(object, '__asarray__'):
+    return object.__asarray__(dtype)
+  elif isinstance(object, ndarray):
     if dtype and _dtype(object) != dtype:
       return lax.convert_element_type(object, dtype)
     else:",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,3b049e385388df1ef24009c3ee972a5d5643c424,"source sync

PiperOrigin-RevId: 224157599","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 3881dc06b..391630812 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -419,6 +419,14 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)
 
+  def testArrayAsarrayMethod(self):
+    class arraylike(object):
+      def __asarray__(self, dtype=None):
+        return 3.
+    a = arraylike()
+    ans = lnp.array(a)
+    assert ans == 3.
+
   def testAllClose(self):
     rng = onp.random.RandomState(0)
     x = rng.randn(2, 2)","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 3881dc06b..391630812 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -419,6 +419,14 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)
 
+  def testArrayAsarrayMethod(self):
+    class arraylike(object):
+      def __asarray__(self, dtype=None):
+        return 3.
+    a = arraylike()
+    ans = lnp.array(a)
+    assert ans == 3.
+
   def testAllClose(self):
     rng = onp.random.RandomState(0)
     x = rng.randn(2, 2)",No
jax/api.py,jax/api.py,307d195577079925c5c1e78f10b7d9968288495f,5bb8f87e221a8487eb7c3703dd3efa56101ae41c,Wrapped static args to jit to be hashed on id. This is conservative but simple and cheap.,"diff --git a/jax/api.py b/jax/api.py
index 70e290003..4bd4005ea 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -26,7 +26,7 @@ from .core import pack, eval_jaxpr
 from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                         tree_map)
-from .util import unzip2, unzip3, curry, partial, safe_map
+from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
@@ -191,17 +191,16 @@ def argnums_partial(f, dyn_argnums, args):
     dyn_argnums = (dyn_argnums,)
   else:
     dyn_argnums = tuple(dyn_argnums)
-  fixed_args = tuple([None if i in dyn_argnums else arg for i, arg in enumerate(args)])
+  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
+                      for i, arg in enumerate(args)])
   dyn_args = [args[i] for i in dyn_argnums]
   return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args
 
 @lu.transformation
 def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
-  args = list(fixed_args)
+  args = [None if arg is None else arg.val for arg in fixed_args]
   for i, arg in zip(dyn_argnums, dyn_args):
     args[i] = arg
-
-
   ans = yield args
   yield ans
 ","diff --git a/jax/api.py b/jax/api.py
index 70e290003..4bd4005ea 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -26,7 +26,7 @@ from .core import pack, eval_jaxpr
 from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                         tree_map)
-from .util import unzip2, unzip3, curry, partial, safe_map
+from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
@@ -191,17 +191,16 @@ def argnums_partial(f, dyn_argnums, args):
     dyn_argnums = (dyn_argnums,)
   else:
     dyn_argnums = tuple(dyn_argnums)
-  fixed_args = tuple([None if i in dyn_argnums else arg for i, arg in enumerate(args)])
+  fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
+                      for i, arg in enumerate(args)])
   dyn_args = [args[i] for i in dyn_argnums]
   return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args
 
 @lu.transformation
 def argnums_partial_(dyn_argnums, fixed_args, *dyn_args):
-  args = list(fixed_args)
+  args = [None if arg is None else arg.val for arg in fixed_args]
   for i, arg in zip(dyn_argnums, dyn_args):
     args[i] = arg
-
-
   ans = yield args
   yield ans
 ",No
jax/util.py,jax/util.py,307d195577079925c5c1e78f10b7d9968288495f,5bb8f87e221a8487eb7c3703dd3efa56101ae41c,Wrapped static args to jit to be hashed on id. This is conservative but simple and cheap.,"diff --git a/jax/util.py b/jax/util.py
index 62587fe2d..38d22f00f 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -143,3 +143,15 @@ def memoize(fun):
 
 def prod(xs):
   return functools.reduce(mul, xs, 1)
+
+
+class WrapHashably(object):
+  def __init__(self, val):
+    self.val = val
+
+  def __hash__(self):
+    return id(self.val)
+
+  def __eq__(self, other):
+    return self.val == other.val
+","diff --git a/jax/util.py b/jax/util.py
index 62587fe2d..38d22f00f 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -143,3 +143,15 @@ def memoize(fun):
 
 def prod(xs):
   return functools.reduce(mul, xs, 1)
+
+
+class WrapHashably(object):
+  def __init__(self, val):
+    self.val = val
+
+  def __hash__(self):
+    return id(self.val)
+
+  def __eq__(self, other):
+    return self.val == other.val
+",No
jax/__init__.py,jax/__init__.py,709cfe905da28a68afe480b1c047438775960aa1,307d195577079925c5c1e78f10b7d9968288495f,"Set default TF log level to ""1"" to avoid reporting things like CPU frequency at import time. Also import jax.numpy in __init__.py because it has side effects that set up the infix operator overloading.","diff --git a/jax/__init__.py b/jax/__init__.py
index 46a817b68..8ea62cb7e 100644
--- a/jax/__init__.py
+++ b/jax/__init__.py
@@ -12,4 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
+os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '1')
 from jax.api import *
+import jax.numpy as np  # side-effecting import sets up operator overloads","diff --git a/jax/__init__.py b/jax/__init__.py
index 46a817b68..8ea62cb7e 100644
--- a/jax/__init__.py
+++ b/jax/__init__.py
@@ -12,4 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import os
+os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '1')
 from jax.api import *
+import jax.numpy as np  # side-effecting import sets up operator overloads",No
tests/quickercheck.py,tests/quickercheck.py,6fcee12cec31ac8298974d557f4e0d525452caa4,709cfe905da28a68afe480b1c047438775960aa1,Added forward derivative checks,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index c74f50988..145ff8a95 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,8 +2,9 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit
+from jax import jit, jvp
 import itertools as it
+import sys
 
 npr.seed(0)
 
@@ -151,26 +152,61 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
+def gen_inputs(fun):
+  return [gen_array_val(v.vartype) for v in fun.in_vars]
+
+def jvp_fd(fun, args, directions):
+  EPS = 1e-4
+  def eval_eps(eps):
+    return fun(*[x if d is None else x + eps * d
+                 for x, d in zip(args, directions)])
+
+  ys_neg = eval_eps(-EPS)
+  ys_pos = eval_eps(EPS)
+  ys = eval_eps(0.0)
+  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
+  return ys, deriv
+
+def check_all_close(xs, ys, tol=1e-3):
+  for x, y in zip(xs, ys):
+    assert x.shape == y.shape
+    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+    # assert x.dtype == y.dtype
+    assert np.allclose(x, y, rtol=tol, atol=tol), \
+       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 def jit_is_identity(fun):
-  vals = map(gen_array_val, [v.vartype for v in fun.in_vars])
+  vals = gen_inputs(fun)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
-  ans_jitted = jit(fun)(*vals)
-  for i, (x, x_jit) in enumerate(zip(ans, ans_jitted)):
-    assert x.shape == x_jit.shape
-    # assert x.dtype == x_jit.dtype, 'dtype mismatch: {} != {}'.format(x.dtype, x_jit.dtype)
-    assert np.allclose(x, x_jit)
+  static_argnums = thin(range(len(vals)), 0.5)
+  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
+  check_all_close(ans, ans_jitted)
+
+def jvp_matches_fd(fun):
+  vals = gen_inputs(fun)
+  directions = gen_inputs(fun)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, deriv1 = jvp_fd(fun, vals, directions)
+  ans2, deriv2 = jvp(fun, vals, directions)
+  check_all_close(ans1, ans2)
+  check_all_close(deriv1, deriv2)
 
 properties = [
-  jit_is_identity ]
-  # jvp_matches_fd,
-  # vjp_matches_fd,
-  # vmap_matches_map ]
+  jit_is_identity,
+  jvp_matches_fd,
+]
+# vjp_matches_fd,
+# vmap_matches_map ]
 
 def run_tests():
-  sizes = [3, 10, 30]
-  num_examples = 30
-  for size, _, check_prop in it.product(sizes, range(num_examples), properties):
+  sizes = [3, 10]
+  num_examples = 50
+  cases = it.product(sizes, range(num_examples), properties)
+  for i, (size, _, check_prop) in enumerate(cases):
+    sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
 
 if __name__ == ""__main__"":","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index c74f50988..145ff8a95 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,8 +2,9 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit
+from jax import jit, jvp
 import itertools as it
+import sys
 
 npr.seed(0)
 
@@ -151,26 +152,61 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
+def gen_inputs(fun):
+  return [gen_array_val(v.vartype) for v in fun.in_vars]
+
+def jvp_fd(fun, args, directions):
+  EPS = 1e-4
+  def eval_eps(eps):
+    return fun(*[x if d is None else x + eps * d
+                 for x, d in zip(args, directions)])
+
+  ys_neg = eval_eps(-EPS)
+  ys_pos = eval_eps(EPS)
+  ys = eval_eps(0.0)
+  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
+  return ys, deriv
+
+def check_all_close(xs, ys, tol=1e-3):
+  for x, y in zip(xs, ys):
+    assert x.shape == y.shape
+    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+    # assert x.dtype == y.dtype
+    assert np.allclose(x, y, rtol=tol, atol=tol), \
+       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 def jit_is_identity(fun):
-  vals = map(gen_array_val, [v.vartype for v in fun.in_vars])
+  vals = gen_inputs(fun)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
-  ans_jitted = jit(fun)(*vals)
-  for i, (x, x_jit) in enumerate(zip(ans, ans_jitted)):
-    assert x.shape == x_jit.shape
-    # assert x.dtype == x_jit.dtype, 'dtype mismatch: {} != {}'.format(x.dtype, x_jit.dtype)
-    assert np.allclose(x, x_jit)
+  static_argnums = thin(range(len(vals)), 0.5)
+  ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
+  check_all_close(ans, ans_jitted)
+
+def jvp_matches_fd(fun):
+  vals = gen_inputs(fun)
+  directions = gen_inputs(fun)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, deriv1 = jvp_fd(fun, vals, directions)
+  ans2, deriv2 = jvp(fun, vals, directions)
+  check_all_close(ans1, ans2)
+  check_all_close(deriv1, deriv2)
 
 properties = [
-  jit_is_identity ]
-  # jvp_matches_fd,
-  # vjp_matches_fd,
-  # vmap_matches_map ]
+  jit_is_identity,
+  jvp_matches_fd,
+]
+# vjp_matches_fd,
+# vmap_matches_map ]
 
 def run_tests():
-  sizes = [3, 10, 30]
-  num_examples = 30
-  for size, _, check_prop in it.product(sizes, range(num_examples), properties):
+  sizes = [3, 10]
+  num_examples = 50
+  cases = it.product(sizes, range(num_examples), properties)
+  for i, (size, _, check_prop) in enumerate(cases):
+    sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
 
 if __name__ == ""__main__"":",No
tests/quickercheck.py,tests/quickercheck.py,f1d7ea897205b7bad5db688d6ca5c58b83500aad,6fcee12cec31ac8298974d557f4e0d525452caa4,Added reverse-mode checks,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 145ff8a95..8191b8182 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,7 +2,7 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit, jvp
+from jax import jit, jvp, vjp
 import itertools as it
 import sys
 
@@ -152,14 +152,19 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
-def gen_inputs(fun):
-  return [gen_array_val(v.vartype) for v in fun.in_vars]
+def gen_vals(vs):
+  return [gen_array_val(v.vartype) for v in vs]
 
-def jvp_fd(fun, args, directions):
+def inner_prod(xs, ys):
+  xys = zip(xs, ys)
+  assert all(x.shape == y.shape for x, y in xys)
+  return sum(np.sum(x * y) for x, y in xys)
+
+def jvp_fd(fun, args, tangents):
   EPS = 1e-4
   def eval_eps(eps):
-    return fun(*[x if d is None else x + eps * d
-                 for x, d in zip(args, directions)])
+    return fun(*[x if t is None else x + eps * t
+                 for x, t in zip(args, tangents)])
 
   ys_neg = eval_eps(-EPS)
   ys_pos = eval_eps(EPS)
@@ -169,14 +174,18 @@ def jvp_fd(fun, args, directions):
 
 def check_all_close(xs, ys, tol=1e-3):
   for x, y in zip(xs, ys):
-    assert x.shape == y.shape
-    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
-    # assert x.dtype == y.dtype
-    assert np.allclose(x, y, rtol=tol, atol=tol), \
-       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+    check_close(x, y, tol)
+
+def check_close(x, y, tol=1e-3):
+  assert np.shape(x) == np.shape(y)
+  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+  # assert x.dtype == y.dtype
+  assert np.allclose(x, y, rtol=tol, atol=tol), \
+     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 
 def jit_is_identity(fun):
-  vals = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
   static_argnums = thin(range(len(vals)), 0.5)
@@ -184,21 +193,39 @@ def jit_is_identity(fun):
   check_all_close(ans, ans_jitted)
 
 def jvp_matches_fd(fun):
-  vals = gen_inputs(fun)
-  directions = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
+  tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
 
   # TODO: differentiate wrt some inputs only
-  ans1, deriv1 = jvp_fd(fun, vals, directions)
-  ans2, deriv2 = jvp(fun, vals, directions)
+  ans1, deriv1 = jvp_fd(fun, vals, tangents)
+  ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
   check_all_close(deriv1, deriv2)
 
+
+def vjp_matches_fd(fun):
+  # print fun
+  vals = gen_vals(fun.in_vars)
+  in_tangents = gen_vals(fun.in_vars)
+  in_cotangents = gen_vals(fun.out_vars)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+  ans2, vjpfun = vjp(fun, *vals)
+  out_cotangents = vjpfun(in_cotangents)
+  check_all_close(ans1, ans2)
+  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+  check_close(inner_prod_fd, inner_prod_ad)
+
+
 properties = [
   jit_is_identity,
   jvp_matches_fd,
+  vjp_matches_fd,
 ]
-# vjp_matches_fd,
 # vmap_matches_map ]
 
 def run_tests():","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 145ff8a95..8191b8182 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -2,7 +2,7 @@ from collections import namedtuple
 from functools import partial
 import numpy.random as npr
 import jax.numpy as np
-from jax import jit, jvp
+from jax import jit, jvp, vjp
 import itertools as it
 import sys
 
@@ -152,14 +152,19 @@ def gen_subset(xs):
 
   return gen_sized_subset(xs, npr.randint(len(xs) + 1))
 
-def gen_inputs(fun):
-  return [gen_array_val(v.vartype) for v in fun.in_vars]
+def gen_vals(vs):
+  return [gen_array_val(v.vartype) for v in vs]
 
-def jvp_fd(fun, args, directions):
+def inner_prod(xs, ys):
+  xys = zip(xs, ys)
+  assert all(x.shape == y.shape for x, y in xys)
+  return sum(np.sum(x * y) for x, y in xys)
+
+def jvp_fd(fun, args, tangents):
   EPS = 1e-4
   def eval_eps(eps):
-    return fun(*[x if d is None else x + eps * d
-                 for x, d in zip(args, directions)])
+    return fun(*[x if t is None else x + eps * t
+                 for x, t in zip(args, tangents)])
 
   ys_neg = eval_eps(-EPS)
   ys_pos = eval_eps(EPS)
@@ -169,14 +174,18 @@ def jvp_fd(fun, args, directions):
 
 def check_all_close(xs, ys, tol=1e-3):
   for x, y in zip(xs, ys):
-    assert x.shape == y.shape
-    # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
-    # assert x.dtype == y.dtype
-    assert np.allclose(x, y, rtol=tol, atol=tol), \
-       ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+    check_close(x, y, tol)
+
+def check_close(x, y, tol=1e-3):
+  assert np.shape(x) == np.shape(y)
+  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+  # assert x.dtype == y.dtype
+  assert np.allclose(x, y, rtol=tol, atol=tol), \
+     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
 
 def jit_is_identity(fun):
-  vals = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
   ans = fun(*vals)
   static_argnums = thin(range(len(vals)), 0.5)
@@ -184,21 +193,39 @@ def jit_is_identity(fun):
   check_all_close(ans, ans_jitted)
 
 def jvp_matches_fd(fun):
-  vals = gen_inputs(fun)
-  directions = gen_inputs(fun)
+  vals = gen_vals(fun.in_vars)
+  tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
 
   # TODO: differentiate wrt some inputs only
-  ans1, deriv1 = jvp_fd(fun, vals, directions)
-  ans2, deriv2 = jvp(fun, vals, directions)
+  ans1, deriv1 = jvp_fd(fun, vals, tangents)
+  ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
   check_all_close(deriv1, deriv2)
 
+
+def vjp_matches_fd(fun):
+  # print fun
+  vals = gen_vals(fun.in_vars)
+  in_tangents = gen_vals(fun.in_vars)
+  in_cotangents = gen_vals(fun.out_vars)
+  fun = partial(eval_fun, fun)
+
+  # TODO: differentiate wrt some inputs only
+  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+  ans2, vjpfun = vjp(fun, *vals)
+  out_cotangents = vjpfun(in_cotangents)
+  check_all_close(ans1, ans2)
+  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+  check_close(inner_prod_fd, inner_prod_ad)
+
+
 properties = [
   jit_is_identity,
   jvp_matches_fd,
+  vjp_matches_fd,
 ]
-# vjp_matches_fd,
 # vmap_matches_map ]
 
 def run_tests():",No
tests/quickercheck.py,tests/quickercheck.py,2f44eba01d2937f58be9927329679876b26b6274,f1d7ea897205b7bad5db688d6ca5c58b83500aad,Made some subset of vjp/jvp inputs static in quickercheck. Exposing bugs.,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 8191b8182..34162c5a2 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -183,6 +183,16 @@ def check_close(x, y, tol=1e-3):
   assert np.allclose(x, y, rtol=tol, atol=tol), \
      ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
 
+def partial_argnums(f, args, dyn_argnums):
+  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
+  def f_(*dyn_args):
+    args = fixed_args[:]
+    for i, arg in zip(dyn_argnums, dyn_args):
+      args[i] = arg
+    return f(*args)
+
+  dyn_args = [args[i] for i in dyn_argnums]
+  return f_, dyn_args
 
 def jit_is_identity(fun):
   vals = gen_vals(fun.in_vars)
@@ -196,8 +206,9 @@ def jvp_matches_fd(fun):
   vals = gen_vals(fun.in_vars)
   tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  tangents = [tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, deriv1 = jvp_fd(fun, vals, tangents)
   ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
@@ -210,8 +221,9 @@ def vjp_matches_fd(fun):
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  in_tangents = [in_tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
   ans2, vjpfun = vjp(fun, *vals)
   out_cotangents = vjpfun(in_cotangents)
@@ -235,6 +247,7 @@ def run_tests():
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
+  print ""\nok""
 
 if __name__ == ""__main__"":
   run_tests()","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 8191b8182..34162c5a2 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -183,6 +183,16 @@ def check_close(x, y, tol=1e-3):
   assert np.allclose(x, y, rtol=tol, atol=tol), \
      ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
 
+def partial_argnums(f, args, dyn_argnums):
+  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
+  def f_(*dyn_args):
+    args = fixed_args[:]
+    for i, arg in zip(dyn_argnums, dyn_args):
+      args[i] = arg
+    return f(*args)
+
+  dyn_args = [args[i] for i in dyn_argnums]
+  return f_, dyn_args
 
 def jit_is_identity(fun):
   vals = gen_vals(fun.in_vars)
@@ -196,8 +206,9 @@ def jvp_matches_fd(fun):
   vals = gen_vals(fun.in_vars)
   tangents = gen_vals(fun.in_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  tangents = [tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, deriv1 = jvp_fd(fun, vals, tangents)
   ans2, deriv2 = jvp(fun, vals, tangents)
   check_all_close(ans1, ans2)
@@ -210,8 +221,9 @@ def vjp_matches_fd(fun):
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
   fun = partial(eval_fun, fun)
-
-  # TODO: differentiate wrt some inputs only
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  in_tangents = [in_tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
   ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
   ans2, vjpfun = vjp(fun, *vals)
   out_cotangents = vjpfun(in_cotangents)
@@ -235,6 +247,7 @@ def run_tests():
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
     check_prop(gen_fun_and_types(size))
+  print ""\nok""
 
 if __name__ == ""__main__"":
   run_tests()",No
jax/api.py,jax/api.py,2e4ff400faa0bb8c7137810639cf769a683730c5,2f44eba01d2937f58be9927329679876b26b6274,Fixed bug in vjp with constant-zero tangent outputs,"diff --git a/jax/api.py b/jax/api.py
index 4bd4005ea..9a2240d58 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -123,8 +123,8 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
   def fun(*args):
     primals = pack(args) # doesn't matter what these are-they'll be ignored
     tangents = pack(args)
-    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
-    return list(pe.merge_pvals(ans, out_pval))[1]
+    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
+    return pe.merge_pvals(ans, out_pval)
 
   return unflatten_fun(fun, io_tree, *py_args)
 ","diff --git a/jax/api.py b/jax/api.py
index 4bd4005ea..9a2240d58 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -123,8 +123,8 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
   def fun(*args):
     primals = pack(args) # doesn't matter what these are-they'll be ignored
     tangents = pack(args)
-    ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
-    return list(pe.merge_pvals(ans, out_pval))[1]
+    _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
+    return pe.merge_pvals(ans, out_pval)
 
   return unflatten_fun(fun, io_tree, *py_args)
 ",No
jax/interpreters/ad.py,jax/interpreters/ad.py,2e4ff400faa0bb8c7137810639cf769a683730c5,2f44eba01d2937f58be9927329679876b26b6274,Fixed bug in vjp with constant-zero tangent outputs,"diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f..3f205b1db 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -64,21 +64,40 @@ def linearize(traceable, *primals):
   in_pvals = (pe.PartialVal((None, pack(primals))),
               pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
   jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
-  out_pv, out_const = out_pval
-  assert out_pv is None or out_pv[0] is None
-  primal_out = tuple(out_const)[0]
-  return primal_out, out_pval, jaxpr, consts
-
+  pval_primal, pval_tangent = unpair_pval(out_pval)
+  aval_primal, const_primal = pval_primal
+  assert aval_primal is None
+  return const_primal, pval_tangent, jaxpr, consts
 
 def vjp(traceable, primals):
-  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
+  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
   def vjp_(ct):
+    ct = ignore_consts(ct, pval)
     dummy_primal_and_ct = pack((core.unit, ct))
     _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
     return instantiate_zeros(pack(primals), arg_cts[1])
 
   return out_primal, vjp_
 
+def ignore_consts(ct, pval):
+  aval, const = pval
+  if isinstance(aval, core.AbstractValue):
+    return ct
+  elif isinstance(aval, pe.JaxprTracerTuple):
+    return pack(map(ignore_consts, ct, zip(aval, const)))
+  elif aval is None:
+    return core.unit
+  else:
+    raise TypeError(aval)
+
+def unpair_pval(pval):
+  aval, const = pval
+  const_1, const_2 = const
+  if aval is None:
+    return (None, const_1), (None, const_2)
+  else:
+    aval_1, aval_2 = aval
+    return (aval_1, const_1), (aval_2, const_2)
 
 def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
   def write_cotangent(v, ct):
@@ -116,8 +135,8 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
-    for var, ct in builtins.zip(eqn.invars, cts_out):
+
+    for var, ct in zip(eqn.invars, cts_out):
       write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 10c8af15f..3f205b1db 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -64,21 +64,40 @@ def linearize(traceable, *primals):
   in_pvals = (pe.PartialVal((None, pack(primals))),
               pe.PartialVal((core.AbstractTuple(tangent_avals), core.unit)))
   jaxpr, out_pval, consts = pe.trace_to_jaxpr(jvpfun, in_pvals)
-  out_pv, out_const = out_pval
-  assert out_pv is None or out_pv[0] is None
-  primal_out = tuple(out_const)[0]
-  return primal_out, out_pval, jaxpr, consts
-
+  pval_primal, pval_tangent = unpair_pval(out_pval)
+  aval_primal, const_primal = pval_primal
+  assert aval_primal is None
+  return const_primal, pval_tangent, jaxpr, consts
 
 def vjp(traceable, primals):
-  out_primal, _, jaxpr, consts = linearize(traceable, *primals)
+  out_primal, pval, jaxpr, consts = linearize(traceable, *primals)
   def vjp_(ct):
+    ct = ignore_consts(ct, pval)
     dummy_primal_and_ct = pack((core.unit, ct))
     _, arg_cts = backward_pass(jaxpr, consts, (), dummy_primal_and_ct)
     return instantiate_zeros(pack(primals), arg_cts[1])
 
   return out_primal, vjp_
 
+def ignore_consts(ct, pval):
+  aval, const = pval
+  if isinstance(aval, core.AbstractValue):
+    return ct
+  elif isinstance(aval, pe.JaxprTracerTuple):
+    return pack(map(ignore_consts, ct, zip(aval, const)))
+  elif aval is None:
+    return core.unit
+  else:
+    raise TypeError(aval)
+
+def unpair_pval(pval):
+  aval, const = pval
+  const_1, const_2 = const
+  if aval is None:
+    return (None, const_1), (None, const_2)
+  else:
+    aval_1, aval_2 = aval
+    return (aval_1, const_1), (aval_2, const_2)
 
 def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
   def write_cotangent(v, ct):
@@ -116,8 +135,8 @@ def backward_pass(jaxpr, consts, freevar_vals, cotangent_in):
 
     if cts_out is zero:
       cts_out = [zero for _ in eqn.invars]
-    # TODO(phawkins,dougalm): eqn.invars and cts_out can have different lengths
-    for var, ct in builtins.zip(eqn.invars, cts_out):
+
+    for var, ct in zip(eqn.invars, cts_out):
       write_cotangent(var, ct)
 
   cotangents_out = map(read_cotangent, jaxpr.invars)",No
tests/quickercheck.py,tests/quickercheck.py,2e4ff400faa0bb8c7137810639cf769a683730c5,2f44eba01d2937f58be9927329679876b26b6274,Fixed bug in vjp with constant-zero tangent outputs,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 34162c5a2..828e12c13 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -216,7 +216,6 @@ def jvp_matches_fd(fun):
 
 
 def vjp_matches_fd(fun):
-  # print fun
   vals = gen_vals(fun.in_vars)
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
@@ -232,7 +231,6 @@ def vjp_matches_fd(fun):
   inner_prod_ad = inner_prod(in_tangents, out_cotangents)
   check_close(inner_prod_fd, inner_prod_ad)
 
-
 properties = [
   jit_is_identity,
   jvp_matches_fd,
@@ -246,7 +244,13 @@ def run_tests():
   cases = it.product(sizes, range(num_examples), properties)
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
-    check_prop(gen_fun_and_types(size))
+    try:
+      fun = gen_fun_and_types(size)
+      check_prop(fun)
+    except:
+      print fun
+      raise
+
   print ""\nok""
 
 if __name__ == ""__main__"":","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 34162c5a2..828e12c13 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -216,7 +216,6 @@ def jvp_matches_fd(fun):
 
 
 def vjp_matches_fd(fun):
-  # print fun
   vals = gen_vals(fun.in_vars)
   in_tangents = gen_vals(fun.in_vars)
   in_cotangents = gen_vals(fun.out_vars)
@@ -232,7 +231,6 @@ def vjp_matches_fd(fun):
   inner_prod_ad = inner_prod(in_tangents, out_cotangents)
   check_close(inner_prod_fd, inner_prod_ad)
 
-
 properties = [
   jit_is_identity,
   jvp_matches_fd,
@@ -246,7 +244,13 @@ def run_tests():
   cases = it.product(sizes, range(num_examples), properties)
   for i, (size, _, check_prop) in enumerate(cases):
     sys.stderr.write('\rTested: {}'.format(i))
-    check_prop(gen_fun_and_types(size))
+    try:
+      fun = gen_fun_and_types(size)
+      check_prop(fun)
+    except:
+      print fun
+      raise
+
   print ""\nok""
 
 if __name__ == ""__main__"":",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,d7b7200884d5a766ffe6398bbcdd105716d915b8,2e4ff400faa0bb8c7137810639cf769a683730c5,Error message for unimplemented numpy functions,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 99bd78dca..de7a52d4d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -822,9 +822,10 @@ def _argminmax(op, a, axis):
   return min(mask_idxs, axis)
 
 
-# TODO plan how to handle unsupported ops
 def _not_implemented(fun):
-  return None
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
 
 argpartition = _not_implemented(onp.argpartition)
 argsort = _not_implemented(onp.argsort)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 99bd78dca..de7a52d4d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -822,9 +822,10 @@ def _argminmax(op, a, axis):
   return min(mask_idxs, axis)
 
 
-# TODO plan how to handle unsupported ops
 def _not_implemented(fun):
-  return None
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
 
 argpartition = _not_implemented(onp.argpartition)
 argsort = _not_implemented(onp.argsort)",No
tests/quickercheck.py,tests/quickercheck.py,727755178dfa6df85e3fff98db27b941131de8a6,d7b7200884d5a766ffe6398bbcdd105716d915b8,Added license header,"diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 828e12c13..ab5579575 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from collections import namedtuple
 from functools import partial
 import numpy.random as npr","diff --git a/tests/quickercheck.py b/tests/quickercheck.py
index 828e12c13..ab5579575 100644
--- a/tests/quickercheck.py
+++ b/tests/quickercheck.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from collections import namedtuple
 from functools import partial
 import numpy.random as npr",No
.gitignore,.gitignore,47ade4136872263ca6b1e5768229c9b163707e78,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,"Adding quickstart notebook, and corresponding gitignore rules","diff --git a/.gitignore b/.gitignore
index 944358884..24a98859f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,3 +4,11 @@
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
 jax.egg-info
+
+.ipynb_checkpoints
+
+/bazel-*
+.bazelrc
+/tensorflow
+
+.DS_Store","diff --git a/.gitignore b/.gitignore
index 944358884..24a98859f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,3 +4,11 @@
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
 jax.egg-info
+
+.ipynb_checkpoints
+
+/bazel-*
+.bazelrc
+/tensorflow
+
+.DS_Store",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,f2795cbdeadf5a27a8294d56c9888297a7bcf441,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,"[JAX] Add a NUMPY_SCALAR_SHAPE constant shape to test_utils.py to allow tests for both numpy scalars as distinct from 0D ndarrays.

Fix mishandling of scalars when passed to np.reshape().

PiperOrigin-RevId: 224326107","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 99bd78dca..5a28af9f2 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -383,7 +383,7 @@ def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
   else:
     raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))
 
-  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
+  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
   computed_newshape = onp.reshape(dummy_val, newshape).shape
   return lax.reshape(a, computed_newshape, dims)
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 99bd78dca..5a28af9f2 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -383,7 +383,7 @@ def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
   else:
     raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))
 
-  dummy_val = onp.broadcast_to(0, a.shape)  # zero strides
+  dummy_val = onp.broadcast_to(0, shape(a))  # zero strides
   computed_newshape = onp.reshape(dummy_val, newshape).shape
   return lax.reshape(a, computed_newshape, dims)
 ",No
jax/test_util.py,jax/test_util.py,f2795cbdeadf5a27a8294d56c9888297a7bcf441,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,"[JAX] Add a NUMPY_SCALAR_SHAPE constant shape to test_utils.py to allow tests for both numpy scalars as distinct from 0D ndarrays.

Fix mishandling of scalars when passed to np.reshape().

PiperOrigin-RevId: 224326107","diff --git a/jax/test_util.py b/jax/test_util.py
index 2c05a0b65..649b47be2 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -167,12 +167,41 @@ def format_test_name_suffix(opname, shapes, dtypes):
   return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))
 
 
+class _NumpyScalar(object):
+
+  def __len__(self):
+    return 0
+
+# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
+# identical to 0-D arrays, and we want to write tests that exercise both paths.
+NUMPY_SCALAR_SHAPE = _NumpyScalar()
+
+
+def _dims_of_shape(shape):
+  """"""Converts `shape` to a tuple of dimensions.""""""
+  return shape if shape != NUMPY_SCALAR_SHAPE else ()
+
+
+def _cast_to_shape(value, shape, dtype):
+  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
+  if shape != NUMPY_SCALAR_SHAPE:
+    return value
+  else:
+    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
+    # scalar.
+    return dtype(value)
+
+
 def format_shape_dtype_string(shape, dtype):
+  typestr = onp.dtype(dtype).name
+  if shape == NUMPY_SCALAR_SHAPE:
+    return typestr
+
   if onp.isscalar(shape):
     shapestr = str(shape) + ','
   else:
     shapestr = ','.join(str(dim) for dim in shape)
-  return '{}[{}]'.format(onp.dtype(dtype).name, shapestr)
+  return '{}[{}]'.format(typestr, shapestr)
 
 
 def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
@@ -191,12 +220,12 @@ def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
     An ndarray of the given shape and dtype using random values based on a call
     to rand but scaled, converted to the appropriate dtype, and post-processed.
   """"""
-  r = lambda: onp.asarray(scale * rand(*shape), dtype)
+  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
   if onp.issubdtype(dtype, onp.complexfloating):
     vals = r() + 1.0j * r()
   else:
     vals = r()
-  return onp.asarray(post(vals), dtype)
+  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)
 
 
 def rand_default():
@@ -255,14 +284,15 @@ def rand_some_inf():
       # only float types have inf
       return base_rand(shape, dtype)
 
-    posinf_flips = rng.rand(*shape) < 0.1
-    neginf_flips = rng.rand(*shape) < 0.1
+    dims = _dims_of_shape(shape)
+    posinf_flips = rng.rand(*dims) < 0.1
+    neginf_flips = rng.rand(*dims) < 0.1
 
     vals = base_rand(shape, dtype)
     vals = onp.where(posinf_flips, onp.inf, vals)
     vals = onp.where(neginf_flips, -onp.inf, vals)
 
-    return onp.asarray(vals, dtype=dtype)
+    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)
 
   return rand
 
@@ -275,19 +305,22 @@ def rand_some_zero():
 
   def rand(shape, dtype):
     """"""The random sampler function.""""""
-    zeros = rng.rand(*shape) < 0.5
+    dims = _dims_of_shape(shape)
+    zeros = rng.rand(*dims) < 0.5
 
     vals = base_rand(shape, dtype)
     vals = onp.where(zeros, 0, vals)
 
-    return onp.asarray(vals, dtype=dtype)
+    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)
 
   return rand
 
 
 def rand_bool():
   rng = npr.RandomState(0)
-  return lambda shape, dtype: rng.rand(*shape) < 0.5
+  def generator(shape, dtype):
+    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
+  return generator
 
 def check_raises(thunk, err_type, msg):
   try:","diff --git a/jax/test_util.py b/jax/test_util.py
index 2c05a0b65..649b47be2 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -167,12 +167,41 @@ def format_test_name_suffix(opname, shapes, dtypes):
   return '{}_{}'.format(opname.capitalize(), '_'.join(arg_descriptions))
 
 
+class _NumpyScalar(object):
+
+  def __len__(self):
+    return 0
+
+# A special singleton ""shape"" that denotes numpy scalars. Numpy scalars are not
+# identical to 0-D arrays, and we want to write tests that exercise both paths.
+NUMPY_SCALAR_SHAPE = _NumpyScalar()
+
+
+def _dims_of_shape(shape):
+  """"""Converts `shape` to a tuple of dimensions.""""""
+  return shape if shape != NUMPY_SCALAR_SHAPE else ()
+
+
+def _cast_to_shape(value, shape, dtype):
+  """"""Casts `value` to the correct Python type for `shape` and `dtype`.""""""
+  if shape != NUMPY_SCALAR_SHAPE:
+    return value
+  else:
+    # A numpy scalar was requested. Explicitly cast in case `value` is a Python
+    # scalar.
+    return dtype(value)
+
+
 def format_shape_dtype_string(shape, dtype):
+  typestr = onp.dtype(dtype).name
+  if shape == NUMPY_SCALAR_SHAPE:
+    return typestr
+
   if onp.isscalar(shape):
     shapestr = str(shape) + ','
   else:
     shapestr = ','.join(str(dim) for dim in shape)
-  return '{}[{}]'.format(onp.dtype(dtype).name, shapestr)
+  return '{}[{}]'.format(typestr, shapestr)
 
 
 def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
@@ -191,12 +220,12 @@ def _rand_dtype(rand, shape, dtype, scale=1., post=lambda x: x):
     An ndarray of the given shape and dtype using random values based on a call
     to rand but scaled, converted to the appropriate dtype, and post-processed.
   """"""
-  r = lambda: onp.asarray(scale * rand(*shape), dtype)
+  r = lambda: onp.asarray(scale * rand(*_dims_of_shape(shape)), dtype)
   if onp.issubdtype(dtype, onp.complexfloating):
     vals = r() + 1.0j * r()
   else:
     vals = r()
-  return onp.asarray(post(vals), dtype)
+  return _cast_to_shape(onp.asarray(post(vals), dtype), shape, dtype)
 
 
 def rand_default():
@@ -255,14 +284,15 @@ def rand_some_inf():
       # only float types have inf
       return base_rand(shape, dtype)
 
-    posinf_flips = rng.rand(*shape) < 0.1
-    neginf_flips = rng.rand(*shape) < 0.1
+    dims = _dims_of_shape(shape)
+    posinf_flips = rng.rand(*dims) < 0.1
+    neginf_flips = rng.rand(*dims) < 0.1
 
     vals = base_rand(shape, dtype)
     vals = onp.where(posinf_flips, onp.inf, vals)
     vals = onp.where(neginf_flips, -onp.inf, vals)
 
-    return onp.asarray(vals, dtype=dtype)
+    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)
 
   return rand
 
@@ -275,19 +305,22 @@ def rand_some_zero():
 
   def rand(shape, dtype):
     """"""The random sampler function.""""""
-    zeros = rng.rand(*shape) < 0.5
+    dims = _dims_of_shape(shape)
+    zeros = rng.rand(*dims) < 0.5
 
     vals = base_rand(shape, dtype)
     vals = onp.where(zeros, 0, vals)
 
-    return onp.asarray(vals, dtype=dtype)
+    return _cast_to_shape(onp.asarray(vals, dtype=dtype), shape, dtype)
 
   return rand
 
 
 def rand_bool():
   rng = npr.RandomState(0)
-  return lambda shape, dtype: rng.rand(*shape) < 0.5
+  def generator(shape, dtype):
+    return _cast_to_shape(rng.rand(*_dims_of_shape(shape)) < 0.5, shape, dtype)
+  return generator
 
 def check_raises(thunk, err_type, msg):
   try:",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,f2795cbdeadf5a27a8294d56c9888297a7bcf441,2597300c7fbee9d2cff4191b1c67fa7c4a6659eb,"[JAX] Add a NUMPY_SCALAR_SHAPE constant shape to test_utils.py to allow tests for both numpy scalars as distinct from 0D ndarrays.

Fix mishandling of scalars when passed to np.reshape().

PiperOrigin-RevId: 224326107","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 391630812..5eae4fdee 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -32,7 +32,11 @@ from jax.config import config
 
 FLAGS = config.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
+# fixed so the tests pass.
+all_shapes = array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -53,10 +57,10 @@ def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
 JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
     op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
     op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
@@ -306,12 +310,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(
-      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
-          ""_"".join(str(d) for d in shape),
-          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
+          jtu.format_shape_dtype_string(shape, fill_value_dtype),
+          onp.dtype(out_dtype).name),
        ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
        ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
-      for shape in all_shapes
+      for shape in array_shapes
       for fill_value_dtype in default_dtypes
       for out_dtype in default_dtypes)
   def testFull(self, shape, fill_value_dtype, out_dtype, rng):
@@ -345,6 +349,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
       for arg_shape, out_shape in [
+          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
+          ((), (1, 1, 1)),
+          ((7, 0), (0, 42, 101)),
           ((3, 4), 12),
           ((3, 4), (12,)),
           ((3, 4), -1),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 391630812..5eae4fdee 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -32,7 +32,11 @@ from jax.config import config
 
 FLAGS = config.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
+# fixed so the tests pass.
+all_shapes = array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -53,10 +57,10 @@ def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
 JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
     op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
     op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
@@ -306,12 +310,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(
-      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
-          ""_"".join(str(d) for d in shape),
-          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+      {""testcase_name"": ""_inshape={}_outdtype={}"".format(
+          jtu.format_shape_dtype_string(shape, fill_value_dtype),
+          onp.dtype(out_dtype).name),
        ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
        ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
-      for shape in all_shapes
+      for shape in array_shapes
       for fill_value_dtype in default_dtypes
       for out_dtype in default_dtypes)
   def testFull(self, shape, fill_value_dtype, out_dtype, rng):
@@ -345,6 +349,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
       for arg_shape, out_shape in [
+          (jtu.NUMPY_SCALAR_SHAPE, (1, 1, 1)),
+          ((), (1, 1, 1)),
+          ((7, 0), (0, 42, 101)),
           ((3, 4), 12),
           ((3, 4), (12,)),
           ((3, 4), -1),",No
WORKSPACE,WORKSPACE,90afb3a15503ccfb3655666a66f6466285fd7f2e,f2795cbdeadf5a27a8294d56c9888297a7bcf441,update tensorflow release,"diff --git a/WORKSPACE b/WORKSPACE
index a3e97ebfa..bbb200c79 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,10 +16,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""1a685ee839eb52e5fd26c0c9fbb0cddbf39a49cc5edaa6506604a3ba0fecfa2e"",
-    strip_prefix=""tensorflow-47fe933d89146a0488563bdfde2c0675a750ed16"",
+    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
+    strip_prefix=""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd""
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/47fe933d89146a0488563bdfde2c0675a750ed16.tar.gz"",
+      ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
     ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index a3e97ebfa..bbb200c79 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,10 +16,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""1a685ee839eb52e5fd26c0c9fbb0cddbf39a49cc5edaa6506604a3ba0fecfa2e"",
-    strip_prefix=""tensorflow-47fe933d89146a0488563bdfde2c0675a750ed16"",
+    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
+    strip_prefix=""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd""
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/47fe933d89146a0488563bdfde2c0675a750ed16.tar.gz"",
+      ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
     ],
 )
 ",No
WORKSPACE,WORKSPACE,b4344a07bc44f4548c557fbfc3dc08b9978ce001,90afb3a15503ccfb3655666a66f6466285fd7f2e,fix typo in WORKSPACE,"diff --git a/WORKSPACE b/WORKSPACE
index bbb200c79..0df920561 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -8,7 +8,6 @@ http_archive(
     ],
 )
 
-
 # To update TensorFlow to a new revision,
 # a) update URL and strip_prefix to the new git commit hash
 # b) get the sha256 hash of the commit by running:
@@ -17,9 +16,9 @@ http_archive(
 http_archive(
     name = ""org_tensorflow"",
     sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
-    strip_prefix=""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd""
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
     ],
 )
 
@@ -29,7 +28,6 @@ http_archive(
 #    path = ""tensorflow"",
 # )
 
-
 load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
 
 tf_workspace(","diff --git a/WORKSPACE b/WORKSPACE
index bbb200c79..0df920561 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -8,7 +8,6 @@ http_archive(
     ],
 )
 
-
 # To update TensorFlow to a new revision,
 # a) update URL and strip_prefix to the new git commit hash
 # b) get the sha256 hash of the commit by running:
@@ -17,9 +16,9 @@ http_archive(
 http_archive(
     name = ""org_tensorflow"",
     sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
-    strip_prefix=""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd""
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
     urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
     ],
 )
 
@@ -29,7 +28,6 @@ http_archive(
 #    path = ""tensorflow"",
 # )
 
-
 load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
 
 tf_workspace(",No
jax/abstract_arrays.py,jax/abstract_arrays.py,c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,b4344a07bc44f4548c557fbfc3dc08b9978ce001,"[JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 61fd8f7cb..66d913c9e 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -152,8 +152,9 @@ def make_shaped_array(x):
   dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
   return ShapedArray(onp.shape(x), dtype)
 
-array_types = [onp.ndarray, onp.float64, onp.float32, onp.int64, onp.int32,
-               onp.bool_, onp.uint64, onp.uint32, float, int, bool]
+array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
+               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32, float,
+               int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 61fd8f7cb..66d913c9e 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -152,8 +152,9 @@ def make_shaped_array(x):
   dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
   return ShapedArray(onp.shape(x), dtype)
 
-array_types = [onp.ndarray, onp.float64, onp.float32, onp.int64, onp.int32,
-               onp.bool_, onp.uint64, onp.uint32, float, int, bool]
+array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
+               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32, float,
+               int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,b4344a07bc44f4548c557fbfc3dc08b9978ce001,"[JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5a28af9f2..180b8b6f8 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -126,10 +126,7 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    all_scalar = _all(isscalar(x) for x in args)
-    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
-    keep_all = all_scalar or some_bools
-    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
+    from_dtypes = (_dtype(x) for x in args)
     to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5a28af9f2..180b8b6f8 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -126,10 +126,7 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    all_scalar = _all(isscalar(x) for x in args)
-    some_bools = _any(_dtype(x) == onp.dtype(""bool"") for x in args)
-    keep_all = all_scalar or some_bools
-    from_dtypes = (_dtype(x) for x in args if keep_all or not isscalar(x))
+    from_dtypes = (_dtype(x) for x in args)
     to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]",No
jax/random.py,jax/random.py,c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,b4344a07bc44f4548c557fbfc3dc08b9978ce001,"[JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.","diff --git a/jax/random.py b/jax/random.py
index 0edd3f1fb..2b7e4f34b 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -187,7 +187,7 @@ def _random_bits(key, bit_width, shape):
   bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
-    bits = (bits[0] << 32) | bits[1]
+    bits = (bits[0] << onp.uint64(32)) | bits[1]
   return lax.reshape(bits, shape)
 
 ","diff --git a/jax/random.py b/jax/random.py
index 0edd3f1fb..2b7e4f34b 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -187,7 +187,7 @@ def _random_bits(key, bit_width, shape):
   bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
-    bits = (bits[0] << 32) | bits[1]
+    bits = (bits[0] << onp.uint64(32)) | bits[1]
   return lax.reshape(bits, shape)
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,b4344a07bc44f4548c557fbfc3dc08b9978ce001,"[JAX] Change semantics of dtype promotion to just call numpy.result_type.

* Enable tests for numpy scalars in lax_numpy_test.py.
* Fix invalid promotion in random.py.
* Split tests for bitwise ops into their own test case and test mixed signedness.
* Add complex64 to the set of types supported by abstractify.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5eae4fdee..d275fc2a4 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -34,13 +34,13 @@ FLAGS = config.FLAGS
 
 array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
 
-# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
-# fixed so the tests pass.
-all_shapes = array_shapes
+all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
 int_dtypes = [onp.int32, onp.int64]
+unsigned_dtypes = (
+    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
@@ -57,10 +57,6 @@ def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
 JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
     op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
     op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
@@ -109,6 +105,17 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
 ]
 
+JAX_BITWISE_OP_RECORDS = [
+    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+]
+
 JAX_REDUCER_RECORDS = [
     op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
@@ -128,6 +135,22 @@ JAX_ARGMINMAX_RECORDS = [
 CombosWithReplacement = itertools.combinations_with_replacement
 
 
+def _dtypes_are_compatible_for_bitwise_ops(args):
+  if len(args) <= 1:
+    return True
+  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
+  width = lambda dtype: onp.iinfo(dtype).bits
+  x, y = args
+  if width(x) > width(y):
+    x, y = y, x
+  # The following condition seems a little ad hoc, but seems to capture what
+  # numpy actually implements.
+  return (
+      is_signed(x) == is_signed(y)
+      or (width(x) == 32 and width(y) == 32)
+      or (width(x) == 32 and width(y) == 64 and is_signed(y)))
+
+
 class LaxBackedNumpyTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed Numpy implementation.""""""
 
@@ -148,6 +171,21 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in JAX_BITWISE_OP_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in filter(
+          _dtypes_are_compatible_for_bitwise_ops,
+          CombosWithReplacement(rec.dtypes, rec.nargs)))
+  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5eae4fdee..d275fc2a4 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -34,13 +34,13 @@ FLAGS = config.FLAGS
 
 array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
 
-# TODO(b/120546360): add jtu.NUMPY_SCALAR_SHAPE when the coercion rules are
-# fixed so the tests pass.
-all_shapes = array_shapes
+all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
 int_dtypes = [onp.int32, onp.int64]
+unsigned_dtypes = (
+    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
@@ -57,10 +57,6 @@ def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
 JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""bitwise_and"", 2, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, int_dtypes, jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, int_dtypes, jtu.rand_bool(), []),
     op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
     op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
@@ -109,6 +105,17 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
 ]
 
+JAX_BITWISE_OP_RECORDS = [
+    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
+              jtu.rand_bool(), []),
+]
+
 JAX_REDUCER_RECORDS = [
     op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
@@ -128,6 +135,22 @@ JAX_ARGMINMAX_RECORDS = [
 CombosWithReplacement = itertools.combinations_with_replacement
 
 
+def _dtypes_are_compatible_for_bitwise_ops(args):
+  if len(args) <= 1:
+    return True
+  is_signed = lambda dtype: onp.issubdtype(dtype, onp.signedinteger)
+  width = lambda dtype: onp.iinfo(dtype).bits
+  x, y = args
+  if width(x) > width(y):
+    x, y = y, x
+  # The following condition seems a little ad hoc, but seems to capture what
+  # numpy actually implements.
+  return (
+      is_signed(x) == is_signed(y)
+      or (width(x) == 32 and width(y) == 32)
+      or (width(x) == 32 and width(y) == 64 and is_signed(y)))
+
+
 class LaxBackedNumpyTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed Numpy implementation.""""""
 
@@ -148,6 +171,21 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in JAX_BITWISE_OP_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in filter(
+          _dtypes_are_compatible_for_bitwise_ops,
+          CombosWithReplacement(rec.dtypes, rec.nargs)))
+  def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),",No
WORKSPACE,WORKSPACE,29113dd606ce77c74c0af785542abee0b1e6499d,678bcee7baaaed27b4e3bc0d7e75f6c88dbe27b7,Made tests runnable with bazel,"diff --git a/WORKSPACE b/WORKSPACE
index a3e97ebfa..e69de29bb 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -1,38 +0,0 @@
-http_archive(
-    name = ""io_bazel_rules_closure"",
-    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
-    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
-    urls = [
-        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
-        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
-    ],
-)
-
-
-# To update TensorFlow to a new revision,
-# a) update URL and strip_prefix to the new git commit hash
-# b) get the sha256 hash of the commit by running:
-#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
-#    and update the sha256 with the result.
-http_archive(
-    name = ""org_tensorflow"",
-    sha256 = ""1a685ee839eb52e5fd26c0c9fbb0cddbf39a49cc5edaa6506604a3ba0fecfa2e"",
-    strip_prefix=""tensorflow-47fe933d89146a0488563bdfde2c0675a750ed16"",
-    urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/47fe933d89146a0488563bdfde2c0675a750ed16.tar.gz"",
-    ],
-)
-
-# For development, one can use a local TF repository instead.
-# local_repository(
-#    name = ""org_tensorflow"",
-#    path = ""tensorflow"",
-# )
-
-
-load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
-
-tf_workspace(
-    path_prefix = """",
-    tf_repo_name = ""org_tensorflow"",
-)","diff --git a/WORKSPACE b/WORKSPACE
index a3e97ebfa..e69de29bb 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -1,38 +0,0 @@
-http_archive(
-    name = ""io_bazel_rules_closure"",
-    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
-    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
-    urls = [
-        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
-        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
-    ],
-)
-
-
-# To update TensorFlow to a new revision,
-# a) update URL and strip_prefix to the new git commit hash
-# b) get the sha256 hash of the commit by running:
-#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
-#    and update the sha256 with the result.
-http_archive(
-    name = ""org_tensorflow"",
-    sha256 = ""1a685ee839eb52e5fd26c0c9fbb0cddbf39a49cc5edaa6506604a3ba0fecfa2e"",
-    strip_prefix=""tensorflow-47fe933d89146a0488563bdfde2c0675a750ed16"",
-    urls = [
-      ""https://github.com/tensorflow/tensorflow/archive/47fe933d89146a0488563bdfde2c0675a750ed16.tar.gz"",
-    ],
-)
-
-# For development, one can use a local TF repository instead.
-# local_repository(
-#    name = ""org_tensorflow"",
-#    path = ""tensorflow"",
-# )
-
-
-load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
-
-tf_workspace(
-    path_prefix = """",
-    tf_repo_name = ""org_tensorflow"",
-)",No
tests/BUILD,tests/BUILD,29113dd606ce77c74c0af785542abee0b1e6499d,678bcee7baaaed27b4e3bc0d7e75f6c88dbe27b7,Made tests runnable with bazel,"diff --git a/tests/BUILD b/tests/BUILD
index 0669300c8..bc6eca82d 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -80,17 +80,14 @@ jax_test(
 jax_test(
     name = ""stax_test"",
     srcs = [""stax_test.py""],
-    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
     srcs = [""minmax_test.py""],
-    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
     srcs = [""lapax_test.py""],
-    deps = [""//jax:lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index 0669300c8..bc6eca82d 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -80,17 +80,14 @@ jax_test(
 jax_test(
     name = ""stax_test"",
     srcs = [""stax_test.py""],
-    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
     srcs = [""minmax_test.py""],
-    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
     srcs = [""lapax_test.py""],
-    deps = [""//jax:lapax""],
 )",No
tests/BUILD,tests/BUILD,c3374a9d5fdae5b8a11c4f245eb765ca0de7bc2d,29113dd606ce77c74c0af785542abee0b1e6499d,Added build rule for generated_fun_test (formerly quickish_check),"diff --git a/tests/BUILD b/tests/BUILD
index bc6eca82d..9c65b16b0 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -91,3 +91,8 @@ jax_test(
     name = ""lapax_test"",
     srcs = [""lapax_test.py""],
 )
+
+jax_test(
+    name = ""generated_fun_test"",
+    srcs = [""generated_fun_test.py""],
+)","diff --git a/tests/BUILD b/tests/BUILD
index bc6eca82d..9c65b16b0 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -91,3 +91,8 @@ jax_test(
     name = ""lapax_test"",
     srcs = [""lapax_test.py""],
 )
+
+jax_test(
+    name = ""generated_fun_test"",
+    srcs = [""generated_fun_test.py""],
+)",No
tests/quickercheck.py,tests/generated_fun_test.py,c3374a9d5fdae5b8a11c4f245eb765ca0de7bc2d,29113dd606ce77c74c0af785542abee0b1e6499d,Added build rule for generated_fun_test (formerly quickish_check),"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
new file mode 100644
index 000000000..c5f6f0b73
--- /dev/null
+++ b/tests/generated_fun_test.py
@@ -0,0 +1,267 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from collections import namedtuple
+from functools import partial
+import numpy.random as npr
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import jax.numpy as np
+from jax import jit, jvp, vjp
+from jax.test_util as jtu
+
+npr.seed(0)
+
+from jax.util import unzip2, safe_zip, safe_map
+
+map = safe_map
+zip = safe_zip
+
+subfun_prob = 0.5
+thin_prob = 0.1
+size_reduction_factor = 3
+
+Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
+Prim = namedtuple('Prim', ['fun'])
+ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
+Var = namedtuple('Var', ['name', 'vartype'])
+Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])
+
+def gen_fun_and_types(size):
+  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
+  fun, _ = gen_function(size, in_types)
+  return fun
+
+def gen_function(size, in_types):
+  eqns = []
+  in_vars = map(fresh_var, in_types)
+  cur_vars = in_vars[:]
+  for _ in range(gen_nonneg_int(size)):
+    if not cur_vars:
+      break
+    if npr.rand() < subfun_prob:
+      arg_vars = gen_subset(cur_vars)
+      arg_types = [v.vartype for v in arg_vars]
+      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
+      fun = partial(eval_fun, fun)
+    else:
+      arity = choice(primitive_generators.keys())
+      arg_vars = gen_sized_subset(cur_vars, arity)
+      arg_types = [v.vartype for v in arg_vars]
+      prim_gen = weighted_choice(primitive_generators[arity])
+      fun, out_type = prim_gen(size, *arg_types)
+      fun = wrap_singleton(fun)
+      out_types = [out_type]
+
+    out_vars = map(fresh_var, out_types)
+    eqns.append(Eqn(arg_vars, out_vars, fun))
+    cur_vars.extend(out_vars)
+    cur_vars = thin(cur_vars, thin_prob)
+
+  out_vars = gen_subset(cur_vars)
+  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]
+
+def eval_fun(fun, *args):
+  def read(v):
+    return env[v]
+  def write(v, x):
+    env[v] = x
+
+  env = {}
+  map(write, fun.in_vars, args)
+  for in_vars, out_vars, f in fun.eqns:
+    out_vals = f(*map(read, in_vars))
+    map(write, out_vars, out_vals)
+
+  return map(read, fun.out_vars)
+
+counter = it.count()
+def fresh_var(ty):
+  return Var(counter.next(), ty)
+
+def gen_array_type(size):
+  # TODO(dougalm): randomize this
+  return ArrayType((2,2), np.float32)
+
+def gen_array_val(array_type):
+  # TODO(dougalm): different sizes and dtypes
+  return npr.randn(*array_type.shape)
+
+def gen_neg(size, t):
+  return (lambda x: -x), t
+
+def gen_trig(size, t):
+  op = choice([np.sin, np.cos])
+  return op, t
+
+def gen_binop(size, t1, t2):
+  unifier, t_out = gen_broadcasting_unifier(t1, t2)
+  binop = choice([lambda x, y: x + y,
+                  lambda x, y: x * y])
+  def unify_and_binop(x, y):
+    x_, y_ = unifier(x, y)
+    return binop(x_, y_)
+
+  return unify_and_binop, t_out
+
+def thin(xs, p):
+  return [x for x in xs if npr.rand() > p]
+
+def gen_broadcasting_unifier(t1, t2):
+  assert t1.shape == t2.shape
+  return lambda x, y: (x,y), t1
+  # TODO: generate slices and paddings to match shapes
+
+def wrap_singleton(f):
+  return lambda *xs: (f(*xs),)
+
+unary_primitive_generators = [
+  (3, gen_trig),
+  (1, gen_neg) ]
+
+binary_primitive_generators = [
+  (1, gen_binop)]
+
+primitive_generators = { 1: unary_primitive_generators,
+                         2: binary_primitive_generators }
+
+def gen_nonneg_int(size):
+  return npr.randint(size)
+
+choice = npr.choice
+
+def weighted_choice(weighted_choices):
+  weights, choices = unzip2(weighted_choices)
+  return npr_choice(choices, weights)
+
+def npr_choice(xs, weights=None):
+  # npr.choice isn't actually RS -> [a] -> a
+  # because it inspects the components to see if they're array-like
+  assert xs
+  n = len(xs)
+  if weights is None:
+    i = npr.randint(n)
+  else:
+    normalizer = float(sum(weights))
+    weights = [w / normalizer for w in weights]
+    i = npr.choice(range(n), p=weights)
+  return xs[i]
+
+def gen_sized_subset(xs, size):
+  return [npr_choice(xs) for _ in range(size)]
+
+def gen_subset(xs):
+  if not xs:
+    return []
+
+  return gen_sized_subset(xs, npr.randint(len(xs) + 1))
+
+def gen_vals(vs):
+  return [gen_array_val(v.vartype) for v in vs]
+
+def inner_prod(xs, ys):
+  xys = zip(xs, ys)
+  assert all(x.shape == y.shape for x, y in xys)
+  return sum(np.sum(x * y) for x, y in xys)
+
+def jvp_fd(fun, args, tangents):
+  EPS = 1e-4
+  def eval_eps(eps):
+    return fun(*[x if t is None else x + eps * t
+                 for x, t in zip(args, tangents)])
+
+  ys_neg = eval_eps(-EPS)
+  ys_pos = eval_eps(EPS)
+  ys = eval_eps(0.0)
+  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
+  return ys, deriv
+
+def check_all_close(xs, ys, tol=1e-3):
+  for x, y in zip(xs, ys):
+    check_close(x, y, tol)
+
+def check_close(x, y, tol=1e-3):
+  assert np.shape(x) == np.shape(y)
+  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+  # assert x.dtype == y.dtype
+  assert np.allclose(x, y, rtol=tol, atol=tol), \
+     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
+def partial_argnums(f, args, dyn_argnums):
+  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
+  def f_(*dyn_args):
+    args = fixed_args[:]
+    for i, arg in zip(dyn_argnums, dyn_args):
+      args[i] = arg
+    return f(*args)
+
+  dyn_args = [args[i] for i in dyn_argnums]
+  return f_, dyn_args
+
+def jvp_matches_fd(fun):
+  vals = gen_vals(fun.in_vars)
+  tangents = gen_vals(fun.in_vars)
+  fun = partial(eval_fun, fun)
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  tangents = [tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
+  ans1, deriv1 = jvp_fd(fun, vals, tangents)
+  ans2, deriv2 = jvp(fun, vals, tangents)
+  check_all_close(ans1, ans2)
+  check_all_close(deriv1, deriv2)
+
+
+def vjp_matches_fd(fun):
+  vals = gen_vals(fun.in_vars)
+  in_tangents = gen_vals(fun.in_vars)
+  in_cotangents = gen_vals(fun.out_vars)
+  fun = partial(eval_fun, fun)
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  in_tangents = [in_tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
+  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+  ans2, vjpfun = vjp(fun, *vals)
+  out_cotangents = vjpfun(in_cotangents)
+  check_all_close(ans1, ans2)
+  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+  check_close(inner_prod_fd, inner_prod_ad)
+
+counter = it.count()
+fresh = counter.next
+
+class GeneratedFunTest(jtu.JaxTestCase):
+  """"""Tests of transformations on randomly generated functions.""""""
+
+  @parameterized.named_parameters(take(
+    {""testcase_name"": 'rand_fun_jit_test_{}'.format(fresh()),
+     ""fun"" : gen_fun_and_types(size) }
+     for _ in it.count()))
+  def testJitIsIdentity(self, fun):
+    vals = gen_vals(fun.in_vars)
+    fun = partial(eval_fun, fun)
+    ans = fun(*vals)
+    static_argnums = thin(range(len(vals)), 0.5)
+    ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
+    try:
+      check_all_close(ans, ans_jitted)
+    except:
+      print fun
+      raise
+
+if __name__ == ""__main__"":
+  config.config_with_absl()
+  absltest.main()","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
new file mode 100644
index 000000000..c5f6f0b73
--- /dev/null
+++ b/tests/generated_fun_test.py
@@ -0,0 +1,267 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from collections import namedtuple
+from functools import partial
+import numpy.random as npr
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import jax.numpy as np
+from jax import jit, jvp, vjp
+from jax.test_util as jtu
+
+npr.seed(0)
+
+from jax.util import unzip2, safe_zip, safe_map
+
+map = safe_map
+zip = safe_zip
+
+subfun_prob = 0.5
+thin_prob = 0.1
+size_reduction_factor = 3
+
+Eqn = namedtuple('Eqn', ['in_vars', 'out_vars', 'fun'])
+Prim = namedtuple('Prim', ['fun'])
+ArrayType = namedtuple('ArrayType', ['shape', 'dtype'])
+Var = namedtuple('Var', ['name', 'vartype'])
+Fun = namedtuple('Fun', ['in_vars', 'out_vars', 'eqns'])
+
+def gen_fun_and_types(size):
+  in_types = [gen_array_type(size) for _ in range(gen_nonneg_int(size))]
+  fun, _ = gen_function(size, in_types)
+  return fun
+
+def gen_function(size, in_types):
+  eqns = []
+  in_vars = map(fresh_var, in_types)
+  cur_vars = in_vars[:]
+  for _ in range(gen_nonneg_int(size)):
+    if not cur_vars:
+      break
+    if npr.rand() < subfun_prob:
+      arg_vars = gen_subset(cur_vars)
+      arg_types = [v.vartype for v in arg_vars]
+      fun, out_types = gen_function(size / size_reduction_factor, arg_types)
+      fun = partial(eval_fun, fun)
+    else:
+      arity = choice(primitive_generators.keys())
+      arg_vars = gen_sized_subset(cur_vars, arity)
+      arg_types = [v.vartype for v in arg_vars]
+      prim_gen = weighted_choice(primitive_generators[arity])
+      fun, out_type = prim_gen(size, *arg_types)
+      fun = wrap_singleton(fun)
+      out_types = [out_type]
+
+    out_vars = map(fresh_var, out_types)
+    eqns.append(Eqn(arg_vars, out_vars, fun))
+    cur_vars.extend(out_vars)
+    cur_vars = thin(cur_vars, thin_prob)
+
+  out_vars = gen_subset(cur_vars)
+  return Fun(in_vars, out_vars, eqns), [v.vartype for v in out_vars]
+
+def eval_fun(fun, *args):
+  def read(v):
+    return env[v]
+  def write(v, x):
+    env[v] = x
+
+  env = {}
+  map(write, fun.in_vars, args)
+  for in_vars, out_vars, f in fun.eqns:
+    out_vals = f(*map(read, in_vars))
+    map(write, out_vars, out_vals)
+
+  return map(read, fun.out_vars)
+
+counter = it.count()
+def fresh_var(ty):
+  return Var(counter.next(), ty)
+
+def gen_array_type(size):
+  # TODO(dougalm): randomize this
+  return ArrayType((2,2), np.float32)
+
+def gen_array_val(array_type):
+  # TODO(dougalm): different sizes and dtypes
+  return npr.randn(*array_type.shape)
+
+def gen_neg(size, t):
+  return (lambda x: -x), t
+
+def gen_trig(size, t):
+  op = choice([np.sin, np.cos])
+  return op, t
+
+def gen_binop(size, t1, t2):
+  unifier, t_out = gen_broadcasting_unifier(t1, t2)
+  binop = choice([lambda x, y: x + y,
+                  lambda x, y: x * y])
+  def unify_and_binop(x, y):
+    x_, y_ = unifier(x, y)
+    return binop(x_, y_)
+
+  return unify_and_binop, t_out
+
+def thin(xs, p):
+  return [x for x in xs if npr.rand() > p]
+
+def gen_broadcasting_unifier(t1, t2):
+  assert t1.shape == t2.shape
+  return lambda x, y: (x,y), t1
+  # TODO: generate slices and paddings to match shapes
+
+def wrap_singleton(f):
+  return lambda *xs: (f(*xs),)
+
+unary_primitive_generators = [
+  (3, gen_trig),
+  (1, gen_neg) ]
+
+binary_primitive_generators = [
+  (1, gen_binop)]
+
+primitive_generators = { 1: unary_primitive_generators,
+                         2: binary_primitive_generators }
+
+def gen_nonneg_int(size):
+  return npr.randint(size)
+
+choice = npr.choice
+
+def weighted_choice(weighted_choices):
+  weights, choices = unzip2(weighted_choices)
+  return npr_choice(choices, weights)
+
+def npr_choice(xs, weights=None):
+  # npr.choice isn't actually RS -> [a] -> a
+  # because it inspects the components to see if they're array-like
+  assert xs
+  n = len(xs)
+  if weights is None:
+    i = npr.randint(n)
+  else:
+    normalizer = float(sum(weights))
+    weights = [w / normalizer for w in weights]
+    i = npr.choice(range(n), p=weights)
+  return xs[i]
+
+def gen_sized_subset(xs, size):
+  return [npr_choice(xs) for _ in range(size)]
+
+def gen_subset(xs):
+  if not xs:
+    return []
+
+  return gen_sized_subset(xs, npr.randint(len(xs) + 1))
+
+def gen_vals(vs):
+  return [gen_array_val(v.vartype) for v in vs]
+
+def inner_prod(xs, ys):
+  xys = zip(xs, ys)
+  assert all(x.shape == y.shape for x, y in xys)
+  return sum(np.sum(x * y) for x, y in xys)
+
+def jvp_fd(fun, args, tangents):
+  EPS = 1e-4
+  def eval_eps(eps):
+    return fun(*[x if t is None else x + eps * t
+                 for x, t in zip(args, tangents)])
+
+  ys_neg = eval_eps(-EPS)
+  ys_pos = eval_eps(EPS)
+  ys = eval_eps(0.0)
+  deriv = [(y_pos - y_neg) / (2 * EPS) for y_neg, y_pos in zip(ys_neg, ys_pos)]
+  return ys, deriv
+
+def check_all_close(xs, ys, tol=1e-3):
+  for x, y in zip(xs, ys):
+    check_close(x, y, tol)
+
+def check_close(x, y, tol=1e-3):
+  assert np.shape(x) == np.shape(y)
+  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs
+  # assert x.dtype == y.dtype
+  assert np.allclose(x, y, rtol=tol, atol=tol), \
+     ""Value mismatch:\n{}\n  vs\n{}\n"".format(x, y)
+
+def partial_argnums(f, args, dyn_argnums):
+  fixed_args = [None if i in dyn_argnums else arg for i, arg in enumerate(args)]
+  def f_(*dyn_args):
+    args = fixed_args[:]
+    for i, arg in zip(dyn_argnums, dyn_args):
+      args[i] = arg
+    return f(*args)
+
+  dyn_args = [args[i] for i in dyn_argnums]
+  return f_, dyn_args
+
+def jvp_matches_fd(fun):
+  vals = gen_vals(fun.in_vars)
+  tangents = gen_vals(fun.in_vars)
+  fun = partial(eval_fun, fun)
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  tangents = [tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
+  ans1, deriv1 = jvp_fd(fun, vals, tangents)
+  ans2, deriv2 = jvp(fun, vals, tangents)
+  check_all_close(ans1, ans2)
+  check_all_close(deriv1, deriv2)
+
+
+def vjp_matches_fd(fun):
+  vals = gen_vals(fun.in_vars)
+  in_tangents = gen_vals(fun.in_vars)
+  in_cotangents = gen_vals(fun.out_vars)
+  fun = partial(eval_fun, fun)
+  dyn_argnums = thin(range(len(vals)), 0.5)
+  in_tangents = [in_tangents[i] for i in dyn_argnums]
+  fun, vals = partial_argnums(fun, vals, dyn_argnums)
+  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+  ans2, vjpfun = vjp(fun, *vals)
+  out_cotangents = vjpfun(in_cotangents)
+  check_all_close(ans1, ans2)
+  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+  check_close(inner_prod_fd, inner_prod_ad)
+
+counter = it.count()
+fresh = counter.next
+
+class GeneratedFunTest(jtu.JaxTestCase):
+  """"""Tests of transformations on randomly generated functions.""""""
+
+  @parameterized.named_parameters(take(
+    {""testcase_name"": 'rand_fun_jit_test_{}'.format(fresh()),
+     ""fun"" : gen_fun_and_types(size) }
+     for _ in it.count()))
+  def testJitIsIdentity(self, fun):
+    vals = gen_vals(fun.in_vars)
+    fun = partial(eval_fun, fun)
+    ans = fun(*vals)
+    static_argnums = thin(range(len(vals)), 0.5)
+    ans_jitted = jit(fun, static_argnums=static_argnums)(*vals)
+    try:
+      check_all_close(ans, ans_jitted)
+    except:
+      print fun
+      raise
+
+if __name__ == ""__main__"":
+  config.config_with_absl()
+  absltest.main()",No
jax/test_util.py,jax/test_util.py,3dbf41f3e6f9a01b0d32b925d22e9bfabc524fad,c3374a9d5fdae5b8a11c4f245eb765ca0de7bc2d,Generated function tests working with bazel,"diff --git a/jax/test_util.py b/jax/test_util.py
index 2c05a0b65..730d60eaf 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -18,6 +18,8 @@ from __future__ import print_function
 
 import functools
 import re
+import itertools as it
+import random
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -39,6 +41,10 @@ flags.DEFINE_enum(
     'Describes the device under test in case special consideration is required.'
 )
 
+flags.DEFINE_integer(
+  'num_generated_cases',
+  100,
+  help='Number of generated cases to test')
 
 EPS = 1e-4
 ATOL = 1e-4
@@ -304,6 +310,26 @@ def check_raises_regexp(thunk, err_type, pattern):
     assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
 
 
+random.seed(0)
+
+
+def take(xs):
+  return dedup(it.islice(xs, FLAGS.num_generated_cases))
+
+
+def dedup(xs):
+  seen = set()
+  for x in xs:
+    name = x[""testcase_name""]
+    if name not in seen:
+      seen.add(name)
+      yield x
+
+
+def sample(xs):
+  return [random.choice(xs)]
+
+
 class JaxTestCase(parameterized.TestCase):
   """"""Base class for JAX tests including numerical checks and boilerplate.""""""
 ","diff --git a/jax/test_util.py b/jax/test_util.py
index 2c05a0b65..730d60eaf 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -18,6 +18,8 @@ from __future__ import print_function
 
 import functools
 import re
+import itertools as it
+import random
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -39,6 +41,10 @@ flags.DEFINE_enum(
     'Describes the device under test in case special consideration is required.'
 )
 
+flags.DEFINE_integer(
+  'num_generated_cases',
+  100,
+  help='Number of generated cases to test')
 
 EPS = 1e-4
 ATOL = 1e-4
@@ -304,6 +310,26 @@ def check_raises_regexp(thunk, err_type, pattern):
     assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
 
 
+random.seed(0)
+
+
+def take(xs):
+  return dedup(it.islice(xs, FLAGS.num_generated_cases))
+
+
+def dedup(xs):
+  seen = set()
+  for x in xs:
+    name = x[""testcase_name""]
+    if name not in seen:
+      seen.add(name)
+      yield x
+
+
+def sample(xs):
+  return [random.choice(xs)]
+
+
 class JaxTestCase(parameterized.TestCase):
   """"""Base class for JAX tests including numerical checks and boilerplate.""""""
 ",No
tests/generated_fun_test.py,tests/generated_fun_test.py,3dbf41f3e6f9a01b0d32b925d22e9bfabc524fad,c3374a9d5fdae5b8a11c4f245eb765ca0de7bc2d,Generated function tests working with bazel,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index c5f6f0b73..4b5716c21 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -19,9 +19,11 @@ import numpy.random as npr
 from absl.testing import absltest
 from absl.testing import parameterized
 
+import itertools as it
 import jax.numpy as np
 from jax import jit, jvp, vjp
-from jax.test_util as jtu
+import jax.test_util as jtu
+from jax.config import config
 
 npr.seed(0)
 
@@ -211,44 +213,15 @@ def partial_argnums(f, args, dyn_argnums):
   dyn_args = [args[i] for i in dyn_argnums]
   return f_, dyn_args
 
-def jvp_matches_fd(fun):
-  vals = gen_vals(fun.in_vars)
-  tangents = gen_vals(fun.in_vars)
-  fun = partial(eval_fun, fun)
-  dyn_argnums = thin(range(len(vals)), 0.5)
-  tangents = [tangents[i] for i in dyn_argnums]
-  fun, vals = partial_argnums(fun, vals, dyn_argnums)
-  ans1, deriv1 = jvp_fd(fun, vals, tangents)
-  ans2, deriv2 = jvp(fun, vals, tangents)
-  check_all_close(ans1, ans2)
-  check_all_close(deriv1, deriv2)
-
-
-def vjp_matches_fd(fun):
-  vals = gen_vals(fun.in_vars)
-  in_tangents = gen_vals(fun.in_vars)
-  in_cotangents = gen_vals(fun.out_vars)
-  fun = partial(eval_fun, fun)
-  dyn_argnums = thin(range(len(vals)), 0.5)
-  in_tangents = [in_tangents[i] for i in dyn_argnums]
-  fun, vals = partial_argnums(fun, vals, dyn_argnums)
-  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
-  ans2, vjpfun = vjp(fun, *vals)
-  out_cotangents = vjpfun(in_cotangents)
-  check_all_close(ans1, ans2)
-  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
-  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
-  check_close(inner_prod_fd, inner_prod_ad)
-
 counter = it.count()
 fresh = counter.next
 
 class GeneratedFunTest(jtu.JaxTestCase):
   """"""Tests of transformations on randomly generated functions.""""""
 
-  @parameterized.named_parameters(take(
-    {""testcase_name"": 'rand_fun_jit_test_{}'.format(fresh()),
-     ""fun"" : gen_fun_and_types(size) }
+  @parameterized.named_parameters(jtu.take(
+    {""testcase_name"": str(fresh()),
+     ""fun"" : gen_fun_and_types(10) }
      for _ in it.count()))
   def testJitIsIdentity(self, fun):
     vals = gen_vals(fun.in_vars)
@@ -262,6 +235,43 @@ class GeneratedFunTest(jtu.JaxTestCase):
       print fun
       raise
 
+  @parameterized.named_parameters(jtu.take(
+    {""testcase_name"": str(fresh()),
+     ""fun"" : gen_fun_and_types(10) }
+     for _ in it.count()))
+  def testJVPMatchesFD(self, fun):
+    vals = gen_vals(fun.in_vars)
+    tangents = gen_vals(fun.in_vars)
+    fun = partial(eval_fun, fun)
+    dyn_argnums = thin(range(len(vals)), 0.5)
+    tangents = [tangents[i] for i in dyn_argnums]
+    fun, vals = partial_argnums(fun, vals, dyn_argnums)
+    ans1, deriv1 = jvp_fd(fun, vals, tangents)
+    ans2, deriv2 = jvp(fun, vals, tangents)
+    check_all_close(ans1, ans2)
+    check_all_close(deriv1, deriv2)
+
+  @parameterized.named_parameters(jtu.take(
+    {""testcase_name"": str(fresh()),
+     ""fun"" : gen_fun_and_types(10) }
+     for _ in it.count()))
+  def vjp_matches_fd(self, fun):
+    vals = gen_vals(fun.in_vars)
+    in_tangents = gen_vals(fun.in_vars)
+    in_cotangents = gen_vals(fun.out_vars)
+    fun = partial(eval_fun, fun)
+    dyn_argnums = thin(range(len(vals)), 0.5)
+    in_tangents = [in_tangents[i] for i in dyn_argnums]
+    fun, vals = partial_argnums(fun, vals, dyn_argnums)
+    ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+    ans2, vjpfun = vjp(fun, *vals)
+    out_cotangents = vjpfun(in_cotangents)
+    check_all_close(ans1, ans2)
+    inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+    inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+    check_close(inner_prod_fd, inner_prod_ad)
+
+
 if __name__ == ""__main__"":
   config.config_with_absl()
   absltest.main()","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index c5f6f0b73..4b5716c21 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -19,9 +19,11 @@ import numpy.random as npr
 from absl.testing import absltest
 from absl.testing import parameterized
 
+import itertools as it
 import jax.numpy as np
 from jax import jit, jvp, vjp
-from jax.test_util as jtu
+import jax.test_util as jtu
+from jax.config import config
 
 npr.seed(0)
 
@@ -211,44 +213,15 @@ def partial_argnums(f, args, dyn_argnums):
   dyn_args = [args[i] for i in dyn_argnums]
   return f_, dyn_args
 
-def jvp_matches_fd(fun):
-  vals = gen_vals(fun.in_vars)
-  tangents = gen_vals(fun.in_vars)
-  fun = partial(eval_fun, fun)
-  dyn_argnums = thin(range(len(vals)), 0.5)
-  tangents = [tangents[i] for i in dyn_argnums]
-  fun, vals = partial_argnums(fun, vals, dyn_argnums)
-  ans1, deriv1 = jvp_fd(fun, vals, tangents)
-  ans2, deriv2 = jvp(fun, vals, tangents)
-  check_all_close(ans1, ans2)
-  check_all_close(deriv1, deriv2)
-
-
-def vjp_matches_fd(fun):
-  vals = gen_vals(fun.in_vars)
-  in_tangents = gen_vals(fun.in_vars)
-  in_cotangents = gen_vals(fun.out_vars)
-  fun = partial(eval_fun, fun)
-  dyn_argnums = thin(range(len(vals)), 0.5)
-  in_tangents = [in_tangents[i] for i in dyn_argnums]
-  fun, vals = partial_argnums(fun, vals, dyn_argnums)
-  ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
-  ans2, vjpfun = vjp(fun, *vals)
-  out_cotangents = vjpfun(in_cotangents)
-  check_all_close(ans1, ans2)
-  inner_prod_fd = inner_prod(out_tangents, in_cotangents)
-  inner_prod_ad = inner_prod(in_tangents, out_cotangents)
-  check_close(inner_prod_fd, inner_prod_ad)
-
 counter = it.count()
 fresh = counter.next
 
 class GeneratedFunTest(jtu.JaxTestCase):
   """"""Tests of transformations on randomly generated functions.""""""
 
-  @parameterized.named_parameters(take(
-    {""testcase_name"": 'rand_fun_jit_test_{}'.format(fresh()),
-     ""fun"" : gen_fun_and_types(size) }
+  @parameterized.named_parameters(jtu.take(
+    {""testcase_name"": str(fresh()),
+     ""fun"" : gen_fun_and_types(10) }
      for _ in it.count()))
   def testJitIsIdentity(self, fun):
     vals = gen_vals(fun.in_vars)
@@ -262,6 +235,43 @@ class GeneratedFunTest(jtu.JaxTestCase):
       print fun
       raise
 
+  @parameterized.named_parameters(jtu.take(
+    {""testcase_name"": str(fresh()),
+     ""fun"" : gen_fun_and_types(10) }
+     for _ in it.count()))
+  def testJVPMatchesFD(self, fun):
+    vals = gen_vals(fun.in_vars)
+    tangents = gen_vals(fun.in_vars)
+    fun = partial(eval_fun, fun)
+    dyn_argnums = thin(range(len(vals)), 0.5)
+    tangents = [tangents[i] for i in dyn_argnums]
+    fun, vals = partial_argnums(fun, vals, dyn_argnums)
+    ans1, deriv1 = jvp_fd(fun, vals, tangents)
+    ans2, deriv2 = jvp(fun, vals, tangents)
+    check_all_close(ans1, ans2)
+    check_all_close(deriv1, deriv2)
+
+  @parameterized.named_parameters(jtu.take(
+    {""testcase_name"": str(fresh()),
+     ""fun"" : gen_fun_and_types(10) }
+     for _ in it.count()))
+  def vjp_matches_fd(self, fun):
+    vals = gen_vals(fun.in_vars)
+    in_tangents = gen_vals(fun.in_vars)
+    in_cotangents = gen_vals(fun.out_vars)
+    fun = partial(eval_fun, fun)
+    dyn_argnums = thin(range(len(vals)), 0.5)
+    in_tangents = [in_tangents[i] for i in dyn_argnums]
+    fun, vals = partial_argnums(fun, vals, dyn_argnums)
+    ans1, out_tangents = jvp_fd(fun, vals, in_tangents)
+    ans2, vjpfun = vjp(fun, *vals)
+    out_cotangents = vjpfun(in_cotangents)
+    check_all_close(ans1, ans2)
+    inner_prod_fd = inner_prod(out_tangents, in_cotangents)
+    inner_prod_ad = inner_prod(in_tangents, out_cotangents)
+    check_close(inner_prod_fd, inner_prod_ad)
+
+
 if __name__ == ""__main__"":
   config.config_with_absl()
   absltest.main()",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,bfd58854e915439dc9aad057a30dddfee343e681,c1b9eb19eabb9b02b4140a360bb1ddee17240b2a,Fix test case to test for x64 mode during test rather than init time.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index d275fc2a4..da799b237 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -39,8 +39,7 @@ all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
 int_dtypes = [onp.int32, onp.int64]
-unsigned_dtypes = (
-    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
+unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
@@ -182,6 +181,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           _dtypes_are_compatible_for_bitwise_ops,
           CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    if not FLAGS.jax_enable_x64 and any(
+        onp.iinfo(dtype).bits == 64 for dtype in dtypes):
+      self.skipTest(""x64 types are disabled by jax_enable_x64"")
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index d275fc2a4..da799b237 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -39,8 +39,7 @@ all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
 int_dtypes = [onp.int32, onp.int64]
-unsigned_dtypes = (
-    [onp.uint32, onp.uint64] if FLAGS.jax_enable_x64 else [onp.uint32])
+unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
@@ -182,6 +181,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           _dtypes_are_compatible_for_bitwise_ops,
           CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    if not FLAGS.jax_enable_x64 and any(
+        onp.iinfo(dtype).bits == 64 for dtype in dtypes):
+      self.skipTest(""x64 types are disabled by jax_enable_x64"")
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)",No
jax/test_util.py,jax/test_util.py,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,3dbf41f3e6f9a01b0d32b925d22e9bfabc524fad,Step through sizes in test case generation,"diff --git a/jax/test_util.py b/jax/test_util.py
index 730d60eaf..b09daa37e 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -310,24 +310,17 @@ def check_raises_regexp(thunk, err_type, pattern):
     assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
 
 
-random.seed(0)
+def cases_from_list(xs):
+  assert False
 
+random.seed(0) # TODO: consider managing prng state more carefully
 
-def take(xs):
-  return dedup(it.islice(xs, FLAGS.num_generated_cases))
-
-
-def dedup(xs):
-  seen = set()
-  for x in xs:
-    name = x[""testcase_name""]
-    if name not in seen:
-      seen.add(name)
-      yield x
-
-
-def sample(xs):
-  return [random.choice(xs)]
+def cases_from_gens(*gens):
+  sizes = [1, 3, 10]
+  cases_per_size = int(FLAGS.num_generated_cases / len(sizes))
+  for size in sizes:
+    for i in xrange(cases_per_size):
+      yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)
 
 
 class JaxTestCase(parameterized.TestCase):","diff --git a/jax/test_util.py b/jax/test_util.py
index 730d60eaf..b09daa37e 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -310,24 +310,17 @@ def check_raises_regexp(thunk, err_type, pattern):
     assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
 
 
-random.seed(0)
+def cases_from_list(xs):
+  assert False
 
+random.seed(0) # TODO: consider managing prng state more carefully
 
-def take(xs):
-  return dedup(it.islice(xs, FLAGS.num_generated_cases))
-
-
-def dedup(xs):
-  seen = set()
-  for x in xs:
-    name = x[""testcase_name""]
-    if name not in seen:
-      seen.add(name)
-      yield x
-
-
-def sample(xs):
-  return [random.choice(xs)]
+def cases_from_gens(*gens):
+  sizes = [1, 3, 10]
+  cases_per_size = int(FLAGS.num_generated_cases / len(sizes))
+  for size in sizes:
+    for i in xrange(cases_per_size):
+      yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)
 
 
 class JaxTestCase(parameterized.TestCase):",No
tests/generated_fun_test.py,tests/generated_fun_test.py,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,3dbf41f3e6f9a01b0d32b925d22e9bfabc524fad,Step through sizes in test case generation,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 4b5716c21..18571e142 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -213,16 +213,11 @@ def partial_argnums(f, args, dyn_argnums):
   dyn_args = [args[i] for i in dyn_argnums]
   return f_, dyn_args
 
-counter = it.count()
-fresh = counter.next
 
 class GeneratedFunTest(jtu.JaxTestCase):
   """"""Tests of transformations on randomly generated functions.""""""
 
-  @parameterized.named_parameters(jtu.take(
-    {""testcase_name"": str(fresh()),
-     ""fun"" : gen_fun_and_types(10) }
-     for _ in it.count()))
+  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
   def testJitIsIdentity(self, fun):
     vals = gen_vals(fun.in_vars)
     fun = partial(eval_fun, fun)
@@ -235,10 +230,7 @@ class GeneratedFunTest(jtu.JaxTestCase):
       print fun
       raise
 
-  @parameterized.named_parameters(jtu.take(
-    {""testcase_name"": str(fresh()),
-     ""fun"" : gen_fun_and_types(10) }
-     for _ in it.count()))
+  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
   def testJVPMatchesFD(self, fun):
     vals = gen_vals(fun.in_vars)
     tangents = gen_vals(fun.in_vars)
@@ -251,10 +243,7 @@ class GeneratedFunTest(jtu.JaxTestCase):
     check_all_close(ans1, ans2)
     check_all_close(deriv1, deriv2)
 
-  @parameterized.named_parameters(jtu.take(
-    {""testcase_name"": str(fresh()),
-     ""fun"" : gen_fun_and_types(10) }
-     for _ in it.count()))
+  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
   def vjp_matches_fd(self, fun):
     vals = gen_vals(fun.in_vars)
     in_tangents = gen_vals(fun.in_vars)","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 4b5716c21..18571e142 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -213,16 +213,11 @@ def partial_argnums(f, args, dyn_argnums):
   dyn_args = [args[i] for i in dyn_argnums]
   return f_, dyn_args
 
-counter = it.count()
-fresh = counter.next
 
 class GeneratedFunTest(jtu.JaxTestCase):
   """"""Tests of transformations on randomly generated functions.""""""
 
-  @parameterized.named_parameters(jtu.take(
-    {""testcase_name"": str(fresh()),
-     ""fun"" : gen_fun_and_types(10) }
-     for _ in it.count()))
+  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
   def testJitIsIdentity(self, fun):
     vals = gen_vals(fun.in_vars)
     fun = partial(eval_fun, fun)
@@ -235,10 +230,7 @@ class GeneratedFunTest(jtu.JaxTestCase):
       print fun
       raise
 
-  @parameterized.named_parameters(jtu.take(
-    {""testcase_name"": str(fresh()),
-     ""fun"" : gen_fun_and_types(10) }
-     for _ in it.count()))
+  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
   def testJVPMatchesFD(self, fun):
     vals = gen_vals(fun.in_vars)
     tangents = gen_vals(fun.in_vars)
@@ -251,10 +243,7 @@ class GeneratedFunTest(jtu.JaxTestCase):
     check_all_close(ans1, ans2)
     check_all_close(deriv1, deriv2)
 
-  @parameterized.named_parameters(jtu.take(
-    {""testcase_name"": str(fresh()),
-     ""fun"" : gen_fun_and_types(10) }
-     for _ in it.count()))
+  @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))
   def vjp_matches_fd(self, fun):
     vals = gen_vals(fun.in_vars)
     in_tangents = gen_vals(fun.in_vars)",No
jax/config.py,jax/config.py,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,Number of test cases settable with command-line flag,"diff --git a/jax/config.py b/jax/config.py
index e92dfb0eb..6f5bb11c9 100644
--- a/jax/config.py
+++ b/jax/config.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import sys
+
 
 class Config(object):
   def __init__(self):
@@ -75,6 +77,10 @@ class Config(object):
     for name, _ in self.values.items():
       self.update(name, getattr(absl_flags.FLAGS, name))
 
+  def parse_flags_with_absl(self):
+    import absl.flags
+    self.config_with_absl()
+    absl.flags.FLAGS(sys.argv)
 
 class NameSpace(object):
   def __init__(self, getter):","diff --git a/jax/config.py b/jax/config.py
index e92dfb0eb..6f5bb11c9 100644
--- a/jax/config.py
+++ b/jax/config.py
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import sys
+
 
 class Config(object):
   def __init__(self):
@@ -75,6 +77,10 @@ class Config(object):
     for name, _ in self.values.items():
       self.update(name, getattr(absl_flags.FLAGS, name))
 
+  def parse_flags_with_absl(self):
+    import absl.flags
+    self.config_with_absl()
+    absl.flags.FLAGS(sys.argv)
 
 class NameSpace(object):
   def __init__(self, getter):",No
jax/test_util.py,jax/test_util.py,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,Number of test cases settable with command-line flag,"diff --git a/jax/test_util.py b/jax/test_util.py
index b09daa37e..cbad6a79c 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -309,15 +309,16 @@ def check_raises_regexp(thunk, err_type, pattern):
   except err_type as e:
     assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
 
+random.seed(0) # TODO: consider managing prng state more carefully
 
 def cases_from_list(xs):
-  assert False
-
-random.seed(0) # TODO: consider managing prng state more carefully
+  xs = list(xs)
+  k = min(len(xs), FLAGS.num_generated_cases)
+  return random.sample(xs, k)
 
 def cases_from_gens(*gens):
   sizes = [1, 3, 10]
-  cases_per_size = int(FLAGS.num_generated_cases / len(sizes))
+  cases_per_size = int(FLAGS.num_generated_cases / len(sizes)) + 1
   for size in sizes:
     for i in xrange(cases_per_size):
       yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)","diff --git a/jax/test_util.py b/jax/test_util.py
index b09daa37e..cbad6a79c 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -309,15 +309,16 @@ def check_raises_regexp(thunk, err_type, pattern):
   except err_type as e:
     assert re.match(pattern, str(e)), ""{}\n\n{}\n"".format(e, pattern)
 
+random.seed(0) # TODO: consider managing prng state more carefully
 
 def cases_from_list(xs):
-  assert False
-
-random.seed(0) # TODO: consider managing prng state more carefully
+  xs = list(xs)
+  k = min(len(xs), FLAGS.num_generated_cases)
+  return random.sample(xs, k)
 
 def cases_from_gens(*gens):
   sizes = [1, 3, 10]
-  cases_per_size = int(FLAGS.num_generated_cases / len(sizes))
+  cases_per_size = int(FLAGS.num_generated_cases / len(sizes)) + 1
   for size in sizes:
     for i in xrange(cases_per_size):
       yield ('_{}_{}'.format(size, i),) + tuple(gen(size) for gen in gens)",No
tests/generated_fun_test.py,tests/generated_fun_test.py,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,Number of test cases settable with command-line flag,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 18571e142..90c090e70 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -25,6 +25,8 @@ from jax import jit, jvp, vjp
 import jax.test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
+
 npr.seed(0)
 
 from jax.util import unzip2, safe_zip, safe_map
@@ -262,5 +264,4 @@ class GeneratedFunTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 18571e142..90c090e70 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -25,6 +25,8 @@ from jax import jit, jvp, vjp
 import jax.test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
+
 npr.seed(0)
 
 from jax.util import unzip2, safe_zip, safe_map
@@ -262,5 +264,4 @@ class GeneratedFunTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,Number of test cases settable with command-line flag,"diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 024c3e991..92fa80a4d 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -31,6 +31,7 @@ from jax import numpy as lnp
 from jax import test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 # We disable the whitespace continuation check in this file because otherwise it
@@ -60,7 +61,7 @@ def check_grads(f, args, order, atol=None, rtol=None, eps=None):
 class IndexingTest(jtu.JaxTestCase):
   """"""Tests for Numpy indexing translation rules.""""""
 
-  @parameterized.named_parameters({
+  @parameterized.named_parameters(jtu.cases_from_list({
       ""testcase_name"":
           ""{}_inshape={}_indexer={}"".format(
               name, jtu.format_shape_dtype_string( shape, dtype), indexer),
@@ -152,14 +153,14 @@ class IndexingTest(jtu.JaxTestCase):
           IndexSpec(shape=(3, 4), indexer=()),
       ]),
   ] for shape, indexer in index_specs for dtype in all_dtypes
-                                  for rng in [jtu.rand_default()])
+                                  for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype)]
     fun = lambda x: x[indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters({
+  @parameterized.named_parameters(jtu.cases_from_list({
       ""testcase_name"":
           ""{}_inshape={}_indexer={}"".format(name,
                                             jtu.format_shape_dtype_string(
@@ -231,7 +232,7 @@ class IndexingTest(jtu.JaxTestCase):
       #   IndexSpec(shape=(3, 4), indexer=()),
       #   ]),
   ] for shape, indexer in index_specs for dtype in float_dtypes
-                                  for rng in [jtu.rand_default()])
+                                  for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -255,7 +256,7 @@ class IndexingTest(jtu.JaxTestCase):
     else:
       return idx, lambda x: x
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -278,7 +279,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -290,7 +291,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self.assertRaises(IndexError, lambda: fun(*args_maker()))
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -310,7 +311,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -321,7 +322,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -343,7 +344,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
     # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -357,7 +358,7 @@ class IndexingTest(jtu.JaxTestCase):
     arr = rng(shape, dtype)
     check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -409,13 +410,13 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype), indexer]
     fun = lambda x, idx: x[idx]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -467,14 +468,14 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     arg = rng(shape, dtype)
     fun = lambda x: x[indexer]**2
     check_grads(fun, (arg,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -530,7 +531,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
                             for e in indexer]
@@ -587,5 +588,4 @@ class IndexingTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 024c3e991..92fa80a4d 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -31,6 +31,7 @@ from jax import numpy as lnp
 from jax import test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 # We disable the whitespace continuation check in this file because otherwise it
@@ -60,7 +61,7 @@ def check_grads(f, args, order, atol=None, rtol=None, eps=None):
 class IndexingTest(jtu.JaxTestCase):
   """"""Tests for Numpy indexing translation rules.""""""
 
-  @parameterized.named_parameters({
+  @parameterized.named_parameters(jtu.cases_from_list({
       ""testcase_name"":
           ""{}_inshape={}_indexer={}"".format(
               name, jtu.format_shape_dtype_string( shape, dtype), indexer),
@@ -152,14 +153,14 @@ class IndexingTest(jtu.JaxTestCase):
           IndexSpec(shape=(3, 4), indexer=()),
       ]),
   ] for shape, indexer in index_specs for dtype in all_dtypes
-                                  for rng in [jtu.rand_default()])
+                                  for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype)]
     fun = lambda x: x[indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters({
+  @parameterized.named_parameters(jtu.cases_from_list({
       ""testcase_name"":
           ""{}_inshape={}_indexer={}"".format(name,
                                             jtu.format_shape_dtype_string(
@@ -231,7 +232,7 @@ class IndexingTest(jtu.JaxTestCase):
       #   IndexSpec(shape=(3, 4), indexer=()),
       #   ]),
   ] for shape, indexer in index_specs for dtype in float_dtypes
-                                  for rng in [jtu.rand_default()])
+                                  for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -255,7 +256,7 @@ class IndexingTest(jtu.JaxTestCase):
     else:
       return idx, lambda x: x
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -278,7 +279,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -290,7 +291,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self.assertRaises(IndexError, lambda: fun(*args_maker()))
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -310,7 +311,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -321,7 +322,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -343,7 +344,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
     # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -357,7 +358,7 @@ class IndexingTest(jtu.JaxTestCase):
     arr = rng(shape, dtype)
     check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -409,13 +410,13 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype), indexer]
     fun = lambda x, idx: x[idx]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -467,14 +468,14 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     arg = rng(shape, dtype)
     fun = lambda x: x[indexer]**2
     check_grads(fun, (arg,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -530,7 +531,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
                             for e in indexer]
@@ -587,5 +588,4 @@ class IndexingTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
tests/stax_test.py,tests/stax_test.py,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,ebc6cd1e03c5c0f4046d5cf77a8b1aff63f25b43,Number of test cases settable with command-line flag,"diff --git a/tests/stax_test.py b/tests/stax_test.py
index 3b48dadfb..5d5a38fb1 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -27,6 +27,8 @@ from jax import random
 from jax.config import config
 from jax.experimental import stax
 
+config.parse_flags_with_absl()
+
 
 def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   result_shape, params = init_fun(input_shape)
@@ -38,21 +40,21 @@ def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
 
 class StaxTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
-      for shape in [(2, 3), (5,)])
+      for shape in [(2, 3), (5,)]))
   def testRandnInitShape(self, shape):
     out = stax.randn()(shape)
     self.assertEqual(out.shape, shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
-      for shape in [(2, 3), (2, 3, 4)])
+      for shape in [(2, 3), (2, 3, 4)]))
   def testGlorotInitShape(self, shape):
     out = stax.glorot()(shape)
     self.assertEqual(out.shape, shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_channels={}_filter_shape={}_padding={}_strides={}_input_shape={}""
        .format(channels, filter_shape, padding, strides, input_shape),
@@ -62,32 +64,32 @@ class StaxTest(jtu.JaxTestCase):
       for filter_shape in [(1, 1), (2, 3)]
       for padding in [""SAME"", ""VALID""]
       for strides in [None, (2, 1)]
-      for input_shape in [(2, 10, 11, 1)])
+      for input_shape in [(2, 10, 11, 1)]))
   def testConvShape(self, channels, filter_shape, padding, strides,
                     input_shape):
     init_fun, apply_fun = stax.Conv(channels, filter_shape, strides=strides,
                                     padding=padding)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_out_dim={}_input_shape={}""
                         .format(out_dim, input_shape),
        ""out_dim"": out_dim, ""input_shape"": input_shape}
       for out_dim in [3, 4]
-      for input_shape in [(2, 3), (3, 4)])
+      for input_shape in [(2, 3), (3, 4)]))
   def testDenseShape(self, out_dim, input_shape):
     init_fun, apply_fun = stax.Dense(out_dim)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
-      for input_shape in [(2, 3), (2, 3, 4)])
+      for input_shape in [(2, 3), (2, 3, 4)]))
   def testReluShape(self, input_shape):
     init_fun, apply_fun = stax.Relu
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
                         .format(window_shape, padding, strides, input_shape),
        ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
@@ -95,40 +97,39 @@ class StaxTest(jtu.JaxTestCase):
       for window_shape in [(1, 1), (2, 3)]
       for padding in [""VALID""]
       for strides in [None, (2, 1)]
-      for input_shape in [(2, 5, 6, 1)])
+      for input_shape in [(2, 5, 6, 1)]))
   def testPoolingShape(self, window_shape, padding, strides, input_shape):
     init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
                                        strides=strides)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
-      for input_shape in [(2, 3), (2, 3, 4)])
+      for input_shape in [(2, 3), (2, 3, 4)]))
   def testFlattenShape(self, input_shape):
     init_fun, apply_fun = stax.Flatten
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_input_shape={}_spec={}"".format(input_shape, i),
        ""input_shape"": input_shape, ""spec"": spec}
       for input_shape in [(2, 5, 6, 1)]
       for i, spec in enumerate([
           [stax.Conv(3, (2, 2))],
-          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]]))
+          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]])))
   def testSerialComposeLayersShape(self, input_shape, spec):
     init_fun, apply_fun = stax.serial(*spec)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
-      for input_shape in [(3, 4), (2, 5, 6, 1)])
+      for input_shape in [(3, 4), (2, 5, 6, 1)]))
   def testDropoutShape(self, input_shape):
     init_fun, apply_fun = stax.Dropout(0.9)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 3b48dadfb..5d5a38fb1 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -27,6 +27,8 @@ from jax import random
 from jax.config import config
 from jax.experimental import stax
 
+config.parse_flags_with_absl()
+
 
 def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   result_shape, params = init_fun(input_shape)
@@ -38,21 +40,21 @@ def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
 
 class StaxTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
-      for shape in [(2, 3), (5,)])
+      for shape in [(2, 3), (5,)]))
   def testRandnInitShape(self, shape):
     out = stax.randn()(shape)
     self.assertEqual(out.shape, shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
-      for shape in [(2, 3), (2, 3, 4)])
+      for shape in [(2, 3), (2, 3, 4)]))
   def testGlorotInitShape(self, shape):
     out = stax.glorot()(shape)
     self.assertEqual(out.shape, shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_channels={}_filter_shape={}_padding={}_strides={}_input_shape={}""
        .format(channels, filter_shape, padding, strides, input_shape),
@@ -62,32 +64,32 @@ class StaxTest(jtu.JaxTestCase):
       for filter_shape in [(1, 1), (2, 3)]
       for padding in [""SAME"", ""VALID""]
       for strides in [None, (2, 1)]
-      for input_shape in [(2, 10, 11, 1)])
+      for input_shape in [(2, 10, 11, 1)]))
   def testConvShape(self, channels, filter_shape, padding, strides,
                     input_shape):
     init_fun, apply_fun = stax.Conv(channels, filter_shape, strides=strides,
                                     padding=padding)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_out_dim={}_input_shape={}""
                         .format(out_dim, input_shape),
        ""out_dim"": out_dim, ""input_shape"": input_shape}
       for out_dim in [3, 4]
-      for input_shape in [(2, 3), (3, 4)])
+      for input_shape in [(2, 3), (3, 4)]))
   def testDenseShape(self, out_dim, input_shape):
     init_fun, apply_fun = stax.Dense(out_dim)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
-      for input_shape in [(2, 3), (2, 3, 4)])
+      for input_shape in [(2, 3), (2, 3, 4)]))
   def testReluShape(self, input_shape):
     init_fun, apply_fun = stax.Relu
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
                         .format(window_shape, padding, strides, input_shape),
        ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
@@ -95,40 +97,39 @@ class StaxTest(jtu.JaxTestCase):
       for window_shape in [(1, 1), (2, 3)]
       for padding in [""VALID""]
       for strides in [None, (2, 1)]
-      for input_shape in [(2, 5, 6, 1)])
+      for input_shape in [(2, 5, 6, 1)]))
   def testPoolingShape(self, window_shape, padding, strides, input_shape):
     init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
                                        strides=strides)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
-      for input_shape in [(2, 3), (2, 3, 4)])
+      for input_shape in [(2, 3), (2, 3, 4)]))
   def testFlattenShape(self, input_shape):
     init_fun, apply_fun = stax.Flatten
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_input_shape={}_spec={}"".format(input_shape, i),
        ""input_shape"": input_shape, ""spec"": spec}
       for input_shape in [(2, 5, 6, 1)]
       for i, spec in enumerate([
           [stax.Conv(3, (2, 2))],
-          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]]))
+          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]])))
   def testSerialComposeLayersShape(self, input_shape, spec):
     init_fun, apply_fun = stax.serial(*spec)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
-      for input_shape in [(3, 4), (2, 5, 6, 1)])
+      for input_shape in [(3, 4), (2, 5, 6, 1)]))
   def testDropoutShape(self, input_shape):
     init_fun, apply_fun = stax.Dropout(0.9)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
tests/lapax_test.py,tests/lapax_test.py,bb8899f60d5a6afc2b91c307bee5f6b73f037a10,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,Updated existing parameterized tests to subsample,"diff --git a/tests/lapax_test.py b/tests/lapax_test.py
index 444156171..e63fb24b3 100644
--- a/tests/lapax_test.py
+++ b/tests/lapax_test.py
@@ -28,6 +28,7 @@ from jax import test_util as jtu
 from jax.config import config
 from jax.experimental import lapax
 
+config.parse_flags_with_absl()
 
 class LapaxTest(jtu.JaxTestCase):
 
@@ -130,7 +131,7 @@ class LapaxTest(jtu.JaxTestCase):
     check(fun, arr)
     check(fun, arr2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -144,7 +145,7 @@ class LapaxTest(jtu.JaxTestCase):
           ((2, 4, 4), (2, 4, 6) if left else (2, 6, 4)),
       ]
       for dtype in [onp.float32, onp.float64]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSolveTriangular(self, lower, left_side, transpose_a, lhs_shape,
                           rhs_shape, dtype, rng):
     # pylint: disable=invalid-name
@@ -165,7 +166,7 @@ class LapaxTest(jtu.JaxTestCase):
     self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
     # pylint: enable=invalid-name
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -179,7 +180,7 @@ class LapaxTest(jtu.JaxTestCase):
           ((2, 8, 8), (2, 8, 10) if left else (2, 10, 8)),
       ]
       for dtype in [onp.float32, onp.float64]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSolveTriangularBlocked(self, lower, left_side, transpose_a, lhs_shape,
                                  rhs_shape, dtype, rng):
     # pylint: disable=invalid-name
@@ -203,5 +204,4 @@ class LapaxTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
index 444156171..e63fb24b3 100644
--- a/tests/lapax_test.py
+++ b/tests/lapax_test.py
@@ -28,6 +28,7 @@ from jax import test_util as jtu
 from jax.config import config
 from jax.experimental import lapax
 
+config.parse_flags_with_absl()
 
 class LapaxTest(jtu.JaxTestCase):
 
@@ -130,7 +131,7 @@ class LapaxTest(jtu.JaxTestCase):
     check(fun, arr)
     check(fun, arr2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -144,7 +145,7 @@ class LapaxTest(jtu.JaxTestCase):
           ((2, 4, 4), (2, 4, 6) if left else (2, 6, 4)),
       ]
       for dtype in [onp.float32, onp.float64]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSolveTriangular(self, lower, left_side, transpose_a, lhs_shape,
                           rhs_shape, dtype, rng):
     # pylint: disable=invalid-name
@@ -165,7 +166,7 @@ class LapaxTest(jtu.JaxTestCase):
     self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
     # pylint: enable=invalid-name
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -179,7 +180,7 @@ class LapaxTest(jtu.JaxTestCase):
           ((2, 8, 8), (2, 8, 10) if left else (2, 10, 8)),
       ]
       for dtype in [onp.float32, onp.float64]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSolveTriangularBlocked(self, lower, left_side, transpose_a, lhs_shape,
                                  rhs_shape, dtype, rng):
     # pylint: disable=invalid-name
@@ -203,5 +204,4 @@ class LapaxTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,bb8899f60d5a6afc2b91c307bee5f6b73f037a10,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,Updated existing parameterized tests to subsample,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 391630812..6a451d442 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -30,6 +30,7 @@ from jax import numpy as lnp
 from jax import test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
@@ -130,7 +131,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
   def _GetArgsMaker(self, rng, shapes, dtypes):
     return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                     dtypes),
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
@@ -138,13 +139,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                  JAX_COMPOUND_OP_RECORDS)
       for shapes in CombosWithReplacement(all_shapes, rec.nargs)
-      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),
           jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
@@ -154,7 +155,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for rec in JAX_REDUCER_RECORDS
       for shape in all_shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
-      for keepdims in [False, True])
+      for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
     lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
@@ -162,7 +163,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}"".format(
           rec.test_name.capitalize(),
           jtu.format_shape_dtype_string(shape, dtype), axis),
@@ -171,7 +172,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""axis"": axis}
       for rec in JAX_ARGMINMAX_RECORDS
       for shape in all_shapes for dtype in rec.dtypes
-      for axis in range(-len(shape), len(shape)))
+      for axis in range(-len(shape), len(shape))))
   def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
 
     def onp_fun(array_to_reduce):
@@ -184,7 +185,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_{}_{}"".format(
           name,
           jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
@@ -204,13 +205,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (4, 3, 2), (2, 5)),
           (""matrix-tensor"", (5, 2), (3, 2, 4)),
           (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
   def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_{}_{}"".format(
           name,
           jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
@@ -230,20 +231,20 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (5, 2, 3), (3, 2)),
           (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
           (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
   def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                             check_dtypes=True)
     self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_amin={}_amax={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
        ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
        ""rng"": jtu.rand_default()}
       for shape in all_shapes for dtype in float_dtypes
-      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
+      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)]))
   def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
     onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
     lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
@@ -251,13 +252,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_decimals={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), decimals),
        ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
        ""rng"": jtu.rand_default()}
       for shape in all_shapes for dtype in float_dtypes
-      for decimals in [0, 1, -2])
+      for decimals in [0, 1, -2]))
   def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
     onp_fun = lambda x: onp.round(x, decimals=decimals)
     lnp_fun = lambda x: lnp.round(x, decimals=decimals)
@@ -265,7 +266,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
           axis, "","".join(str(d) for d in base_shape),
           "","".join(onp.dtype(dtype).name for dtype in dtypes)),
@@ -274,7 +275,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for num_arrs in [3]
       for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
-      for axis in range(-len(base_shape)+1, len(base_shape)))
+      for axis in range(-len(base_shape)+1, len(base_shape))))
   def testConcatenate(self, axis, base_shape, dtypes, rng):
     wrapped_axis = axis % len(base_shape)
     shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
@@ -288,7 +289,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
        ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
@@ -300,12 +301,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
         [onp.float32, onp.int32, onp.float64],
       ]
       for shape in [(), (2,), (3, 4), (1, 100)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testStack(self, shape, dtypes, rng):
     args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
     self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
           ""_"".join(str(d) for d in shape),
           onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
@@ -313,7 +314,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
       for shape in all_shapes
       for fill_value_dtype in default_dtypes
-      for out_dtype in default_dtypes)
+      for out_dtype in default_dtypes))
   def testFull(self, shape, fill_value_dtype, out_dtype, rng):
     onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
     lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
@@ -321,7 +322,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_axis={}_{}sections"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
        ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
@@ -329,7 +330,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for shape, axis, num_sections in [
           ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
           ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
-      for dtype in default_dtypes)
+      for dtype in default_dtypes))
   def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
     onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
     lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
@@ -337,7 +338,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -350,7 +351,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           ((3, 4), -1),
           ((2, 1, 4), (-1,)),
           ((2, 2, 4), (2, 8))
-      ])
+      ]))
   def testReshape(self, arg_shape, out_shape, dtype, rng):
     onp_fun = lambda x: onp.reshape(x, out_shape)
     lnp_fun = lambda x: lnp.reshape(x, out_shape)
@@ -358,14 +359,14 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_expanddim={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype), dim),
        ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
        ""rng"": jtu.rand_default()}
       for arg_shape in [(), (3,), (3, 4)]
       for dtype in default_dtypes
-      for dim in range(-len(arg_shape)+1, len(arg_shape)))
+      for dim in range(-len(arg_shape)+1, len(arg_shape))))
   def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
     onp_fun = lambda x: onp.expand_dims(x, dim)
     lnp_fun = lambda x: lnp.expand_dims(x, dim)
@@ -373,7 +374,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
        ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
@@ -381,7 +382,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for arg_shape, ax1, ax2 in [
           ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
           ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
-      for dtype in default_dtypes)
+      for dtype in default_dtypes))
   def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
     onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
     lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
@@ -389,7 +390,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype), ax),
        ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
@@ -399,7 +400,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           ((3, 1), 1),
           ((1, 3, 1), (0, 2)),
           ((1, 4, 1), (0,))]
-      for dtype in default_dtypes)
+      for dtype in default_dtypes))
   def testSqueeze(self, arg_shape, dtype, ax, rng):
     onp_fun = lambda x: onp.squeeze(x, ax)
     lnp_fun = lambda x: lnp.squeeze(x, ax)
@@ -407,13 +408,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
       for i, arg in enumerate([
           [1, 2, 3], [1., 2., 3.],
           [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
           [[3, onp.array(2), 1], onp.arange(3.)],
-      ]))
+      ])))
   def testArray(self, arg):
     args_maker = lambda: [arg]
     self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
@@ -550,5 +551,4 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 391630812..6a451d442 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -30,6 +30,7 @@ from jax import numpy as lnp
 from jax import test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
@@ -130,7 +131,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
   def _GetArgsMaker(self, rng, shapes, dtypes):
     return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
                                                     dtypes),
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
@@ -138,13 +139,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                  JAX_COMPOUND_OP_RECORDS)
       for shapes in CombosWithReplacement(all_shapes, rec.nargs)
-      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),
           jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
@@ -154,7 +155,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for rec in JAX_REDUCER_RECORDS
       for shape in all_shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
-      for keepdims in [False, True])
+      for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
     lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
@@ -162,7 +163,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}"".format(
           rec.test_name.capitalize(),
           jtu.format_shape_dtype_string(shape, dtype), axis),
@@ -171,7 +172,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""axis"": axis}
       for rec in JAX_ARGMINMAX_RECORDS
       for shape in all_shapes for dtype in rec.dtypes
-      for axis in range(-len(shape), len(shape)))
+      for axis in range(-len(shape), len(shape))))
   def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
 
     def onp_fun(array_to_reduce):
@@ -184,7 +185,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_{}_{}"".format(
           name,
           jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
@@ -204,13 +205,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (4, 3, 2), (2, 5)),
           (""matrix-tensor"", (5, 2), (3, 2, 4)),
           (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
   def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_{}_{}"".format(
           name,
           jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
@@ -230,20 +231,20 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (5, 2, 3), (3, 2)),
           (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
           (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
   def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
                             check_dtypes=True)
     self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_amin={}_amax={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
        ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
        ""rng"": jtu.rand_default()}
       for shape in all_shapes for dtype in float_dtypes
-      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
+      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)]))
   def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
     onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
     lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
@@ -251,13 +252,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_decimals={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), decimals),
        ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
        ""rng"": jtu.rand_default()}
       for shape in all_shapes for dtype in float_dtypes
-      for decimals in [0, 1, -2])
+      for decimals in [0, 1, -2]))
   def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
     onp_fun = lambda x: onp.round(x, decimals=decimals)
     lnp_fun = lambda x: lnp.round(x, decimals=decimals)
@@ -265,7 +266,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
           axis, "","".join(str(d) for d in base_shape),
           "","".join(onp.dtype(dtype).name for dtype in dtypes)),
@@ -274,7 +275,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for num_arrs in [3]
       for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
-      for axis in range(-len(base_shape)+1, len(base_shape)))
+      for axis in range(-len(base_shape)+1, len(base_shape))))
   def testConcatenate(self, axis, base_shape, dtypes, rng):
     wrapped_axis = axis % len(base_shape)
     shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
@@ -288,7 +289,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
        ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
@@ -300,12 +301,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
         [onp.float32, onp.int32, onp.float64],
       ]
       for shape in [(), (2,), (3, 4), (1, 100)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testStack(self, shape, dtypes, rng):
     args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
     self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
           ""_"".join(str(d) for d in shape),
           onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
@@ -313,7 +314,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
       for shape in all_shapes
       for fill_value_dtype in default_dtypes
-      for out_dtype in default_dtypes)
+      for out_dtype in default_dtypes))
   def testFull(self, shape, fill_value_dtype, out_dtype, rng):
     onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
     lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
@@ -321,7 +322,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_axis={}_{}sections"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
        ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
@@ -329,7 +330,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for shape, axis, num_sections in [
           ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
           ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
-      for dtype in default_dtypes)
+      for dtype in default_dtypes))
   def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
     onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
     lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
@@ -337,7 +338,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -350,7 +351,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           ((3, 4), -1),
           ((2, 1, 4), (-1,)),
           ((2, 2, 4), (2, 8))
-      ])
+      ]))
   def testReshape(self, arg_shape, out_shape, dtype, rng):
     onp_fun = lambda x: onp.reshape(x, out_shape)
     lnp_fun = lambda x: lnp.reshape(x, out_shape)
@@ -358,14 +359,14 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_expanddim={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype), dim),
        ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
        ""rng"": jtu.rand_default()}
       for arg_shape in [(), (3,), (3, 4)]
       for dtype in default_dtypes
-      for dim in range(-len(arg_shape)+1, len(arg_shape)))
+      for dim in range(-len(arg_shape)+1, len(arg_shape))))
   def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
     onp_fun = lambda x: onp.expand_dims(x, dim)
     lnp_fun = lambda x: lnp.expand_dims(x, dim)
@@ -373,7 +374,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
        ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
@@ -381,7 +382,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for arg_shape, ax1, ax2 in [
           ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
           ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
-      for dtype in default_dtypes)
+      for dtype in default_dtypes))
   def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
     onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
     lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
@@ -389,7 +390,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype), ax),
        ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
@@ -399,7 +400,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           ((3, 1), 1),
           ((1, 3, 1), (0, 2)),
           ((1, 4, 1), (0,))]
-      for dtype in default_dtypes)
+      for dtype in default_dtypes))
   def testSqueeze(self, arg_shape, dtype, ax, rng):
     onp_fun = lambda x: onp.squeeze(x, ax)
     lnp_fun = lambda x: lnp.squeeze(x, ax)
@@ -407,13 +408,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
       for i, arg in enumerate([
           [1, 2, 3], [1., 2., 3.],
           [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
           [[3, onp.array(2), 1], onp.arange(3.)],
-      ]))
+      ])))
   def testArray(self, arg):
     args_maker = lambda: [arg]
     self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
@@ -550,5 +551,4 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,bb8899f60d5a6afc2b91c307bee5f6b73f037a10,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,Updated existing parameterized tests to subsample,"diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 53c1e688f..dd658799e 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -35,6 +35,7 @@ from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
 from jax.scipy import stats as lsp_stats
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
@@ -72,14 +73,14 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
   def _GetArgsMaker(self, rng, shapes, dtypes):
     return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_axis={}_keepdims={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
        ""rng"": jtu.rand_default(), ""shape"": shape, ""dtype"": dtype,
        ""axis"": axis, ""keepdims"": keepdims}
       for shape in all_shapes for dtype in float_dtypes
       for axis in range(-len(shape), len(shape))
-      for keepdims in [False, True])
+      for keepdims in [False, True]))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
     # TODO(mattjj): test autodiff
@@ -93,7 +94,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.test_name, shapes, dtypes),
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
@@ -102,7 +103,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
        ""lax_op"": getattr(lsp_special, rec.name)}
       for rec in JAX_SPECIAL_FUNCTION_RECORDS
       for shapes in CombosWithReplacement(all_shapes, rec.nargs)
-      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
     # TODO(mattjj): unskip this test combination when real() on tpu is improved
     # TODO(mattjj): test autodiff
@@ -116,13 +117,13 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           """", shapes, dtypes),
        ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
       for shapes in CombosWithReplacement(all_shapes, 3)
       for dtypes in CombosWithReplacement(default_dtypes, 3)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
     # TODO(mattjj): test autodiff
@@ -135,13 +136,13 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           """", shapes, dtypes),
        ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
       for shapes in CombosWithReplacement(all_shapes, 2)
       for dtypes in CombosWithReplacement(default_dtypes, 2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
     # TODO(mattjj): test autodiff
     scale = 0.5
@@ -154,5 +155,4 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 53c1e688f..dd658799e 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -35,6 +35,7 @@ from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
 from jax.scipy import stats as lsp_stats
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
@@ -72,14 +73,14 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
   def _GetArgsMaker(self, rng, shapes, dtypes):
     return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_axis={}_keepdims={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
        ""rng"": jtu.rand_default(), ""shape"": shape, ""dtype"": dtype,
        ""axis"": axis, ""keepdims"": keepdims}
       for shape in all_shapes for dtype in float_dtypes
       for axis in range(-len(shape), len(shape))
-      for keepdims in [False, True])
+      for keepdims in [False, True]))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
     # TODO(mattjj): test autodiff
@@ -93,7 +94,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.test_name, shapes, dtypes),
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
@@ -102,7 +103,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
        ""lax_op"": getattr(lsp_special, rec.name)}
       for rec in JAX_SPECIAL_FUNCTION_RECORDS
       for shapes in CombosWithReplacement(all_shapes, rec.nargs)
-      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
     # TODO(mattjj): unskip this test combination when real() on tpu is improved
     # TODO(mattjj): test autodiff
@@ -116,13 +117,13 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           """", shapes, dtypes),
        ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
       for shapes in CombosWithReplacement(all_shapes, 3)
       for dtypes in CombosWithReplacement(default_dtypes, 3)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
     # TODO(mattjj): test autodiff
@@ -135,13 +136,13 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           """", shapes, dtypes),
        ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
       for shapes in CombosWithReplacement(all_shapes, 2)
       for dtypes in CombosWithReplacement(default_dtypes, 2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
     # TODO(mattjj): test autodiff
     scale = 0.5
@@ -154,5 +155,4 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
tests/lax_test.py,tests/lax_test.py,bb8899f60d5a6afc2b91c307bee5f6b73f037a10,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,Updated existing parameterized tests to subsample,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index f1c3d8580..561ebc0cb 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -36,6 +36,7 @@ from jax.config import config
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 
@@ -136,19 +137,19 @@ CombosWithReplacement = itertools.combinations_with_replacement
 class LaxTest(jtu.JaxTestCase):
   """"""Numerical tests for LAX operations.""""""
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.op.__name__, shapes, itertools.repeat(dtype)),
        ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
       for rec in LAX_OPS
       for shape_group in compatible_shapes
       for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes)
+      for dtype in rec.dtypes))
   def testOp(self, op, rng, shapes, dtype):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.op.__name__, shapes, itertools.repeat(dtype)),
        ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
@@ -156,7 +157,7 @@ class LaxTest(jtu.JaxTestCase):
       for rec in LAX_OPS
       for shape_group in compatible_shapes
       for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes)
+      for dtype in rec.dtypes))
   def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     numpy_op = getattr(lax_reference, op.__name__)
@@ -164,57 +165,57 @@ class LaxTest(jtu.JaxTestCase):
 
   # TODO test shift_left, shift_right_arithmetic, shift_right_logical
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
           from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConvertElementType(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.convert_element_type(x, to_dtype)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}""
        .format(from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.convert_element_type(x, to_dtype)
     numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}""
        .format(from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBitcastConvertType(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.bitcast_convert_type(x, to_dtype)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}""
        .format(from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.bitcast_convert_type(x, to_dtype)
     numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
           jtu.format_shape_dtype_string(min_shape, dtype),
           jtu.format_shape_dtype_string(operand_shape, dtype),
@@ -228,13 +229,13 @@ class LaxTest(jtu.JaxTestCase):
           [(2, 3), (2, 3), (2, 3)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
     shapes = [min_shape, operand_shape, max_shape]
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
           jtu.format_shape_dtype_string(min_shape, dtype),
           jtu.format_shape_dtype_string(operand_shape, dtype),
@@ -248,14 +249,14 @@ class LaxTest(jtu.JaxTestCase):
           [(2, 3), (2, 3), (2, 3)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
                             rng):
     shapes = [min_shape, operand_shape, max_shape]
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
           dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
           num_arrs),
@@ -265,7 +266,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in default_dtypes
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
       for dim in range(len(base_shape))
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
     shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
               for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
@@ -273,7 +274,7 @@ class LaxTest(jtu.JaxTestCase):
     op = lambda *args: lax.concatenate(args, dim)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
           dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
           num_arrs),
@@ -283,7 +284,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in default_dtypes
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
       for dim in range(len(base_shape))
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
     shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
               for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
@@ -292,7 +293,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda *args: lax_reference.concatenate(args, dim)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -305,7 +306,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in [onp.float32]
       for strides in [(1, 1), (1, 2), (2, 1)]
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
 
@@ -314,7 +315,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -327,7 +328,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in [onp.float32]
       for strides in [(1, 1), (1, 2), (2, 1)]
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
                            rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -335,7 +336,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -351,7 +352,7 @@ class LaxTest(jtu.JaxTestCase):
       for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
       for lhs_dilation, rhs_dilation in itertools.product(
           [(1, 1), (1, 2), (2, 2)], repeat=2)
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
                                  padding, lhs_dilation, rhs_dilation, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -362,7 +363,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -378,7 +379,7 @@ class LaxTest(jtu.JaxTestCase):
       for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
       for lhs_dilation, rhs_dilation in itertools.product(
           [(1, 1), (1, 2), (2, 2)], repeat=2)
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
       self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
       rhs_dilation, rng):
@@ -395,7 +396,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CheckAgainstNumpy(fun, numpy_fun, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}""
        ""_dims={}"".format(
@@ -418,7 +419,7 @@ class LaxTest(jtu.JaxTestCase):
       for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
                               ([0, 1, 2, 3], [0, 1, 2, 3])),
                              ((""NHWC"", ""HWIO"", ""NHWC""),
-                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
+                              ([0, 2, 3, 1], [2, 3, 1, 0]))]))
   def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
                              padding, lhs_dilation, rhs_dilation,
                              dimension_numbers, perms, rng):
@@ -437,7 +438,7 @@ class LaxTest(jtu.JaxTestCase):
 
   # TODO(mattjj): test conv_general_dilated against numpy
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype)),
@@ -445,12 +446,12 @@ class LaxTest(jtu.JaxTestCase):
        ""rng"": rng}
       for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDot(self, lhs_shape, rhs_shape, dtype, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
     self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype)),
@@ -458,12 +459,12 @@ class LaxTest(jtu.JaxTestCase):
        ""rng"": rng}
       for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
     self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -483,7 +484,7 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 2), (2, 4), [1], [0]],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
                                  lhs_contracting, rhs_contracting, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -494,7 +495,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -507,7 +508,7 @@ class LaxTest(jtu.JaxTestCase):
           ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
                                      dimension_numbers, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -517,7 +518,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -530,7 +531,7 @@ class LaxTest(jtu.JaxTestCase):
           ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
                                  dimension_numbers, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -538,7 +539,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
           shape, onp.dtype(dtype).name, broadcast_sizes),
        ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
@@ -546,13 +547,13 @@ class LaxTest(jtu.JaxTestCase):
       for shape in [(), (2, 3)]
       for dtype in default_dtypes
       for broadcast_sizes in [(), (2,), (1, 2)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.broadcast(x, broadcast_sizes)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
        ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
@@ -560,14 +561,14 @@ class LaxTest(jtu.JaxTestCase):
       for shape in [(), (2, 3)]
       for dtype in default_dtypes
       for broadcast_sizes in [(), (2,), (1, 2)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.broadcast(x, broadcast_sizes)
     numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
           jtu.format_shape_dtype_string(inshape, dtype),
           outshape, broadcast_dimensions),
@@ -580,13 +581,13 @@ class LaxTest(jtu.JaxTestCase):
           ([], [2, 3], []),
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
     args_maker = lambda: [rng(inshape, dtype)]
     op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
           jtu.format_shape_dtype_string(inshape, dtype),
           outshape, broadcast_dimensions),
@@ -599,7 +600,7 @@ class LaxTest(jtu.JaxTestCase):
           ([], [2, 3], []),
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
                                      dimensions, rng):
     args_maker = lambda: [rng(inshape, dtype)]
@@ -607,7 +608,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -617,13 +618,13 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape, out_shape in [
           [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReshape(self, arg_shape, out_shape, dtype, rng):
     args_maker = lambda: [rng(arg_shape, dtype)]
     op = lambda x: lax.reshape(x, out_shape)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -633,32 +634,32 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape, out_shape in [
           [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
     args_maker = lambda: [rng(arg_shape, dtype)]
     op = lambda x: lax.reshape(x, out_shape)
     numpy_op = lambda x: lax_reference.reshape(x, out_shape)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_pads={}""
        .format(jtu.format_shape_dtype_string(shape, dtype), pads),
        ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
       for shape in [(2, 3)]
       for dtype in default_dtypes
-      for pads in [[(1, 2, 1), (0, 1, 0)]])
+      for pads in [[(1, 2, 1), (0, 1, 0)]]))
   def testPad(self, shape, dtype, pads, rng):
     args_maker = lambda: [rng(shape, dtype)]
     fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_pads={}""
        .format(jtu.format_shape_dtype_string(shape, dtype), pads),
        ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
       for shape in [(2, 3)]
       for dtype in default_dtypes
-      for pads in [[(1, 2, 1), (0, 1, 0)]])
+      for pads in [[(1, 2, 1), (0, 1, 0)]]))
   def testPadAgainstNumpy(self, shape, dtype, pads, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
@@ -677,7 +678,7 @@ class LaxTest(jtu.JaxTestCase):
                         rev(onp.array([[1, 2, 3], [4, 5, 6]])),
                         check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_predshape={}_argshapes={}"".format(
           jtu.format_shape_dtype_string(pred_shape, onp.bool_),
           jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
@@ -686,7 +687,7 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape in [(), (3,), (2, 3)]
       for pred_shape in ([(), arg_shape] if arg_shape else [()])
       for arg_dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):
 
     def args_maker():
@@ -695,7 +696,7 @@ class LaxTest(jtu.JaxTestCase):
 
     return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_predshape={}_argshapes={}"".format(
           jtu.format_shape_dtype_string(pred_shape, onp.bool_),
           jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
@@ -704,7 +705,7 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape in [(), (3,), (2, 3)]
       for pred_shape in ([(), arg_shape] if arg_shape else [()])
       for arg_dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):
 
     def args_maker():
@@ -713,7 +714,7 @@ class LaxTest(jtu.JaxTestCase):
 
     return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
@@ -732,13 +733,13 @@ class LaxTest(jtu.JaxTestCase):
         [(5, 3), (1, 1), (5, 3), (2, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSlice(self, shape, dtype, starts, limits, strides, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.slice(x, starts, limits, strides)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
@@ -757,7 +758,7 @@ class LaxTest(jtu.JaxTestCase):
         [(5, 3), (1, 1), (5, 3), (2, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
                             strides, rng):
     args_maker = lambda: [rng(shape, dtype)]
@@ -765,7 +766,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, size_indices),
@@ -777,13 +778,13 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
     args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
     op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, size_indices),
@@ -795,7 +796,7 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
                                    size_indices, rng):
     args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
@@ -803,7 +804,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, update_shape),
@@ -815,7 +816,7 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
                              rng):
 
@@ -826,7 +827,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
                           check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, update_shape),
@@ -838,7 +839,7 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, start_indices,
                                          update_shape, rng):
 
@@ -849,7 +850,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CheckAgainstNumpy(lax.dynamic_update_slice,
                             lax_reference.dynamic_update_slice, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
        ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
@@ -860,13 +861,13 @@ class LaxTest(jtu.JaxTestCase):
         [(3, 4, 5), (1, 0, 2)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testTranspose(self, shape, dtype, perm, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.transpose(x, perm)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
        ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
@@ -877,14 +878,14 @@ class LaxTest(jtu.JaxTestCase):
         [(3, 4, 5), (1, 0, 2)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.transpose(x, perm)
     numpy_op = lambda x: lax_reference.transpose(x, perm)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
        .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
        ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
@@ -907,7 +908,7 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
           [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
       ]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testReduce(self, op, init_val, shape, dtype, dims, rng):
     init_val = onp.asarray(init_val, dtype=dtype)
     fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
@@ -920,7 +921,7 @@ class LaxTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype)]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_dtype={}_padding={}""
        .format(op.__name__, onp.dtype(dtype).name, padding),
        ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
@@ -932,7 +933,7 @@ class LaxTest(jtu.JaxTestCase):
       ]
       for dtype in dtypes
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testReduceWindow(self, op, init_val, dtype, padding, rng):
     init_val = onp.asarray(init_val, dtype=dtype)
 
@@ -972,14 +973,14 @@ class LaxTest(jtu.JaxTestCase):
     # pylint: enable=cell-var-from-loop
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis),
        ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
       for dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(5,), (5, 7)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSort(self, shape, dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort only implemented for R1 on non-TPU backends""
@@ -990,14 +991,14 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis),
        ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
       for dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(5,), (5, 7)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortAgainstNumpy(self, shape, dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort only implemented for R1 on non-TPU backends""
@@ -1009,7 +1010,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, key_dtype),
           jtu.format_shape_dtype_string(shape, val_dtype),
@@ -1020,7 +1021,7 @@ class LaxTest(jtu.JaxTestCase):
       for val_dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(3,), (5, 3)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort_key_val only implemented for R1 non-TPU backends""
@@ -1040,7 +1041,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, key_dtype),
           jtu.format_shape_dtype_string(shape, val_dtype),
@@ -1051,7 +1052,7 @@ class LaxTest(jtu.JaxTestCase):
       for val_dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(3,), (5, 3)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort_key_val only implemented for R1 non-TPU backends""
@@ -1275,7 +1276,7 @@ class LaxTest(jtu.JaxTestCase):
       self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
       self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
                jtu.format_shape_dtype_string(rhs_shape, dtype)),
@@ -1285,7 +1286,7 @@ class LaxTest(jtu.JaxTestCase):
                                    ((5, 3, 2), (5, 2, 4)),
                                    ((1, 2, 2, 3), (1, 2, 3, 1))]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
     arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
     self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)
@@ -1301,7 +1302,7 @@ class LaxTest(jtu.JaxTestCase):
     self.assertEqual((2, 3, 4),
                      collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
        ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
@@ -1312,14 +1313,14 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexTake(self, shape, dtype, idxs, axes, rng):
     rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
     args_maker = lambda: [rng(shape, dtype), rand_idxs()]
     fun = lambda src, idxs: lax.index_take(src, idxs, axes)
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
        ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
@@ -1331,7 +1332,7 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
     # We call lax.index_take to get the shapes right
     src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
@@ -1429,7 +1430,7 @@ def check_grads_bilinear(f, args, order, atol=None, rtol=None):
 
 class LaxAutodiffTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.op.__name__, shapes, itertools.repeat(dtype)),
        ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
@@ -1438,7 +1439,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for shape_group in compatible_shapes
       for shapes in CombosWithReplacement(shape_group, rec.nargs)
       for dtype in rec.dtypes
-  )
+  ))
   def testOpGrad(self, op, rng, shapes, dtype, order):
     if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
       if dtype is onp.complex64:
@@ -1449,19 +1450,19 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     args = tuple(rng(shape, dtype) for shape in shapes)
     check_grads(op, args, order, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
           from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.float64], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
     args = (rng((2, 3), from_dtype),)
     convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
     check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
           jtu.format_shape_dtype_string(min_shape, dtype),
           jtu.format_shape_dtype_string(operand_shape, dtype),
@@ -1474,7 +1475,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(2, 3), (2, 3), (2, 3)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     shapes = [min_shape, operand_shape, max_shape]
@@ -1482,7 +1483,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
     check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
           dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
           num_arrs),
@@ -1492,7 +1493,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for dtype in float_dtypes
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
       for dim in range(len(base_shape))
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
@@ -1501,7 +1502,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     concatenate = lambda *args: lax.concatenate(args, dim)
     check_grads(concatenate, operands, 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -1516,7 +1517,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        for strides in all_strides
        for dtype in [onp.float32]
        for padding in [""VALID"", ""SAME""]
-       for rng in [jtu.rand_small()])
+       for rng in [jtu.rand_small()]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
     lhs = rng(lhs_shape, dtype)
@@ -1524,7 +1525,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     conv = partial(lax.conv, window_strides=strides, padding=padding)
     check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
        ""rhs_dilation={}""
@@ -1547,7 +1548,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        for lhs_dil in lhs_dils
        for dtype in [onp.float32]
        for padding in all_pads
-       for rng in [jtu.rand_small()])
+       for rng in [jtu.rand_small()]))
   @jtu.skip_on_devices(""tpu"")
   def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                      padding, lhs_dil, rhs_dil, rng):
@@ -1557,7 +1558,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
                    padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
     check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
        ""rhs_dilation={}_dims={}""
@@ -1581,7 +1582,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]
       for dim_nums, perms in [
           ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
-          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                  padding, lhs_dil, rhs_dil, dimension_numbers,
@@ -1595,14 +1596,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
                    dimension_numbers=dimension_numbers)
     check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype)),
        ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
        ""rng"": jtu.rand_default()}
       for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
-      for dtype in float_dtypes)
+      for dtype in float_dtypes))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
     tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
@@ -1610,7 +1611,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     rhs = rng(rhs_shape, dtype)
     check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -1624,7 +1625,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           ((5, 3), (5, 2), (([0], [0]), ([], []))),
           ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
       ]
-      for dtype in float_dtypes)
+      for dtype in float_dtypes))
   @jtu.skip_on_devices(""tpu"")
   def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
                                           dimension_numbers, rng):
@@ -1634,7 +1635,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
     check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
           shape, onp.dtype(dtype).name, broadcast_sizes),
        ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
@@ -1642,14 +1643,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for shape in [(), (2, 3)]
       for dtype in float_dtypes
       for broadcast_sizes in [(), (2,), (1, 2)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     args = (rng(shape, dtype),)
     broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
     check_grads(broadcast, args, 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
           jtu.format_shape_dtype_string(inshape, dtype),
           outshape, broadcast_dimensions),
@@ -1662,14 +1663,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           ([], [2, 3], []),
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(inshape, dtype)
     broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
     check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -1679,20 +1680,20 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for arg_shape, out_shape in [
           [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(arg_shape, dtype)
     reshape = lambda x: lax.reshape(x, out_shape)
     check_grads(reshape, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_pads={}""
        .format(jtu.format_shape_dtype_string(shape, dtype), pads),
        ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
       for shape in [(2, 3)]
       for dtype in float_dtypes
-      for pads in [[(1, 2, 1), (0, 1, 0)]])
+      for pads in [[(1, 2, 1), (0, 1, 0)]]))
   def testPadGrad(self, shape, dtype, pads, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
 
@@ -1714,7 +1715,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dimensions = [0, 1]
     check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_predshape={}_argshapes={}"".format(
           jtu.format_shape_dtype_string(pred_shape, onp.bool_),
           jtu.format_shape_dtype_string(arg_shape, dtype)),
@@ -1723,7 +1724,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for arg_shape in [(), (3,), (2, 3)]
       for pred_shape in ([(), arg_shape] if arg_shape else [()])
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     pred = rng(pred_shape, onp.bool_)
@@ -1732,7 +1733,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
     check_grads(select, (on_true, on_false), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
@@ -1751,14 +1752,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(5, 3), (1, 1), (5, 3), (2, 1)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
     slice = lambda x: lax.slice(x, starts, limits, strides)
     check_grads(slice, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, size_indices),
@@ -1770,7 +1771,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
                            rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -1778,7 +1779,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
     check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, update_shape),
@@ -1790,7 +1791,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
                                  update_shape, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -1807,7 +1808,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)
     check_grads(dus, (update,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
        ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
@@ -1818,14 +1819,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(3, 4, 5), (1, 0, 2)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testTransposeGrad(self, shape, dtype, perm, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
     transpose = lambda x: lax.transpose(x, perm)
     check_grads(transpose, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
        .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
        ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
@@ -1842,7 +1843,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(3, 4, 5), (0, 2)],
           [(3, 4, 5), (0, 1, 2)]
       ]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
@@ -1850,7 +1851,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
     check_grads(reduce, (operand,), 1, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_dtype={}_padding={}""
        .format(op.__name__, onp.dtype(dtype).name, padding),
        ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
@@ -1862,7 +1863,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       ]
       for dtype in dtypes
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
     init_val = onp.asarray(init_val, dtype=dtype)
 
@@ -1901,14 +1902,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     # pylint: enable=cell-var-from-loop
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis),
        ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
       for dtype in [onp.float32]
       for shape in [(5,), (5, 7)]
       for axis in [len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortGrad(self, shape, dtype, axis, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
@@ -1916,7 +1917,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     check_grads(sort, (operand,), 2, tol, tol, tol)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, key_dtype),
           jtu.format_shape_dtype_string(shape, val_dtype),
@@ -1927,7 +1928,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for val_dtype in [onp.float32]
       for shape in [(3,), (5, 3)]
       for axis in [len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
     # This test relies on the property that wherever keys are tied, values are
     # too, since we don't guarantee the same ordering of values with equal keys.
@@ -1943,7 +1944,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
     check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
        ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
@@ -1954,14 +1955,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
     idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
     src = rng(shape, dtype)
     index_take = lambda src: lax.index_take(src, idxs, axes)
     check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
        ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
@@ -1973,7 +1974,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
     # We call lax.index_take to get the shapes right
     src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
@@ -1986,5 +1987,4 @@ class LaxAutodiffTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/lax_test.py b/tests/lax_test.py
index f1c3d8580..561ebc0cb 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -36,6 +36,7 @@ from jax.config import config
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 
@@ -136,19 +137,19 @@ CombosWithReplacement = itertools.combinations_with_replacement
 class LaxTest(jtu.JaxTestCase):
   """"""Numerical tests for LAX operations.""""""
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.op.__name__, shapes, itertools.repeat(dtype)),
        ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
       for rec in LAX_OPS
       for shape_group in compatible_shapes
       for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes)
+      for dtype in rec.dtypes))
   def testOp(self, op, rng, shapes, dtype):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.op.__name__, shapes, itertools.repeat(dtype)),
        ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
@@ -156,7 +157,7 @@ class LaxTest(jtu.JaxTestCase):
       for rec in LAX_OPS
       for shape_group in compatible_shapes
       for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes)
+      for dtype in rec.dtypes))
   def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     numpy_op = getattr(lax_reference, op.__name__)
@@ -164,57 +165,57 @@ class LaxTest(jtu.JaxTestCase):
 
   # TODO test shift_left, shift_right_arithmetic, shift_right_logical
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
           from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConvertElementType(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.convert_element_type(x, to_dtype)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}""
        .format(from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.convert_element_type(x, to_dtype)
     numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}""
        .format(from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBitcastConvertType(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.bitcast_convert_type(x, to_dtype)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}""
        .format(from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
     args_maker = lambda: [rng((2, 3), from_dtype)]
     op = lambda x: lax.bitcast_convert_type(x, to_dtype)
     numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
           jtu.format_shape_dtype_string(min_shape, dtype),
           jtu.format_shape_dtype_string(operand_shape, dtype),
@@ -228,13 +229,13 @@ class LaxTest(jtu.JaxTestCase):
           [(2, 3), (2, 3), (2, 3)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
     shapes = [min_shape, operand_shape, max_shape]
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
           jtu.format_shape_dtype_string(min_shape, dtype),
           jtu.format_shape_dtype_string(operand_shape, dtype),
@@ -248,14 +249,14 @@ class LaxTest(jtu.JaxTestCase):
           [(2, 3), (2, 3), (2, 3)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
                             rng):
     shapes = [min_shape, operand_shape, max_shape]
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
           dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
           num_arrs),
@@ -265,7 +266,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in default_dtypes
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
       for dim in range(len(base_shape))
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
     shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
               for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
@@ -273,7 +274,7 @@ class LaxTest(jtu.JaxTestCase):
     op = lambda *args: lax.concatenate(args, dim)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
           dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
           num_arrs),
@@ -283,7 +284,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in default_dtypes
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
       for dim in range(len(base_shape))
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
     shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
               for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
@@ -292,7 +293,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda *args: lax_reference.concatenate(args, dim)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -305,7 +306,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in [onp.float32]
       for strides in [(1, 1), (1, 2), (2, 1)]
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
 
@@ -314,7 +315,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -327,7 +328,7 @@ class LaxTest(jtu.JaxTestCase):
       for dtype in [onp.float32]
       for strides in [(1, 1), (1, 2), (2, 1)]
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
                            rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -335,7 +336,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -351,7 +352,7 @@ class LaxTest(jtu.JaxTestCase):
       for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
       for lhs_dilation, rhs_dilation in itertools.product(
           [(1, 1), (1, 2), (2, 2)], repeat=2)
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
                                  padding, lhs_dilation, rhs_dilation, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -362,7 +363,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}"".format(
            jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -378,7 +379,7 @@ class LaxTest(jtu.JaxTestCase):
       for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
       for lhs_dilation, rhs_dilation in itertools.product(
           [(1, 1), (1, 2), (2, 2)], repeat=2)
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
       self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
       rhs_dilation, rng):
@@ -395,7 +396,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CheckAgainstNumpy(fun, numpy_fun, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}""
        ""_dims={}"".format(
@@ -418,7 +419,7 @@ class LaxTest(jtu.JaxTestCase):
       for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
                               ([0, 1, 2, 3], [0, 1, 2, 3])),
                              ((""NHWC"", ""HWIO"", ""NHWC""),
-                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
+                              ([0, 2, 3, 1], [2, 3, 1, 0]))]))
   def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
                              padding, lhs_dilation, rhs_dilation,
                              dimension_numbers, perms, rng):
@@ -437,7 +438,7 @@ class LaxTest(jtu.JaxTestCase):
 
   # TODO(mattjj): test conv_general_dilated against numpy
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype)),
@@ -445,12 +446,12 @@ class LaxTest(jtu.JaxTestCase):
        ""rng"": rng}
       for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDot(self, lhs_shape, rhs_shape, dtype, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
     self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype)),
@@ -458,12 +459,12 @@ class LaxTest(jtu.JaxTestCase):
        ""rng"": rng}
       for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
     self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -483,7 +484,7 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 2), (2, 4), [1], [0]],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
                                  lhs_contracting, rhs_contracting, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -494,7 +495,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -507,7 +508,7 @@ class LaxTest(jtu.JaxTestCase):
           ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
                                      dimension_numbers, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -517,7 +518,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -530,7 +531,7 @@ class LaxTest(jtu.JaxTestCase):
           ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
                                  dimension_numbers, rng):
     args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
@@ -538,7 +539,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
           shape, onp.dtype(dtype).name, broadcast_sizes),
        ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
@@ -546,13 +547,13 @@ class LaxTest(jtu.JaxTestCase):
       for shape in [(), (2, 3)]
       for dtype in default_dtypes
       for broadcast_sizes in [(), (2,), (1, 2)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.broadcast(x, broadcast_sizes)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
        ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
@@ -560,14 +561,14 @@ class LaxTest(jtu.JaxTestCase):
       for shape in [(), (2, 3)]
       for dtype in default_dtypes
       for broadcast_sizes in [(), (2,), (1, 2)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.broadcast(x, broadcast_sizes)
     numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
           jtu.format_shape_dtype_string(inshape, dtype),
           outshape, broadcast_dimensions),
@@ -580,13 +581,13 @@ class LaxTest(jtu.JaxTestCase):
           ([], [2, 3], []),
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
     args_maker = lambda: [rng(inshape, dtype)]
     op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
           jtu.format_shape_dtype_string(inshape, dtype),
           outshape, broadcast_dimensions),
@@ -599,7 +600,7 @@ class LaxTest(jtu.JaxTestCase):
           ([], [2, 3], []),
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
                                      dimensions, rng):
     args_maker = lambda: [rng(inshape, dtype)]
@@ -607,7 +608,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -617,13 +618,13 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape, out_shape in [
           [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReshape(self, arg_shape, out_shape, dtype, rng):
     args_maker = lambda: [rng(arg_shape, dtype)]
     op = lambda x: lax.reshape(x, out_shape)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -633,32 +634,32 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape, out_shape in [
           [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
     args_maker = lambda: [rng(arg_shape, dtype)]
     op = lambda x: lax.reshape(x, out_shape)
     numpy_op = lambda x: lax_reference.reshape(x, out_shape)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_pads={}""
        .format(jtu.format_shape_dtype_string(shape, dtype), pads),
        ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
       for shape in [(2, 3)]
       for dtype in default_dtypes
-      for pads in [[(1, 2, 1), (0, 1, 0)]])
+      for pads in [[(1, 2, 1), (0, 1, 0)]]))
   def testPad(self, shape, dtype, pads, rng):
     args_maker = lambda: [rng(shape, dtype)]
     fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_pads={}""
        .format(jtu.format_shape_dtype_string(shape, dtype), pads),
        ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
       for shape in [(2, 3)]
       for dtype in default_dtypes
-      for pads in [[(1, 2, 1), (0, 1, 0)]])
+      for pads in [[(1, 2, 1), (0, 1, 0)]]))
   def testPadAgainstNumpy(self, shape, dtype, pads, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
@@ -677,7 +678,7 @@ class LaxTest(jtu.JaxTestCase):
                         rev(onp.array([[1, 2, 3], [4, 5, 6]])),
                         check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_predshape={}_argshapes={}"".format(
           jtu.format_shape_dtype_string(pred_shape, onp.bool_),
           jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
@@ -686,7 +687,7 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape in [(), (3,), (2, 3)]
       for pred_shape in ([(), arg_shape] if arg_shape else [()])
       for arg_dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):
 
     def args_maker():
@@ -695,7 +696,7 @@ class LaxTest(jtu.JaxTestCase):
 
     return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_predshape={}_argshapes={}"".format(
           jtu.format_shape_dtype_string(pred_shape, onp.bool_),
           jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
@@ -704,7 +705,7 @@ class LaxTest(jtu.JaxTestCase):
       for arg_shape in [(), (3,), (2, 3)]
       for pred_shape in ([(), arg_shape] if arg_shape else [()])
       for arg_dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):
 
     def args_maker():
@@ -713,7 +714,7 @@ class LaxTest(jtu.JaxTestCase):
 
     return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
@@ -732,13 +733,13 @@ class LaxTest(jtu.JaxTestCase):
         [(5, 3), (1, 1), (5, 3), (2, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSlice(self, shape, dtype, starts, limits, strides, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.slice(x, starts, limits, strides)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
@@ -757,7 +758,7 @@ class LaxTest(jtu.JaxTestCase):
         [(5, 3), (1, 1), (5, 3), (2, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
                             strides, rng):
     args_maker = lambda: [rng(shape, dtype)]
@@ -765,7 +766,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, size_indices),
@@ -777,13 +778,13 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
     args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
     op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, size_indices),
@@ -795,7 +796,7 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
                                    size_indices, rng):
     args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
@@ -803,7 +804,7 @@ class LaxTest(jtu.JaxTestCase):
     numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, update_shape),
@@ -815,7 +816,7 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
                              rng):
 
@@ -826,7 +827,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
                           check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, update_shape),
@@ -838,7 +839,7 @@ class LaxTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicUpdateSliceAgainstNumpy(self, shape, dtype, start_indices,
                                          update_shape, rng):
 
@@ -849,7 +850,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CheckAgainstNumpy(lax.dynamic_update_slice,
                             lax_reference.dynamic_update_slice, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
        ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
@@ -860,13 +861,13 @@ class LaxTest(jtu.JaxTestCase):
         [(3, 4, 5), (1, 0, 2)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testTranspose(self, shape, dtype, perm, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.transpose(x, perm)
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
        ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
@@ -877,14 +878,14 @@ class LaxTest(jtu.JaxTestCase):
         [(3, 4, 5), (1, 0, 2)],
       ]
       for dtype in default_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
     args_maker = lambda: [rng(shape, dtype)]
     op = lambda x: lax.transpose(x, perm)
     numpy_op = lambda x: lax_reference.transpose(x, perm)
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
        .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
        ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
@@ -907,7 +908,7 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
           [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
       ]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testReduce(self, op, init_val, shape, dtype, dims, rng):
     init_val = onp.asarray(init_val, dtype=dtype)
     fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
@@ -920,7 +921,7 @@ class LaxTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype)]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_dtype={}_padding={}""
        .format(op.__name__, onp.dtype(dtype).name, padding),
        ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
@@ -932,7 +933,7 @@ class LaxTest(jtu.JaxTestCase):
       ]
       for dtype in dtypes
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testReduceWindow(self, op, init_val, dtype, padding, rng):
     init_val = onp.asarray(init_val, dtype=dtype)
 
@@ -972,14 +973,14 @@ class LaxTest(jtu.JaxTestCase):
     # pylint: enable=cell-var-from-loop
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis),
        ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
       for dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(5,), (5, 7)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSort(self, shape, dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort only implemented for R1 on non-TPU backends""
@@ -990,14 +991,14 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis),
        ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
       for dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(5,), (5, 7)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortAgainstNumpy(self, shape, dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort only implemented for R1 on non-TPU backends""
@@ -1009,7 +1010,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CheckAgainstNumpy(op, numpy_op, args_maker)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, key_dtype),
           jtu.format_shape_dtype_string(shape, val_dtype),
@@ -1020,7 +1021,7 @@ class LaxTest(jtu.JaxTestCase):
       for val_dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(3,), (5, 3)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort_key_val only implemented for R1 non-TPU backends""
@@ -1040,7 +1041,7 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, key_dtype),
           jtu.format_shape_dtype_string(shape, val_dtype),
@@ -1051,7 +1052,7 @@ class LaxTest(jtu.JaxTestCase):
       for val_dtype in [onp.float32, onp.int32, onp.uint32]
       for shape in [(3,), (5, 3)]
       for axis in [-1, len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
     if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
       msg = ""sort_key_val only implemented for R1 non-TPU backends""
@@ -1275,7 +1276,7 @@ class LaxTest(jtu.JaxTestCase):
       self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
       self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
                jtu.format_shape_dtype_string(rhs_shape, dtype)),
@@ -1285,7 +1286,7 @@ class LaxTest(jtu.JaxTestCase):
                                    ((5, 3, 2), (5, 2, 4)),
                                    ((1, 2, 2, 3), (1, 2, 3, 1))]
       for dtype in float_dtypes
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
     arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
     self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)
@@ -1301,7 +1302,7 @@ class LaxTest(jtu.JaxTestCase):
     self.assertEqual((2, 3, 4),
                      collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
        ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
@@ -1312,14 +1313,14 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexTake(self, shape, dtype, idxs, axes, rng):
     rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
     args_maker = lambda: [rng(shape, dtype), rand_idxs()]
     fun = lambda src, idxs: lax.index_take(src, idxs, axes)
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
        ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
@@ -1331,7 +1332,7 @@ class LaxTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
     # We call lax.index_take to get the shapes right
     src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
@@ -1429,7 +1430,7 @@ def check_grads_bilinear(f, args, order, atol=None, rtol=None):
 
 class LaxAutodiffTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": jtu.format_test_name_suffix(
           rec.op.__name__, shapes, itertools.repeat(dtype)),
        ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
@@ -1438,7 +1439,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for shape_group in compatible_shapes
       for shapes in CombosWithReplacement(shape_group, rec.nargs)
       for dtype in rec.dtypes
-  )
+  ))
   def testOpGrad(self, op, rng, shapes, dtype, order):
     if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
       if dtype is onp.complex64:
@@ -1449,19 +1450,19 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     args = tuple(rng(shape, dtype) for shape in shapes)
     check_grads(op, args, order, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
           from_dtype, to_dtype),
        ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
       for from_dtype, to_dtype in itertools.product(
           [onp.float32, onp.float64], repeat=2)
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
     args = (rng((2, 3), from_dtype),)
     convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
     check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
           jtu.format_shape_dtype_string(min_shape, dtype),
           jtu.format_shape_dtype_string(operand_shape, dtype),
@@ -1474,7 +1475,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(2, 3), (2, 3), (2, 3)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     shapes = [min_shape, operand_shape, max_shape]
@@ -1482,7 +1483,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
     check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
           dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
           num_arrs),
@@ -1492,7 +1493,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for dtype in float_dtypes
       for base_shape in [(4,), (3, 4), (2, 3, 4)]
       for dim in range(len(base_shape))
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
@@ -1501,7 +1502,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     concatenate = lambda *args: lax.concatenate(args, dim)
     check_grads(concatenate, operands, 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -1516,7 +1517,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        for strides in all_strides
        for dtype in [onp.float32]
        for padding in [""VALID"", ""SAME""]
-       for rng in [jtu.rand_small()])
+       for rng in [jtu.rand_small()]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
     lhs = rng(lhs_shape, dtype)
@@ -1524,7 +1525,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     conv = partial(lax.conv, window_strides=strides, padding=padding)
     check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
        ""rhs_dilation={}""
@@ -1547,7 +1548,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        for lhs_dil in lhs_dils
        for dtype in [onp.float32]
        for padding in all_pads
-       for rng in [jtu.rand_small()])
+       for rng in [jtu.rand_small()]))
   @jtu.skip_on_devices(""tpu"")
   def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                      padding, lhs_dil, rhs_dil, rng):
@@ -1557,7 +1558,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
                    padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
     check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
        ""rhs_dilation={}_dims={}""
@@ -1581,7 +1582,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]
       for dim_nums, perms in [
           ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
-          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                  padding, lhs_dil, rhs_dil, dimension_numbers,
@@ -1595,14 +1596,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
                    dimension_numbers=dimension_numbers)
     check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
           jtu.format_shape_dtype_string(lhs_shape, dtype),
           jtu.format_shape_dtype_string(rhs_shape, dtype)),
        ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
        ""rng"": jtu.rand_default()}
       for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
-      for dtype in float_dtypes)
+      for dtype in float_dtypes))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
   def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
     tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
@@ -1610,7 +1611,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     rhs = rng(rhs_shape, dtype)
     check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
        .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
@@ -1624,7 +1625,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           ((5, 3), (5, 2), (([0], [0]), ([], []))),
           ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
       ]
-      for dtype in float_dtypes)
+      for dtype in float_dtypes))
   @jtu.skip_on_devices(""tpu"")
   def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
                                           dimension_numbers, rng):
@@ -1634,7 +1635,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
     check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
           shape, onp.dtype(dtype).name, broadcast_sizes),
        ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
@@ -1642,14 +1643,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for shape in [(), (2, 3)]
       for dtype in float_dtypes
       for broadcast_sizes in [(), (2,), (1, 2)]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     args = (rng(shape, dtype),)
     broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
     check_grads(broadcast, args, 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
           jtu.format_shape_dtype_string(inshape, dtype),
           outshape, broadcast_dimensions),
@@ -1662,14 +1663,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           ([], [2, 3], []),
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(inshape, dtype)
     broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
     check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_outshape={}"".format(
           jtu.format_shape_dtype_string(arg_shape, dtype),
           jtu.format_shape_dtype_string(out_shape, dtype)),
@@ -1679,20 +1680,20 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for arg_shape, out_shape in [
           [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(arg_shape, dtype)
     reshape = lambda x: lax.reshape(x, out_shape)
     check_grads(reshape, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_inshape={}_pads={}""
        .format(jtu.format_shape_dtype_string(shape, dtype), pads),
        ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
       for shape in [(2, 3)]
       for dtype in float_dtypes
-      for pads in [[(1, 2, 1), (0, 1, 0)]])
+      for pads in [[(1, 2, 1), (0, 1, 0)]]))
   def testPadGrad(self, shape, dtype, pads, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
 
@@ -1714,7 +1715,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dimensions = [0, 1]
     check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_predshape={}_argshapes={}"".format(
           jtu.format_shape_dtype_string(pred_shape, onp.bool_),
           jtu.format_shape_dtype_string(arg_shape, dtype)),
@@ -1723,7 +1724,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for arg_shape in [(), (3,), (2, 3)]
       for pred_shape in ([(), arg_shape] if arg_shape else [()])
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     pred = rng(pred_shape, onp.bool_)
@@ -1732,7 +1733,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
     check_grads(select, (on_true, on_false), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
@@ -1751,14 +1752,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(5, 3), (1, 1), (5, 3), (2, 1)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
     slice = lambda x: lax.slice(x, starts, limits, strides)
     check_grads(slice, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, size_indices),
@@ -1770,7 +1771,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
                            rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -1778,7 +1779,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
     check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),
           start_indices, update_shape),
@@ -1790,7 +1791,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
                                  update_shape, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -1807,7 +1808,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)
     check_grads(dus, (update,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_perm={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), perm),
        ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
@@ -1818,14 +1819,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
         [(3, 4, 5), (1, 0, 2)],
       ]
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testTransposeGrad(self, shape, dtype, perm, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
     transpose = lambda x: lax.transpose(x, perm)
     check_grads(transpose, (operand,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
        .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
        ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
@@ -1842,7 +1843,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(3, 4, 5), (0, 2)],
           [(3, 4, 5), (0, 1, 2)]
       ]
-      for rng in [jtu.rand_small()])
+      for rng in [jtu.rand_small()]))
   def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
@@ -1850,7 +1851,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
     check_grads(reduce, (operand,), 1, tol, tol, tol)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_op={}_dtype={}_padding={}""
        .format(op.__name__, onp.dtype(dtype).name, padding),
        ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
@@ -1862,7 +1863,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       ]
       for dtype in dtypes
       for padding in [""VALID"", ""SAME""]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
     init_val = onp.asarray(init_val, dtype=dtype)
 
@@ -1901,14 +1902,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     # pylint: enable=cell-var-from-loop
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis),
        ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
       for dtype in [onp.float32]
       for shape in [(5,), (5, 7)]
       for axis in [len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortGrad(self, shape, dtype, axis, rng):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     operand = rng(shape, dtype)
@@ -1916,7 +1917,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     check_grads(sort, (operand,), 2, tol, tol, tol)
 
   # TODO(b/205052657): enable more tests when supported
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
           jtu.format_shape_dtype_string(shape, key_dtype),
           jtu.format_shape_dtype_string(shape, val_dtype),
@@ -1927,7 +1928,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for val_dtype in [onp.float32]
       for shape in [(3,), (5, 3)]
       for axis in [len(shape) - 1]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
     # This test relies on the property that wherever keys are tied, values are
     # too, since we don't guarantee the same ordering of values with equal keys.
@@ -1943,7 +1944,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
     check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
        ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
@@ -1954,14 +1955,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
     idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
     src = rng(shape, dtype)
     index_take = lambda src: lax.index_take(src, idxs, axes)
     check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
           jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
        ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
@@ -1973,7 +1974,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
           [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
       ]
-      for rng in [jtu.rand_default()])
+      for rng in [jtu.rand_default()]))
   def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
     # We call lax.index_take to get the shapes right
     src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
@@ -1986,5 +1987,4 @@ class LaxAutodiffTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()",No
tests/random_test.py,tests/random_test.py,bb8899f60d5a6afc2b91c307bee5f6b73f037a10,8b88027df0fe4fa94c65bca81f28062f4d8f43f4,Updated existing parameterized tests to subsample,"diff --git a/tests/random_test.py b/tests/random_test.py
index e90582b92..39a6046b0 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -29,6 +29,7 @@ from jax import random
 from jax import test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 
@@ -48,9 +49,9 @@ class LaxRandomTest(jtu.JaxTestCase):
     statistic = scipy.stats.kstest(samples, cdf).statistic
     self.assertLess(1. - scipy.special.kolmogorov(statistic), fail_prob)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64])
+      for dtype in [onp.float32, onp.float64]))
   def testNumpyAndXLAAgreeOnFloatEndianness(self, dtype):
     if not FLAGS.jax_enable_x64 and onp.issubdtype(dtype, onp.float64):
       return absltest.unittest.skip(""can't test float64 agreement"")
@@ -83,9 +84,9 @@ class LaxRandomTest(jtu.JaxTestCase):
         onp.uint32([0x243f6a88, 0x85a308d3]))
     self.assertEqual(expected, result_to_hex(result))
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64])
+      for dtype in [onp.float32, onp.float64]))
   def testRngUniform(self, dtype):
     key = random.PRNGKey(0)
     rand = lambda key: random.uniform(key, (10000,), dtype)
@@ -98,9 +99,9 @@ class LaxRandomTest(jtu.JaxTestCase):
       self._CheckCollisions(samples, onp.finfo(dtype).nmant)
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.uniform().cdf)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.int32, onp.int64])
+      for dtype in [onp.int32, onp.int64]))
   def testRngRandint(self, dtype):
     lo = 5
     hi = 10
@@ -117,9 +118,9 @@ class LaxRandomTest(jtu.JaxTestCase):
       self.assertTrue(onp.all(samples < hi))
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64])
+      for dtype in [onp.float32, onp.float64]))
   def testNormal(self, dtype):
     key = random.PRNGKey(0)
     rand = lambda key: random.normal(key, (10000,), dtype)
@@ -131,9 +132,9 @@ class LaxRandomTest(jtu.JaxTestCase):
     for samples in [uncompiled_samples, compiled_samples]:
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64])
+      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64]))
   def testShuffle(self, dtype):
     key = random.PRNGKey(0)
     x = onp.arange(100).astype(dtype)
@@ -150,5 +151,4 @@ class LaxRandomTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
index e90582b92..39a6046b0 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -29,6 +29,7 @@ from jax import random
 from jax import test_util as jtu
 from jax.config import config
 
+config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
 
@@ -48,9 +49,9 @@ class LaxRandomTest(jtu.JaxTestCase):
     statistic = scipy.stats.kstest(samples, cdf).statistic
     self.assertLess(1. - scipy.special.kolmogorov(statistic), fail_prob)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64])
+      for dtype in [onp.float32, onp.float64]))
   def testNumpyAndXLAAgreeOnFloatEndianness(self, dtype):
     if not FLAGS.jax_enable_x64 and onp.issubdtype(dtype, onp.float64):
       return absltest.unittest.skip(""can't test float64 agreement"")
@@ -83,9 +84,9 @@ class LaxRandomTest(jtu.JaxTestCase):
         onp.uint32([0x243f6a88, 0x85a308d3]))
     self.assertEqual(expected, result_to_hex(result))
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64])
+      for dtype in [onp.float32, onp.float64]))
   def testRngUniform(self, dtype):
     key = random.PRNGKey(0)
     rand = lambda key: random.uniform(key, (10000,), dtype)
@@ -98,9 +99,9 @@ class LaxRandomTest(jtu.JaxTestCase):
       self._CheckCollisions(samples, onp.finfo(dtype).nmant)
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.uniform().cdf)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.int32, onp.int64])
+      for dtype in [onp.int32, onp.int64]))
   def testRngRandint(self, dtype):
     lo = 5
     hi = 10
@@ -117,9 +118,9 @@ class LaxRandomTest(jtu.JaxTestCase):
       self.assertTrue(onp.all(samples < hi))
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64])
+      for dtype in [onp.float32, onp.float64]))
   def testNormal(self, dtype):
     key = random.PRNGKey(0)
     rand = lambda key: random.normal(key, (10000,), dtype)
@@ -131,9 +132,9 @@ class LaxRandomTest(jtu.JaxTestCase):
     for samples in [uncompiled_samples, compiled_samples]:
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
 
-  @parameterized.named_parameters(
+  @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
-      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64])
+      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64]))
   def testShuffle(self, dtype):
     key = random.PRNGKey(0)
     x = onp.arange(100).astype(dtype)
@@ -150,5 +151,4 @@ class LaxRandomTest(jtu.JaxTestCase):
 
 
 if __name__ == ""__main__"":
-  config.config_with_absl()
   absltest.main()",No
.gitignore,.gitignore,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/.gitignore b/.gitignore
index 24a98859f..7a4f3c259 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,12 +3,9 @@
 /jax/lib/pywrap_xla.py
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
-jax.egg-info
-
+*.egg-info
 .ipynb_checkpoints
-
 /bazel-*
 .bazelrc
 /tensorflow
-
-.DS_Store
+.DS_Store
\ No newline at end of file","diff --git a/.gitignore b/.gitignore
index 24a98859f..7a4f3c259 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,12 +3,9 @@
 /jax/lib/pywrap_xla.py
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
-jax.egg-info
-
+*.egg-info
 .ipynb_checkpoints
-
 /bazel-*
 .bazelrc
 /tensorflow
-
-.DS_Store
+.DS_Store
\ No newline at end of file",No
WORKSPACE,build/WORKSPACE,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/build/WORKSPACE b/build/WORKSPACE
new file mode 100644
index 000000000..0df920561
--- /dev/null
+++ b/build/WORKSPACE
@@ -0,0 +1,36 @@
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
+    urls = [
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/build/WORKSPACE b/build/WORKSPACE
new file mode 100644
index 000000000..0df920561
--- /dev/null
+++ b/build/WORKSPACE
@@ -0,0 +1,36 @@
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
+    urls = [
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
build.py,build/build.py,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/build/build.py b/build/build.py
new file mode 100755
index 000000000..4e75a6afd
--- /dev/null
+++ b/build/build.py
@@ -0,0 +1,278 @@
+#!/usr/bin/python
+#
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# Helper script for building JAX's libjax easily.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import collections
+import hashlib
+import os
+import platform
+import re
+import shutil
+import stat
+import subprocess
+import sys
+import urllib
+
+# pylint: disable=g-import-not-at-top
+if hasattr(urllib, ""urlretrieve""):
+  urlretrieve = urllib.urlretrieve
+else:
+  import urllib.request
+  urlretrieve = urllib.request.urlretrieve
+
+if hasattr(shutil, ""which""):
+  which = shutil.which
+else:
+  from distutils.spawn import find_executable as which
+# pylint: enable=g-import-not-at-top
+
+
+def shell(cmd):
+  output = subprocess.check_output(cmd)
+  return output.decode(""UTF-8"").strip()
+
+
+# Python
+
+def get_python_bin_path(python_bin_path_flag):
+  """"""Returns the path to the Python interpreter to use.""""""
+  return python_bin_path_flag or sys.executable
+
+
+# Bazel
+
+BAZEL_BASE_URI = ""https://github.com/bazelbuild/bazel/releases/download/0.19.2/""
+BazelPackage = collections.namedtuple(""BazelPackage"", [""file"", ""sha256""])
+bazel_packages = {
+    ""Linux"":
+        BazelPackage(
+            file=""bazel-0.19.2-linux-x86_64"",
+            sha256=
+            ""2ee9f23b49fb47725f725579c47f4f50272f4f9d23643e32add1fdef6aa0c5e0""),
+    ""Darwin"":
+        BazelPackage(
+            file=""bazel-0.19.2-darwin-x86_64"",
+            sha256=
+            ""74ae65127b46b59305fc5ea0c6baca355fce7e87c8624448e06f8cf2366b507e""),
+}
+
+
+def download_and_verify_bazel():
+  """"""Downloads a bazel binary from Github, verifying its SHA256 hash.""""""
+  package = bazel_packages.get(platform.system())
+  if package is None:
+    return None
+
+  if not os.access(package.file, os.X_OK):
+    uri = BAZEL_BASE_URI + package.file
+    sys.stdout.write(""Downloading bazel from: {}\n"".format(uri))
+
+    def progress(block_count, block_size, total_size):
+      if total_size <= 0:
+        total_size = 170**6
+      progress = (block_count * block_size) / total_size
+      num_chars = 40
+      progress_chars = int(num_chars * progress)
+      sys.stdout.write(""{} [{}{}] {}%\r"".format(
+          package.file, ""#"" * progress_chars,
+          ""."" * (num_chars - progress_chars), int(progress * 100.0)))
+
+    tmp_path, _ = urlretrieve(uri, None, progress)
+    sys.stdout.write(""\n"")
+
+    # Verify that the downloaded Bazel binary has the expected SHA256.
+    downloaded_file = open(tmp_path, ""rb"")
+    contents = downloaded_file.read()
+    downloaded_file.close()
+    digest = hashlib.sha256(contents).hexdigest()
+    if digest != package.sha256:
+      print(
+          ""Checksum mismatch for downloaded bazel binary (expected {}; got {}).""
+          .format(package.sha256, digest))
+      sys.exit(-1)
+
+    # Write the file as the bazel file name.
+    out_file = open(package.file, ""wb"")
+    out_file.write(contents)
+    out_file.close()
+
+    # Mark the file as executable.
+    st = os.stat(package.file)
+    os.chmod(package.file,
+             st.st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)
+
+  return ""./"" + package.file
+
+
+def get_bazel_path(bazel_path_flag):
+  """"""Returns the path to a Bazel binary, downloading Bazel if not found.""""""
+  if bazel_path_flag:
+    return bazel_path_flag
+
+  bazel = which(""bazel"")
+  if bazel:
+    return bazel
+
+  bazel = download_and_verify_bazel()
+  if bazel:
+    return bazel
+
+  print(""Cannot find or download bazel. Please install bazel."")
+  sys.exit(-1)
+
+
+def check_bazel_version(bazel_path, min_version):
+  """"""Checks Bazel's version is at least `min_version`.""""""
+  version_output = shell([bazel_path, ""--bazelrc=/dev/null"", ""version""])
+  match = re.search(""Build label: *([0-9\\.]+)[^0-9\\.]"", version_output)
+  if match is None:
+    print(""Warning: bazel installation is not a release version. Make sure ""
+          ""bazel is at least 0.19.2"")
+    return
+  version = match.group(1)
+  min_ints = [int(x) for x in min_version.split(""."")]
+  actual_ints = [int(x) for x in match.group(1).split(""."")]
+  if min_ints > actual_ints:
+    print(""Outdated bazel revision (>= {} required, found {})"".format(
+        min_version, version))
+    sys.exit(0)
+
+
+BAZELRC_TEMPLATE = """"""
+build --action_env PYTHON_BIN_PATH=""{python_bin_path}""
+build --python_path=""{python_bin_path}""
+build --action_env TF_NEED_CUDA=""{tf_need_cuda}""
+build --action_env CUDA_TOOLKIT_PATH=""{cuda_toolkit_path}""
+build --action_env CUDNN_INSTALL_PATH=""{cudnn_install_path}""
+build:opt --copt=-march=native
+build:opt --copt=-Wno-sign-compare
+build:opt --host_copt=-march=native
+""""""
+
+
+def write_bazelrc(**kwargs):
+  f = open("".bazelrc"", ""w"")
+  f.write(BAZELRC_TEMPLATE.format(**kwargs))
+  f.close()
+
+
+BANNER = r""""""
+     _   _  __  __
+    | | / \ \ \/ /
+ _  | |/ _ \ \  /
+| |_| / ___ \/  \
+ \___/_/   \/_/\_\
+
+""""""
+
+EPILOG = """"""
+
+From the 'build' directory in the JAX repository, run
+    python build.py
+or
+    python3 build.py
+to download and build JAX's XLA (jaxlib) dependency.
+""""""
+
+
+def _parse_string_as_bool(s):
+  """"""Parses a string as a boolean argument.""""""
+  lower = s.lower()
+  if lower == ""true"":
+    return True
+  elif lower == ""false"":
+    return False
+  else:
+    raise ValueError(""Expected either 'true' or 'false'; got {}"".format(s))
+
+
+def add_boolean_argument(parser, name, default=False, help_str=None):
+  """"""Creates a boolean flag.""""""
+  group = parser.add_mutually_exclusive_group()
+  group.add_argument(
+      ""--"" + name,
+      nargs=""?"",
+      default=default,
+      const=True,
+      type=_parse_string_as_bool,
+      help=help_str)
+  group.add_argument(""--no"" + name, dest=name, action=""store_false"")
+
+
+def main():
+  parser = argparse.ArgumentParser(
+      description=""Builds libjax from source."", epilog=EPILOG)
+  parser.add_argument(
+      ""--bazel_path"",
+      help=""Path to the Bazel binary to use. The default is to find bazel via ""
+      ""the PATH; if none is found, downloads a fresh copy of bazel from ""
+      ""GitHub."")
+  parser.add_argument(
+      ""--python_bin_path"",
+      help=""Path to Python binary to use. The default is the Python ""
+      ""interpreter used to run the build script."")
+  add_boolean_argument(
+      parser,
+      ""enable_cuda"",
+      help_str=""Should we build with CUDA enabled? Requires CUDA and CuDNN."")
+  parser.add_argument(
+      ""--cuda_path"",
+      default=""/usr/local/cuda"",
+      help=""Path to the CUDA toolkit."")
+  parser.add_argument(
+      ""--cudnn_path"",
+      default=""/usr/local/cuda"",
+      help=""Path to CUDNN libraries."")
+  args = parser.parse_args()
+
+  print(BANNER)
+  os.chdir(os.path.dirname(__file__))
+
+  # Find a working Bazel.
+  bazel_path = get_bazel_path(args.bazel_path)
+  check_bazel_version(bazel_path, ""0.19.2"")
+  print(""Bazel binary path: {}"".format(bazel_path))
+
+  python_bin_path = get_python_bin_path(args.python_bin_path)
+  print(""Python binary path: {}"".format(python_bin_path))
+
+  cuda_toolkit_path = args.cuda_path
+  cudnn_install_path = args.cudnn_path
+  print(""CUDA enabled: {}"".format(""yes"" if args.enable_cuda else ""no""))
+  if args.enable_cuda:
+    print(""CUDA toolkit path: {}"".format(cuda_toolkit_path))
+    print(""CUDNN library path: {}"".format(cudnn_install_path))
+  write_bazelrc(
+      python_bin_path=python_bin_path,
+      tf_need_cuda=1 if args.enable_cuda else 0,
+      cuda_toolkit_path=cuda_toolkit_path,
+      cudnn_install_path=cudnn_install_path)
+
+  print(""\nBuilding XLA and installing it in the jaxlib source tree..."")
+  shell([
+      bazel_path, ""run"", ""-c"", ""opt"", "":install_xla_in_source_tree"",
+      os.getcwd()
+  ])
+
+
+if __name__ == ""__main__"":
+  main()","diff --git a/build/build.py b/build/build.py
new file mode 100755
index 000000000..4e75a6afd
--- /dev/null
+++ b/build/build.py
@@ -0,0 +1,278 @@
+#!/usr/bin/python
+#
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# Helper script for building JAX's libjax easily.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import collections
+import hashlib
+import os
+import platform
+import re
+import shutil
+import stat
+import subprocess
+import sys
+import urllib
+
+# pylint: disable=g-import-not-at-top
+if hasattr(urllib, ""urlretrieve""):
+  urlretrieve = urllib.urlretrieve
+else:
+  import urllib.request
+  urlretrieve = urllib.request.urlretrieve
+
+if hasattr(shutil, ""which""):
+  which = shutil.which
+else:
+  from distutils.spawn import find_executable as which
+# pylint: enable=g-import-not-at-top
+
+
+def shell(cmd):
+  output = subprocess.check_output(cmd)
+  return output.decode(""UTF-8"").strip()
+
+
+# Python
+
+def get_python_bin_path(python_bin_path_flag):
+  """"""Returns the path to the Python interpreter to use.""""""
+  return python_bin_path_flag or sys.executable
+
+
+# Bazel
+
+BAZEL_BASE_URI = ""https://github.com/bazelbuild/bazel/releases/download/0.19.2/""
+BazelPackage = collections.namedtuple(""BazelPackage"", [""file"", ""sha256""])
+bazel_packages = {
+    ""Linux"":
+        BazelPackage(
+            file=""bazel-0.19.2-linux-x86_64"",
+            sha256=
+            ""2ee9f23b49fb47725f725579c47f4f50272f4f9d23643e32add1fdef6aa0c5e0""),
+    ""Darwin"":
+        BazelPackage(
+            file=""bazel-0.19.2-darwin-x86_64"",
+            sha256=
+            ""74ae65127b46b59305fc5ea0c6baca355fce7e87c8624448e06f8cf2366b507e""),
+}
+
+
+def download_and_verify_bazel():
+  """"""Downloads a bazel binary from Github, verifying its SHA256 hash.""""""
+  package = bazel_packages.get(platform.system())
+  if package is None:
+    return None
+
+  if not os.access(package.file, os.X_OK):
+    uri = BAZEL_BASE_URI + package.file
+    sys.stdout.write(""Downloading bazel from: {}\n"".format(uri))
+
+    def progress(block_count, block_size, total_size):
+      if total_size <= 0:
+        total_size = 170**6
+      progress = (block_count * block_size) / total_size
+      num_chars = 40
+      progress_chars = int(num_chars * progress)
+      sys.stdout.write(""{} [{}{}] {}%\r"".format(
+          package.file, ""#"" * progress_chars,
+          ""."" * (num_chars - progress_chars), int(progress * 100.0)))
+
+    tmp_path, _ = urlretrieve(uri, None, progress)
+    sys.stdout.write(""\n"")
+
+    # Verify that the downloaded Bazel binary has the expected SHA256.
+    downloaded_file = open(tmp_path, ""rb"")
+    contents = downloaded_file.read()
+    downloaded_file.close()
+    digest = hashlib.sha256(contents).hexdigest()
+    if digest != package.sha256:
+      print(
+          ""Checksum mismatch for downloaded bazel binary (expected {}; got {}).""
+          .format(package.sha256, digest))
+      sys.exit(-1)
+
+    # Write the file as the bazel file name.
+    out_file = open(package.file, ""wb"")
+    out_file.write(contents)
+    out_file.close()
+
+    # Mark the file as executable.
+    st = os.stat(package.file)
+    os.chmod(package.file,
+             st.st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)
+
+  return ""./"" + package.file
+
+
+def get_bazel_path(bazel_path_flag):
+  """"""Returns the path to a Bazel binary, downloading Bazel if not found.""""""
+  if bazel_path_flag:
+    return bazel_path_flag
+
+  bazel = which(""bazel"")
+  if bazel:
+    return bazel
+
+  bazel = download_and_verify_bazel()
+  if bazel:
+    return bazel
+
+  print(""Cannot find or download bazel. Please install bazel."")
+  sys.exit(-1)
+
+
+def check_bazel_version(bazel_path, min_version):
+  """"""Checks Bazel's version is at least `min_version`.""""""
+  version_output = shell([bazel_path, ""--bazelrc=/dev/null"", ""version""])
+  match = re.search(""Build label: *([0-9\\.]+)[^0-9\\.]"", version_output)
+  if match is None:
+    print(""Warning: bazel installation is not a release version. Make sure ""
+          ""bazel is at least 0.19.2"")
+    return
+  version = match.group(1)
+  min_ints = [int(x) for x in min_version.split(""."")]
+  actual_ints = [int(x) for x in match.group(1).split(""."")]
+  if min_ints > actual_ints:
+    print(""Outdated bazel revision (>= {} required, found {})"".format(
+        min_version, version))
+    sys.exit(0)
+
+
+BAZELRC_TEMPLATE = """"""
+build --action_env PYTHON_BIN_PATH=""{python_bin_path}""
+build --python_path=""{python_bin_path}""
+build --action_env TF_NEED_CUDA=""{tf_need_cuda}""
+build --action_env CUDA_TOOLKIT_PATH=""{cuda_toolkit_path}""
+build --action_env CUDNN_INSTALL_PATH=""{cudnn_install_path}""
+build:opt --copt=-march=native
+build:opt --copt=-Wno-sign-compare
+build:opt --host_copt=-march=native
+""""""
+
+
+def write_bazelrc(**kwargs):
+  f = open("".bazelrc"", ""w"")
+  f.write(BAZELRC_TEMPLATE.format(**kwargs))
+  f.close()
+
+
+BANNER = r""""""
+     _   _  __  __
+    | | / \ \ \/ /
+ _  | |/ _ \ \  /
+| |_| / ___ \/  \
+ \___/_/   \/_/\_\
+
+""""""
+
+EPILOG = """"""
+
+From the 'build' directory in the JAX repository, run
+    python build.py
+or
+    python3 build.py
+to download and build JAX's XLA (jaxlib) dependency.
+""""""
+
+
+def _parse_string_as_bool(s):
+  """"""Parses a string as a boolean argument.""""""
+  lower = s.lower()
+  if lower == ""true"":
+    return True
+  elif lower == ""false"":
+    return False
+  else:
+    raise ValueError(""Expected either 'true' or 'false'; got {}"".format(s))
+
+
+def add_boolean_argument(parser, name, default=False, help_str=None):
+  """"""Creates a boolean flag.""""""
+  group = parser.add_mutually_exclusive_group()
+  group.add_argument(
+      ""--"" + name,
+      nargs=""?"",
+      default=default,
+      const=True,
+      type=_parse_string_as_bool,
+      help=help_str)
+  group.add_argument(""--no"" + name, dest=name, action=""store_false"")
+
+
+def main():
+  parser = argparse.ArgumentParser(
+      description=""Builds libjax from source."", epilog=EPILOG)
+  parser.add_argument(
+      ""--bazel_path"",
+      help=""Path to the Bazel binary to use. The default is to find bazel via ""
+      ""the PATH; if none is found, downloads a fresh copy of bazel from ""
+      ""GitHub."")
+  parser.add_argument(
+      ""--python_bin_path"",
+      help=""Path to Python binary to use. The default is the Python ""
+      ""interpreter used to run the build script."")
+  add_boolean_argument(
+      parser,
+      ""enable_cuda"",
+      help_str=""Should we build with CUDA enabled? Requires CUDA and CuDNN."")
+  parser.add_argument(
+      ""--cuda_path"",
+      default=""/usr/local/cuda"",
+      help=""Path to the CUDA toolkit."")
+  parser.add_argument(
+      ""--cudnn_path"",
+      default=""/usr/local/cuda"",
+      help=""Path to CUDNN libraries."")
+  args = parser.parse_args()
+
+  print(BANNER)
+  os.chdir(os.path.dirname(__file__))
+
+  # Find a working Bazel.
+  bazel_path = get_bazel_path(args.bazel_path)
+  check_bazel_version(bazel_path, ""0.19.2"")
+  print(""Bazel binary path: {}"".format(bazel_path))
+
+  python_bin_path = get_python_bin_path(args.python_bin_path)
+  print(""Python binary path: {}"".format(python_bin_path))
+
+  cuda_toolkit_path = args.cuda_path
+  cudnn_install_path = args.cudnn_path
+  print(""CUDA enabled: {}"".format(""yes"" if args.enable_cuda else ""no""))
+  if args.enable_cuda:
+    print(""CUDA toolkit path: {}"".format(cuda_toolkit_path))
+    print(""CUDNN library path: {}"".format(cudnn_install_path))
+  write_bazelrc(
+      python_bin_path=python_bin_path,
+      tf_need_cuda=1 if args.enable_cuda else 0,
+      cuda_toolkit_path=cuda_toolkit_path,
+      cudnn_install_path=cudnn_install_path)
+
+  print(""\nBuilding XLA and installing it in the jaxlib source tree..."")
+  shell([
+      bazel_path, ""run"", ""-c"", ""opt"", "":install_xla_in_source_tree"",
+      os.getcwd()
+  ])
+
+
+if __name__ == ""__main__"":
+  main()",No
build/install_xla_in_source_tree.sh,build/install_xla_in_source_tree.sh,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/build/install_xla_in_source_tree.sh b/build/install_xla_in_source_tree.sh
index 55e8ff8bf..6ace93ead 100755
--- a/build/install_xla_in_source_tree.sh
+++ b/build/install_xla_in_source_tree.sh
@@ -45,23 +45,22 @@ if [[ $# -ne 1 ]]; then
 fi
 TARGET=""$1""
 
-if [[ ! -r ""${TARGET}/jax/lax.py"" ]]; then
-  echo ""Target directory ${TARGET} does not seem to be a JAX source tree"" \
-       ""(missing jax/lax.py)""
+if [[ ! -d ""${TARGET}/jaxlib"" ]]; then
+  echo ""Target directory ${TARGET} does not have a jaxlib directory""
   exit 1
 fi
 
 # Copy the XLA dependencies into jax/lib, fixing up some imports to point to the
 # new location.
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/xla_data_pb2.py)"" \
-  ""${TARGET}/jax/lib""
+  ""${TARGET}/jaxlib""
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/pywrap_xla.py)"" \
-  ""${TARGET}/jax/lib""
+  ""${TARGET}/jaxlib""
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/_pywrap_xla.so)"" \
-  ""${TARGET}/jax/lib""
+  ""${TARGET}/jaxlib""
 sed \
   -e 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' \
   -e 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' \
   -e '/from tensorflow.compiler.xla.service import hlo_pb2/d' \
   < ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/xla_client.py)"" \
-  > ""${TARGET}/jax/lib/xla_client.py""
+  > ""${TARGET}/jaxlib/xla_client.py""","diff --git a/build/install_xla_in_source_tree.sh b/build/install_xla_in_source_tree.sh
index 55e8ff8bf..6ace93ead 100755
--- a/build/install_xla_in_source_tree.sh
+++ b/build/install_xla_in_source_tree.sh
@@ -45,23 +45,22 @@ if [[ $# -ne 1 ]]; then
 fi
 TARGET=""$1""
 
-if [[ ! -r ""${TARGET}/jax/lax.py"" ]]; then
-  echo ""Target directory ${TARGET} does not seem to be a JAX source tree"" \
-       ""(missing jax/lax.py)""
+if [[ ! -d ""${TARGET}/jaxlib"" ]]; then
+  echo ""Target directory ${TARGET} does not have a jaxlib directory""
   exit 1
 fi
 
 # Copy the XLA dependencies into jax/lib, fixing up some imports to point to the
 # new location.
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/xla_data_pb2.py)"" \
-  ""${TARGET}/jax/lib""
+  ""${TARGET}/jaxlib""
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/pywrap_xla.py)"" \
-  ""${TARGET}/jax/lib""
+  ""${TARGET}/jaxlib""
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/_pywrap_xla.so)"" \
-  ""${TARGET}/jax/lib""
+  ""${TARGET}/jaxlib""
 sed \
   -e 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' \
   -e 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' \
   -e '/from tensorflow.compiler.xla.service import hlo_pb2/d' \
   < ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/xla_client.py)"" \
-  > ""${TARGET}/jax/lib/xla_client.py""
+  > ""${TARGET}/jaxlib/xla_client.py""",No
build/jaxlib/__init__.py,build/jaxlib/__init__.py,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/build/jaxlib/__init__.py b/build/jaxlib/__init__.py
new file mode 100644
index 000000000..e69de29bb","diff --git a/build/jaxlib/__init__.py b/build/jaxlib/__init__.py
new file mode 100644
index 000000000..e69de29bb",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 053c6bbe9..96db8a1b2 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -29,8 +29,8 @@ import warnings
 from ..config import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
-from . import xla_data_pb2
-from . import xla_client
+from jaxlib import xla_data_pb2
+from jaxlib import xla_client
 
 
 FLAGS = flags.FLAGS","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 053c6bbe9..96db8a1b2 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -29,8 +29,8 @@ import warnings
 from ..config import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
-from . import xla_data_pb2
-from . import xla_client
+from jaxlib import xla_data_pb2
+from jaxlib import xla_client
 
 
 FLAGS = flags.FLAGS",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 180b8b6f8..4bf144320 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -24,9 +24,9 @@ import numpy as onp
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
-from ..lib import xla_bridge
 import jax.lax as lax
 from ..util import memoize
+from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
 # and also use some common names (like 'shape' and 'dtype') at the top-level.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 180b8b6f8..4bf144320 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -24,9 +24,9 @@ import numpy as onp
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
-from ..lib import xla_bridge
 import jax.lax as lax
 from ..util import memoize
+from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
 # and also use some common names (like 'shape' and 'dtype') at the top-level.",No
jax/random.py,jax/random.py,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/jax/random.py b/jax/random.py
index 2b7e4f34b..a03599f97 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -25,8 +25,8 @@ import numpy as onp
 from . import lax
 from . import numpy as np
 from . import tree_util
-from .lib import xla_bridge
 from .api import jit
+from jax.lib import xla_bridge
 
 
 class PRNGKey(object):","diff --git a/jax/random.py b/jax/random.py
index 2b7e4f34b..a03599f97 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -25,8 +25,8 @@ import numpy as onp
 from . import lax
 from . import numpy as np
 from . import tree_util
-from .lib import xla_bridge
 from .api import jit
+from jax.lib import xla_bridge
 
 
 class PRNGKey(object):",No
setup.py,setup.py,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,c0360033316a7e648481284fd3436433db881280,"Split out `jax` and `jaxlib` packages (#11)

factor out 'jaxlib' as separate package","diff --git a/setup.py b/setup.py
index b634177d3..e605c2f76 100644
--- a/setup.py
+++ b/setup.py
@@ -13,18 +13,17 @@
 # limitations under the License.
 
 from setuptools import setup
-from glob import glob
 
 setup(
     name='jax',
-    version='0.0',
+    version='0.1',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
-    author_email='jax-team@google.com',
+    author_email='jax-dev@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py',
+                      'opt_einsum'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
-    package_data={'jax.lib': glob('jax/lib/*.so')},
 )","diff --git a/setup.py b/setup.py
index b634177d3..e605c2f76 100644
--- a/setup.py
+++ b/setup.py
@@ -13,18 +13,17 @@
 # limitations under the License.
 
 from setuptools import setup
-from glob import glob
 
 setup(
     name='jax',
-    version='0.0',
+    version='0.1',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
-    author_email='jax-team@google.com',
+    author_email='jax-dev@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py',
+                      'opt_einsum'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
-    package_data={'jax.lib': glob('jax/lib/*.so')},
 )",No
images/jax_logo.png,images/jax_logo.png,febfc84fdad6563068132acd5a263a6d8af4de36,bbc92ce6eb6c942fea8e20a841d1d7473cfe9c90,add jax logo png,"diff --git a/images/jax_logo.png b/images/jax_logo.png
new file mode 100644
index 000000000..a05e7c8d6
Binary files /dev/null and b/images/jax_logo.png differ","diff --git a/images/jax_logo.png b/images/jax_logo.png
new file mode 100644
index 000000000..a05e7c8d6
Binary files /dev/null and b/images/jax_logo.png differ",No
jax/lax.py,jax/lax.py,1627827ac60610d12e243b5d00ae46b64aedd8b8,8a3241e36b7a7aa0eb6e951d504cbc7e287d0c47,Fixed a couple of bugs,"diff --git a/jax/lax.py b/jax/lax.py
index 131a59c4e..9e5ae12a3 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1643,7 +1643,7 @@ def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                  operand_shape):
   assert operand is None
   zeros = broadcast(_const(t, 0), operand_shape)
-  return [dynamic_update_slice(zeros, t, start_indices)]
+  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]
 
 dynamic_slice_p = standard_primitive(
     dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',","diff --git a/jax/lax.py b/jax/lax.py
index 131a59c4e..9e5ae12a3 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1643,7 +1643,7 @@ def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
                                  operand_shape):
   assert operand is None
   zeros = broadcast(_const(t, 0), operand_shape)
-  return [dynamic_update_slice(zeros, t, start_indices)]
+  return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]
 
 dynamic_slice_p = standard_primitive(
     dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',",No
jax/test_util.py,jax/test_util.py,1627827ac60610d12e243b5d00ae46b64aedd8b8,8a3241e36b7a7aa0eb6e951d504cbc7e287d0c47,Fixed a couple of bugs,"diff --git a/jax/test_util.py b/jax/test_util.py
index e0f2e212c..21525c462 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -333,7 +333,7 @@ def check_raises(thunk, err_type, msg):
     thunk()
     assert False
   except err_type as e:
-    assert str(e) == msg, ""{}\n\n{}\n"".format(e, msg)
+    assert str(e).startswith(msg), ""\n{}\n\n{}\n"".format(e, msg)
 
 def check_raises_regexp(thunk, err_type, pattern):
   try:","diff --git a/jax/test_util.py b/jax/test_util.py
index e0f2e212c..21525c462 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -333,7 +333,7 @@ def check_raises(thunk, err_type, msg):
     thunk()
     assert False
   except err_type as e:
-    assert str(e) == msg, ""{}\n\n{}\n"".format(e, msg)
+    assert str(e).startswith(msg), ""\n{}\n\n{}\n"".format(e, msg)
 
 def check_raises_regexp(thunk, err_type, pattern):
   try:",No
tests/api_test.py,tests/api_test.py,1627827ac60610d12e243b5d00ae46b64aedd8b8,8a3241e36b7a7aa0eb6e951d504cbc7e287d0c47,Fixed a couple of bugs,"diff --git a/tests/api_test.py b/tests/api_test.py
index 39eb326b7..ff3fbb7e6 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -123,18 +123,15 @@ class APITest(jtu.JaxTestCase):
 
   def test_grad_tuple_output(self):
     jtu.check_raises(lambda: grad(lambda x: (x,x))(1.0), TypeError,
-                     ""Gradient only defined for scalar-output functions. ""
-                     ""Output was: (1.0, 1.0)"")
+                     ""Gradient only defined for scalar-output functions. "")
 
   def test_grad_unit_output(self):
     jtu.check_raises(lambda: grad(lambda x: ())(onp.zeros(3)), TypeError,
-                     ""Gradient only defined for scalar-output functions. ""
-                     ""Output was: ()"")
+                     ""Gradient only defined for scalar-output functions. "")
 
   def test_grad_nonscalar_output(self):
     jtu.check_raises(lambda: grad(lambda x: x)(onp.zeros(3)), TypeError,
-                     ""Gradient only defined for scalar-output functions. ""
-                     ""Output was: [ 0.  0.  0.]"")
+                     ""Gradient only defined for scalar-output functions. "")
 
   def test_unwrapped_numpy(self):
     def f(x):","diff --git a/tests/api_test.py b/tests/api_test.py
index 39eb326b7..ff3fbb7e6 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -123,18 +123,15 @@ class APITest(jtu.JaxTestCase):
 
   def test_grad_tuple_output(self):
     jtu.check_raises(lambda: grad(lambda x: (x,x))(1.0), TypeError,
-                     ""Gradient only defined for scalar-output functions. ""
-                     ""Output was: (1.0, 1.0)"")
+                     ""Gradient only defined for scalar-output functions. "")
 
   def test_grad_unit_output(self):
     jtu.check_raises(lambda: grad(lambda x: ())(onp.zeros(3)), TypeError,
-                     ""Gradient only defined for scalar-output functions. ""
-                     ""Output was: ()"")
+                     ""Gradient only defined for scalar-output functions. "")
 
   def test_grad_nonscalar_output(self):
     jtu.check_raises(lambda: grad(lambda x: x)(onp.zeros(3)), TypeError,
-                     ""Gradient only defined for scalar-output functions. ""
-                     ""Output was: [ 0.  0.  0.]"")
+                     ""Gradient only defined for scalar-output functions. "")
 
   def test_unwrapped_numpy(self):
     def f(x):",No
tests/core_test.py,tests/core_test.py,1627827ac60610d12e243b5d00ae46b64aedd8b8,8a3241e36b7a7aa0eb6e951d504cbc7e287d0c47,Fixed a couple of bugs,"diff --git a/tests/core_test.py b/tests/core_test.py
index 6091d92ef..fce93167e 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -205,7 +205,6 @@ class CoreTest(jtu.JaxTestCase):
 
   @parameterized.parameters(test_specs)
   def test_jvp(self, f, args):
-    print(f)
     jtu.check_jvp(f, partial(jvp, f), args)
 
   def test_jvp_zeros(self):
@@ -223,7 +222,7 @@ class CoreTest(jtu.JaxTestCase):
         return np.multiply(np.sin(x), y)
       return call(bar, x)
 
-    print(api.trace_to_jaxpr(foo, (__,)))
+    api.trace_to_jaxpr(foo, (__,))
 
   def DISABLED_test_nested_grad(self):
     def foo(x):
@@ -246,16 +245,14 @@ class CoreTest(jtu.JaxTestCase):
 
       return call(bar, x)
 
-    print(api.trace_to_jaxpr(foo, (__,)))
+    api.trace_to_jaxpr(foo, (__,))
 
   @parameterized.parameters(test_specs)
   def test_jvp_linearized(self, f, args):
-    print(f)
     jtu.check_jvp(f, partial(jvp_unlinearized, f), args)
 
   @parameterized.parameters(test_specs)
   def test_vjp(self, f, args):
-    print(f)
     jtu.check_vjp(f, partial(vjp, f), args)
 
   def test_jvp_closure(self):","diff --git a/tests/core_test.py b/tests/core_test.py
index 6091d92ef..fce93167e 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -205,7 +205,6 @@ class CoreTest(jtu.JaxTestCase):
 
   @parameterized.parameters(test_specs)
   def test_jvp(self, f, args):
-    print(f)
     jtu.check_jvp(f, partial(jvp, f), args)
 
   def test_jvp_zeros(self):
@@ -223,7 +222,7 @@ class CoreTest(jtu.JaxTestCase):
         return np.multiply(np.sin(x), y)
       return call(bar, x)
 
-    print(api.trace_to_jaxpr(foo, (__,)))
+    api.trace_to_jaxpr(foo, (__,))
 
   def DISABLED_test_nested_grad(self):
     def foo(x):
@@ -246,16 +245,14 @@ class CoreTest(jtu.JaxTestCase):
 
       return call(bar, x)
 
-    print(api.trace_to_jaxpr(foo, (__,)))
+    api.trace_to_jaxpr(foo, (__,))
 
   @parameterized.parameters(test_specs)
   def test_jvp_linearized(self, f, args):
-    print(f)
     jtu.check_jvp(f, partial(jvp_unlinearized, f), args)
 
   @parameterized.parameters(test_specs)
   def test_vjp(self, f, args):
-    print(f)
     jtu.check_vjp(f, partial(vjp, f), args)
 
   def test_jvp_closure(self):",No
images/jax_logo_500px.png,images/jax_logo_500px.png,c8f93511ecb977d02fa5b0eee17075706fd6fd93,febfc84fdad6563068132acd5a263a6d8af4de36,add 500px jax logo,"diff --git a/images/jax_logo_500px.png b/images/jax_logo_500px.png
new file mode 100644
index 000000000..66823f059
Binary files /dev/null and b/images/jax_logo_500px.png differ","diff --git a/images/jax_logo_500px.png b/images/jax_logo_500px.png
new file mode 100644
index 000000000..66823f059
Binary files /dev/null and b/images/jax_logo_500px.png differ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,61aa47a1a803f68626d99a702455b5430b7cf99c,1627827ac60610d12e243b5d00ae46b64aedd8b8,Fixed paren in parameterized test specification,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 8f22a1434..2b062a411 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -355,7 +355,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, fill_value_dtype),
           onp.dtype(out_dtype).name),
        ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
-       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()})
+       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
       for shape in array_shapes
       for fill_value_dtype in default_dtypes
       for out_dtype in default_dtypes))","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 8f22a1434..2b062a411 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -355,7 +355,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, fill_value_dtype),
           onp.dtype(out_dtype).name),
        ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
-       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()})
+       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
       for shape in array_shapes
       for fill_value_dtype in default_dtypes
       for out_dtype in default_dtypes))",No
jax/interpreters/batching.py,jax/interpreters/batching.py,ae7df43e9bcdcd4ff70228bfddb1f52973315b8e,61aa47a1a803f68626d99a702455b5430b7cf99c,Fixed bug due to input_shape kwarg not being modified in batching rule for reducers. Fixes b/120595235,"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 4611f0daa..56e764616 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -224,6 +224,8 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
   bdim, = batch_dims
   axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
   bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
+  if 'input_shape' in kwargs:
+    kwargs['input_shape'] = operand.shape
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
 def add_batched(batched_args, batch_dims):","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 4611f0daa..56e764616 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -224,6 +224,8 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
   bdim, = batch_dims
   axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
   bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
+  if 'input_shape' in kwargs:
+    kwargs['input_shape'] = operand.shape
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
 def add_batched(batched_args, batch_dims):",No
jax/lax.py,jax/lax.py,ae7df43e9bcdcd4ff70228bfddb1f52973315b8e,61aa47a1a803f68626d99a702455b5430b7cf99c,Fixed bug due to input_shape kwarg not being modified in batching rule for reducers. Fixes b/120595235,"diff --git a/jax/lax.py b/jax/lax.py
index 9e5ae12a3..5d6d63f58 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1790,6 +1790,8 @@ batching.defreducer(reduce_p)
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):
+  assert operand.shape == input_shape, ('{} != {}'
+                                        .format(operand.shape, input_shape))
   return tuple(onp.delete(operand.shape, axes))
 
 def reduce_sum_translation_rule(c, operand, axes, input_shape):","diff --git a/jax/lax.py b/jax/lax.py
index 9e5ae12a3..5d6d63f58 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1790,6 +1790,8 @@ batching.defreducer(reduce_p)
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):
+  assert operand.shape == input_shape, ('{} != {}'
+                                        .format(operand.shape, input_shape))
   return tuple(onp.delete(operand.shape, axes))
 
 def reduce_sum_translation_rule(c, operand, axes, input_shape):",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,80689dcc1354681c19904711f123a092b511de3d,c8f93511ecb977d02fa5b0eee17075706fd6fd93,Update the quickstart notebook.,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index d6c94a106..e1502e48a 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -1,572 +1,562 @@
 {
- ""cells"": [
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
+  ""nbformat"": 4,
+  ""nbformat_minor"": 0,
+  ""metadata"": {
     ""colab"": {
-     ""base_uri"": ""https://localhost:8080/"",
-     ""height"": 224
-    },
-    ""colab_type"": ""code"",
-    ""id"": ""PaW85yP_BrCF"",
-    ""outputId"": ""16fe6dee-a773-4889-fdb3-ebe75f48abe5""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""xtWX4x9DCF5_""
-   },
-   ""source"": [
-    ""# JAX Quickstart\n"",
-    ""dougalm@, phawkins@, mattjj@, frostig@, alexbw@\n"",
-    ""\n"",
-    ""### TODO: LOGO\n"",
-    ""\n"",
-    ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
-    ""\n"",
-    ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
-    ""can automatically differentiate native Python and NumPy code. It can\n"",
-    ""differentiate through a large subset of Pythons features, including loops, ifs,\n"",
-    ""recursion, and closures, and it can even take derivatives of derivatives of\n"",
-    ""derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n"",
-    ""to any order.\n"",
-    ""\n"",
-    ""Whats new is that JAX uses\n"",
-    ""[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)\n"",
-    ""to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n"",
-    ""Compilation happens under the hood by default, with library calls getting\n"",
-    ""just-in-time compiled and executed. But JAX even lets you just-in-time compile\n"",
-    ""your own Python functions into XLA-optimized kernels using a one-function API.\n"",
-    ""Compilation and automatic differentiation can be composed arbitrarily, so you\n"",
-    ""can express sophisticated algorithms and get maximal performance without having\n"",
-    ""to leave Python.\n""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""roHm4XGpf3rl""
-   },
-   ""source"": [
-    ""## The basics of JAX""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""SY8mDvEvCGqk""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""from __future__ import print_function, division\n"",
-    ""import numpy as onp\n"",
-    ""from tqdm import tqdm\n"",
-    ""import jax.numpy as np\n"",
-    ""from jax import grad, jit, vmap\n"",
-    ""from jax import device_put\n"",
-    ""from jax import random""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""FQ89jHCYfhpg""
-   },
-   ""source"": [
-    ""### Multiplying Matrices""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""Xpy1dSgNqCP4""
-   },
-   ""source"": [
-    ""We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you ask for random numbers. We needed to make this change to support some of the great features we talk about below.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""u0nseKZNqOoH""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""key = random.PRNGKey(0)\n"",
-    ""x = random.normal(key, (10,))\n"",
-    ""print(x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""hDJF0UPKnuqB""
-   },
-   ""source"": [
-    ""Let's dive right in and multiply two big matrices.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""eXn8GUl6CG5N""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""size = 3000\n"",
-    ""x = random.normal(key, (size, size), dtype=np.float32)\n"",
-    ""%timeit np.dot(x, x.T)  # runs on the GPU""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""0AlN7EbonyaR""
-   },
-   ""source"": [
-    ""JAX NumPy functions work on raw NumPy arrays as well.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""ZPl0MuwYrM7t""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""import numpy as onp  # original CPU-backed NumPy\n"",
-    ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
-    ""%timeit np.dot(x, x.T)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""_SrcB2IurUuE""
-   },
-   ""source"": [
-    ""That's slower beacuse it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""Jj7M7zyRskF0""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
-    ""x = device_put(x)\n"",
-    ""%timeit np.dot(x, x.T)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""clO9djnen8qi""
-   },
-   ""source"": [
-    ""The output of `device_put` still acts like an NDArray. By the way, the implementation of `device_put` is just `device_put = jit(lambda x: x)`.""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""ghkfKNQttDpg""
-   },
-   ""source"": [
-    ""All of these calls above are faster than original NumPy on the CPU.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""RzXK8GnIs7VV""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
-    ""%timeit onp.dot(x, x.T)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""iOzp0P_GoJhb""
-   },
-   ""source"": [
-    ""JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numeric code. For now, there's three main ones:\n"",
-    ""\n"",
-    "" - `jit`, for speeding up your code\n"",
-    "" - `grad`, for taking derivatives\n"",
-    "" - `vmap`, for automatic vectorization or batching.\n"",
-    ""\n"",
-    ""Let's go over these, one-by-one. We'll also end up composing these in interesting ways.""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""bTTrTbWvgLUK""
-   },
-   ""source"": [
-    ""### Using `jit` to speed up functions""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""YrqE32mvE3b7""
-   },
-   ""source"": [
-    ""JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, JAX will incur overhead. Fortunately, JAX has a `@jit` decorator which will fuse multiple operations together. Let's try that.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""qLGdCtFKFLOR""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def selu_raw(x, alpha=1.67, lmbda=1.05):\n"",
-    ""  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n"",
-    ""\n"",
-    ""x = np.zeros(1000000)\n"",
-    ""%timeit selu_raw(x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""a_V8SruVHrD_""
-   },
-   ""source"": [
-    ""We can speed it up with @jit, which will jit-compile the first time `selu` is called and will be cached thereafter.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""fh4w_3NpFYTp""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""selu = jit(selu)\n"",
-    ""%timeit selu(x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""HxpBc4WmfsEU""
-   },
-   ""source"": [
-    ""### Taking derivatives with `grad`\n"",
-    ""\n"",
-    ""We don't just want to compute with NumPy arrays, we also want to tranform numeric programs, like by taking their derivative. In JAX, just like in Autograd, there is a one-function API for taking derivatives: the `grad` function.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""IMAgNJaMJwPD""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def sum_logistic(x):\n"",
-    ""  return np.sum(1.0 / (1.0 + np.exp(-x)))\n"",
-    ""\n"",
-    ""x_small = np.ones((3,))\n"",
-    ""derivative_fn = grad(sum_logistic)\n"",
-    ""print(derivative_fn(x_small))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""PtNs881Ohioc""
-   },
-   ""source"": [
-    ""Let's verify with finite differences that our result is correct.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""JXI7_OZuKZVO""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def first_finite_differences(f, x):\n"",
-    ""  eps = 1e-3\n"",
-    ""  return np.array([(f(x + eps * basis_vect) - f(x - eps * basis_vect)) / (2 * eps)\n"",
-    ""                   for basis_vect in onp.eye(len(x))])\n"",
-    ""\n"",
-    ""\n"",
-    ""print(first_finite_differences(sum_logistic, x_small))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""Q2CUZjOWNZ-3""
-   },
-   ""source"": [
-    ""Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""TO4g8ny-OEi4""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""yCJ5feKvhnBJ""
-   },
-   ""source"": [
-    ""For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. We used them with `vmap` (which we'll describe in a moment) to write `jax.jacfwd` and `jax.jacrev` for computing full Jacobian matrices. Here's one way to compose those to make a function that efficiently computes full Hessian matrices:""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""Z-JxbiNyhxEW""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""from jax import jacfwd, jacrev\n"",
-    ""def hessian(fun):\n"",
-    ""  return jacfwd(jacrev(fun))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""TI4nPsGafxbL""
-   },
-   ""source"": [
-    ""### Auto-vectorization with `vmap`""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""PcxkONy5aius""
-   },
-   ""source"": [
-    ""JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a functions primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand.""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""TPiX4y-bWLFS""
-   },
-   ""source"": [
-    ""We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using vmap. Although this is trivial to do by hand in this specific case, the same technique can apply to more complicated functions.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""cCQf0AAAa3ta""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def apply_matrix(v):\n"",
-    ""  return np.dot(mat, v)""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""8w0Gpsn8WYYj""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""mat = random.normal(key, (150, 100))\n"",
-    ""batched_x = random.normal(key, (10, 100))""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""KWVc9BsZv0Ki""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def naively_batched_apply_matrix(v_batched):\n"",
-    ""  return np.stack([apply_matrix(v) for v in v_batched])\n"",
-    ""\n"",
-    ""print('Naively batched')\n"",
-    ""%timeit naively_batched_apply_matrix(batched_x)""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""ipei6l8nvrzH""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""@jit\n"",
-    ""def batched_apply_matrix(v_batched):\n"",
-    ""  return np.dot(v_batched, mat.T)\n"",
-    ""\n"",
-    ""print('Manually batched')\n"",
-    ""%timeit batched_apply_matrix(batched_x)""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""67Oeknf5vuCl""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""@jit\n"",
-    ""def vmap_batched_apply_matrix(batched_x):\n"",
-    ""  return vmap(apply_matrix, batched_x)\n"",
-    ""\n"",
-    ""print('Auto-vectorized with vmap')\n"",
-    ""%timeit vmap_batched_apply_matrix(batched_x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""pYVl3Z2nbZhO""
-   },
-   ""source"": [
-    ""Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation. Now, let's put it all together.\n"",
-    ""\n""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""xC1CMcVNYwxm""
-   },
-   ""source"": [
-    ""We've now used the whole of the JAX API: `grad` for derivatives, `jit` for speedups and `vmap` for auto-vectorization.\n"",
-    ""We used NumPy to specify all of our computation, and borrowed the great data loaders from PyTorch, and ran the whole thing on the GPU.""
-   ]
-  }
- ],
- ""metadata"": {
-  ""accelerator"": ""GPU"",
-  ""colab"": {
-   ""collapsed_sections"": [],
-   ""name"": ""JAX Quickstart.ipynb"",
-   ""provenance"": [],
-   ""toc_visible"": true,
-   ""version"": ""0.3.2""
-  },
-  ""kernelspec"": {
-   ""display_name"": ""Python 3"",
-   ""language"": ""python"",
-   ""name"": ""python3""
-  },
-  ""language_info"": {
-   ""codemirror_mode"": {
-    ""name"": ""ipython"",
-    ""version"": 3
-   },
-   ""file_extension"": "".py"",
-   ""mimetype"": ""text/x-python"",
-   ""name"": ""python"",
-   ""nbconvert_exporter"": ""python"",
-   ""pygments_lexer"": ""ipython3"",
-   ""version"": ""3.5.3""
-  }
- },
- ""nbformat"": 4,
- ""nbformat_minor"": 1
-}
+      ""name"": ""JAX Quickstart.ipynb"",
+      ""version"": ""0.3.2"",
+      ""provenance"": [],
+      ""collapsed_sections"": [],
+      ""toc_visible"": true
+    },
+    ""kernelspec"": {
+      ""display_name"": ""Python 3"",
+      ""language"": ""python"",
+      ""name"": ""python3""
+    },
+    ""accelerator"": ""GPU""
+  },
+  ""cells"": [
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""PaW85yP_BrCF"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""xtWX4x9DCF5_""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""# JAX Quickstart\n"",
+        ""Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary\n"",
+        ""\n"",
+        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_500px.png =250x144)\n"",
+        ""\n"",
+        ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
+        ""\n"",
+        ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
+        ""can automatically differentiate native Python and NumPy code. It can\n"",
+        ""differentiate through a large subset of Pythons features, including loops, ifs,\n"",
+        ""recursion, and closures, and it can even take derivatives of derivatives of\n"",
+        ""derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n"",
+        ""to any order.\n"",
+        ""\n"",
+        ""Whats new is that JAX uses\n"",
+        ""[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)\n"",
+        ""to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n"",
+        ""Compilation happens under the hood by default, with library calls getting\n"",
+        ""just-in-time compiled and executed. But JAX even lets you just-in-time compile\n"",
+        ""your own Python functions into XLA-optimized kernels using a one-function API.\n"",
+        ""Compilation and automatic differentiation can be composed arbitrarily, so you\n"",
+        ""can express sophisticated algorithms and get maximal performance without having\n"",
+        ""to leave Python.\n""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""SY8mDvEvCGqk"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""from __future__ import print_function, division\n"",
+        ""import jax.numpy as np\n"",
+        ""from jax import grad, jit, vmap\n"",
+        ""from jax import random""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""FQ89jHCYfhpg""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Multiplying Matrices""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""Xpy1dSgNqCP4""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see the readme.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""u0nseKZNqOoH"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""key = random.PRNGKey(0)\n"",
+        ""x = random.normal(key, (10,))\n"",
+        ""print(x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""hDJF0UPKnuqB""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Let's dive right in and multiply two big matrices.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""eXn8GUl6CG5N"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""size = 3000\n"",
+        ""x = random.normal(key, (size, size), dtype=np.float32)\n"",
+        ""%timeit np.dot(x, x.T)  # runs on the GPU""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""0AlN7EbonyaR""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX NumPy functions work on regular NumPy arrays.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""ZPl0MuwYrM7t"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""import numpy as onp  # original CPU-backed NumPy\n"",
+        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
+        ""%timeit np.dot(x, x.T)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""_SrcB2IurUuE""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""That's slower because it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""Jj7M7zyRskF0"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""from jax import device_put\n"",
+        ""\n"",
+        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
+        ""x = device_put(x)\n"",
+        ""%timeit np.dot(x, x.T)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""clO9djnen8qi""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""The output of `device_put` still acts like an NDArray. By the way, the [implementation](https://github.com/google/jax/blob/c8f93511ecb977d02fa5b0eee17075706fd6fd93/jax/api.py#L162) of `device_put` is just `device_put = jit(lambda x: x)`.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""ghkfKNQttDpg""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""If you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""RzXK8GnIs7VV"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
+        ""%timeit onp.dot(x, x.T)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""iOzp0P_GoJhb""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there's three main ones:\n"",
+        ""\n"",
+        "" - `jit`, for speeding up your code\n"",
+        "" - `grad`, for taking derivatives\n"",
+        "" - `vmap`, for automatic vectorization or batching.\n"",
+        ""\n"",
+        ""Let's go over these, one-by-one. We'll also end up composing these in interesting ways.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""bTTrTbWvgLUK""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Using `jit` to speed up functions""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""YrqE32mvE3b7""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md). Let's try that.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""qLGdCtFKFLOR"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def selu(x, alpha=1.67, lmbda=1.05):\n"",
+        ""  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n"",
+        ""\n"",
+        ""x = random.normal(key, (1000000,))\n"",
+        ""%timeit selu(x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""a_V8SruVHrD_""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We can speed it up with `@jit`, which will jit-compile the first time `selu` is called and will be cached thereafter.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""fh4w_3NpFYTp"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""selu_jit = jit(selu)\n"",
+        ""%timeit selu_jit(x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""HxpBc4WmfsEU""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Taking derivatives with `grad`\n"",
+        ""\n"",
+        ""In addition to evaluating numerical functions, we also want to transform them. One transformation is [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). In JAX, just like in [Autograd](https://github.com/HIPS/autograd), you can compute gradients with the `grad` function.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""IMAgNJaMJwPD"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def sum_logistic(x):\n"",
+        ""  return np.sum(1.0 / (1.0 + np.exp(-x)))\n"",
+        ""\n"",
+        ""x_small = np.arange(3.)\n"",
+        ""derivative_fn = grad(sum_logistic)\n"",
+        ""print(derivative_fn(x_small))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""PtNs881Ohioc""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Let's verify with finite differences that our result is correct.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""JXI7_OZuKZVO"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def first_finite_differences(f, x):\n"",
+        ""  eps = 1e-3\n"",
+        ""  return np.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n"",
+        ""                   for v in onp.eye(len(x))])\n"",
+        ""\n"",
+        ""\n"",
+        ""print(first_finite_differences(sum_logistic, x_small))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""Q2CUZjOWNZ-3""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""TO4g8ny-OEi4"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""yCJ5feKvhnBJ""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""Z-JxbiNyhxEW"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""from jax import jacfwd, jacrev\n"",
+        ""def hessian(fun):\n"",
+        ""  return jit(jacfwd(jacrev(fun)))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""TI4nPsGafxbL""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Auto-vectorization with `vmap`""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""PcxkONy5aius""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a functions primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""TPiX4y-bWLFS""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using `vmap`. Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""8w0Gpsn8WYYj"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""mat = random.normal(key, (150, 100))\n"",
+        ""batched_x = random.normal(key, (10, 100))\n"",
+        ""\n"",
+        ""def apply_matrix(v):\n"",
+        ""  return np.dot(mat, v)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""id"": ""0zWsc0RisQWx"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Given a function such as `apply_matrix`, we can loop over a batch dimension in Python, but usually the performance of doing so is poor.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""KWVc9BsZv0Ki"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def naively_batched_apply_matrix(v_batched):\n"",
+        ""  return np.stack([apply_matrix(v) for v in v_batched])\n"",
+        ""\n"",
+        ""print('Naively batched')\n"",
+        ""%timeit naively_batched_apply_matrix(batched_x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""id"": ""qHfKaLE9stbA"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We know how to batch this operation manually. In this case, `np.dot` handles extra batch dimensions transparently.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""ipei6l8nvrzH"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""@jit\n"",
+        ""def batched_apply_matrix(v_batched):\n"",
+        ""  return np.dot(v_batched, mat.T)\n"",
+        ""\n"",
+        ""print('Manually batched')\n"",
+        ""%timeit batched_apply_matrix(batched_x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""id"": ""1eF8Nhb-szAb"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""However, suppose we had a more complicated function without batching support. We can use `vmap` to add batching support automatically.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""67Oeknf5vuCl"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""@jit\n"",
+        ""def vmap_batched_apply_matrix(batched_x):\n"",
+        ""  return vmap(apply_matrix, batched_x)\n"",
+        ""\n"",
+        ""print('Auto-vectorized with vmap')\n"",
+        ""%timeit vmap_batched_apply_matrix(batched_x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""pYVl3Z2nbZhO""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""id"": ""WwNnjaI4th_8"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""This is just a taste of what JAX can do. We're really excited to see what you do with it!""
+      ]
+    }
+  ]
+}
\ No newline at end of file","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index d6c94a106..e1502e48a 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -1,572 +1,562 @@
 {
- ""cells"": [
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
+  ""nbformat"": 4,
+  ""nbformat_minor"": 0,
+  ""metadata"": {
     ""colab"": {
-     ""base_uri"": ""https://localhost:8080/"",
-     ""height"": 224
+      ""name"": ""JAX Quickstart.ipynb"",
+      ""version"": ""0.3.2"",
+      ""provenance"": [],
+      ""collapsed_sections"": [],
+      ""toc_visible"": true
     },
-    ""colab_type"": ""code"",
-    ""id"": ""PaW85yP_BrCF"",
-    ""outputId"": ""16fe6dee-a773-4889-fdb3-ebe75f48abe5""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
-   ]
+    ""kernelspec"": {
+      ""display_name"": ""Python 3"",
+      ""language"": ""python"",
+      ""name"": ""python3""
+    },
+    ""accelerator"": ""GPU""
   },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""xtWX4x9DCF5_""
-   },
-   ""source"": [
-    ""# JAX Quickstart\n"",
-    ""dougalm@, phawkins@, mattjj@, frostig@, alexbw@\n"",
-    ""\n"",
-    ""### TODO: LOGO\n"",
-    ""\n"",
-    ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
-    ""\n"",
-    ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
-    ""can automatically differentiate native Python and NumPy code. It can\n"",
-    ""differentiate through a large subset of Pythons features, including loops, ifs,\n"",
-    ""recursion, and closures, and it can even take derivatives of derivatives of\n"",
-    ""derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n"",
-    ""to any order.\n"",
-    ""\n"",
-    ""Whats new is that JAX uses\n"",
-    ""[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)\n"",
-    ""to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n"",
-    ""Compilation happens under the hood by default, with library calls getting\n"",
-    ""just-in-time compiled and executed. But JAX even lets you just-in-time compile\n"",
-    ""your own Python functions into XLA-optimized kernels using a one-function API.\n"",
-    ""Compilation and automatic differentiation can be composed arbitrarily, so you\n"",
-    ""can express sophisticated algorithms and get maximal performance without having\n"",
-    ""to leave Python.\n""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""roHm4XGpf3rl""
-   },
-   ""source"": [
-    ""## The basics of JAX""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""SY8mDvEvCGqk""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""from __future__ import print_function, division\n"",
-    ""import numpy as onp\n"",
-    ""from tqdm import tqdm\n"",
-    ""import jax.numpy as np\n"",
-    ""from jax import grad, jit, vmap\n"",
-    ""from jax import device_put\n"",
-    ""from jax import random""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""FQ89jHCYfhpg""
-   },
-   ""source"": [
-    ""### Multiplying Matrices""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""Xpy1dSgNqCP4""
-   },
-   ""source"": [
-    ""We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you ask for random numbers. We needed to make this change to support some of the great features we talk about below.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""u0nseKZNqOoH""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""key = random.PRNGKey(0)\n"",
-    ""x = random.normal(key, (10,))\n"",
-    ""print(x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""hDJF0UPKnuqB""
-   },
-   ""source"": [
-    ""Let's dive right in and multiply two big matrices.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""eXn8GUl6CG5N""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""size = 3000\n"",
-    ""x = random.normal(key, (size, size), dtype=np.float32)\n"",
-    ""%timeit np.dot(x, x.T)  # runs on the GPU""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""0AlN7EbonyaR""
-   },
-   ""source"": [
-    ""JAX NumPy functions work on raw NumPy arrays as well.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""ZPl0MuwYrM7t""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""import numpy as onp  # original CPU-backed NumPy\n"",
-    ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
-    ""%timeit np.dot(x, x.T)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""_SrcB2IurUuE""
-   },
-   ""source"": [
-    ""That's slower beacuse it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""Jj7M7zyRskF0""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
-    ""x = device_put(x)\n"",
-    ""%timeit np.dot(x, x.T)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""clO9djnen8qi""
-   },
-   ""source"": [
-    ""The output of `device_put` still acts like an NDArray. By the way, the implementation of `device_put` is just `device_put = jit(lambda x: x)`.""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""ghkfKNQttDpg""
-   },
-   ""source"": [
-    ""All of these calls above are faster than original NumPy on the CPU.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""RzXK8GnIs7VV""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
-    ""%timeit onp.dot(x, x.T)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""iOzp0P_GoJhb""
-   },
-   ""source"": [
-    ""JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numeric code. For now, there's three main ones:\n"",
-    ""\n"",
-    "" - `jit`, for speeding up your code\n"",
-    "" - `grad`, for taking derivatives\n"",
-    "" - `vmap`, for automatic vectorization or batching.\n"",
-    ""\n"",
-    ""Let's go over these, one-by-one. We'll also end up composing these in interesting ways.""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""bTTrTbWvgLUK""
-   },
-   ""source"": [
-    ""### Using `jit` to speed up functions""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""YrqE32mvE3b7""
-   },
-   ""source"": [
-    ""JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, JAX will incur overhead. Fortunately, JAX has a `@jit` decorator which will fuse multiple operations together. Let's try that.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""qLGdCtFKFLOR""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def selu_raw(x, alpha=1.67, lmbda=1.05):\n"",
-    ""  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n"",
-    ""\n"",
-    ""x = np.zeros(1000000)\n"",
-    ""%timeit selu_raw(x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""a_V8SruVHrD_""
-   },
-   ""source"": [
-    ""We can speed it up with @jit, which will jit-compile the first time `selu` is called and will be cached thereafter.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""fh4w_3NpFYTp""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""selu = jit(selu)\n"",
-    ""%timeit selu(x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""HxpBc4WmfsEU""
-   },
-   ""source"": [
-    ""### Taking derivatives with `grad`\n"",
-    ""\n"",
-    ""We don't just want to compute with NumPy arrays, we also want to tranform numeric programs, like by taking their derivative. In JAX, just like in Autograd, there is a one-function API for taking derivatives: the `grad` function.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""IMAgNJaMJwPD""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def sum_logistic(x):\n"",
-    ""  return np.sum(1.0 / (1.0 + np.exp(-x)))\n"",
-    ""\n"",
-    ""x_small = np.ones((3,))\n"",
-    ""derivative_fn = grad(sum_logistic)\n"",
-    ""print(derivative_fn(x_small))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""PtNs881Ohioc""
-   },
-   ""source"": [
-    ""Let's verify with finite differences that our result is correct.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""JXI7_OZuKZVO""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def first_finite_differences(f, x):\n"",
-    ""  eps = 1e-3\n"",
-    ""  return np.array([(f(x + eps * basis_vect) - f(x - eps * basis_vect)) / (2 * eps)\n"",
-    ""                   for basis_vect in onp.eye(len(x))])\n"",
-    ""\n"",
-    ""\n"",
-    ""print(first_finite_differences(sum_logistic, x_small))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""Q2CUZjOWNZ-3""
-   },
-   ""source"": [
-    ""Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""TO4g8ny-OEi4""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""yCJ5feKvhnBJ""
-   },
-   ""source"": [
-    ""For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. We used them with `vmap` (which we'll describe in a moment) to write `jax.jacfwd` and `jax.jacrev` for computing full Jacobian matrices. Here's one way to compose those to make a function that efficiently computes full Hessian matrices:""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""Z-JxbiNyhxEW""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""from jax import jacfwd, jacrev\n"",
-    ""def hessian(fun):\n"",
-    ""  return jacfwd(jacrev(fun))""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""TI4nPsGafxbL""
-   },
-   ""source"": [
-    ""### Auto-vectorization with `vmap`""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""PcxkONy5aius""
-   },
-   ""source"": [
-    ""JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a functions primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand.""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""TPiX4y-bWLFS""
-   },
-   ""source"": [
-    ""We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using vmap. Although this is trivial to do by hand in this specific case, the same technique can apply to more complicated functions.""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""cCQf0AAAa3ta""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def apply_matrix(v):\n"",
-    ""  return np.dot(mat, v)""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""8w0Gpsn8WYYj""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""mat = random.normal(key, (150, 100))\n"",
-    ""batched_x = random.normal(key, (10, 100))""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""KWVc9BsZv0Ki""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""def naively_batched_apply_matrix(v_batched):\n"",
-    ""  return np.stack([apply_matrix(v) for v in v_batched])\n"",
-    ""\n"",
-    ""print('Naively batched')\n"",
-    ""%timeit naively_batched_apply_matrix(batched_x)""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""ipei6l8nvrzH""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""@jit\n"",
-    ""def batched_apply_matrix(v_batched):\n"",
-    ""  return np.dot(v_batched, mat.T)\n"",
-    ""\n"",
-    ""print('Manually batched')\n"",
-    ""%timeit batched_apply_matrix(batched_x)""
-   ]
-  },
-  {
-   ""cell_type"": ""code"",
-   ""execution_count"": null,
-   ""metadata"": {
-    ""colab"": {},
-    ""colab_type"": ""code"",
-    ""id"": ""67Oeknf5vuCl""
-   },
-   ""outputs"": [],
-   ""source"": [
-    ""@jit\n"",
-    ""def vmap_batched_apply_matrix(batched_x):\n"",
-    ""  return vmap(apply_matrix, batched_x)\n"",
-    ""\n"",
-    ""print('Auto-vectorized with vmap')\n"",
-    ""%timeit vmap_batched_apply_matrix(batched_x)""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""pYVl3Z2nbZhO""
-   },
-   ""source"": [
-    ""Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation. Now, let's put it all together.\n"",
-    ""\n""
-   ]
-  },
-  {
-   ""cell_type"": ""markdown"",
-   ""metadata"": {
-    ""colab_type"": ""text"",
-    ""id"": ""xC1CMcVNYwxm""
-   },
-   ""source"": [
-    ""We've now used the whole of the JAX API: `grad` for derivatives, `jit` for speedups and `vmap` for auto-vectorization.\n"",
-    ""We used NumPy to specify all of our computation, and borrowed the great data loaders from PyTorch, and ran the whole thing on the GPU.""
-   ]
-  }
- ],
- ""metadata"": {
-  ""accelerator"": ""GPU"",
-  ""colab"": {
-   ""collapsed_sections"": [],
-   ""name"": ""JAX Quickstart.ipynb"",
-   ""provenance"": [],
-   ""toc_visible"": true,
-   ""version"": ""0.3.2""
-  },
-  ""kernelspec"": {
-   ""display_name"": ""Python 3"",
-   ""language"": ""python"",
-   ""name"": ""python3""
-  },
-  ""language_info"": {
-   ""codemirror_mode"": {
-    ""name"": ""ipython"",
-    ""version"": 3
-   },
-   ""file_extension"": "".py"",
-   ""mimetype"": ""text/x-python"",
-   ""name"": ""python"",
-   ""nbconvert_exporter"": ""python"",
-   ""pygments_lexer"": ""ipython3"",
-   ""version"": ""3.5.3""
-  }
- },
- ""nbformat"": 4,
- ""nbformat_minor"": 1
-}
+  ""cells"": [
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""PaW85yP_BrCF"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""xtWX4x9DCF5_""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""# JAX Quickstart\n"",
+        ""Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary\n"",
+        ""\n"",
+        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_500px.png =250x144)\n"",
+        ""\n"",
+        ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
+        ""\n"",
+        ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
+        ""can automatically differentiate native Python and NumPy code. It can\n"",
+        ""differentiate through a large subset of Pythons features, including loops, ifs,\n"",
+        ""recursion, and closures, and it can even take derivatives of derivatives of\n"",
+        ""derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n"",
+        ""to any order.\n"",
+        ""\n"",
+        ""Whats new is that JAX uses\n"",
+        ""[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)\n"",
+        ""to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n"",
+        ""Compilation happens under the hood by default, with library calls getting\n"",
+        ""just-in-time compiled and executed. But JAX even lets you just-in-time compile\n"",
+        ""your own Python functions into XLA-optimized kernels using a one-function API.\n"",
+        ""Compilation and automatic differentiation can be composed arbitrarily, so you\n"",
+        ""can express sophisticated algorithms and get maximal performance without having\n"",
+        ""to leave Python.\n""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""SY8mDvEvCGqk"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""from __future__ import print_function, division\n"",
+        ""import jax.numpy as np\n"",
+        ""from jax import grad, jit, vmap\n"",
+        ""from jax import random""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""FQ89jHCYfhpg""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Multiplying Matrices""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""Xpy1dSgNqCP4""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see the readme.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""u0nseKZNqOoH"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""key = random.PRNGKey(0)\n"",
+        ""x = random.normal(key, (10,))\n"",
+        ""print(x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""hDJF0UPKnuqB""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Let's dive right in and multiply two big matrices.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""eXn8GUl6CG5N"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""size = 3000\n"",
+        ""x = random.normal(key, (size, size), dtype=np.float32)\n"",
+        ""%timeit np.dot(x, x.T)  # runs on the GPU""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""0AlN7EbonyaR""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX NumPy functions work on regular NumPy arrays.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""ZPl0MuwYrM7t"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""import numpy as onp  # original CPU-backed NumPy\n"",
+        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
+        ""%timeit np.dot(x, x.T)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""_SrcB2IurUuE""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""That's slower because it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""Jj7M7zyRskF0"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""from jax import device_put\n"",
+        ""\n"",
+        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
+        ""x = device_put(x)\n"",
+        ""%timeit np.dot(x, x.T)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""clO9djnen8qi""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""The output of `device_put` still acts like an NDArray. By the way, the [implementation](https://github.com/google/jax/blob/c8f93511ecb977d02fa5b0eee17075706fd6fd93/jax/api.py#L162) of `device_put` is just `device_put = jit(lambda x: x)`.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""ghkfKNQttDpg""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""If you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""RzXK8GnIs7VV"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""x = onp.random.normal(size=(size, size)).astype(onp.float32)\n"",
+        ""%timeit onp.dot(x, x.T)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""iOzp0P_GoJhb""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there's three main ones:\n"",
+        ""\n"",
+        "" - `jit`, for speeding up your code\n"",
+        "" - `grad`, for taking derivatives\n"",
+        "" - `vmap`, for automatic vectorization or batching.\n"",
+        ""\n"",
+        ""Let's go over these, one-by-one. We'll also end up composing these in interesting ways.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""bTTrTbWvgLUK""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Using `jit` to speed up functions""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""YrqE32mvE3b7""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md). Let's try that.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""qLGdCtFKFLOR"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def selu(x, alpha=1.67, lmbda=1.05):\n"",
+        ""  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n"",
+        ""\n"",
+        ""x = random.normal(key, (1000000,))\n"",
+        ""%timeit selu(x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""a_V8SruVHrD_""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We can speed it up with `@jit`, which will jit-compile the first time `selu` is called and will be cached thereafter.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""fh4w_3NpFYTp"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""selu_jit = jit(selu)\n"",
+        ""%timeit selu_jit(x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""HxpBc4WmfsEU""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Taking derivatives with `grad`\n"",
+        ""\n"",
+        ""In addition to evaluating numerical functions, we also want to transform them. One transformation is [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). In JAX, just like in [Autograd](https://github.com/HIPS/autograd), you can compute gradients with the `grad` function.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""IMAgNJaMJwPD"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def sum_logistic(x):\n"",
+        ""  return np.sum(1.0 / (1.0 + np.exp(-x)))\n"",
+        ""\n"",
+        ""x_small = np.arange(3.)\n"",
+        ""derivative_fn = grad(sum_logistic)\n"",
+        ""print(derivative_fn(x_small))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""PtNs881Ohioc""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Let's verify with finite differences that our result is correct.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""JXI7_OZuKZVO"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def first_finite_differences(f, x):\n"",
+        ""  eps = 1e-3\n"",
+        ""  return np.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n"",
+        ""                   for v in onp.eye(len(x))])\n"",
+        ""\n"",
+        ""\n"",
+        ""print(first_finite_differences(sum_logistic, x_small))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""Q2CUZjOWNZ-3""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""TO4g8ny-OEi4"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""yCJ5feKvhnBJ""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""Z-JxbiNyhxEW"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""from jax import jacfwd, jacrev\n"",
+        ""def hessian(fun):\n"",
+        ""  return jit(jacfwd(jacrev(fun)))""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""TI4nPsGafxbL""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""### Auto-vectorization with `vmap`""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""PcxkONy5aius""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a functions primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""TPiX4y-bWLFS""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using `vmap`. Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""8w0Gpsn8WYYj"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""mat = random.normal(key, (150, 100))\n"",
+        ""batched_x = random.normal(key, (10, 100))\n"",
+        ""\n"",
+        ""def apply_matrix(v):\n"",
+        ""  return np.dot(mat, v)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""id"": ""0zWsc0RisQWx"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Given a function such as `apply_matrix`, we can loop over a batch dimension in Python, but usually the performance of doing so is poor.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""KWVc9BsZv0Ki"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""def naively_batched_apply_matrix(v_batched):\n"",
+        ""  return np.stack([apply_matrix(v) for v in v_batched])\n"",
+        ""\n"",
+        ""print('Naively batched')\n"",
+        ""%timeit naively_batched_apply_matrix(batched_x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""id"": ""qHfKaLE9stbA"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""We know how to batch this operation manually. In this case, `np.dot` handles extra batch dimensions transparently.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""ipei6l8nvrzH"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""@jit\n"",
+        ""def batched_apply_matrix(v_batched):\n"",
+        ""  return np.dot(v_batched, mat.T)\n"",
+        ""\n"",
+        ""print('Manually batched')\n"",
+        ""%timeit batched_apply_matrix(batched_x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""id"": ""1eF8Nhb-szAb"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""However, suppose we had a more complicated function without batching support. We can use `vmap` to add batching support automatically.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""67Oeknf5vuCl"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""@jit\n"",
+        ""def vmap_batched_apply_matrix(batched_x):\n"",
+        ""  return vmap(apply_matrix, batched_x)\n"",
+        ""\n"",
+        ""print('Auto-vectorized with vmap')\n"",
+        ""%timeit vmap_batched_apply_matrix(batched_x)""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
+    {
+      ""metadata"": {
+        ""colab_type"": ""text"",
+        ""id"": ""pYVl3Z2nbZhO""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation.""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""id"": ""WwNnjaI4th_8"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""This is just a taste of what JAX can do. We're really excited to see what you do with it!""
+      ]
+    }
+  ]
+}
\ No newline at end of file",Yes
images/jax_logo_250px.png,images/jax_logo_250px.png,6f2b42bf4cd76c6fc7da4fc2a1f24db11a6eb9d9,3857762931922ffd1e7d6190f9967bcb6d2cf5c2,Fixing logo size so resize is not required,"diff --git a/images/jax_logo_250px.png b/images/jax_logo_250px.png
new file mode 100644
index 000000000..92a545225
Binary files /dev/null and b/images/jax_logo_250px.png differ","diff --git a/images/jax_logo_250px.png b/images/jax_logo_250px.png
new file mode 100644
index 000000000..92a545225
Binary files /dev/null and b/images/jax_logo_250px.png differ",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,6f2b42bf4cd76c6fc7da4fc2a1f24db11a6eb9d9,3857762931922ffd1e7d6190f9967bcb6d2cf5c2,Fixing logo size so resize is not required,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index e1502e48a..95e6fe470 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -40,7 +40,7 @@
         ""# JAX Quickstart\n"",
         ""Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary\n"",
         ""\n"",
-        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_500px.png =250x144)\n"",
+        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
         ""\n"",
         ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
         ""\n"",
@@ -559,4 +559,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index e1502e48a..95e6fe470 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -40,7 +40,7 @@
         ""# JAX Quickstart\n"",
         ""Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary\n"",
         ""\n"",
-        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_500px.png =250x144)\n"",
+        ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
         ""\n"",
         ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
         ""\n"",
@@ -559,4 +559,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,d673d78131a461b2f496ef05da782c8d778193a3,ae1a3fc04786a6445641d8768543775c2ef1956c,Add copyright notice to quickstart notebook.,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 95e6fe470..68df26850 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -19,16 +19,35 @@
   ""cells"": [
     {
       ""metadata"": {
-        ""colab_type"": ""code"",
-        ""id"": ""PaW85yP_BrCF"",
-        ""colab"": {}
+        ""id"": ""logZcM_HEnve"",
+        ""colab_type"": ""text""
       },
-      ""cell_type"": ""code"",
+      ""cell_type"": ""markdown"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
-      ],
-      ""execution_count"": 0,
-      ""outputs"": []
+        ""##### Copyright 2018 Google LLC.\n"",
+        ""\n"",
+        ""Licensed under the Apache License, Version 2.0 (the \""License\"");""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""id"": ""QwN47xiBEsKz"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
+        ""you may not use this file except in compliance with the License.\n"",
+        ""You may obtain a copy of the License at\n"",
+        ""\n"",
+        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
+        ""\n"",
+        ""Unless required by applicable law or agreed to in writing, software\n"",
+        ""distributed under the License is distributed on an \""AS IS\"" BASIS,\n"",
+        ""WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n"",
+        ""See the License for the specific language governing permissions and\n"",
+        ""limitations under the License.""
+      ]
     },
     {
       ""metadata"": {
@@ -62,6 +81,19 @@
         ""to leave Python.\n""
       ]
     },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""PaW85yP_BrCF"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
     {
       ""metadata"": {
         ""colab_type"": ""code"",","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 95e6fe470..68df26850 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -19,16 +19,35 @@
   ""cells"": [
     {
       ""metadata"": {
-        ""colab_type"": ""code"",
-        ""id"": ""PaW85yP_BrCF"",
-        ""colab"": {}
+        ""id"": ""logZcM_HEnve"",
+        ""colab_type"": ""text""
       },
-      ""cell_type"": ""code"",
+      ""cell_type"": ""markdown"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
-      ],
-      ""execution_count"": 0,
-      ""outputs"": []
+        ""##### Copyright 2018 Google LLC.\n"",
+        ""\n"",
+        ""Licensed under the Apache License, Version 2.0 (the \""License\"");""
+      ]
+    },
+    {
+      ""metadata"": {
+        ""id"": ""QwN47xiBEsKz"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""Licensed under the Apache License, Version 2.0 (the \""License\"");\n"",
+        ""you may not use this file except in compliance with the License.\n"",
+        ""You may obtain a copy of the License at\n"",
+        ""\n"",
+        ""https://www.apache.org/licenses/LICENSE-2.0\n"",
+        ""\n"",
+        ""Unless required by applicable law or agreed to in writing, software\n"",
+        ""distributed under the License is distributed on an \""AS IS\"" BASIS,\n"",
+        ""WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n"",
+        ""See the License for the specific language governing permissions and\n"",
+        ""limitations under the License.""
+      ]
     },
     {
       ""metadata"": {
@@ -62,6 +81,19 @@
         ""to leave Python.\n""
       ]
     },
+    {
+      ""metadata"": {
+        ""colab_type"": ""code"",
+        ""id"": ""PaW85yP_BrCF"",
+        ""colab"": {}
+      },
+      ""cell_type"": ""code"",
+      ""source"": [
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+      ],
+      ""execution_count"": 0,
+      ""outputs"": []
+    },
     {
       ""metadata"": {
         ""colab_type"": ""code"",",No
jax/api.py,jax/api.py,50624bd97819dc9bcc816fef2aa2bc7f8e2d3f4b,c8f93511ecb977d02fa5b0eee17075706fd6fd93,"rename in_bdims, out_bdims --> in_axes, out_axes","diff --git a/jax/api.py b/jax/api.py
index 70e290003..f5ba279c5 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -65,7 +65,7 @@ def jacfwd(fun, x):
   fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
-  y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+  y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 @curry
@@ -73,26 +73,26 @@ def jacrev(fun, x):
   fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-  jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+  jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
 def vmap(fun, *args, **kwargs):
-  in_bdims = kwargs.pop(""in_bdims"", 0)
-  out_bdim = kwargs.pop(""out_bdim"", 0)
+  in_axes = kwargs.pop(""in_axes"", 0)
+  out_axes = kwargs.pop(""out_axes"", 0)
   if kwargs:
-    msg = ""vmap keyword args must be 'in_bdims' and/or 'out_bdim', got {}.""
+    msg = ""vmap keyword args must be 'in_axes' and/or 'out_axes', got {}.""
     raise TypeError(msg.format(', '.join(kwargs)))
 
-  if type(in_bdims) is int:
-    in_bdims = (in_bdims,) * len(args)
+  if type(in_axes) is int:
+    in_axes = (in_axes,) * len(args)
   if not isinstance(fun, lu.WrappedFun):
     fun = lu.wrap_init(fun)
   in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
   flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_flat = batching.batch(flat_fun, in_flat, in_bdims, out_bdim)
+  out_flat = batching.batch(flat_fun, in_flat, in_axes, out_axes)
   return build_tree(out_tree(), out_flat)
 
 def jvp(fun, primals, tangents):","diff --git a/jax/api.py b/jax/api.py
index 70e290003..f5ba279c5 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -65,7 +65,7 @@ def jacfwd(fun, x):
   fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
-  y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+  y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 @curry
@@ -73,26 +73,26 @@ def jacrev(fun, x):
   fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-  jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+  jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
 def vmap(fun, *args, **kwargs):
-  in_bdims = kwargs.pop(""in_bdims"", 0)
-  out_bdim = kwargs.pop(""out_bdim"", 0)
+  in_axes = kwargs.pop(""in_axes"", 0)
+  out_axes = kwargs.pop(""out_axes"", 0)
   if kwargs:
-    msg = ""vmap keyword args must be 'in_bdims' and/or 'out_bdim', got {}.""
+    msg = ""vmap keyword args must be 'in_axes' and/or 'out_axes', got {}.""
     raise TypeError(msg.format(', '.join(kwargs)))
 
-  if type(in_bdims) is int:
-    in_bdims = (in_bdims,) * len(args)
+  if type(in_axes) is int:
+    in_axes = (in_axes,) * len(args)
   if not isinstance(fun, lu.WrappedFun):
     fun = lu.wrap_init(fun)
   in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
   flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_flat = batching.batch(flat_fun, in_flat, in_bdims, out_bdim)
+  out_flat = batching.batch(flat_fun, in_flat, in_axes, out_axes)
   return build_tree(out_tree(), out_flat)
 
 def jvp(fun, primals, tangents):",No
tests/batching_test.py,tests/batching_test.py,50624bd97819dc9bcc816fef2aa2bc7f8e2d3f4b,c8f93511ecb977d02fa5b0eee17075706fd6fd93,"rename in_bdims, out_bdims --> in_axes, out_axes","diff --git a/tests/batching_test.py b/tests/batching_test.py
index fd372bc47..23d507279 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -42,10 +42,10 @@ class BatchingTest(jtu.JaxTestCase):
 
   def testNestedBatchingMatMat(self):
     def matvec(A, b):
-      return vmap(np.vdot, A, b, in_bdims=(0, None))
+      return vmap(np.vdot, A, b, in_axes=(0, None))
 
     def matmat(A, B):
-      return vmap(matvec, A, B, in_bdims=(None, 1), out_bdim=1)
+      return vmap(matvec, A, B, in_axes=(None, 1), out_axes=1)
 
     R = onp.random.RandomState(0).randn
     A = R(4, 3)
@@ -107,13 +107,13 @@ class BatchingTest(jtu.JaxTestCase):
     def jacbwd(f, x):
       y, pullback = vjp(f, x)
       std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-      jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+      jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     def jacfwd(f, x):
       pushfwd = lambda v: jvp(f, (x,), (v,))
       std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
-      y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+      y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     R = onp.random.RandomState(0).randn","diff --git a/tests/batching_test.py b/tests/batching_test.py
index fd372bc47..23d507279 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -42,10 +42,10 @@ class BatchingTest(jtu.JaxTestCase):
 
   def testNestedBatchingMatMat(self):
     def matvec(A, b):
-      return vmap(np.vdot, A, b, in_bdims=(0, None))
+      return vmap(np.vdot, A, b, in_axes=(0, None))
 
     def matmat(A, B):
-      return vmap(matvec, A, B, in_bdims=(None, 1), out_bdim=1)
+      return vmap(matvec, A, B, in_axes=(None, 1), out_axes=1)
 
     R = onp.random.RandomState(0).randn
     A = R(4, 3)
@@ -107,13 +107,13 @@ class BatchingTest(jtu.JaxTestCase):
     def jacbwd(f, x):
       y, pullback = vjp(f, x)
       std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-      jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+      jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     def jacfwd(f, x):
       pushfwd = lambda v: jvp(f, (x,), (v,))
       std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
-      y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+      y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     R = onp.random.RandomState(0).randn",No
build/Dockerfile,build/Dockerfile,d4f32dff7b9c9dd58f0e34d1cf2cd738812f03bd,e93f682d58e947426f36a7aa1f6f293192bab8f3,update build script for split packages,"diff --git a/build/Dockerfile b/build/Dockerfile
index 3b02bf44d..ecde7c269 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -13,7 +13,7 @@ WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/744ac4821afafaff86b0f1820f05ac7c1186e276/build/build_wheel_docker_entrypoint.sh
+RUN curl -O https://raw.githubusercontent.com/google/jax/c34a48bd86745ab4a221be058f725597d43c75eb/build/build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build","diff --git a/build/Dockerfile b/build/Dockerfile
index 3b02bf44d..ecde7c269 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -13,7 +13,7 @@ WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/744ac4821afafaff86b0f1820f05ac7c1186e276/build/build_wheel_docker_entrypoint.sh
+RUN curl -O https://raw.githubusercontent.com/google/jax/c34a48bd86745ab4a221be058f725597d43c75eb/build/build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build",No
build/build.py,build/build.py,d4f32dff7b9c9dd58f0e34d1cf2cd738812f03bd,e93f682d58e947426f36a7aa1f6f293192bab8f3,update build script for split packages,"diff --git a/build/build.py b/build/build.py
index 4e75a6afd..dd2ba18ef 100755
--- a/build/build.py
+++ b/build/build.py
@@ -245,7 +245,7 @@ def main():
   args = parser.parse_args()
 
   print(BANNER)
-  os.chdir(os.path.dirname(__file__))
+  os.chdir(os.path.dirname(__file__ or args.prog) or '.')
 
   # Find a working Bazel.
   bazel_path = get_bazel_path(args.bazel_path)","diff --git a/build/build.py b/build/build.py
index 4e75a6afd..dd2ba18ef 100755
--- a/build/build.py
+++ b/build/build.py
@@ -245,7 +245,7 @@ def main():
   args = parser.parse_args()
 
   print(BANNER)
-  os.chdir(os.path.dirname(__file__))
+  os.chdir(os.path.dirname(__file__ or args.prog) or '.')
 
   # Find a working Bazel.
   bazel_path = get_bazel_path(args.bazel_path)",No
build/build_wheel_docker_entrypoint.sh,build/build_wheel_docker_entrypoint.sh,d4f32dff7b9c9dd58f0e34d1cf2cd738812f03bd,e93f682d58e947426f36a7aa1f6f293192bab8f3,update build script for split packages,"diff --git a/build/build_wheel_docker_entrypoint.sh b/build/build_wheel_docker_entrypoint.sh
index 12e110e6c..55757d898 100755
--- a/build/build_wheel_docker_entrypoint.sh
+++ b/build/build_wheel_docker_entrypoint.sh
@@ -1,4 +1,5 @@
-#!/bin/bash -xev
+#!/bin/bash
+set -xev
 if [ ! -d ""/dist"" ]
 then
   echo ""/dist must be mounted to produce output""
@@ -6,10 +7,10 @@ then
 fi
 
 git clone -b binary-distros https://github.com/google/jax /build/jax
-cd /build/jax
+cd /build/jax/build
 
 usage() {
-  echo ""usage: ${0##*/} [python2|python3] [cuda-included|cuda|nocuda]""
+  echo ""usage: ${0##*/} [py2|py3] [cuda-included|cuda|nocuda]""
   exit 1
 }
 
@@ -19,10 +20,10 @@ then
 fi
 
 case $1 in
-  py2)
+  py3)
     update-alternatives --install /usr/bin/python python /usr/bin/python3 10
     ;;
-  py3)
+  py2)
     ;;
   *)
     usage
@@ -31,7 +32,7 @@ esac
 case $2 in
   cuda-included)
     python build.py --enable_cuda --cudnn_path /usr/lib/x86_64-linux-gnu/
-    python build/include_cuda.py
+    python include_cuda.py
     ;;
   cuda)
     python build.py --enable_cuda --cudnn_path /usr/lib/x86_64-linux-gnu/
@@ -43,5 +44,5 @@ case $2 in
     usage
 esac
 
-python setup.py bdist bdist_wheel
+python setup.py bdist_wheel
 cp -r dist/* /dist","diff --git a/build/build_wheel_docker_entrypoint.sh b/build/build_wheel_docker_entrypoint.sh
index 12e110e6c..55757d898 100755
--- a/build/build_wheel_docker_entrypoint.sh
+++ b/build/build_wheel_docker_entrypoint.sh
@@ -1,4 +1,5 @@
-#!/bin/bash -xev
+#!/bin/bash
+set -xev
 if [ ! -d ""/dist"" ]
 then
   echo ""/dist must be mounted to produce output""
@@ -6,10 +7,10 @@ then
 fi
 
 git clone -b binary-distros https://github.com/google/jax /build/jax
-cd /build/jax
+cd /build/jax/build
 
 usage() {
-  echo ""usage: ${0##*/} [python2|python3] [cuda-included|cuda|nocuda]""
+  echo ""usage: ${0##*/} [py2|py3] [cuda-included|cuda|nocuda]""
   exit 1
 }
 
@@ -19,10 +20,10 @@ then
 fi
 
 case $1 in
-  py2)
+  py3)
     update-alternatives --install /usr/bin/python python /usr/bin/python3 10
     ;;
-  py3)
+  py2)
     ;;
   *)
     usage
@@ -31,7 +32,7 @@ esac
 case $2 in
   cuda-included)
     python build.py --enable_cuda --cudnn_path /usr/lib/x86_64-linux-gnu/
-    python build/include_cuda.py
+    python include_cuda.py
     ;;
   cuda)
     python build.py --enable_cuda --cudnn_path /usr/lib/x86_64-linux-gnu/
@@ -43,5 +44,5 @@ case $2 in
     usage
 esac
 
-python setup.py bdist bdist_wheel
+python setup.py bdist_wheel
 cp -r dist/* /dist",No
images/lifecycle.png,images/lifecycle.png,3efe36917ab71752a6b960636df7299f0437dbea,9fd120113d4d802698f7cbf46fbaf7a7ca0e65da,add simplified lifecycle image,"diff --git a/images/lifecycle.png b/images/lifecycle.png
new file mode 100644
index 000000000..fefabb0c9
Binary files /dev/null and b/images/lifecycle.png differ","diff --git a/images/lifecycle.png b/images/lifecycle.png
new file mode 100644
index 000000000..fefabb0c9
Binary files /dev/null and b/images/lifecycle.png differ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,18920337f30bd6727062382024746d1d3f7b6fbb,3efe36917ab71752a6b960636df7299f0437dbea,add eye in lax_numpy,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 4bf144320..f11d67aaa 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -616,6 +616,7 @@ def allclose(a, b, rtol=1e-05, atol=1e-08):
 
 
 arange = onp.arange
+eye = onp.eye
 
 
 @_wraps(onp.stack)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 4bf144320..f11d67aaa 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -616,6 +616,7 @@ def allclose(a, b, rtol=1e-05, atol=1e-08):
 
 
 arange = onp.arange
+eye = onp.eye
 
 
 @_wraps(onp.stack)",No
build/Dockerfile,build/Dockerfile,f6d31ad5f1309ce1fd28422379f369744d16cbb0,18920337f30bd6727062382024746d1d3f7b6fbb,update dockerfile entrypoint script git hash,"diff --git a/build/Dockerfile b/build/Dockerfile
index ecde7c269..e637a3e00 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -13,7 +13,7 @@ WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/c34a48bd86745ab4a221be058f725597d43c75eb/build/build_wheel_docker_entrypoint.sh
+RUN curl -O https://raw.githubusercontent.com/google/jax/18920337f30bd6727062382024746d1d3f7b6fbb/build/build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build","diff --git a/build/Dockerfile b/build/Dockerfile
index ecde7c269..e637a3e00 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -13,7 +13,7 @@ WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/c34a48bd86745ab4a221be058f725597d43c75eb/build/build_wheel_docker_entrypoint.sh
+RUN curl -O https://raw.githubusercontent.com/google/jax/18920337f30bd6727062382024746d1d3f7b6fbb/build/build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build",No
build/setup.py,build/setup.py,18201fab7bb234e35e2878ab0347a9342a7deac3,c8348c0070e0df143e026f20dfcf8b24b48fee31,bump version number for jaxlib,"diff --git a/build/setup.py b/build/setup.py
index 80a0246c5..fb08bafad 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.0',
+    version='0.1',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index 80a0246c5..fb08bafad 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.0',
+    version='0.1',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
build/build_wheels.sh,build/build_jaxlib_wheels.sh,91f8247d32c2e2af800f312531fa4e125ac1e7e8,18201fab7bb234e35e2878ab0347a9342a7deac3,update wheel name,"diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
new file mode 100755
index 000000000..32125c59f
--- /dev/null
+++ b/build/build_jaxlib_wheels.sh
@@ -0,0 +1,29 @@
+#!/bin/bash -xev
+JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
+
+PYTHON_VERSIONS=""py2 py3""
+CUDA_VERSIONS=""9.2""  # ""9.2 10.0""
+CUDA_VARIANTS=""cuda""  # ""cuda cuda-included""
+
+mkdir -p dist
+for CUDA_VERSION in $CUDA_VERSIONS
+do
+  docker build -t jaxbuild jax/build/ --build-arg CUDA_VERSION=$CUDA_VERSION
+
+  for PYTHON_VERSION in $PYTHON_VERSIONS
+  do
+    mkdir -p dist/nocuda/
+    nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
+    mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+
+    for CUDA_VARIANT in $CUDA_VARIANTS
+    do
+      mkdir -p dist/cuda${CUDA_VERSION//.}
+      nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
+      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+    done
+  done
+done
+
+echo ""now you might want to run something like:""
+echo ""python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/nocuda/*.whl --verbose""","diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
new file mode 100755
index 000000000..32125c59f
--- /dev/null
+++ b/build/build_jaxlib_wheels.sh
@@ -0,0 +1,29 @@
+#!/bin/bash -xev
+JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
+
+PYTHON_VERSIONS=""py2 py3""
+CUDA_VERSIONS=""9.2""  # ""9.2 10.0""
+CUDA_VARIANTS=""cuda""  # ""cuda cuda-included""
+
+mkdir -p dist
+for CUDA_VERSION in $CUDA_VERSIONS
+do
+  docker build -t jaxbuild jax/build/ --build-arg CUDA_VERSION=$CUDA_VERSION
+
+  for PYTHON_VERSION in $PYTHON_VERSIONS
+  do
+    mkdir -p dist/nocuda/
+    nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
+    mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+
+    for CUDA_VARIANT in $CUDA_VARIANTS
+    do
+      mkdir -p dist/cuda${CUDA_VERSION//.}
+      nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
+      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+    done
+  done
+done
+
+echo ""now you might want to run something like:""
+echo ""python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/nocuda/*.whl --verbose""",No
build/build_wheel_docker_entrypoint.sh,build/build_wheel_docker_entrypoint.sh,762abcf29b4a155c3de325c27ecffa5d4a3da28c,91f8247d32c2e2af800f312531fa4e125ac1e7e8,update wheel builder docker entrypoint script,"diff --git a/build/build_wheel_docker_entrypoint.sh b/build/build_wheel_docker_entrypoint.sh
index 55757d898..afc69d211 100755
--- a/build/build_wheel_docker_entrypoint.sh
+++ b/build/build_wheel_docker_entrypoint.sh
@@ -6,7 +6,7 @@ then
   exit 1
 fi
 
-git clone -b binary-distros https://github.com/google/jax /build/jax
+git clone https://github.com/google/jax /build/jax
 cd /build/jax/build
 
 usage() {","diff --git a/build/build_wheel_docker_entrypoint.sh b/build/build_wheel_docker_entrypoint.sh
index 55757d898..afc69d211 100755
--- a/build/build_wheel_docker_entrypoint.sh
+++ b/build/build_wheel_docker_entrypoint.sh
@@ -6,7 +6,7 @@ then
   exit 1
 fi
 
-git clone -b binary-distros https://github.com/google/jax /build/jax
+git clone https://github.com/google/jax /build/jax
 cd /build/jax/build
 
 usage() {",No
build/Dockerfile,build/Dockerfile,8a90d37da2e23fc1f7b962211813669000c6c49f,762abcf29b4a155c3de325c27ecffa5d4a3da28c,update wheel building dockerfile,"diff --git a/build/Dockerfile b/build/Dockerfile
index e637a3e00..e20c320b2 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -13,7 +13,7 @@ WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/18920337f30bd6727062382024746d1d3f7b6fbb/build/build_wheel_docker_entrypoint.sh
+RUN curl -O https://raw.githubusercontent.com/google/jax/762abcf29b4a155c3de325c27ecffa5d4a3da28c/build/build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build","diff --git a/build/Dockerfile b/build/Dockerfile
index e637a3e00..e20c320b2 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -13,7 +13,7 @@ WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/18920337f30bd6727062382024746d1d3f7b6fbb/build/build_wheel_docker_entrypoint.sh
+RUN curl -O https://raw.githubusercontent.com/google/jax/762abcf29b4a155c3de325c27ecffa5d4a3da28c/build/build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build",No
build/WORKSPACE,build/WORKSPACE,7093c4740f5fd4e35281e89ef8574b7866cf722a,bd8bef6849e917666032ab04e796d55fe4c0e8bf,revive WORKSPACE file,"diff --git a/build/WORKSPACE b/build/WORKSPACE
index e69de29bb..0df920561 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -0,0 +1,36 @@
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
+    urls = [
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/build/WORKSPACE b/build/WORKSPACE
index e69de29bb..0df920561 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -0,0 +1,36 @@
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
+    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
+    urls = [
+        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
build/build_jaxlib_wheels.sh,build/build_jaxlib_wheels.sh,7093c4740f5fd4e35281e89ef8574b7866cf722a,bd8bef6849e917666032ab04e796d55fe4c0e8bf,revive WORKSPACE file,"diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 32125c59f..4456bfa89 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -1,4 +1,5 @@
-#!/bin/bash -xev
+#!/bin/bash
+set -xev
 JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
 
 PYTHON_VERSIONS=""py2 py3""","diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 32125c59f..4456bfa89 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -1,4 +1,5 @@
-#!/bin/bash -xev
+#!/bin/bash
+set -xev
 JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
 
 PYTHON_VERSIONS=""py2 py3""",No
build/build_jaxlib_wheels.sh,build/build_jaxlib_wheels.sh,a9640b3ebbcc537791a7a34b2c4ad50313270895,7093c4740f5fd4e35281e89ef8574b7866cf722a,tweak wheel names in build script,"diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 4456bfa89..31cbf27a9 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -21,7 +21,7 @@ do
     do
       mkdir -p dist/cuda${CUDA_VERSION//.}
       nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
-      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
     done
   done
 done","diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 4456bfa89..31cbf27a9 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -21,7 +21,7 @@ do
     do
       mkdir -p dist/cuda${CUDA_VERSION//.}
       nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
-      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
     done
   done
 done",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,95eb58f448dba0bcec471a2301902c6588fa552c,a9640b3ebbcc537791a7a34b2c4ad50313270895,Implement np.repeat() for scalar repeats.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7a65c63c7..12a816edc 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -739,6 +739,35 @@ def ones(shape, dtype=onp.dtype(""float64"")):
   return onp.broadcast_to(onp.ones((), dtype), tuple(shape))
 
 
+@_wraps(onp.repeat)
+def repeat(a, repeats, axis=None):
+  if not isscalar(repeats):
+    raise NotImplementedError(
+        ""np.repeat implementation only supports scalar repeats"")
+  if axis is None or isscalar(a):
+    a = ravel(a)
+    axis = 0
+  a_shape = list(shape(a))
+  num_dims = len(a_shape)
+  if axis < 0:
+    axis = axis + num_dims
+
+  if axis < 0 or axis >= num_dims:
+    raise ValueError(
+        ""axis {} is out of bounds for array of dimension {}"".format(
+            axis, num_dims))
+
+  # Broadcasts to [..., X, repeats, ...] and reshapes to [..., X * repeats, ...]
+  broadcast_shape = list(a_shape)
+  broadcast_shape.insert(axis + 1, repeats)
+  broadcast_dims = onp.concatenate((onp.arange(0, axis + 1),
+                                    onp.arange(axis + 2, num_dims + 1)))
+  a_shape[axis] *= repeats
+  return lax.reshape(
+      lax.broadcast_in_dim(a, broadcast_shape, broadcast_dims),
+      a_shape)
+
+
 ### Tensor contraction operations
 
 
@@ -836,7 +865,6 @@ insert = _not_implemented(onp.insert)
 linspace = _not_implemented(onp.linspace)
 nonzero = _not_implemented(onp.nonzero)
 ptp = _not_implemented(onp.ptp)
-repeat = _not_implemented(onp.repeat)
 searchsorted = _not_implemented(onp.searchsorted)
 take = _not_implemented(onp.take)
 trace = _not_implemented(onp.trace)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7a65c63c7..12a816edc 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -739,6 +739,35 @@ def ones(shape, dtype=onp.dtype(""float64"")):
   return onp.broadcast_to(onp.ones((), dtype), tuple(shape))
 
 
+@_wraps(onp.repeat)
+def repeat(a, repeats, axis=None):
+  if not isscalar(repeats):
+    raise NotImplementedError(
+        ""np.repeat implementation only supports scalar repeats"")
+  if axis is None or isscalar(a):
+    a = ravel(a)
+    axis = 0
+  a_shape = list(shape(a))
+  num_dims = len(a_shape)
+  if axis < 0:
+    axis = axis + num_dims
+
+  if axis < 0 or axis >= num_dims:
+    raise ValueError(
+        ""axis {} is out of bounds for array of dimension {}"".format(
+            axis, num_dims))
+
+  # Broadcasts to [..., X, repeats, ...] and reshapes to [..., X * repeats, ...]
+  broadcast_shape = list(a_shape)
+  broadcast_shape.insert(axis + 1, repeats)
+  broadcast_dims = onp.concatenate((onp.arange(0, axis + 1),
+                                    onp.arange(axis + 2, num_dims + 1)))
+  a_shape[axis] *= repeats
+  return lax.reshape(
+      lax.broadcast_in_dim(a, broadcast_shape, broadcast_dims),
+      a_shape)
+
+
 ### Tensor contraction operations
 
 
@@ -836,7 +865,6 @@ insert = _not_implemented(onp.insert)
 linspace = _not_implemented(onp.linspace)
 nonzero = _not_implemented(onp.nonzero)
 ptp = _not_implemented(onp.ptp)
-repeat = _not_implemented(onp.repeat)
 searchsorted = _not_implemented(onp.searchsorted)
 take = _not_implemented(onp.take)
 trace = _not_implemented(onp.trace)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,95eb58f448dba0bcec471a2301902c6588fa552c,a9640b3ebbcc537791a7a34b2c4ad50313270895,Implement np.repeat() for scalar repeats.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 2b062a411..2f9b823f5 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -333,6 +333,24 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape=[{}]_axis={}_repeats={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, repeats),
+       ""axis"": axis, ""shape"": shape, ""dtype"": dtype, ""repeats"": repeats,
+       ""rng"": jtu.rand_default()}
+      for repeats in [0, 1, 2]
+      for dtype in default_dtypes
+      for shape in all_shapes
+      for axis in [None] + list(range(-len(shape), len(shape)))))
+  def testRepeat(self, axis, shape, dtype, repeats, rng):
+    onp_fun = lambda arg: onp.repeat(arg, repeats=repeats, axis=axis)
+    lnp_fun = lambda arg: lnp.repeat(arg, repeats=repeats, axis=axis)
+
+    args_maker = lambda: [rng(shape, dtype)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 2b062a411..2f9b823f5 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -333,6 +333,24 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape=[{}]_axis={}_repeats={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, repeats),
+       ""axis"": axis, ""shape"": shape, ""dtype"": dtype, ""repeats"": repeats,
+       ""rng"": jtu.rand_default()}
+      for repeats in [0, 1, 2]
+      for dtype in default_dtypes
+      for shape in all_shapes
+      for axis in [None] + list(range(-len(shape), len(shape)))))
+  def testRepeat(self, axis, shape, dtype, repeats, rng):
+    onp_fun = lambda arg: onp.repeat(arg, repeats=repeats, axis=axis)
+    lnp_fun = lambda arg: lnp.repeat(arg, repeats=repeats, axis=axis)
+
+    args_maker = lambda: [rng(shape, dtype)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),",No
build/jaxlib/__init__.py,build/jaxlib/__init__.py,2e24f91a48d03d436d57f3a1eae99ccc6017289f,a9640b3ebbcc537791a7a34b2c4ad50313270895,Added license header,"diff --git a/build/jaxlib/__init__.py b/build/jaxlib/__init__.py
index e69de29bb..b0c7da3d7 100644
--- a/build/jaxlib/__init__.py
+++ b/build/jaxlib/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.","diff --git a/build/jaxlib/__init__.py b/build/jaxlib/__init__.py
index e69de29bb..b0c7da3d7 100644
--- a/build/jaxlib/__init__.py
+++ b/build/jaxlib/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.",No
tests/generated_fun_test.py,tests/generated_fun_test.py,6fb8a8bc8923877576dc48b4d2ed539c138d60ac,8ca66c55059308ae90b4469092eb59aaff2b278b,use print as function,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 90c090e70..50c9fcfbc 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from collections import namedtuple
 from functools import partial
 import numpy.random as npr
@@ -229,7 +233,7 @@ class GeneratedFunTest(jtu.JaxTestCase):
     try:
       check_all_close(ans, ans_jitted)
     except:
-      print fun
+      print(fun)
       raise
 
   @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 90c090e70..50c9fcfbc 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -12,6 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from collections import namedtuple
 from functools import partial
 import numpy.random as npr
@@ -229,7 +233,7 @@ class GeneratedFunTest(jtu.JaxTestCase):
     try:
       check_all_close(ans, ans_jitted)
     except:
-      print fun
+      print(fun)
       raise
 
   @parameterized.named_parameters(jtu.cases_from_gens(gen_fun_and_types))",No
jax/test_util.py,jax/test_util.py,db748a046f5f8856cb65dbce0d8d7bc99256d65f,6fb8a8bc8923877576dc48b4d2ed539c138d60ac,py3 compatibility fix,"diff --git a/jax/test_util.py b/jax/test_util.py
index 21525c462..a605bb781 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -27,6 +27,8 @@ from absl.testing import parameterized
 import numpy as onp
 import numpy.random as npr
 
+from six.moves import xrange
+
 from . import api
 from .config import flags
 from .util import partial","diff --git a/jax/test_util.py b/jax/test_util.py
index 21525c462..a605bb781 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -27,6 +27,8 @@ from absl.testing import parameterized
 import numpy as onp
 import numpy.random as npr
 
+from six.moves import xrange
+
 from . import api
 from .config import flags
 from .util import partial",No
tests/generated_fun_test.py,tests/generated_fun_test.py,479f256abdeb9b0c64587bdf86de52b8dae98e8b,db748a046f5f8856cb65dbce0d8d7bc99256d65f,py3 iterator compatibility,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 50c9fcfbc..67d81b240 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -98,7 +98,7 @@ def eval_fun(fun, *args):
 
 counter = it.count()
 def fresh_var(ty):
-  return Var(counter.next(), ty)
+  return Var(next(counter), ty)
 
 def gen_array_type(size):
   # TODO(dougalm): randomize this","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 50c9fcfbc..67d81b240 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -98,7 +98,7 @@ def eval_fun(fun, *args):
 
 counter = it.count()
 def fresh_var(ty):
-  return Var(counter.next(), ty)
+  return Var(next(counter), ty)
 
 def gen_array_type(size):
   # TODO(dougalm): randomize this",No
tests/generated_fun_test.py,tests/generated_fun_test.py,fe0a26936c66285d60cef86a6da03c9ab0d28736,479f256abdeb9b0c64587bdf86de52b8dae98e8b,use custom choice function in generated tests,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 67d81b240..c005e5106 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -149,13 +149,7 @@ primitive_generators = { 1: unary_primitive_generators,
 def gen_nonneg_int(size):
   return npr.randint(size)
 
-choice = npr.choice
-
-def weighted_choice(weighted_choices):
-  weights, choices = unzip2(weighted_choices)
-  return npr_choice(choices, weights)
-
-def npr_choice(xs, weights=None):
+def choice(xs, weights=None):
   # npr.choice isn't actually RS -> [a] -> a
   # because it inspects the components to see if they're array-like
   assert xs
@@ -168,6 +162,10 @@ def npr_choice(xs, weights=None):
     i = npr.choice(range(n), p=weights)
   return xs[i]
 
+def weighted_choice(weighted_choices):
+  weights, choices = unzip2(weighted_choices)
+  return choice(choices, weights)
+
 def gen_sized_subset(xs, size):
   return [npr_choice(xs) for _ in range(size)]
 ","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 67d81b240..c005e5106 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -149,13 +149,7 @@ primitive_generators = { 1: unary_primitive_generators,
 def gen_nonneg_int(size):
   return npr.randint(size)
 
-choice = npr.choice
-
-def weighted_choice(weighted_choices):
-  weights, choices = unzip2(weighted_choices)
-  return npr_choice(choices, weights)
-
-def npr_choice(xs, weights=None):
+def choice(xs, weights=None):
   # npr.choice isn't actually RS -> [a] -> a
   # because it inspects the components to see if they're array-like
   assert xs
@@ -168,6 +162,10 @@ def npr_choice(xs, weights=None):
     i = npr.choice(range(n), p=weights)
   return xs[i]
 
+def weighted_choice(weighted_choices):
+  weights, choices = unzip2(weighted_choices)
+  return choice(choices, weights)
+
 def gen_sized_subset(xs, size):
   return [npr_choice(xs) for _ in range(size)]
 ",No
tests/generated_fun_test.py,tests/generated_fun_test.py,6c2cdfc318cf98cab00044eed49540dc8f5e377b,fe0a26936c66285d60cef86a6da03c9ab0d28736,py3 compatibility: list of keys,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index c005e5106..2cb2aaad6 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -66,7 +66,7 @@ def gen_function(size, in_types):
       fun, out_types = gen_function(size / size_reduction_factor, arg_types)
       fun = partial(eval_fun, fun)
     else:
-      arity = choice(primitive_generators.keys())
+      arity = choice(list(primitive_generators))
       arg_vars = gen_sized_subset(cur_vars, arity)
       arg_types = [v.vartype for v in arg_vars]
       prim_gen = weighted_choice(primitive_generators[arity])","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index c005e5106..2cb2aaad6 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -66,7 +66,7 @@ def gen_function(size, in_types):
       fun, out_types = gen_function(size / size_reduction_factor, arg_types)
       fun = partial(eval_fun, fun)
     else:
-      arity = choice(primitive_generators.keys())
+      arity = choice(list(primitive_generators))
       arg_vars = gen_sized_subset(cur_vars, arity)
       arg_types = [v.vartype for v in arg_vars]
       prim_gen = weighted_choice(primitive_generators[arity])",No
tests/generated_fun_test.py,tests/generated_fun_test.py,e5d89ce908d5f6782df7b6a57b7313dcc5a6c198,6c2cdfc318cf98cab00044eed49540dc8f5e377b,update call to choice,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 2cb2aaad6..3fbc71ebd 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -167,7 +167,7 @@ def weighted_choice(weighted_choices):
   return choice(choices, weights)
 
 def gen_sized_subset(xs, size):
-  return [npr_choice(xs) for _ in range(size)]
+  return [choice(xs) for _ in range(size)]
 
 def gen_subset(xs):
   if not xs:","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 2cb2aaad6..3fbc71ebd 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -167,7 +167,7 @@ def weighted_choice(weighted_choices):
   return choice(choices, weights)
 
 def gen_sized_subset(xs, size):
-  return [npr_choice(xs) for _ in range(size)]
+  return [choice(xs) for _ in range(size)]
 
 def gen_subset(xs):
   if not xs:",No
setup.py,setup.py,ee1728c2afa4aed72acc33e69596cf246c285b78,e5d89ce908d5f6782df7b6a57b7313dcc5a6c198,bump version for pypi,"diff --git a/setup.py b/setup.py
index e605c2f76..d7e145826 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1',
+    version='0.1.1',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index e605c2f76..d7e145826 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1',
+    version='0.1.1',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,948a8db0adf233f333f3e5f64d324f308c277773,ee1728c2afa4aed72acc33e69596cf246c285b78,populate readme with ill content,"diff --git a/README.md b/README.md
index 22e31d080..d34c9329b 100644
--- a/README.md
+++ b/README.md
@@ -1,5 +1,630 @@
-JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd).
-Here's a [two-page abstract](https://www.sysml.cc/doc/146.pdf) about an early version.
-Watch this space for updates!
+# JAX: Autograd and XLA
 
-This is not an official Google product.
+![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)
+
+[JAX](http://go/jax) is [Autograd](https://github.com/hips/autograd) and
+[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
+brought together for high-performance machine learning research.
+
+With its updated version of [Autograd](https://github.com/hips/autograd), JAX
+can automatically differentiate native Python and NumPy code. It can
+differentiate through a large subset of Pythons features, including loops,
+ifs, recursion, and closures, and it can even take derivatives of derivatives
+of derivatives. It supports reverse-mode differentiation (a.k.a.
+backpropagation) as well as forward-mode differentiation, and the two can be
+composed arbitrarily to any order.
+
+Whats new is that JAX uses
+[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
+to compile and run your NumPy code on accelerators, like GPUs and TPUs.
+Compilation happens under the hood by default, with library calls getting
+just-in-time compiled and executed. But JAX even lets you just-in-time compile
+your own Python functions into XLA-optimized kernels using a one-function API.
+Compilation and automatic differentiation can be composed arbitrarily, so you
+can express sophisticated algorithms and get maximal performance without having
+to leave Python.
+
+This is a research project, not an official Google product. Expect bugs and
+sharp edges. Please help by trying it out, [reporting
+bugs](https://github.com/google/jax/issues), and letting us know what you
+think!
+
+```python
+import jax.numpy as np
+from jax import grad, jit, vmap
+from functools import partial
+
+def predict(params, inputs):
+  for W, b in params:
+    outputs = np.dot(inputs, W) + b
+    inputs = np.tanh(outputs)
+  return outputs
+
+def logprob_fun(params, inputs, targets):
+  preds = predict(params, inputs)
+  return np.sum((preds - targets)**2)
+
+grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function
+perex_grads = jit(lambda params, inputs, targets:  # fast per-example gradients
+                  vmap(partial(grad_fun, params), inputs, targets))
+```
+
+JAX started as a research project by [Matt Johnson](https://github.com/mattjj),
+[Roy Frostig](https://github.com/froystig), [Dougal
+Maclaurin](https://github.com/dougalm), and [Chris
+Leary](https://github.com/learyg), and is now developed [in the
+open](https://github.com/google/jax) by a growing number of
+[contributors](#contributors).
+
+## Quickstart: Colab in the Cloud
+Jump right in using [a notebook in your
+browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
+connected to a Google Cloud GPU.
+
+## Installation
+JAX is written in pure Python, but it depends on XLA, which needs to be
+compiled and installed as the `jaxlib` package. Use the following instructions
+to [build XLA from source](#building-jax-from-source) or [install a binary
+package with pip](#pip-installation).
+
+### Building JAX from source
+First, obtain the JAX source code:
+
+```bash
+git clone https://github.com/google/jax
+cd jax
+```
+
+To build XLA with CUDA support, you can run
+
+```bash
+python build/build.py --enable_cuda
+pip install -e build  # install jaxlib
+pip install -e .  # install jax
+```
+
+See `python build/build.py --help` for configuration options, including ways to
+specify the paths to CUDA and CUDNN, which you must have installed. The build
+also depends on NumPy, and a compiler toolchain corresponding to that of
+Ubuntu 16.04 or newer.
+
+To build XLA without CUDA GPU support (CPU only), just run
+
+```bash
+python build/build.py
+pip install -e build  # install jaxlib
+pip install -e .  # install jax
+```
+
+To update to the latest version from GitHub, just run `git pull` from the JAX
+repository root, and rebuild by running `build.py` if necessary. You only have
+to reinstall if new files are added because `pip install -e` sets up symbolic
+links from site-packages into the repository.
+
+### pip installation
+
+Installing XLA with prebuilt binaries via `pip` is still experimental,
+especially with GPU support. Let us know on [the issue
+tracker](https://github.com/google/jax/issues) if you run into any errors.
+
+To install a CPU-only version, which might be useful for doing local
+development on a laptop, you can run
+
+```bash
+pip install jax jaxlib
+```
+
+If you want to install JAX with both CPU and GPU support, using existing CUDA
+and CUDNN7 installations on your machine (for example, preinstalled on your
+cloud VM), you can run
+
+```bash
+# install jaxlib
+PYTHON_VERSION=py2  # alternatives: py2, py3
+CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
+PLATFORM=linux_x86_64  # alternatives: linux_x86_64, mac
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+
+pip install jax  # install jax
+```
+
+The library package name must correspond to the version of the existing CUDA
+installation you want to use, with `cuda100` for CUDA 10.0, `cuda92` for CUDA
+9.2, and `cuda90` for CUDA 9.0. To find your CUDA and CUDNN versions, you can
+run command like these, depending on your CUDNN install path:
+
+```bash
+nvcc --version
+grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
+```
+
+## A brief tour
+
+```python
+In [1]: import jax.numpy as np
+
+In [2]: from jax import random
+
+In [3]: key = random.PRNGKey(0)
+
+In [4]: x = random.normal(key, (5000, 5000))
+
+In [5]: print(np.dot(x, x.T) / 2)  # fast!
+[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...
+
+In [6]: print(np.dot(x, x.T) / 2)  # even faster!
+[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...
+```
+
+Whats happening behind-the-scenes is that JAX is using XLA to just-in-time
+(JIT) compile and execute these individual operations on the GPU. First the
+`random.normal` call is compiled and the array referred to by `x` is generated
+on the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and
+`divide`) is JIT-compiled and executed, each keeping its results on the device.
+Its only when a value needs to be printed, plotted, saved, or passed into a raw
+NumPy function that a read-only copy of the value is brought back to the host as
+an ndarray and cached. The second call to `dot` is even faster because the
+JIT-compiled code is cached and reused, saving the compilation time.
+
+The fun really starts when you use `grad` for automatic differentiation and
+`jit` to compile your own functions end-to-end. Heres a more complete toy
+example:
+
+```python
+from jax import grad, jit
+import jax.numpy as np
+
+def sigmoid(x):
+    return 0.5 * (np.tanh(x / 2.) + 1)
+
+# Outputs probability of a label being true according to logistic model.
+def logistic_predictions(weights, inputs):
+    return sigmoid(np.dot(inputs, weights))
+
+# Training loss is the negative log-likelihood of the training labels.
+def loss(weights, inputs, targets):
+    preds = logistic_predictions(weights, inputs)
+    label_probs = preds * targets + (1 - preds) * (1 - targets)
+    return -np.sum(np.log(label_probs))
+
+# Build a toy dataset.
+inputs = np.array([[0.52, 1.12,  0.77],
+                   [0.88, -1.08, 0.15],
+                   [0.52, 0.06, -1.30],
+                   [0.74, -2.49, 1.39]])
+targets = np.array([True, True, False, True])
+
+# Define a compiled function that returns gradients of the training loss
+training_gradient_fun = jit(grad(loss))
+
+# Optimize weights using gradient descent.
+weights = np.array([0.0, 0.0, 0.0])
+print(""Initial loss: {:0.2f}"".format(loss(weights, inputs, targets)))
+for i in range(100):
+    weights -= 0.1 * training_gradient_fun(weights, inputs, targets)
+
+print(""Trained loss: {:0.2f}"".format(loss(weights, inputs, targets)))
+```
+
+To see more, check out the [quickstart
+notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb),
+a [simple MNIST classifier
+example](https://github.com/google/jax/blob/master/examples/mnist_classifier.py)
+and the rest of the [JAX
+examples](https://github.com/google/jax/blob/master/examples/).
+
+## What's supported
+
+If youre using JAX just as an accelerator-backed NumPy, without using `grad` or
+`jit` in your code, then in principle there are no constraints, though some
+NumPy features havent been implemented. Generally using `np.dot(A, B)` is
+better than `A.dot(B)` because the former gives us more opportunities to run the
+computation on the device. NumPy also does a lot of work to cast any array-like
+function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
+typically requires explicit casting of array arguments, like
+`np.sum(np.array([x, y]))`.
+
+For automatic differentiation with `grad`, JAX has the same basic requirements
+as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
+works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
+x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
+dicts freely. Using `np.dot(A, B)` rather than `A.dot(B)` is required for
+automatic differentiation when `A` is a raw ndarray.
+
+For compiling your own functions with `jit` there are a few more requirements.
+Because `jit` aims to specialize Python functions only on shapes and dtypes
+during tracing, rather than on concrete values, Python control flow that depends
+on concrete values wont be able to execute and will instead raise an error. If
+you want compiled control flow, use structured control flow primitives like
+lax.cond and lax.while. Some indexing features, like slice-based indexing
+`A[i:i+5]` for argument-dependent `i`, or boolean-based indexing `A[bool_ind]`
+for argument-dependent `bool_ind`, produce abstract values of unknown shape and
+are thus unsupported in `jit` functions.
+
+> TLDR **Do use**
+>
+> *   Functional programming
+> *   [Many](https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py) of NumPys
+>     functions (help us add more!)
+> *   [Some](https://github.com/google/jax/tree/master/jax/scipy) SciPy functions
+> *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`
+> *   Explicit array creation from lists like `A = np.array([x, y])`
+>
+> **Dont use**
+>
+> *   Assignment into arrays like `A[0, 0] = x`
+> *   Implicit casting to arrays like `np.sum([x, y])` (use `np.sum(np.array([x,
+>     y])` instead)
+> *   `A.dot(B)` method syntax for functions of more than one argument (use
+>     `np.dot(A, B)` instead)
+> *   Side-effects like mutation of arguments or mutation of global variables
+> *   The `out` argument of NumPy functions
+>
+> **For jit functions, also dont use**
+>
+> *   Control flow based on dynamic values `if x > 0: ...`. Control flow based
+>     on shapes is fine: `if x.shape[0] > 2: ...` and `for subarr in array`.
+> *   Slicing `A[i:i+5]` for dynamic index `i` (use `lax.dynamic_slice` instead)
+>     or boolean indexing `A[bool_ind]` for traced values `bool_ind`.
+
+You should get loud errors if your code violates any of these.
+
+## Transformations
+
+JAX is at its core an extensible system for transforming numerical functions.
+Here are three key transformations for machine learning research.
+
+### Automatic differentiation with grad
+
+JAX has roughly the same API as [Autograd](https://github.com/hips/autograd).
+The most popular function is `grad` for reverse-mode gradients:
+
+```python
+from jax import grad
+import jax.numpy as np
+
+def tanh(x):  # Define a function
+  y = np.exp(-2.0 * x)
+  return (1.0 - y) / (1.0 + y)
+
+grad_tanh = grad(tanh)  # Obtain its gradient function
+print(grad_tanh(1.0))   # Evaluate it at x = 1.0
+# prints 0.41997434161402603
+```
+
+You can differentiate to any order with `grad`.
+
+For more advanced autodiff, you can use `jax.vjp` for reverse-mode
+vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector
+products. The two can be composed arbitrarily with one another, and with other
+JAX transformations. Here's one way to compose
+those to make a function that efficiently computes full Hessian matrices:
+
+```python
+from jax import jit, jacfwd, jacrev
+def hessian(fun):
+  return jit(jacfwd(jacrev(fun)))
+```
+
+As with Autograd, you're free to use differentiation with Python control
+structures:
+
+```python
+def abs_val(x):
+  if x > 0:
+    return x
+  else:
+    return -x
+
+abs_val_grad = grad(abs_val)
+print(abs_val_grad)(1.0)   # prints 1.0
+print(abs_val_grad)(-1.0)  # prints -1.0 (abs_val is re-evaluated)
+```
+
+### Compilation with jit
+
+You can use XLA to compile your functions end-to-end with `jit`, used either as
+an `@jit` decorator or as a higher-order function.
+
+```python
+import jax.numpy as np
+from jax import jit
+
+def slow_f(x):
+  # Element-wise ops see a large benefit from fusion
+  return x * x + x * 2.0
+
+x = np.ones((5000, 5000))
+fast_f = jit(slow_f)
+%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X
+%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)
+```
+
+You can mix `jit` and `grad` and any other JAX transformation however you like.
+
+### Auto-vectorization with vmap
+
+JAX enables more program transformations than just forward- and reverse-mode
+differentiation and compilation. Another example is `vmap`, the vectorizing map.
+It has the familiar semantics of mapping a function along array axes, but
+instead of keeping the loop on the outside, it pushes the loop down into a
+functions primitive operations for better performance.
+
+Using `vmap` can save you from having to carry around batch dimensions in your
+code. For example, consider this simple *unbatched* neural network prediction
+function:
+
+```python
+def predict(params, input_vec):
+  assert input_vec.ndim == 1
+  for W, b in params:
+    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!
+    input_vec = np.tanh(output_vec)
+  return output_vec
+```
+
+We often instead write `np.dot(inputs, W)` to allow for a batch dimension on the
+left side of `inputs`, but weve written this particular prediction function to
+apply only to single input vectors. If we wanted to apply this function to a
+batch of inputs at once, semantically we could just write
+
+```python
+from functools import partial
+predictions = np.stack(list(map(partial(predict, params), input_batch)))
+```
+
+But pushing one example through the network at a time would be slow! Its better
+to vectorize the computation, so that at every layer were doing matrix-matrix
+multiplies rather than matrix-vector multiplies.
+
+The `vmap` function does that transformation for us. That is, if we write
+
+```python
+from jax import vmap
+predictions = vmap(partial(predict, params), input_batch)
+```
+
+then the `vmap` function will push the outer loop inside the function, and our
+machine will end up executing matrix-matrix multiplications exactly as if wed
+done the batching by hand.
+
+Its easy enough to manually batch a simple neural network without `vmap`, but
+in other cases manual vectorization can be impractical or impossible. Take the
+problem of efficiently computing per-example gradients: that is, for a fixed set
+of parameters, we want to compute the gradient of our loss function evaluated
+separately at each example in a batch. With `vmap`, its easy:
+
+```python
+per_example_gradients = vmap(partial(grad(loss), params), inputs, targets)
+```
+
+Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other
+JAX transformation! We use `vmap` with both forward- and reverse-mode automatic
+differentiation for fast Jacobian and Hessian matrix calculations in
+`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.
+
+
+## Random numbers are different
+
+JAX needs a pseudo-random number generator (PRNG) system to provide
+reproducible results invariant to compilation boundaries and backends, while
+also maximizing performance by enabling vectorized generation and
+parallelization across random calls. The `numpy.random` library doesnt have
+those properties. The `jax.random` library meets those needs: its functionally
+pure, but it doesnt require you to pass stateful random objects back out of
+every function.
+
+The `jax.random` library uses
+[count-based PRNGs](http://www.thesalmons.org/john/random123/papers/random123sc11.pdf)
+and a functional array-oriented
+[splitting model](http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf).
+To generate random values, you call a function like `jax.random.normal` and give
+it a PRNG key:
+
+```python
+import jax.random as random
+
+key = random.PRNGKey(0)
+print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
+```
+
+If we make the same call again with the same key, we get the same values:
+
+```python
+print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
+```
+
+The key never gets updated. So how do we get fresh random values? We use
+`jax.random.split` to create new keys from existing ones. A common pattern is to
+split off a new key for every function call that needs random values:
+
+```python
+key = random.PRNGKey(0)
+
+key, subkey = random.split(key)
+print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]
+
+key, subkey = random.split(key)
+print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]
+```
+
+By splitting the PRNG key, not only do we avoid having to thread random states
+back out of every function call, but also we can generate multiple random arrays
+in parallel because we can avoid unnecessary sequential dependencies.
+
+## Mini-libraries
+
+JAX provides some small, experimental libraries for machine learning. These
+libraries are in part about providing tools and in part about serving as
+examples for how to build such libraries using JAX. Each one is only a few
+hundred lines of code, so take a look inside and adapt them as you need!
+
+### Neural-net building with Stax
+
+**Stax** is a functional neural network building library. The basic idea is that
+a single layer or an entire network can be modeled as an `(init_fun, apply_fun)`
+pair. The `init_fun` is used to initialize network parameters and the
+`apply_fun` takes parameters and inputs to produce outputs. There are
+constructor functions for common basic pairs, like `Conv` and `Relu`, and these
+pairs can be composed in serial using `stax.serial` or in parallel using
+`stax.parallel`.
+
+Heres an example:
+
+```python
+from jax.experimental import stax
+from jax.experimental.stax import Conv
+from jax.experimental.stax import Dense
+from jax.experimental.stax import MaxPool
+from jax.experimental.stax import Relu
+from jax.experimental.stax import LogSoftmax
+
+# Set up network initialization and evaluation functions
+net_init, net_apply = stax.serial(
+    Conv(32, (3, 3), padding='SAME'), Relu,
+    Conv(64, (3, 3), padding='SAME'), Relu
+    MaxPool((2, 2)), Flatten,
+    Dense(128), Relu,
+    Dense(10), SoftMax,
+)
+
+# Initialize parameters, not committing to a batch shape
+in_shape = (-1, 28 * 28)
+out_shape, net_params = net_init(in_shape)
+
+# Apply network
+predictions = net_apply(net_params, inputs)
+```
+
+### First-order optimization with Minmax
+
+**Minmax** is an optimization library focused on stochastic first-order
+optimizers. Every optimizer is modeled as an `(init_fun, update_fun)` pair. The
+`init_fun` is used to initialize the optimizer state, which could include things
+like momentum variables, and the `update_fun` accepts a gradient and an
+optimizer state to produce a new optimizer state. The parameters being optimized
+can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can
+store your parameters however youd like.
+
+Heres an example, using `jit` to compile the whole update end-to-end:
+
+```python
+from jax.experimental import minmax
+from jax import jit
+
+# Set up an optimizer
+opt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)
+
+# Define a compiled update step
+@jit
+def step(i, opt_state, batch):
+  params = minmax.get_params(opt_state)
+  g = grad(loss)(params, batch)
+  return opt_update(i, g, opt_state)
+
+# Optimize parameters in a loop
+opt_state = opt_init(net_params)
+for i in range(num_steps):
+  opt_state = step(i, opt_state, next(data_generator))
+net_params = minmax.get_params(opt_state)
+```
+
+## How it works
+
+Programming in machine learning is about expressing and transforming functions.
+Transformations include automatic differentiation, compilation for accelerators,
+and even automatic batching. High-level languages like Python are great for
+expressing functions, but usually all we can do with them is apply them. We lose
+access to their internal structure which would let us perform transformations.
+
+JAX is a tool for specializing and translating high-level Python+NumPy functions
+into a representation that can be transformed and then lifted back into a Python
+function.
+
+![simplified-lifecycle](https://raw.githubusercontent.com/google/jax/master/images/lifecycle.png)
+
+JAX specializes Python functions by tracing. Tracing a function means monitoring
+all the basic operations that are applied to its input to produce its output,
+and recording these operations and the data-flow between them in a directed
+acyclic graph (DAG). To perform tracing, JAX wraps primitive operations, like
+basic numerical kernels, so that when theyre called they add themselves to a
+list of operations performed along with their inputs and outputs. To keep track
+of how data flows between these primitives, values being tracked are wrapped in
+instances of the `Tracer` class.
+
+When a Python function is provided to `grad` or `jit`, its wrapped for tracing
+and returned. When the wrapped function is called, we abstract the concrete
+arguments provided into instances of the `AbstractValue` class, box them for
+tracing in instances of the `Tracer` class, and call the function on them.
+Abstract arguments represent sets of possible values rather than specific
+values: for example, `jit` abstracts ndarray arguments to abstract values that
+represent all ndarrays with the same shape and dtype. In contrast, `grad`
+abstracts ndarray arguments to represent a small neighborhood of the underlying
+value. By tracing the Python function on these abstract values, we ensure that
+its specialized enough so that its tractable to transform, and that its still
+general enough so that the transformed result is useful. These transformed
+functions are then lifted back into Python callables in a way that allows them
+to be traced and transformed again as needed.
+
+The primitive functions that JAX traces are mostly in 1:1 correspondence with
+[XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined
+in [lax.py](https://github.com/google/jax/blob/master/jax/lax.py). This 1:1
+correspondence makes most of the translations to XLA essentially trivial, and
+ensures we only have a small set of primitives to cover for other
+transformations like automatic differentiation. The [`jax.numpy`
+layer](https://github.com/google/jax/blob/master/jax/numpy/) is written in pure
+Python simply by expressing NumPy functions in terms of the LAX functions (and
+other NumPy functions weve already written). That makes `jax.numpy` easy to
+extend.
+
+When you use `jax.numpy`, the underlying LAX primitives are `jit`-compiled
+behind the scenes, allowing you to write unrestricted Python+Numpy code while
+still executing each primitive operation on an accelerator.
+
+But JAX can do more: instead of just compiling and dispatching to a fixed set of
+individual primitives, you can use `jit` on larger and larger functions to be
+end-to-end compiled and optimized. For example, instead of just compiling and
+dispatching a convolution op, you can compile a whole network, or a whole
+gradient evaluation and optimizer update step.
+
+The tradeoff is that `jit` functions have to satisfy some additional
+specialization requirements: since we want to compile traces that are
+specialized on shapes and dtypes, but not specialized all the way to concrete
+values, the Python code under a `jit` decorator must be applicable to abstract
+values. If we try to evaluate `x > 0` on an abstract `x`, the result is an
+abstract value representing the set `{True, False}`, and so a Python branch like
+`if x > 0` will raise an error: it doesnt know which way to go! 
+See [Whats supported](#whats-supported) for more
+information about `jit` requirements.
+
+The good news about this tradeoff is that `jit` is opt-in: JAX libraries use
+`jit` on individual operations and functions behind the scenes, allowing you to
+write unrestricted Python+Numpy and still make use of a hardware accelerator.
+But when you want to maximize performance, you can often use `jit` in your own
+code to compile and end-to-end optimize much bigger functions.
+
+## What we're working on
+1. Documentation!
+2. Cloud TPU support
+3. Multi-GPU and multi-TPU support
+4. Full NumPy coverage and some SciPy coverage
+5. Full coverage for vmap
+6. Make everything faster
+    * Lowering the XLA function dispatch overhead
+    * Linear algebra routines (MKL on CPU, MAGMA on GPU)
+7. `cond` and `while` primitives with efficient automatic differentiation
+
+## Current gotchas
+
+Some things we don't handle that might surprise NumPy users:
+1. No in-place mutation syntax. Functional code. Can use lax.dynamic\_update\_slice.
+2. PRNG can be awkward, and linearity is not checked with a warning.
+
+## Contributors
+
+So far, JAX includes lots of help and contributions from [Peter
+Hawkins](https://github.com/hawkinsp), [Alex
+Wiltschko](http://github.com/alexbw), George Dahl, [Eli
+Bendersky](https://github.com/eliben), Zak Stone, [Alexey
+Radul](https://github.com/axch), Michael Isard, Skye Wanderman-Milne, and many
+others.","diff --git a/README.md b/README.md
index 22e31d080..d34c9329b 100644
--- a/README.md
+++ b/README.md
@@ -1,5 +1,630 @@
-JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd).
-Here's a [two-page abstract](https://www.sysml.cc/doc/146.pdf) about an early version.
-Watch this space for updates!
+# JAX: Autograd and XLA
 
-This is not an official Google product.
+![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)
+
+[JAX](http://go/jax) is [Autograd](https://github.com/hips/autograd) and
+[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
+brought together for high-performance machine learning research.
+
+With its updated version of [Autograd](https://github.com/hips/autograd), JAX
+can automatically differentiate native Python and NumPy code. It can
+differentiate through a large subset of Pythons features, including loops,
+ifs, recursion, and closures, and it can even take derivatives of derivatives
+of derivatives. It supports reverse-mode differentiation (a.k.a.
+backpropagation) as well as forward-mode differentiation, and the two can be
+composed arbitrarily to any order.
+
+Whats new is that JAX uses
+[XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
+to compile and run your NumPy code on accelerators, like GPUs and TPUs.
+Compilation happens under the hood by default, with library calls getting
+just-in-time compiled and executed. But JAX even lets you just-in-time compile
+your own Python functions into XLA-optimized kernels using a one-function API.
+Compilation and automatic differentiation can be composed arbitrarily, so you
+can express sophisticated algorithms and get maximal performance without having
+to leave Python.
+
+This is a research project, not an official Google product. Expect bugs and
+sharp edges. Please help by trying it out, [reporting
+bugs](https://github.com/google/jax/issues), and letting us know what you
+think!
+
+```python
+import jax.numpy as np
+from jax import grad, jit, vmap
+from functools import partial
+
+def predict(params, inputs):
+  for W, b in params:
+    outputs = np.dot(inputs, W) + b
+    inputs = np.tanh(outputs)
+  return outputs
+
+def logprob_fun(params, inputs, targets):
+  preds = predict(params, inputs)
+  return np.sum((preds - targets)**2)
+
+grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function
+perex_grads = jit(lambda params, inputs, targets:  # fast per-example gradients
+                  vmap(partial(grad_fun, params), inputs, targets))
+```
+
+JAX started as a research project by [Matt Johnson](https://github.com/mattjj),
+[Roy Frostig](https://github.com/froystig), [Dougal
+Maclaurin](https://github.com/dougalm), and [Chris
+Leary](https://github.com/learyg), and is now developed [in the
+open](https://github.com/google/jax) by a growing number of
+[contributors](#contributors).
+
+## Quickstart: Colab in the Cloud
+Jump right in using [a notebook in your
+browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
+connected to a Google Cloud GPU.
+
+## Installation
+JAX is written in pure Python, but it depends on XLA, which needs to be
+compiled and installed as the `jaxlib` package. Use the following instructions
+to [build XLA from source](#building-jax-from-source) or [install a binary
+package with pip](#pip-installation).
+
+### Building JAX from source
+First, obtain the JAX source code:
+
+```bash
+git clone https://github.com/google/jax
+cd jax
+```
+
+To build XLA with CUDA support, you can run
+
+```bash
+python build/build.py --enable_cuda
+pip install -e build  # install jaxlib
+pip install -e .  # install jax
+```
+
+See `python build/build.py --help` for configuration options, including ways to
+specify the paths to CUDA and CUDNN, which you must have installed. The build
+also depends on NumPy, and a compiler toolchain corresponding to that of
+Ubuntu 16.04 or newer.
+
+To build XLA without CUDA GPU support (CPU only), just run
+
+```bash
+python build/build.py
+pip install -e build  # install jaxlib
+pip install -e .  # install jax
+```
+
+To update to the latest version from GitHub, just run `git pull` from the JAX
+repository root, and rebuild by running `build.py` if necessary. You only have
+to reinstall if new files are added because `pip install -e` sets up symbolic
+links from site-packages into the repository.
+
+### pip installation
+
+Installing XLA with prebuilt binaries via `pip` is still experimental,
+especially with GPU support. Let us know on [the issue
+tracker](https://github.com/google/jax/issues) if you run into any errors.
+
+To install a CPU-only version, which might be useful for doing local
+development on a laptop, you can run
+
+```bash
+pip install jax jaxlib
+```
+
+If you want to install JAX with both CPU and GPU support, using existing CUDA
+and CUDNN7 installations on your machine (for example, preinstalled on your
+cloud VM), you can run
+
+```bash
+# install jaxlib
+PYTHON_VERSION=py2  # alternatives: py2, py3
+CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
+PLATFORM=linux_x86_64  # alternatives: linux_x86_64, mac
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+
+pip install jax  # install jax
+```
+
+The library package name must correspond to the version of the existing CUDA
+installation you want to use, with `cuda100` for CUDA 10.0, `cuda92` for CUDA
+9.2, and `cuda90` for CUDA 9.0. To find your CUDA and CUDNN versions, you can
+run command like these, depending on your CUDNN install path:
+
+```bash
+nvcc --version
+grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
+```
+
+## A brief tour
+
+```python
+In [1]: import jax.numpy as np
+
+In [2]: from jax import random
+
+In [3]: key = random.PRNGKey(0)
+
+In [4]: x = random.normal(key, (5000, 5000))
+
+In [5]: print(np.dot(x, x.T) / 2)  # fast!
+[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...
+
+In [6]: print(np.dot(x, x.T) / 2)  # even faster!
+[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...
+```
+
+Whats happening behind-the-scenes is that JAX is using XLA to just-in-time
+(JIT) compile and execute these individual operations on the GPU. First the
+`random.normal` call is compiled and the array referred to by `x` is generated
+on the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and
+`divide`) is JIT-compiled and executed, each keeping its results on the device.
+Its only when a value needs to be printed, plotted, saved, or passed into a raw
+NumPy function that a read-only copy of the value is brought back to the host as
+an ndarray and cached. The second call to `dot` is even faster because the
+JIT-compiled code is cached and reused, saving the compilation time.
+
+The fun really starts when you use `grad` for automatic differentiation and
+`jit` to compile your own functions end-to-end. Heres a more complete toy
+example:
+
+```python
+from jax import grad, jit
+import jax.numpy as np
+
+def sigmoid(x):
+    return 0.5 * (np.tanh(x / 2.) + 1)
+
+# Outputs probability of a label being true according to logistic model.
+def logistic_predictions(weights, inputs):
+    return sigmoid(np.dot(inputs, weights))
+
+# Training loss is the negative log-likelihood of the training labels.
+def loss(weights, inputs, targets):
+    preds = logistic_predictions(weights, inputs)
+    label_probs = preds * targets + (1 - preds) * (1 - targets)
+    return -np.sum(np.log(label_probs))
+
+# Build a toy dataset.
+inputs = np.array([[0.52, 1.12,  0.77],
+                   [0.88, -1.08, 0.15],
+                   [0.52, 0.06, -1.30],
+                   [0.74, -2.49, 1.39]])
+targets = np.array([True, True, False, True])
+
+# Define a compiled function that returns gradients of the training loss
+training_gradient_fun = jit(grad(loss))
+
+# Optimize weights using gradient descent.
+weights = np.array([0.0, 0.0, 0.0])
+print(""Initial loss: {:0.2f}"".format(loss(weights, inputs, targets)))
+for i in range(100):
+    weights -= 0.1 * training_gradient_fun(weights, inputs, targets)
+
+print(""Trained loss: {:0.2f}"".format(loss(weights, inputs, targets)))
+```
+
+To see more, check out the [quickstart
+notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb),
+a [simple MNIST classifier
+example](https://github.com/google/jax/blob/master/examples/mnist_classifier.py)
+and the rest of the [JAX
+examples](https://github.com/google/jax/blob/master/examples/).
+
+## What's supported
+
+If youre using JAX just as an accelerator-backed NumPy, without using `grad` or
+`jit` in your code, then in principle there are no constraints, though some
+NumPy features havent been implemented. Generally using `np.dot(A, B)` is
+better than `A.dot(B)` because the former gives us more opportunities to run the
+computation on the device. NumPy also does a lot of work to cast any array-like
+function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
+typically requires explicit casting of array arguments, like
+`np.sum(np.array([x, y]))`.
+
+For automatic differentiation with `grad`, JAX has the same basic requirements
+as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
+works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
+x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
+dicts freely. Using `np.dot(A, B)` rather than `A.dot(B)` is required for
+automatic differentiation when `A` is a raw ndarray.
+
+For compiling your own functions with `jit` there are a few more requirements.
+Because `jit` aims to specialize Python functions only on shapes and dtypes
+during tracing, rather than on concrete values, Python control flow that depends
+on concrete values wont be able to execute and will instead raise an error. If
+you want compiled control flow, use structured control flow primitives like
+lax.cond and lax.while. Some indexing features, like slice-based indexing
+`A[i:i+5]` for argument-dependent `i`, or boolean-based indexing `A[bool_ind]`
+for argument-dependent `bool_ind`, produce abstract values of unknown shape and
+are thus unsupported in `jit` functions.
+
+> TLDR **Do use**
+>
+> *   Functional programming
+> *   [Many](https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py) of NumPys
+>     functions (help us add more!)
+> *   [Some](https://github.com/google/jax/tree/master/jax/scipy) SciPy functions
+> *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`
+> *   Explicit array creation from lists like `A = np.array([x, y])`
+>
+> **Dont use**
+>
+> *   Assignment into arrays like `A[0, 0] = x`
+> *   Implicit casting to arrays like `np.sum([x, y])` (use `np.sum(np.array([x,
+>     y])` instead)
+> *   `A.dot(B)` method syntax for functions of more than one argument (use
+>     `np.dot(A, B)` instead)
+> *   Side-effects like mutation of arguments or mutation of global variables
+> *   The `out` argument of NumPy functions
+>
+> **For jit functions, also dont use**
+>
+> *   Control flow based on dynamic values `if x > 0: ...`. Control flow based
+>     on shapes is fine: `if x.shape[0] > 2: ...` and `for subarr in array`.
+> *   Slicing `A[i:i+5]` for dynamic index `i` (use `lax.dynamic_slice` instead)
+>     or boolean indexing `A[bool_ind]` for traced values `bool_ind`.
+
+You should get loud errors if your code violates any of these.
+
+## Transformations
+
+JAX is at its core an extensible system for transforming numerical functions.
+Here are three key transformations for machine learning research.
+
+### Automatic differentiation with grad
+
+JAX has roughly the same API as [Autograd](https://github.com/hips/autograd).
+The most popular function is `grad` for reverse-mode gradients:
+
+```python
+from jax import grad
+import jax.numpy as np
+
+def tanh(x):  # Define a function
+  y = np.exp(-2.0 * x)
+  return (1.0 - y) / (1.0 + y)
+
+grad_tanh = grad(tanh)  # Obtain its gradient function
+print(grad_tanh(1.0))   # Evaluate it at x = 1.0
+# prints 0.41997434161402603
+```
+
+You can differentiate to any order with `grad`.
+
+For more advanced autodiff, you can use `jax.vjp` for reverse-mode
+vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector
+products. The two can be composed arbitrarily with one another, and with other
+JAX transformations. Here's one way to compose
+those to make a function that efficiently computes full Hessian matrices:
+
+```python
+from jax import jit, jacfwd, jacrev
+def hessian(fun):
+  return jit(jacfwd(jacrev(fun)))
+```
+
+As with Autograd, you're free to use differentiation with Python control
+structures:
+
+```python
+def abs_val(x):
+  if x > 0:
+    return x
+  else:
+    return -x
+
+abs_val_grad = grad(abs_val)
+print(abs_val_grad)(1.0)   # prints 1.0
+print(abs_val_grad)(-1.0)  # prints -1.0 (abs_val is re-evaluated)
+```
+
+### Compilation with jit
+
+You can use XLA to compile your functions end-to-end with `jit`, used either as
+an `@jit` decorator or as a higher-order function.
+
+```python
+import jax.numpy as np
+from jax import jit
+
+def slow_f(x):
+  # Element-wise ops see a large benefit from fusion
+  return x * x + x * 2.0
+
+x = np.ones((5000, 5000))
+fast_f = jit(slow_f)
+%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X
+%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)
+```
+
+You can mix `jit` and `grad` and any other JAX transformation however you like.
+
+### Auto-vectorization with vmap
+
+JAX enables more program transformations than just forward- and reverse-mode
+differentiation and compilation. Another example is `vmap`, the vectorizing map.
+It has the familiar semantics of mapping a function along array axes, but
+instead of keeping the loop on the outside, it pushes the loop down into a
+functions primitive operations for better performance.
+
+Using `vmap` can save you from having to carry around batch dimensions in your
+code. For example, consider this simple *unbatched* neural network prediction
+function:
+
+```python
+def predict(params, input_vec):
+  assert input_vec.ndim == 1
+  for W, b in params:
+    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!
+    input_vec = np.tanh(output_vec)
+  return output_vec
+```
+
+We often instead write `np.dot(inputs, W)` to allow for a batch dimension on the
+left side of `inputs`, but weve written this particular prediction function to
+apply only to single input vectors. If we wanted to apply this function to a
+batch of inputs at once, semantically we could just write
+
+```python
+from functools import partial
+predictions = np.stack(list(map(partial(predict, params), input_batch)))
+```
+
+But pushing one example through the network at a time would be slow! Its better
+to vectorize the computation, so that at every layer were doing matrix-matrix
+multiplies rather than matrix-vector multiplies.
+
+The `vmap` function does that transformation for us. That is, if we write
+
+```python
+from jax import vmap
+predictions = vmap(partial(predict, params), input_batch)
+```
+
+then the `vmap` function will push the outer loop inside the function, and our
+machine will end up executing matrix-matrix multiplications exactly as if wed
+done the batching by hand.
+
+Its easy enough to manually batch a simple neural network without `vmap`, but
+in other cases manual vectorization can be impractical or impossible. Take the
+problem of efficiently computing per-example gradients: that is, for a fixed set
+of parameters, we want to compute the gradient of our loss function evaluated
+separately at each example in a batch. With `vmap`, its easy:
+
+```python
+per_example_gradients = vmap(partial(grad(loss), params), inputs, targets)
+```
+
+Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other
+JAX transformation! We use `vmap` with both forward- and reverse-mode automatic
+differentiation for fast Jacobian and Hessian matrix calculations in
+`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.
+
+
+## Random numbers are different
+
+JAX needs a pseudo-random number generator (PRNG) system to provide
+reproducible results invariant to compilation boundaries and backends, while
+also maximizing performance by enabling vectorized generation and
+parallelization across random calls. The `numpy.random` library doesnt have
+those properties. The `jax.random` library meets those needs: its functionally
+pure, but it doesnt require you to pass stateful random objects back out of
+every function.
+
+The `jax.random` library uses
+[count-based PRNGs](http://www.thesalmons.org/john/random123/papers/random123sc11.pdf)
+and a functional array-oriented
+[splitting model](http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf).
+To generate random values, you call a function like `jax.random.normal` and give
+it a PRNG key:
+
+```python
+import jax.random as random
+
+key = random.PRNGKey(0)
+print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
+```
+
+If we make the same call again with the same key, we get the same values:
+
+```python
+print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]
+```
+
+The key never gets updated. So how do we get fresh random values? We use
+`jax.random.split` to create new keys from existing ones. A common pattern is to
+split off a new key for every function call that needs random values:
+
+```python
+key = random.PRNGKey(0)
+
+key, subkey = random.split(key)
+print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]
+
+key, subkey = random.split(key)
+print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]
+```
+
+By splitting the PRNG key, not only do we avoid having to thread random states
+back out of every function call, but also we can generate multiple random arrays
+in parallel because we can avoid unnecessary sequential dependencies.
+
+## Mini-libraries
+
+JAX provides some small, experimental libraries for machine learning. These
+libraries are in part about providing tools and in part about serving as
+examples for how to build such libraries using JAX. Each one is only a few
+hundred lines of code, so take a look inside and adapt them as you need!
+
+### Neural-net building with Stax
+
+**Stax** is a functional neural network building library. The basic idea is that
+a single layer or an entire network can be modeled as an `(init_fun, apply_fun)`
+pair. The `init_fun` is used to initialize network parameters and the
+`apply_fun` takes parameters and inputs to produce outputs. There are
+constructor functions for common basic pairs, like `Conv` and `Relu`, and these
+pairs can be composed in serial using `stax.serial` or in parallel using
+`stax.parallel`.
+
+Heres an example:
+
+```python
+from jax.experimental import stax
+from jax.experimental.stax import Conv
+from jax.experimental.stax import Dense
+from jax.experimental.stax import MaxPool
+from jax.experimental.stax import Relu
+from jax.experimental.stax import LogSoftmax
+
+# Set up network initialization and evaluation functions
+net_init, net_apply = stax.serial(
+    Conv(32, (3, 3), padding='SAME'), Relu,
+    Conv(64, (3, 3), padding='SAME'), Relu
+    MaxPool((2, 2)), Flatten,
+    Dense(128), Relu,
+    Dense(10), SoftMax,
+)
+
+# Initialize parameters, not committing to a batch shape
+in_shape = (-1, 28 * 28)
+out_shape, net_params = net_init(in_shape)
+
+# Apply network
+predictions = net_apply(net_params, inputs)
+```
+
+### First-order optimization with Minmax
+
+**Minmax** is an optimization library focused on stochastic first-order
+optimizers. Every optimizer is modeled as an `(init_fun, update_fun)` pair. The
+`init_fun` is used to initialize the optimizer state, which could include things
+like momentum variables, and the `update_fun` accepts a gradient and an
+optimizer state to produce a new optimizer state. The parameters being optimized
+can be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can
+store your parameters however youd like.
+
+Heres an example, using `jit` to compile the whole update end-to-end:
+
+```python
+from jax.experimental import minmax
+from jax import jit
+
+# Set up an optimizer
+opt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)
+
+# Define a compiled update step
+@jit
+def step(i, opt_state, batch):
+  params = minmax.get_params(opt_state)
+  g = grad(loss)(params, batch)
+  return opt_update(i, g, opt_state)
+
+# Optimize parameters in a loop
+opt_state = opt_init(net_params)
+for i in range(num_steps):
+  opt_state = step(i, opt_state, next(data_generator))
+net_params = minmax.get_params(opt_state)
+```
+
+## How it works
+
+Programming in machine learning is about expressing and transforming functions.
+Transformations include automatic differentiation, compilation for accelerators,
+and even automatic batching. High-level languages like Python are great for
+expressing functions, but usually all we can do with them is apply them. We lose
+access to their internal structure which would let us perform transformations.
+
+JAX is a tool for specializing and translating high-level Python+NumPy functions
+into a representation that can be transformed and then lifted back into a Python
+function.
+
+![simplified-lifecycle](https://raw.githubusercontent.com/google/jax/master/images/lifecycle.png)
+
+JAX specializes Python functions by tracing. Tracing a function means monitoring
+all the basic operations that are applied to its input to produce its output,
+and recording these operations and the data-flow between them in a directed
+acyclic graph (DAG). To perform tracing, JAX wraps primitive operations, like
+basic numerical kernels, so that when theyre called they add themselves to a
+list of operations performed along with their inputs and outputs. To keep track
+of how data flows between these primitives, values being tracked are wrapped in
+instances of the `Tracer` class.
+
+When a Python function is provided to `grad` or `jit`, its wrapped for tracing
+and returned. When the wrapped function is called, we abstract the concrete
+arguments provided into instances of the `AbstractValue` class, box them for
+tracing in instances of the `Tracer` class, and call the function on them.
+Abstract arguments represent sets of possible values rather than specific
+values: for example, `jit` abstracts ndarray arguments to abstract values that
+represent all ndarrays with the same shape and dtype. In contrast, `grad`
+abstracts ndarray arguments to represent a small neighborhood of the underlying
+value. By tracing the Python function on these abstract values, we ensure that
+its specialized enough so that its tractable to transform, and that its still
+general enough so that the transformed result is useful. These transformed
+functions are then lifted back into Python callables in a way that allows them
+to be traced and transformed again as needed.
+
+The primitive functions that JAX traces are mostly in 1:1 correspondence with
+[XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined
+in [lax.py](https://github.com/google/jax/blob/master/jax/lax.py). This 1:1
+correspondence makes most of the translations to XLA essentially trivial, and
+ensures we only have a small set of primitives to cover for other
+transformations like automatic differentiation. The [`jax.numpy`
+layer](https://github.com/google/jax/blob/master/jax/numpy/) is written in pure
+Python simply by expressing NumPy functions in terms of the LAX functions (and
+other NumPy functions weve already written). That makes `jax.numpy` easy to
+extend.
+
+When you use `jax.numpy`, the underlying LAX primitives are `jit`-compiled
+behind the scenes, allowing you to write unrestricted Python+Numpy code while
+still executing each primitive operation on an accelerator.
+
+But JAX can do more: instead of just compiling and dispatching to a fixed set of
+individual primitives, you can use `jit` on larger and larger functions to be
+end-to-end compiled and optimized. For example, instead of just compiling and
+dispatching a convolution op, you can compile a whole network, or a whole
+gradient evaluation and optimizer update step.
+
+The tradeoff is that `jit` functions have to satisfy some additional
+specialization requirements: since we want to compile traces that are
+specialized on shapes and dtypes, but not specialized all the way to concrete
+values, the Python code under a `jit` decorator must be applicable to abstract
+values. If we try to evaluate `x > 0` on an abstract `x`, the result is an
+abstract value representing the set `{True, False}`, and so a Python branch like
+`if x > 0` will raise an error: it doesnt know which way to go! 
+See [Whats supported](#whats-supported) for more
+information about `jit` requirements.
+
+The good news about this tradeoff is that `jit` is opt-in: JAX libraries use
+`jit` on individual operations and functions behind the scenes, allowing you to
+write unrestricted Python+Numpy and still make use of a hardware accelerator.
+But when you want to maximize performance, you can often use `jit` in your own
+code to compile and end-to-end optimize much bigger functions.
+
+## What we're working on
+1. Documentation!
+2. Cloud TPU support
+3. Multi-GPU and multi-TPU support
+4. Full NumPy coverage and some SciPy coverage
+5. Full coverage for vmap
+6. Make everything faster
+    * Lowering the XLA function dispatch overhead
+    * Linear algebra routines (MKL on CPU, MAGMA on GPU)
+7. `cond` and `while` primitives with efficient automatic differentiation
+
+## Current gotchas
+
+Some things we don't handle that might surprise NumPy users:
+1. No in-place mutation syntax. Functional code. Can use lax.dynamic\_update\_slice.
+2. PRNG can be awkward, and linearity is not checked with a warning.
+
+## Contributors
+
+So far, JAX includes lots of help and contributions from [Peter
+Hawkins](https://github.com/hawkinsp), [Alex
+Wiltschko](http://github.com/alexbw), George Dahl, [Eli
+Bendersky](https://github.com/eliben), Zak Stone, [Alexey
+Radul](https://github.com/axch), Michael Isard, Skye Wanderman-Milne, and many
+others.",No
README.md,README.md,838d9e12fe2c7d71fc25e7833dd73e99ba1ef316,948a8db0adf233f333f3e5f64d324f308c277773,Removed go/jax link and updated wording,"diff --git a/README.md b/README.md
index d34c9329b..0f077fb0e 100644
--- a/README.md
+++ b/README.md
@@ -2,27 +2,26 @@
 
 ![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)
 
-[JAX](http://go/jax) is [Autograd](https://github.com/hips/autograd) and
+JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
 brought together for high-performance machine learning research.
 
-With its updated version of [Autograd](https://github.com/hips/autograd), JAX
-can automatically differentiate native Python and NumPy code. It can
-differentiate through a large subset of Pythons features, including loops,
-ifs, recursion, and closures, and it can even take derivatives of derivatives
-of derivatives. It supports reverse-mode differentiation (a.k.a.
-backpropagation) as well as forward-mode differentiation, and the two can be
-composed arbitrarily to any order.
+With its updated version of Autograd, JAX can automatically differentiate native
+Python and NumPy functions. It can differentiate through loops, branches,
+recursion, and closures, and it can take derivatives of derivatives of
+derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)
+as well as forward-mode differentiation, and the two can be composed arbitrarily
+to any order.
 
 Whats new is that JAX uses
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
-to compile and run your NumPy code on accelerators, like GPUs and TPUs.
-Compilation happens under the hood by default, with library calls getting
-just-in-time compiled and executed. But JAX even lets you just-in-time compile
-your own Python functions into XLA-optimized kernels using a one-function API.
-Compilation and automatic differentiation can be composed arbitrarily, so you
-can express sophisticated algorithms and get maximal performance without having
-to leave Python.
+to compile and run your NumPy functions on GPUs and TPUs. Compilation happens
+under the hood by default, with library calls getting just-in-time compiled and
+executed. But JAX also lets you just-in-time compile your own Python functions
+into XLA-optimized kernels using a one-function API,
+[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be
+composed arbitrarily, so you can express sophisticated algorithms and get
+maximal performance without leaving Python.
 
 This is a research project, not an official Google product. Expect bugs and
 sharp edges. Please help by trying it out, [reporting","diff --git a/README.md b/README.md
index d34c9329b..0f077fb0e 100644
--- a/README.md
+++ b/README.md
@@ -2,27 +2,26 @@
 
 ![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)
 
-[JAX](http://go/jax) is [Autograd](https://github.com/hips/autograd) and
+JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
 brought together for high-performance machine learning research.
 
-With its updated version of [Autograd](https://github.com/hips/autograd), JAX
-can automatically differentiate native Python and NumPy code. It can
-differentiate through a large subset of Pythons features, including loops,
-ifs, recursion, and closures, and it can even take derivatives of derivatives
-of derivatives. It supports reverse-mode differentiation (a.k.a.
-backpropagation) as well as forward-mode differentiation, and the two can be
-composed arbitrarily to any order.
+With its updated version of Autograd, JAX can automatically differentiate native
+Python and NumPy functions. It can differentiate through loops, branches,
+recursion, and closures, and it can take derivatives of derivatives of
+derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)
+as well as forward-mode differentiation, and the two can be composed arbitrarily
+to any order.
 
 Whats new is that JAX uses
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
-to compile and run your NumPy code on accelerators, like GPUs and TPUs.
-Compilation happens under the hood by default, with library calls getting
-just-in-time compiled and executed. But JAX even lets you just-in-time compile
-your own Python functions into XLA-optimized kernels using a one-function API.
-Compilation and automatic differentiation can be composed arbitrarily, so you
-can express sophisticated algorithms and get maximal performance without having
-to leave Python.
+to compile and run your NumPy functions on GPUs and TPUs. Compilation happens
+under the hood by default, with library calls getting just-in-time compiled and
+executed. But JAX also lets you just-in-time compile your own Python functions
+into XLA-optimized kernels using a one-function API,
+[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be
+composed arbitrarily, so you can express sophisticated algorithms and get
+maximal performance without leaving Python.
 
 This is a research project, not an official Google product. Expect bugs and
 sharp edges. Please help by trying it out, [reporting",No
README.md,README.md,59fc69e0d42c9062fcaaf3a5ab97fdf50af1e5d8,838d9e12fe2c7d71fc25e7833dd73e99ba1ef316,Small edits to readme,"diff --git a/README.md b/README.md
index 0f077fb0e..672ffecd6 100644
--- a/README.md
+++ b/README.md
@@ -15,7 +15,7 @@ to any order.
 
 Whats new is that JAX uses
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
-to compile and run your NumPy functions on GPUs and TPUs. Compilation happens
+to compile and run your NumPy programs on GPUs and TPUs. Compilation happens
 under the hood by default, with library calls getting just-in-time compiled and
 executed. But JAX also lets you just-in-time compile your own Python functions
 into XLA-optimized kernels using a one-function API,
@@ -79,7 +79,7 @@ To build XLA with CUDA support, you can run
 ```bash
 python build/build.py --enable_cuda
 pip install -e build  # install jaxlib
-pip install -e .  # install jax
+pip install -e .      # install jax (pure Python)
 ```
 
 See `python build/build.py --help` for configuration options, including ways to
@@ -87,18 +87,18 @@ specify the paths to CUDA and CUDNN, which you must have installed. The build
 also depends on NumPy, and a compiler toolchain corresponding to that of
 Ubuntu 16.04 or newer.
 
-To build XLA without CUDA GPU support (CPU only), just run
+To build XLA without CUDA GPU support (CPU only), drop the `--enable_cuda`:
 
 ```bash
 python build/build.py
 pip install -e build  # install jaxlib
-pip install -e .  # install jax
+pip install -e .      # install jax
 ```
 
-To update to the latest version from GitHub, just run `git pull` from the JAX
-repository root, and rebuild by running `build.py` if necessary. You only have
-to reinstall if new files are added because `pip install -e` sets up symbolic
-links from site-packages into the repository.
+To upgrade to the latest version from GitHub, just run `git pull` from the JAX
+repository root, and rebuild by running `build.py` if necessary. You shouldn't have
+to reinstall because `pip install -e` sets up symbolic links from site-packages
+into the repository.
 
 ### pip installation
 
@@ -121,7 +121,7 @@ cloud VM), you can run
 # install jaxlib
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
-PLATFORM=linux_x86_64  # alternatives: linux_x86_64, mac
+PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
 pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
@@ -159,10 +159,11 @@ Whats happening behind-the-scenes is that JAX is using XLA to just-in-time
 (JIT) compile and execute these individual operations on the GPU. First the
 `random.normal` call is compiled and the array referred to by `x` is generated
 on the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and
-`divide`) is JIT-compiled and executed, each keeping its results on the device.
+`divide`) is individually JIT-compiled and executed, each keeping its results on
+the device.
 Its only when a value needs to be printed, plotted, saved, or passed into a raw
 NumPy function that a read-only copy of the value is brought back to the host as
-an ndarray and cached. The second call to `dot` is even faster because the
+an ndarray and cached. The second call to `dot` is faster because the
 JIT-compiled code is cached and reused, saving the compilation time.
 
 The fun really starts when you use `grad` for automatic differentiation and
@@ -216,19 +217,19 @@ examples](https://github.com/google/jax/blob/master/examples/).
 
 If youre using JAX just as an accelerator-backed NumPy, without using `grad` or
 `jit` in your code, then in principle there are no constraints, though some
-NumPy features havent been implemented. Generally using `np.dot(A, B)` is
+NumPy functions havent been implemented yet. Generally using `np.dot(A, B)` is
 better than `A.dot(B)` because the former gives us more opportunities to run the
 computation on the device. NumPy also does a lot of work to cast any array-like
 function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
 typically requires explicit casting of array arguments, like
 `np.sum(np.array([x, y]))`.
 
-For automatic differentiation with `grad`, JAX has the same basic requirements
+For automatic differentiation with `grad`, JAX has the same restrictions
 as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
 works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
 x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
-dicts freely. Using `np.dot(A, B)` rather than `A.dot(B)` is required for
-automatic differentiation when `A` is a raw ndarray.
+dicts freely: jax doesn't even see them. Using `np.dot(A, B)` rather than
+`A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.
 
 For compiling your own functions with `jit` there are a few more requirements.
 Because `jit` aims to specialize Python functions only on shapes and dtypes
@@ -240,6 +241,11 @@ lax.cond and lax.while. Some indexing features, like slice-based indexing
 for argument-dependent `bool_ind`, produce abstract values of unknown shape and
 are thus unsupported in `jit` functions.
 
+In general, JAX is intended to be used with a functional style of Python
+programming. Functions passed to transformations like `grad` and `jit` are
+expected to be free of side-effects. You can write print statements for
+debugging but they may only be executed once if they're under a `jit` decorator.
+
 > TLDR **Do use**
 >
 > *   Functional programming
@@ -270,8 +276,8 @@ You should get loud errors if your code violates any of these.
 
 ## Transformations
 
-JAX is at its core an extensible system for transforming numerical functions.
-Here are three key transformations for machine learning research.
+At its core, JAX is an extensible system for transforming numerical functions.
+We currently expose three important transformations: `grad`, `jit`, and `vmap`.
 
 ### Automatic differentiation with grad
 
@@ -343,8 +349,7 @@ You can mix `jit` and `grad` and any other JAX transformation however you like.
 
 ### Auto-vectorization with vmap
 
-JAX enables more program transformations than just forward- and reverse-mode
-differentiation and compilation. Another example is `vmap`, the vectorizing map.
+`vmap` is the vectorizing map.
 It has the familiar semantics of mapping a function along array axes, but
 instead of keeping the loop on the outside, it pushes the loop down into a
 functions primitive operations for better performance.
@@ -405,7 +410,7 @@ differentiation for fast Jacobian and Hessian matrix calculations in
 
 ## Random numbers are different
 
-JAX needs a pseudo-random number generator (PRNG) system to provide
+JAX needs a functional pseudo-random number generator (PRNG) system to provide
 reproducible results invariant to compilation boundaries and backends, while
 also maximizing performance by enabling vectorized generation and
 parallelization across random calls. The `numpy.random` library doesnt have
@@ -451,6 +456,11 @@ By splitting the PRNG key, not only do we avoid having to thread random states
 back out of every function call, but also we can generate multiple random arrays
 in parallel because we can avoid unnecessary sequential dependencies.
 
+There's a gotcha here, which is that it's easy to unintentionally reuse a key
+without splitting. We intend to add a check for this (a sort of dynamic linear
+typing) but for now it's something to be careful about.
+
+
 ## Mini-libraries
 
 JAX provides some small, experimental libraries for machine learning. These
@@ -465,7 +475,7 @@ a single layer or an entire network can be modeled as an `(init_fun, apply_fun)`
 pair. The `init_fun` is used to initialize network parameters and the
 `apply_fun` takes parameters and inputs to produce outputs. There are
 constructor functions for common basic pairs, like `Conv` and `Relu`, and these
-pairs can be composed in serial using `stax.serial` or in parallel using
+pairs can be composed in series using `stax.serial` or in parallel using
 `stax.parallel`.
 
 Heres an example:
@@ -532,7 +542,7 @@ net_params = minmax.get_params(opt_state)
 
 Programming in machine learning is about expressing and transforming functions.
 Transformations include automatic differentiation, compilation for accelerators,
-and even automatic batching. High-level languages like Python are great for
+and automatic batching. High-level languages like Python are great for
 expressing functions, but usually all we can do with them is apply them. We lose
 access to their internal structure which would let us perform transformations.
 
@@ -558,12 +568,13 @@ tracing in instances of the `Tracer` class, and call the function on them.
 Abstract arguments represent sets of possible values rather than specific
 values: for example, `jit` abstracts ndarray arguments to abstract values that
 represent all ndarrays with the same shape and dtype. In contrast, `grad`
-abstracts ndarray arguments to represent a small neighborhood of the underlying
+abstracts ndarray arguments to represent an infinitesimal neighborhood of the
+underlying
 value. By tracing the Python function on these abstract values, we ensure that
 its specialized enough so that its tractable to transform, and that its still
-general enough so that the transformed result is useful. These transformed
-functions are then lifted back into Python callables in a way that allows them
-to be traced and transformed again as needed.
+general enough so that the transformed result is useful, and possibly reusable.
+These transformed functions are then lifted back into Python callables in a way
+that allows them to be traced and transformed again as needed.
 
 The primitive functions that JAX traces are mostly in 1:1 correspondence with
 [XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined","diff --git a/README.md b/README.md
index 0f077fb0e..672ffecd6 100644
--- a/README.md
+++ b/README.md
@@ -15,7 +15,7 @@ to any order.
 
 Whats new is that JAX uses
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
-to compile and run your NumPy functions on GPUs and TPUs. Compilation happens
+to compile and run your NumPy programs on GPUs and TPUs. Compilation happens
 under the hood by default, with library calls getting just-in-time compiled and
 executed. But JAX also lets you just-in-time compile your own Python functions
 into XLA-optimized kernels using a one-function API,
@@ -79,7 +79,7 @@ To build XLA with CUDA support, you can run
 ```bash
 python build/build.py --enable_cuda
 pip install -e build  # install jaxlib
-pip install -e .  # install jax
+pip install -e .      # install jax (pure Python)
 ```
 
 See `python build/build.py --help` for configuration options, including ways to
@@ -87,18 +87,18 @@ specify the paths to CUDA and CUDNN, which you must have installed. The build
 also depends on NumPy, and a compiler toolchain corresponding to that of
 Ubuntu 16.04 or newer.
 
-To build XLA without CUDA GPU support (CPU only), just run
+To build XLA without CUDA GPU support (CPU only), drop the `--enable_cuda`:
 
 ```bash
 python build/build.py
 pip install -e build  # install jaxlib
-pip install -e .  # install jax
+pip install -e .      # install jax
 ```
 
-To update to the latest version from GitHub, just run `git pull` from the JAX
-repository root, and rebuild by running `build.py` if necessary. You only have
-to reinstall if new files are added because `pip install -e` sets up symbolic
-links from site-packages into the repository.
+To upgrade to the latest version from GitHub, just run `git pull` from the JAX
+repository root, and rebuild by running `build.py` if necessary. You shouldn't have
+to reinstall because `pip install -e` sets up symbolic links from site-packages
+into the repository.
 
 ### pip installation
 
@@ -121,7 +121,7 @@ cloud VM), you can run
 # install jaxlib
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
-PLATFORM=linux_x86_64  # alternatives: linux_x86_64, mac
+PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
 pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
@@ -159,10 +159,11 @@ Whats happening behind-the-scenes is that JAX is using XLA to just-in-time
 (JIT) compile and execute these individual operations on the GPU. First the
 `random.normal` call is compiled and the array referred to by `x` is generated
 on the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and
-`divide`) is JIT-compiled and executed, each keeping its results on the device.
+`divide`) is individually JIT-compiled and executed, each keeping its results on
+the device.
 Its only when a value needs to be printed, plotted, saved, or passed into a raw
 NumPy function that a read-only copy of the value is brought back to the host as
-an ndarray and cached. The second call to `dot` is even faster because the
+an ndarray and cached. The second call to `dot` is faster because the
 JIT-compiled code is cached and reused, saving the compilation time.
 
 The fun really starts when you use `grad` for automatic differentiation and
@@ -216,19 +217,19 @@ examples](https://github.com/google/jax/blob/master/examples/).
 
 If youre using JAX just as an accelerator-backed NumPy, without using `grad` or
 `jit` in your code, then in principle there are no constraints, though some
-NumPy features havent been implemented. Generally using `np.dot(A, B)` is
+NumPy functions havent been implemented yet. Generally using `np.dot(A, B)` is
 better than `A.dot(B)` because the former gives us more opportunities to run the
 computation on the device. NumPy also does a lot of work to cast any array-like
 function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
 typically requires explicit casting of array arguments, like
 `np.sum(np.array([x, y]))`.
 
-For automatic differentiation with `grad`, JAX has the same basic requirements
+For automatic differentiation with `grad`, JAX has the same restrictions
 as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
 works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
 x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
-dicts freely. Using `np.dot(A, B)` rather than `A.dot(B)` is required for
-automatic differentiation when `A` is a raw ndarray.
+dicts freely: jax doesn't even see them. Using `np.dot(A, B)` rather than
+`A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.
 
 For compiling your own functions with `jit` there are a few more requirements.
 Because `jit` aims to specialize Python functions only on shapes and dtypes
@@ -240,6 +241,11 @@ lax.cond and lax.while. Some indexing features, like slice-based indexing
 for argument-dependent `bool_ind`, produce abstract values of unknown shape and
 are thus unsupported in `jit` functions.
 
+In general, JAX is intended to be used with a functional style of Python
+programming. Functions passed to transformations like `grad` and `jit` are
+expected to be free of side-effects. You can write print statements for
+debugging but they may only be executed once if they're under a `jit` decorator.
+
 > TLDR **Do use**
 >
 > *   Functional programming
@@ -270,8 +276,8 @@ You should get loud errors if your code violates any of these.
 
 ## Transformations
 
-JAX is at its core an extensible system for transforming numerical functions.
-Here are three key transformations for machine learning research.
+At its core, JAX is an extensible system for transforming numerical functions.
+We currently expose three important transformations: `grad`, `jit`, and `vmap`.
 
 ### Automatic differentiation with grad
 
@@ -343,8 +349,7 @@ You can mix `jit` and `grad` and any other JAX transformation however you like.
 
 ### Auto-vectorization with vmap
 
-JAX enables more program transformations than just forward- and reverse-mode
-differentiation and compilation. Another example is `vmap`, the vectorizing map.
+`vmap` is the vectorizing map.
 It has the familiar semantics of mapping a function along array axes, but
 instead of keeping the loop on the outside, it pushes the loop down into a
 functions primitive operations for better performance.
@@ -405,7 +410,7 @@ differentiation for fast Jacobian and Hessian matrix calculations in
 
 ## Random numbers are different
 
-JAX needs a pseudo-random number generator (PRNG) system to provide
+JAX needs a functional pseudo-random number generator (PRNG) system to provide
 reproducible results invariant to compilation boundaries and backends, while
 also maximizing performance by enabling vectorized generation and
 parallelization across random calls. The `numpy.random` library doesnt have
@@ -451,6 +456,11 @@ By splitting the PRNG key, not only do we avoid having to thread random states
 back out of every function call, but also we can generate multiple random arrays
 in parallel because we can avoid unnecessary sequential dependencies.
 
+There's a gotcha here, which is that it's easy to unintentionally reuse a key
+without splitting. We intend to add a check for this (a sort of dynamic linear
+typing) but for now it's something to be careful about.
+
+
 ## Mini-libraries
 
 JAX provides some small, experimental libraries for machine learning. These
@@ -465,7 +475,7 @@ a single layer or an entire network can be modeled as an `(init_fun, apply_fun)`
 pair. The `init_fun` is used to initialize network parameters and the
 `apply_fun` takes parameters and inputs to produce outputs. There are
 constructor functions for common basic pairs, like `Conv` and `Relu`, and these
-pairs can be composed in serial using `stax.serial` or in parallel using
+pairs can be composed in series using `stax.serial` or in parallel using
 `stax.parallel`.
 
 Heres an example:
@@ -532,7 +542,7 @@ net_params = minmax.get_params(opt_state)
 
 Programming in machine learning is about expressing and transforming functions.
 Transformations include automatic differentiation, compilation for accelerators,
-and even automatic batching. High-level languages like Python are great for
+and automatic batching. High-level languages like Python are great for
 expressing functions, but usually all we can do with them is apply them. We lose
 access to their internal structure which would let us perform transformations.
 
@@ -558,12 +568,13 @@ tracing in instances of the `Tracer` class, and call the function on them.
 Abstract arguments represent sets of possible values rather than specific
 values: for example, `jit` abstracts ndarray arguments to abstract values that
 represent all ndarrays with the same shape and dtype. In contrast, `grad`
-abstracts ndarray arguments to represent a small neighborhood of the underlying
+abstracts ndarray arguments to represent an infinitesimal neighborhood of the
+underlying
 value. By tracing the Python function on these abstract values, we ensure that
 its specialized enough so that its tractable to transform, and that its still
-general enough so that the transformed result is useful. These transformed
-functions are then lifted back into Python callables in a way that allows them
-to be traced and transformed again as needed.
+general enough so that the transformed result is useful, and possibly reusable.
+These transformed functions are then lifted back into Python callables in a way
+that allows them to be traced and transformed again as needed.
 
 The primitive functions that JAX traces are mostly in 1:1 correspondence with
 [XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined",No
README.md,README.md,16658b97c4ff676e67560d57040122e4e86f0614,59fc69e0d42c9062fcaaf3a5ab97fdf50af1e5d8,fix typo in wheel path,"diff --git a/README.md b/README.md
index 672ffecd6..3ef0705d6 100644
--- a/README.md
+++ b/README.md
@@ -122,7 +122,7 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
 ```","diff --git a/README.md b/README.md
index 672ffecd6..3ef0705d6 100644
--- a/README.md
+++ b/README.md
@@ -122,7 +122,7 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jax-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
 ```",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,7523975c2e9ec34998d54266ae6e141eaf068fb8,13ed022245eede526fe36ee572744a71bf655f7c,Updated installs in notebook,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 68df26850..3b8c69f1a 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,8 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install jax"",
       ],
       ""execution_count"": 0,
       ""outputs"": []","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 68df26850..3b8c69f1a 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,8 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install jax"",
       ],
       ""execution_count"": 0,
       ""outputs"": []",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,7be7d42484fa97fe54b5e57ab9e1a3dea246edb5,7523975c2e9ec34998d54266ae6e141eaf068fb8,Fixed json bug,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 3b8c69f1a..962ddb45b 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -90,7 +90,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax"",
+        ""!pip install jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 3b8c69f1a..962ddb45b 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -90,7 +90,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax"",
+        ""!pip install jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,89ec22e5bd13c14acbca788425d4637470f62f07,d6e2dce9f627467de3e6912b1d87418071d2a344,Fixes per review,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 21ffb4304..84315e1a2 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -3,7 +3,7 @@
   ""nbformat_minor"": 0,
   ""metadata"": {
     ""colab"": {
-      ""name"": ""JAX Gufuncs.ipynb"",
+      ""name"": ""JAX generalized ufuncs.ipynb"",
       ""version"": ""0.3.2"",
       ""provenance"": [],
       ""collapsed_sections"": []
@@ -125,7 +125,8 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install -q jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []
@@ -153,9 +154,7 @@
       },
       ""cell_type"": ""markdown"",
       ""source"": [
-        ""### Copied from `numpy.lib.function_base`\n"",
-        ""\n"",
-        ""(but note that it's still Google owned since it was [authored by](https://github.com/numpy/numpy/commit/0a02bb6f62a5c09103cf748bafe7a622f3dfe98b) shoyer)""
+        ""### Copied from `numpy.lib.function_base`""
       ]
     },
     {
@@ -564,7 +563,7 @@
       ""metadata"": {
         ""id"": ""MSyxOrUPixDI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""514bbc0b-05fa-4d56-cc28-2fceab8d1824"",
+        ""outputId"": ""e8d05c0c-a337-4a9c-bdc8-8645616b4b93"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 35
@@ -590,11 +589,11 @@
       ""metadata"": {
         ""id"": ""7UZGOS8FGT_D"",
         ""colab_type"": ""code"",
+        ""outputId"": ""fdb7dad9-721c-439a-ffd6-c469445f371a"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
-        },
-        ""outputId"": ""0bdb89da-b74f-4e47-9fe9-e48228fd5856""
+        }
       },
       ""cell_type"": ""code"",
       ""source"": [
@@ -623,7 +622,7 @@
       ""metadata"": {
         ""id"": ""n2Fz91ptjM_7"",
         ""colab_type"": ""code"",
-        ""outputId"": ""bbda6248-c8fe-4c10-cf87-9b81387c874c"",
+        ""outputId"": ""0a75386a-8238-4fca-c0c8-1d8ac9573e48"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
@@ -651,11 +650,11 @@
       ""metadata"": {
         ""id"": ""G-AyCkAK4RKT"",
         ""colab_type"": ""code"",
+        ""outputId"": ""b4738597-d189-43b9-ad52-67a527b4c179"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
-        },
-        ""outputId"": ""6ef5b64e-a294-4477-b608-dfe4aea3d674""
+        }
       },
       ""cell_type"": ""code"",
       ""source"": [
@@ -679,7 +678,7 @@
       ""metadata"": {
         ""id"": ""_FhnjYMUjZgI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""c796784b-04ca-4d05-a12d-490de5c46b2c"",
+        ""outputId"": ""b81b93ac-5f6a-43ce-ce85-17d6c237891a"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
@@ -718,4 +717,4 @@
       ""outputs"": []
     }
   ]
-}
+}
\ No newline at end of file","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 21ffb4304..84315e1a2 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -3,7 +3,7 @@
   ""nbformat_minor"": 0,
   ""metadata"": {
     ""colab"": {
-      ""name"": ""JAX Gufuncs.ipynb"",
+      ""name"": ""JAX generalized ufuncs.ipynb"",
       ""version"": ""0.3.2"",
       ""provenance"": [],
       ""collapsed_sections"": []
@@ -125,7 +125,8 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jax-0.0-py3-none-any.whl""
+        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install -q jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []
@@ -153,9 +154,7 @@
       },
       ""cell_type"": ""markdown"",
       ""source"": [
-        ""### Copied from `numpy.lib.function_base`\n"",
-        ""\n"",
-        ""(but note that it's still Google owned since it was [authored by](https://github.com/numpy/numpy/commit/0a02bb6f62a5c09103cf748bafe7a622f3dfe98b) shoyer)""
+        ""### Copied from `numpy.lib.function_base`""
       ]
     },
     {
@@ -564,7 +563,7 @@
       ""metadata"": {
         ""id"": ""MSyxOrUPixDI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""514bbc0b-05fa-4d56-cc28-2fceab8d1824"",
+        ""outputId"": ""e8d05c0c-a337-4a9c-bdc8-8645616b4b93"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 35
@@ -590,11 +589,11 @@
       ""metadata"": {
         ""id"": ""7UZGOS8FGT_D"",
         ""colab_type"": ""code"",
+        ""outputId"": ""fdb7dad9-721c-439a-ffd6-c469445f371a"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
-        },
-        ""outputId"": ""0bdb89da-b74f-4e47-9fe9-e48228fd5856""
+        }
       },
       ""cell_type"": ""code"",
       ""source"": [
@@ -623,7 +622,7 @@
       ""metadata"": {
         ""id"": ""n2Fz91ptjM_7"",
         ""colab_type"": ""code"",
-        ""outputId"": ""bbda6248-c8fe-4c10-cf87-9b81387c874c"",
+        ""outputId"": ""0a75386a-8238-4fca-c0c8-1d8ac9573e48"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
@@ -651,11 +650,11 @@
       ""metadata"": {
         ""id"": ""G-AyCkAK4RKT"",
         ""colab_type"": ""code"",
+        ""outputId"": ""b4738597-d189-43b9-ad52-67a527b4c179"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
-        },
-        ""outputId"": ""6ef5b64e-a294-4477-b608-dfe4aea3d674""
+        }
       },
       ""cell_type"": ""code"",
       ""source"": [
@@ -679,7 +678,7 @@
       ""metadata"": {
         ""id"": ""_FhnjYMUjZgI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""c796784b-04ca-4d05-a12d-490de5c46b2c"",
+        ""outputId"": ""b81b93ac-5f6a-43ce-ce85-17d6c237891a"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
           ""height"": 71
@@ -718,4 +717,4 @@
       ""outputs"": []
     }
   ]
-}
+}
\ No newline at end of file",No
README.md,README.md,0505ce29503b431dcd0e1329f16d3404a502398d,5bbf3ee0773063b550efb05f81f9a0a7823d4e66,typo,"diff --git a/README.md b/README.md
index 3ef0705d6..eb3dd140d 100644
--- a/README.md
+++ b/README.md
@@ -491,7 +491,7 @@ from jax.experimental.stax import LogSoftmax
 # Set up network initialization and evaluation functions
 net_init, net_apply = stax.serial(
     Conv(32, (3, 3), padding='SAME'), Relu,
-    Conv(64, (3, 3), padding='SAME'), Relu
+    Conv(64, (3, 3), padding='SAME'), Relu,
     MaxPool((2, 2)), Flatten,
     Dense(128), Relu,
     Dense(10), SoftMax,","diff --git a/README.md b/README.md
index 3ef0705d6..eb3dd140d 100644
--- a/README.md
+++ b/README.md
@@ -491,7 +491,7 @@ from jax.experimental.stax import LogSoftmax
 # Set up network initialization and evaluation functions
 net_init, net_apply = stax.serial(
     Conv(32, (3, 3), padding='SAME'), Relu,
-    Conv(64, (3, 3), padding='SAME'), Relu
+    Conv(64, (3, 3), padding='SAME'), Relu,
     MaxPool((2, 2)), Flatten,
     Dense(128), Relu,
     Dense(10), SoftMax,",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,551601965661c95e406c6e420e06775ac300862d,69b0c2299cbeeec59703a0cdd22125d780fb2360,Fix link in gufuncs notebook,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 84315e1a2..518953942 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -28,7 +28,7 @@
         ""\n"",
         ""## What is a gufunc?\n"",
         ""\n"",
-        ""[Generalized universal functions]((https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
+        ""[Generalized universal functions](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
         ""- \""core dimensions\"" over which an operation is defined.\n"",
         ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
         ""\n"",
@@ -717,4 +717,4 @@
       ""outputs"": []
     }
   ]
-}
\ No newline at end of file
+}","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 84315e1a2..518953942 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -28,7 +28,7 @@
         ""\n"",
         ""## What is a gufunc?\n"",
         ""\n"",
-        ""[Generalized universal functions]((https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
+        ""[Generalized universal functions](https://docs.scipy.org/doc/numpy-1.15.0/reference/c-api.generalized-ufuncs.html) (\""gufuncs\"") are one of my favorite abstractions from NumPy. They generalize NumPy's [broadcasting rules](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html) to handle non-scalar operations. When a gufuncs is applied to arrays, there are:\n"",
         ""- \""core dimensions\"" over which an operation is defined.\n"",
         ""- \""broadcast dimensions\"" over which operations can be automatically vectorized.\n"",
         ""\n"",
@@ -717,4 +717,4 @@
       ""outputs"": []
     }
   ]
-}
\ No newline at end of file
+}",No
setup.py,setup.py,70604f4309df7b7c3ae3a0a512fb197575555c7f,551601965661c95e406c6e420e06775ac300862d,add scipy.stats to setup.py (should use find_packages()),"diff --git a/setup.py b/setup.py
index d7e145826..3f9cbfa4d 100644
--- a/setup.py
+++ b/setup.py
@@ -16,12 +16,12 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.1',
+    version='0.1.2',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
-              'jax.experimental'],
+              'jax.scipy.stats', 'jax.experimental'],
     install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py',
                       'opt_einsum'],
     url='https://github.com/google/jax',","diff --git a/setup.py b/setup.py
index d7e145826..3f9cbfa4d 100644
--- a/setup.py
+++ b/setup.py
@@ -16,12 +16,12 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.1',
+    version='0.1.2',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
-              'jax.experimental'],
+              'jax.scipy.stats', 'jax.experimental'],
     install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py',
                       'opt_einsum'],
     url='https://github.com/google/jax',",No
README.md,README.md,0528192bd93729951c21ba52855df3271a3aee56,70604f4309df7b7c3ae3a0a512fb197575555c7f,"Remove Mac OS X from the list of CUDA pip packages

We haven't created this pip package.","diff --git a/README.md b/README.md
index eb3dd140d..ec8111c2c 100644
--- a/README.md
+++ b/README.md
@@ -121,7 +121,7 @@ cloud VM), you can run
 # install jaxlib
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
-PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
+PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax","diff --git a/README.md b/README.md
index eb3dd140d..ec8111c2c 100644
--- a/README.md
+++ b/README.md
@@ -121,7 +121,7 @@ cloud VM), you can run
 # install jaxlib
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
-PLATFORM=linux_x86_64  # alternatives: linux_x86_64, macosx-10.6-x86_64
+PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax",No
README.md,README.md,0900db35b64400f1e6c3a511e32f614a79ec1418,0528192bd93729951c21ba52855df3271a3aee56,Add ToC to readme,"diff --git a/README.md b/README.md
index ec8111c2c..ad9d70c04 100644
--- a/README.md
+++ b/README.md
@@ -55,6 +55,18 @@ Leary](https://github.com/learyg), and is now developed [in the
 open](https://github.com/google/jax) by a growing number of
 [contributors](#contributors).
 
+### Contents
+* [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)
+* [Installation](#installation)
+* [A brief tour](#a-brief-tour)
+* [What's supported](#whats-supported)
+* [Transformations](#transformations)
+* [Random numbers are different](#random-numbers-are-different)
+* [Mini-libraries](#mini-libraries)
+* [How it works](#how-it-works)
+* [What we're working on](#what-were-working-on)
+* [Current gotchas](#current-gotchas)
+
 ## Quickstart: Colab in the Cloud
 Jump right in using [a notebook in your
 browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)","diff --git a/README.md b/README.md
index ec8111c2c..ad9d70c04 100644
--- a/README.md
+++ b/README.md
@@ -55,6 +55,18 @@ Leary](https://github.com/learyg), and is now developed [in the
 open](https://github.com/google/jax) by a growing number of
 [contributors](#contributors).
 
+### Contents
+* [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)
+* [Installation](#installation)
+* [A brief tour](#a-brief-tour)
+* [What's supported](#whats-supported)
+* [Transformations](#transformations)
+* [Random numbers are different](#random-numbers-are-different)
+* [Mini-libraries](#mini-libraries)
+* [How it works](#how-it-works)
+* [What we're working on](#what-were-working-on)
+* [Current gotchas](#current-gotchas)
+
 ## Quickstart: Colab in the Cloud
 Jump right in using [a notebook in your
 browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)",No
jax/util.py,jax/util.py,30124b6da1eca136a026eb40c94f8fad6885a467,0900db35b64400f1e6c3a511e32f614a79ec1418,Added jit transformations to generated functions. Fixed bug in comparing numpy arrays for equality.,"diff --git a/jax/util.py b/jax/util.py
index 38d22f00f..ea686229b 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -153,5 +153,4 @@ class WrapHashably(object):
     return id(self.val)
 
   def __eq__(self, other):
-    return self.val == other.val
-
+    return self.val is other.val","diff --git a/jax/util.py b/jax/util.py
index 38d22f00f..ea686229b 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -153,5 +153,4 @@ class WrapHashably(object):
     return id(self.val)
 
   def __eq__(self, other):
-    return self.val == other.val
-
+    return self.val is other.val",No
tests/generated_fun_test.py,tests/generated_fun_test.py,30124b6da1eca136a026eb40c94f8fad6885a467,0900db35b64400f1e6c3a511e32f614a79ec1418,Added jit transformations to generated functions. Fixed bug in comparing numpy arrays for equality.,"diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 3fbc71ebd..dec320953 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -65,6 +65,7 @@ def gen_function(size, in_types):
       arg_types = [v.vartype for v in arg_vars]
       fun, out_types = gen_function(size / size_reduction_factor, arg_types)
       fun = partial(eval_fun, fun)
+      fun = maybe_jit(fun, len(arg_types))
     else:
       arity = choice(list(primitive_generators))
       arg_vars = gen_sized_subset(cur_vars, arity)
@@ -96,6 +97,10 @@ def eval_fun(fun, *args):
 
   return map(read, fun.out_vars)
 
+def maybe_jit(f, num_args):
+  static_argnums = thin(range(num_args), 0.5)
+  return jit(f, static_argnums=static_argnums)
+
 counter = it.count()
 def fresh_var(ty):
   return Var(next(counter), ty)","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index 3fbc71ebd..dec320953 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -65,6 +65,7 @@ def gen_function(size, in_types):
       arg_types = [v.vartype for v in arg_vars]
       fun, out_types = gen_function(size / size_reduction_factor, arg_types)
       fun = partial(eval_fun, fun)
+      fun = maybe_jit(fun, len(arg_types))
     else:
       arity = choice(list(primitive_generators))
       arg_vars = gen_sized_subset(cur_vars, arity)
@@ -96,6 +97,10 @@ def eval_fun(fun, *args):
 
   return map(read, fun.out_vars)
 
+def maybe_jit(f, num_args):
+  static_argnums = thin(range(num_args), 0.5)
+  return jit(f, static_argnums=static_argnums)
+
 counter = it.count()
 def fresh_var(ty):
   return Var(next(counter), ty)",No
README.md,README.md,d2b0e30d5405c2ca7bfd7971a6588dc928cfa227,30124b6da1eca136a026eb40c94f8fad6885a467,update readme (accidentally lost a name!),"diff --git a/README.md b/README.md
index ad9d70c04..0e7d92357 100644
--- a/README.md
+++ b/README.md
@@ -644,9 +644,14 @@ Some things we don't handle that might surprise NumPy users:
 
 ## Contributors
 
-So far, JAX includes lots of help and contributions from [Peter
-Hawkins](https://github.com/hawkinsp), [Alex
-Wiltschko](http://github.com/alexbw), George Dahl, [Eli
-Bendersky](https://github.com/eliben), Zak Stone, [Alexey
-Radul](https://github.com/axch), Michael Isard, Skye Wanderman-Milne, and many
-others.
+So far, JAX includes lots of help and contributions from
+[Jamie Townsend](https://github.com/j-towns),
+[Peter Hawkins](https://github.com/hawkinsp),
+[Alex Wiltschko](http://github.com/alexbw),
+George Dahl,
+[Eli Bendersky](https://github.com/eliben),
+Zak Stone,
+[Alexey Radul](https://github.com/axch),
+Michael Isard,
+Skye Wanderman-Milne,
+and many others.","diff --git a/README.md b/README.md
index ad9d70c04..0e7d92357 100644
--- a/README.md
+++ b/README.md
@@ -644,9 +644,14 @@ Some things we don't handle that might surprise NumPy users:
 
 ## Contributors
 
-So far, JAX includes lots of help and contributions from [Peter
-Hawkins](https://github.com/hawkinsp), [Alex
-Wiltschko](http://github.com/alexbw), George Dahl, [Eli
-Bendersky](https://github.com/eliben), Zak Stone, [Alexey
-Radul](https://github.com/axch), Michael Isard, Skye Wanderman-Milne, and many
-others.
+So far, JAX includes lots of help and contributions from
+[Jamie Townsend](https://github.com/j-towns),
+[Peter Hawkins](https://github.com/hawkinsp),
+[Alex Wiltschko](http://github.com/alexbw),
+George Dahl,
+[Eli Bendersky](https://github.com/eliben),
+Zak Stone,
+[Alexey Radul](https://github.com/axch),
+Michael Isard,
+Skye Wanderman-Milne,
+and many others.",No
README.md,README.md,13163ffefe747f8edfb3c7cae4f6948949589a66,d2b0e30d5405c2ca7bfd7971a6588dc928cfa227,"Typo, Python parens","diff --git a/README.md b/README.md
index 0e7d92357..fa518403e 100644
--- a/README.md
+++ b/README.md
@@ -334,8 +334,8 @@ def abs_val(x):
     return -x
 
 abs_val_grad = grad(abs_val)
-print(abs_val_grad)(1.0)   # prints 1.0
-print(abs_val_grad)(-1.0)  # prints -1.0 (abs_val is re-evaluated)
+print(abs_val_grad(1.0))   # prints 1.0
+print(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)
 ```
 
 ### Compilation with jit","diff --git a/README.md b/README.md
index 0e7d92357..fa518403e 100644
--- a/README.md
+++ b/README.md
@@ -334,8 +334,8 @@ def abs_val(x):
     return -x
 
 abs_val_grad = grad(abs_val)
-print(abs_val_grad)(1.0)   # prints 1.0
-print(abs_val_grad)(-1.0)  # prints -1.0 (abs_val is re-evaluated)
+print(abs_val_grad(1.0))   # prints 1.0
+print(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)
 ```
 
 ### Compilation with jit",No
README.md,README.md,9d484b6d4ce1d9131dd645bfa11b47f2ab3ab060,9e739fbfc51d91adefee0c03425caebebf103d5d,attempt to center-justify the jax logo in readme,"diff --git a/README.md b/README.md
index fa518403e..21b79da09 100644
--- a/README.md
+++ b/README.md
@@ -1,6 +1,8 @@
-# JAX: Autograd and XLA
+<div align=""center"">
+<img src=""https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png"" alt=""logo""></img>
+</div>
 
-![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)
+# JAX: Autograd and XLA
 
 JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),","diff --git a/README.md b/README.md
index fa518403e..21b79da09 100644
--- a/README.md
+++ b/README.md
@@ -1,6 +1,8 @@
-# JAX: Autograd and XLA
+<div align=""center"">
+<img src=""https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png"" alt=""logo""></img>
+</div>
 
-![logo](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)
+# JAX: Autograd and XLA
 
 JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),",No
jax/lax.py,jax/lax.py,10b61e08f76a34898471d0b5b52a36c19483ea03,ffd6951719734fd684efe8ed948bc174e8b6e782,fix symbolic zero handling in concat transpose,"diff --git a/jax/lax.py b/jax/lax.py
index 5d6d63f58..fb57be271 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1352,15 +1352,18 @@ def concatenate_translation_rule(c, *operands, **kwargs):
 def concatenate_transpose_rule(t, *operands, **kwargs):
   dimension = kwargs.pop('dimension')
   operand_shapes = kwargs.pop('operand_shapes')
-  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim), dtype=int)
-  starts[1:, dimension] = limit_points[:-1]
-  limits = onp.tile(t.shape, (len(operands), 1))
-  limits[:, dimension] = limit_points
-
-  return [slice(t, start, limit) if o is None else None
-          for o, start, limit in zip(operands, starts, limits)]
+  if t is ad_util.zero:
+    return [ad_util.zero if o is None else None for o in operands]
+  else:
+    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
+    starts = onp.zeros((len(operands), t.ndim), dtype=int)
+    starts[1:, dimension] = limit_points[:-1]
+    limits = onp.tile(t.shape, (len(operands), 1))
+    limits[:, dimension] = limit_points
+
+    return [slice(t, start, limit) if o is None else None
+            for o, start, limit in zip(operands, starts, limits)]
 
 concatenate_p = standard_primitive(
     concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',","diff --git a/jax/lax.py b/jax/lax.py
index 5d6d63f58..fb57be271 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1352,15 +1352,18 @@ def concatenate_translation_rule(c, *operands, **kwargs):
 def concatenate_transpose_rule(t, *operands, **kwargs):
   dimension = kwargs.pop('dimension')
   operand_shapes = kwargs.pop('operand_shapes')
-  limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim), dtype=int)
-  starts[1:, dimension] = limit_points[:-1]
-  limits = onp.tile(t.shape, (len(operands), 1))
-  limits[:, dimension] = limit_points
+  if t is ad_util.zero:
+    return [ad_util.zero if o is None else None for o in operands]
+  else:
+    limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
+    starts = onp.zeros((len(operands), t.ndim), dtype=int)
+    starts[1:, dimension] = limit_points[:-1]
+    limits = onp.tile(t.shape, (len(operands), 1))
+    limits[:, dimension] = limit_points
 
-  return [slice(t, start, limit) if o is None else None
-          for o, start, limit in zip(operands, starts, limits)]
+    return [slice(t, start, limit) if o is None else None
+            for o, start, limit in zip(operands, starts, limits)]
 
 concatenate_p = standard_primitive(
     concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',",Yes
README.md,README.md,0cd84efac162d4094ad152d0e89371e30c150773,31d9443e49b6d73198956d7755387d1c27f49f9f,Small changes to readme,"diff --git a/README.md b/README.md
index 21b79da09..3f30c3d84 100644
--- a/README.md
+++ b/README.md
@@ -77,8 +77,7 @@ connected to a Google Cloud GPU.
 ## Installation
 JAX is written in pure Python, but it depends on XLA, which needs to be
 compiled and installed as the `jaxlib` package. Use the following instructions
-to [build XLA from source](#building-jax-from-source) or [install a binary
-package with pip](#pip-installation).
+to build JAX from source or install a binary package with pip.
 
 ### Building JAX from source
 First, obtain the JAX source code:
@@ -92,7 +91,7 @@ To build XLA with CUDA support, you can run
 
 ```bash
 python build/build.py --enable_cuda
-pip install -e build  # install jaxlib
+pip install -e build  # install jaxlib (includes XLA)
 pip install -e .      # install jax (pure Python)
 ```
 
@@ -105,7 +104,7 @@ To build XLA without CUDA GPU support (CPU only), drop the `--enable_cuda`:
 
 ```bash
 python build/build.py
-pip install -e build  # install jaxlib
+pip install -e build  # install jaxlib (includes XLA)
 pip install -e .      # install jax
 ```
 ","diff --git a/README.md b/README.md
index 21b79da09..3f30c3d84 100644
--- a/README.md
+++ b/README.md
@@ -77,8 +77,7 @@ connected to a Google Cloud GPU.
 ## Installation
 JAX is written in pure Python, but it depends on XLA, which needs to be
 compiled and installed as the `jaxlib` package. Use the following instructions
-to [build XLA from source](#building-jax-from-source) or [install a binary
-package with pip](#pip-installation).
+to build JAX from source or install a binary package with pip.
 
 ### Building JAX from source
 First, obtain the JAX source code:
@@ -92,7 +91,7 @@ To build XLA with CUDA support, you can run
 
 ```bash
 python build/build.py --enable_cuda
-pip install -e build  # install jaxlib
+pip install -e build  # install jaxlib (includes XLA)
 pip install -e .      # install jax (pure Python)
 ```
 
@@ -105,7 +104,7 @@ To build XLA without CUDA GPU support (CPU only), drop the `--enable_cuda`:
 
 ```bash
 python build/build.py
-pip install -e build  # install jaxlib
+pip install -e build  # install jaxlib (includes XLA)
 pip install -e .      # install jax
 ```
 ",No
README.md,README.md,9f61472a913b61685288162150b5f7c9be39fc71,0cd84efac162d4094ad152d0e89371e30c150773,Update README.md,"diff --git a/README.md b/README.md
index 3f30c3d84..8fbee9b7b 100644
--- a/README.md
+++ b/README.md
@@ -123,7 +123,7 @@ To install a CPU-only version, which might be useful for doing local
 development on a laptop, you can run
 
 ```bash
-pip install jax jaxlib
+pip install jax jaxlib  # CPU-only version
 ```
 
 If you want to install JAX with both CPU and GPU support, using existing CUDA","diff --git a/README.md b/README.md
index 3f30c3d84..8fbee9b7b 100644
--- a/README.md
+++ b/README.md
@@ -123,7 +123,7 @@ To install a CPU-only version, which might be useful for doing local
 development on a laptop, you can run
 
 ```bash
-pip install jax jaxlib
+pip install jax jaxlib  # CPU-only version
 ```
 
 If you want to install JAX with both CPU and GPU support, using existing CUDA",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,e22f722e2874d3a525a7c5b98c8be41c00ab6733,13d8cb1e7daa7b8c185497b1c7237e88fff25e56,Fixes for neural network notebook under review,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 7a6786464..4faa52fd4 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -61,7 +61,7 @@
         ""\n"",
         ""![JAX](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
         ""\n"",
-        ""Let's combine everything we showed in the [quickstart notebook](https://colab.research.google.com/github/google/jax/tree/master/notebooks/quickstart.ipynb) to train a simple neural network. We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use PyTorch's data loading API to load images and labels (because it's pretty great, and the world doesn't need yet another data loading library).\n"",
+        ""Let's combine everything we showed in the [quickstart notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb) to train a simple neural network. We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use PyTorch's data loading API to load images and labels (because it's pretty great, and the world doesn't need yet another data loading library).\n"",
         ""\n"",
         ""Of course, you can use JAX with any API that is compatible with NumPy to make specifying the model a bit more plug-and-play. Here, just for explanatory purposes, we won't use any neural network libraries or special APIs for builidng our model.""
       ]
@@ -115,18 +115,21 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
+        ""# A helper function to randomly initialize weights and biases\n"",
+        ""# for a dense neural network layer\n"",
         ""def random_layer_params(m, n, key, scale=1e-2):\n"",
         ""  w_key, b_key = random.split(key)\n"",
         ""  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n"",
         ""\n"",
+        ""# Initialize all layers for a fully-connected neural network with sizes \""sizes\""\n"",
         ""def init_network_params(sizes, key):\n"",
         ""  keys = random.split(key, len(sizes))\n"",
         ""  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n"",
         ""\n"",
-        ""layer_sizes = [784, 1024, 1024, 10]\n"",
-        ""step_size = 1e-2\n"",
+        ""layer_sizes = [784, 512, 512, 10]\n"",
+        ""param_scale = 0.1\n"",
+        ""step_size = 0.001\n"",
         ""num_epochs = 10\n"",
-        ""n_targets = 10\n"",
         ""batch_size = 32\n"",
         ""params = init_network_params(layer_sizes, random.PRNGKey(0))""
       ],
@@ -154,16 +157,20 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""from jax.scipy.misc import logsumexp\n"",
+        ""\n"",
+        ""def relu(x):\n"",
+        ""  return np.maximum(0, x)\n"",
+        ""\n"",
         ""def predict(params, image):\n"",
         ""  # per-example predictions\n"",
         ""  activations = image\n"",
         ""  for w, b in params[:-1]:\n"",
         ""    outputs = np.dot(w, activations) + b\n"",
-        ""    activations = np.tanh(outputs)\n"",
+        ""    activations = relu(outputs)\n"",
         ""  \n"",
         ""  final_w, final_b = params[-1]\n"",
         ""  logits = np.dot(final_w, activations) + final_b\n"",
-        ""  return (logits - logsumexp(logits))""
+        ""  return logits - logsumexp(logits)""
       ],
       ""execution_count"": 0,
       ""outputs"": []
@@ -222,6 +229,11 @@
       ""source"": [
         ""# Let's upgrade it to handle batches using `vmap`\n"",
         ""from jax.util import curry\n"",
+        ""\n"",
+        ""# The `vmap` function alone works like `map`, in that it will\n"",
+        ""# apply a function across inputs.\n"",
+        ""# However, let's curry vmap so that it returns a function\n"",
+        ""# with the original function signature.\n"",
         ""curried_vmap = curry(vmap)\n"",
         ""\n"",
         ""# Make a batched version of the `predict` function\n"",
@@ -234,6 +246,16 @@
       ""execution_count"": 0,
       ""outputs"": []
     },
+    {
+      ""metadata"": {
+        ""id"": ""elsG6nX03BvW"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""At this point, we have all the ingredients we need to define our neural network and train it. We've built an auto-batched version of `predict`, which we should be able to use in a loss function. We should be able to use `grad` to take the derivative of the loss with respect to the neural network parameters. Last, we should be able to use `jit` to speed up everything.""
+      ]
+    },
     {
       ""metadata"": {
         ""id"": ""NwDuFqc9X7ER"",
@@ -428,7 +450,6 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""import time\n"",
-        ""from tqdm import tqdm\n"",
         ""for epoch in range(num_epochs):\n"",
         ""  start_time = time.time()\n"",
         ""  for x, y in training_generator:\n"",","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 7a6786464..4faa52fd4 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -61,7 +61,7 @@
         ""\n"",
         ""![JAX](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
         ""\n"",
-        ""Let's combine everything we showed in the [quickstart notebook](https://colab.research.google.com/github/google/jax/tree/master/notebooks/quickstart.ipynb) to train a simple neural network. We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use PyTorch's data loading API to load images and labels (because it's pretty great, and the world doesn't need yet another data loading library).\n"",
+        ""Let's combine everything we showed in the [quickstart notebook](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb) to train a simple neural network. We will first specify and train a simple MLP on MNIST using JAX for the computation. We will use PyTorch's data loading API to load images and labels (because it's pretty great, and the world doesn't need yet another data loading library).\n"",
         ""\n"",
         ""Of course, you can use JAX with any API that is compatible with NumPy to make specifying the model a bit more plug-and-play. Here, just for explanatory purposes, we won't use any neural network libraries or special APIs for builidng our model.""
       ]
@@ -115,18 +115,21 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
+        ""# A helper function to randomly initialize weights and biases\n"",
+        ""# for a dense neural network layer\n"",
         ""def random_layer_params(m, n, key, scale=1e-2):\n"",
         ""  w_key, b_key = random.split(key)\n"",
         ""  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n"",
         ""\n"",
+        ""# Initialize all layers for a fully-connected neural network with sizes \""sizes\""\n"",
         ""def init_network_params(sizes, key):\n"",
         ""  keys = random.split(key, len(sizes))\n"",
         ""  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n"",
         ""\n"",
-        ""layer_sizes = [784, 1024, 1024, 10]\n"",
-        ""step_size = 1e-2\n"",
+        ""layer_sizes = [784, 512, 512, 10]\n"",
+        ""param_scale = 0.1\n"",
+        ""step_size = 0.001\n"",
         ""num_epochs = 10\n"",
-        ""n_targets = 10\n"",
         ""batch_size = 32\n"",
         ""params = init_network_params(layer_sizes, random.PRNGKey(0))""
       ],
@@ -154,16 +157,20 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""from jax.scipy.misc import logsumexp\n"",
+        ""\n"",
+        ""def relu(x):\n"",
+        ""  return np.maximum(0, x)\n"",
+        ""\n"",
         ""def predict(params, image):\n"",
         ""  # per-example predictions\n"",
         ""  activations = image\n"",
         ""  for w, b in params[:-1]:\n"",
         ""    outputs = np.dot(w, activations) + b\n"",
-        ""    activations = np.tanh(outputs)\n"",
+        ""    activations = relu(outputs)\n"",
         ""  \n"",
         ""  final_w, final_b = params[-1]\n"",
         ""  logits = np.dot(final_w, activations) + final_b\n"",
-        ""  return (logits - logsumexp(logits))""
+        ""  return logits - logsumexp(logits)""
       ],
       ""execution_count"": 0,
       ""outputs"": []
@@ -222,6 +229,11 @@
       ""source"": [
         ""# Let's upgrade it to handle batches using `vmap`\n"",
         ""from jax.util import curry\n"",
+        ""\n"",
+        ""# The `vmap` function alone works like `map`, in that it will\n"",
+        ""# apply a function across inputs.\n"",
+        ""# However, let's curry vmap so that it returns a function\n"",
+        ""# with the original function signature.\n"",
         ""curried_vmap = curry(vmap)\n"",
         ""\n"",
         ""# Make a batched version of the `predict` function\n"",
@@ -234,6 +246,16 @@
       ""execution_count"": 0,
       ""outputs"": []
     },
+    {
+      ""metadata"": {
+        ""id"": ""elsG6nX03BvW"",
+        ""colab_type"": ""text""
+      },
+      ""cell_type"": ""markdown"",
+      ""source"": [
+        ""At this point, we have all the ingredients we need to define our neural network and train it. We've built an auto-batched version of `predict`, which we should be able to use in a loss function. We should be able to use `grad` to take the derivative of the loss with respect to the neural network parameters. Last, we should be able to use `jit` to speed up everything.""
+      ]
+    },
     {
       ""metadata"": {
         ""id"": ""NwDuFqc9X7ER"",
@@ -428,7 +450,6 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""import time\n"",
-        ""from tqdm import tqdm\n"",
         ""for epoch in range(num_epochs):\n"",
         ""  start_time = time.time()\n"",
         ""  for x, y in training_generator:\n"",",No
jax/interpreters/xla.py,jax/interpreters/xla.py,ae36037abfa3eb278955ea775f0ca6c2a7ef95cb,9f61472a913b61685288162150b5f7c9be39fc71,"add ad_util.zeros_like translation rule
PAIR=hawkinsp","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 5f09a2859..296b4757e 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -163,11 +163,18 @@ def translation_rule(p):
 translations = {}
 
 translations[core.pack_p] = lambda c, *xs: c.Tuple(*xs)
-translations[ad_util.add_jaxvals_p] = lambda c, x, y: c.Add(x, y)
 translations[core.call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
                                                            subc_a1[1] + a2)
 translations[core.identity_p] = lambda c, x: x
 
+# TODO(mattjj): zeros_like and add_jaxvals should handle any jaxval
+def zeros_like_translation_rule(c, x):
+  x_shape = c.GetShape(x)
+  return c.Broadcast(c.Constant(onp.array(0, x_shape.element_type())),
+                     x_shape.dimensions())
+translations[ad_util.zeros_like_p] = zeros_like_translation_rule
+translations[ad_util.add_jaxvals_p] = lambda c, x, y: c.Add(x, y)
+
 
 def canonicalize_pyval_dtype(x):
   try:","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 5f09a2859..296b4757e 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -163,11 +163,18 @@ def translation_rule(p):
 translations = {}
 
 translations[core.pack_p] = lambda c, *xs: c.Tuple(*xs)
-translations[ad_util.add_jaxvals_p] = lambda c, x, y: c.Add(x, y)
 translations[core.call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
                                                            subc_a1[1] + a2)
 translations[core.identity_p] = lambda c, x: x
 
+# TODO(mattjj): zeros_like and add_jaxvals should handle any jaxval
+def zeros_like_translation_rule(c, x):
+  x_shape = c.GetShape(x)
+  return c.Broadcast(c.Constant(onp.array(0, x_shape.element_type())),
+                     x_shape.dimensions())
+translations[ad_util.zeros_like_p] = zeros_like_translation_rule
+translations[ad_util.add_jaxvals_p] = lambda c, x, y: c.Add(x, y)
+
 
 def canonicalize_pyval_dtype(x):
   try:",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,f5e6e88b89ab1605be179a31888863935ee06747,e22f722e2874d3a525a7c5b98c8be41c00ab6733,"Delete unused code in data loader, new architecture, some cleanups","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 4faa52fd4..08949104a 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -131,6 +131,7 @@
         ""step_size = 0.001\n"",
         ""num_epochs = 10\n"",
         ""batch_size = 32\n"",
+        ""n_targets = 10\n"",
         ""params = init_network_params(layer_sizes, random.PRNGKey(0))""
       ],
       ""execution_count"": 0,
@@ -333,45 +334,22 @@
         ""import numpy as onp\n"",
         ""from torch.utils import data\n"",
         ""from torchvision.datasets import MNIST\n"",
-        ""import torchvision.transforms as transforms\n"",
-        ""\n"",
-        ""import six\n"",
-        ""if six.PY2:\n"",
-        ""  import collections\n"",
-        ""  container_abcs = collections\n"",
-        ""elif six.PY3:\n"",
-        ""  import collections.abc\n"",
-        ""  container_abcs = collections.abc\n"",
-        ""  from torch.utils import data\n"",
         ""\n"",
         ""def numpy_collate(batch):\n"",
-        ""  elem = batch[0]\n"",
-        ""  elem_type = type(elem)\n"",
-        ""  if isinstance(elem, onp.ndarray):\n"",
+        ""  if isinstance(batch[0], onp.ndarray):\n"",
         ""    return onp.stack(batch)\n"",
-        ""  elif isinstance(elem, (int, float)):\n"",
-        ""    return onp.array(batch)\n"",
-        ""  elif isinstance(elem, str):\n"",
-        ""    return batch\n"",
-        ""  elif isinstance(elem, container_abcs.Mapping):\n"",
-        ""    return {key: numpy_collate([d[key] for d in batch]) for key in elem}\n"",
-        ""  elif isinstance(elem, container_abcs.Sequence):\n"",
+        ""  elif isinstance(batch[0], (tuple,list)):\n"",
         ""    transposed = zip(*batch)\n"",
         ""    return [numpy_collate(samples) for samples in transposed]\n"",
         ""  else:\n"",
-        ""    return np.array(onp.array(batch))\n"",
+        ""    return onp.array(batch)\n"",
         ""\n"",
         ""class NumpyLoader(data.DataLoader):\n"",
-        ""  def __init__(self, dataset,\n"",
-        ""                batch_size=1,\n"",
-        ""                shuffle=False,\n"",
-        ""                sampler=None,\n"",
-        ""                batch_sampler=None,\n"",
-        ""                num_workers=0,\n"",
-        ""                pin_memory=False,\n"",
-        ""                drop_last=False,\n"",
-        ""                timeout=0,\n"",
-        ""                worker_init_fn=None):\n"",
+        ""  def __init__(self, dataset, batch_size=1,\n"",
+        ""                shuffle=False, sampler=None,\n"",
+        ""                batch_sampler=None, num_workers=0,\n"",
+        ""                pin_memory=False, drop_last=False,\n"",
+        ""                timeout=0, worker_init_fn=None):\n"",
         ""    super(self.__class__, self).__init__(dataset,\n"",
         ""        batch_size=batch_size,\n"",
         ""        shuffle=shuffle,\n"",
@@ -385,13 +363,8 @@
         ""        worker_init_fn=worker_init_fn)\n"",
         ""\n"",
         ""class FlattenAndCast(object):\n"",
-        ""  \""\""\""Flattens an image\""\""\""\n"",
-        ""\n"",
         ""  def __call__(self, pic):\n"",
-        ""    return onp.ravel(onp.array(pic, dtype=np.float32))\n"",
-        ""\n"",
-        ""  def __repr__(self):\n"",
-        ""      return self.__class__.__name__ + '()'""
+        ""    return onp.ravel(onp.array(pic, dtype=np.float32))""
       ],
       ""execution_count"": 0,
       ""outputs"": []
@@ -450,6 +423,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""import time\n"",
+        ""\n"",
         ""for epoch in range(num_epochs):\n"",
         ""  start_time = time.time()\n"",
         ""  for x, y in training_generator:\n"",","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 4faa52fd4..08949104a 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -131,6 +131,7 @@
         ""step_size = 0.001\n"",
         ""num_epochs = 10\n"",
         ""batch_size = 32\n"",
+        ""n_targets = 10\n"",
         ""params = init_network_params(layer_sizes, random.PRNGKey(0))""
       ],
       ""execution_count"": 0,
@@ -333,45 +334,22 @@
         ""import numpy as onp\n"",
         ""from torch.utils import data\n"",
         ""from torchvision.datasets import MNIST\n"",
-        ""import torchvision.transforms as transforms\n"",
-        ""\n"",
-        ""import six\n"",
-        ""if six.PY2:\n"",
-        ""  import collections\n"",
-        ""  container_abcs = collections\n"",
-        ""elif six.PY3:\n"",
-        ""  import collections.abc\n"",
-        ""  container_abcs = collections.abc\n"",
-        ""  from torch.utils import data\n"",
         ""\n"",
         ""def numpy_collate(batch):\n"",
-        ""  elem = batch[0]\n"",
-        ""  elem_type = type(elem)\n"",
-        ""  if isinstance(elem, onp.ndarray):\n"",
+        ""  if isinstance(batch[0], onp.ndarray):\n"",
         ""    return onp.stack(batch)\n"",
-        ""  elif isinstance(elem, (int, float)):\n"",
-        ""    return onp.array(batch)\n"",
-        ""  elif isinstance(elem, str):\n"",
-        ""    return batch\n"",
-        ""  elif isinstance(elem, container_abcs.Mapping):\n"",
-        ""    return {key: numpy_collate([d[key] for d in batch]) for key in elem}\n"",
-        ""  elif isinstance(elem, container_abcs.Sequence):\n"",
+        ""  elif isinstance(batch[0], (tuple,list)):\n"",
         ""    transposed = zip(*batch)\n"",
         ""    return [numpy_collate(samples) for samples in transposed]\n"",
         ""  else:\n"",
-        ""    return np.array(onp.array(batch))\n"",
+        ""    return onp.array(batch)\n"",
         ""\n"",
         ""class NumpyLoader(data.DataLoader):\n"",
-        ""  def __init__(self, dataset,\n"",
-        ""                batch_size=1,\n"",
-        ""                shuffle=False,\n"",
-        ""                sampler=None,\n"",
-        ""                batch_sampler=None,\n"",
-        ""                num_workers=0,\n"",
-        ""                pin_memory=False,\n"",
-        ""                drop_last=False,\n"",
-        ""                timeout=0,\n"",
-        ""                worker_init_fn=None):\n"",
+        ""  def __init__(self, dataset, batch_size=1,\n"",
+        ""                shuffle=False, sampler=None,\n"",
+        ""                batch_sampler=None, num_workers=0,\n"",
+        ""                pin_memory=False, drop_last=False,\n"",
+        ""                timeout=0, worker_init_fn=None):\n"",
         ""    super(self.__class__, self).__init__(dataset,\n"",
         ""        batch_size=batch_size,\n"",
         ""        shuffle=shuffle,\n"",
@@ -385,13 +363,8 @@
         ""        worker_init_fn=worker_init_fn)\n"",
         ""\n"",
         ""class FlattenAndCast(object):\n"",
-        ""  \""\""\""Flattens an image\""\""\""\n"",
-        ""\n"",
         ""  def __call__(self, pic):\n"",
-        ""    return onp.ravel(onp.array(pic, dtype=np.float32))\n"",
-        ""\n"",
-        ""  def __repr__(self):\n"",
-        ""      return self.__class__.__name__ + '()'""
+        ""    return onp.ravel(onp.array(pic, dtype=np.float32))""
       ],
       ""execution_count"": 0,
       ""outputs"": []
@@ -450,6 +423,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""import time\n"",
+        ""\n"",
         ""for epoch in range(num_epochs):\n"",
         ""  start_time = time.time()\n"",
         ""  for x, y in training_generator:\n"",",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,70ef70b9651f8901f916fb3f975caa8237b01937,f2f661366fa57f96abfca9b1b8dc56838f27b4cd,minor spelling tweaks,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 12a816edc..7da6fb38a 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -973,7 +973,7 @@ def _rewriting_take(arr, idx, axis=0):
 
 
 def _is_slice_none(idx):
-  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
+  """"""Return True if idx is equal to slice(None), False otherwise.""""""
   if isinstance(idx, slice):
     return idx.start is None and idx.stop is None and idx.step is None
 
@@ -1001,7 +1001,7 @@ def _is_advanced_int_indexer_without_slices(idx):
 
 
 def _is_int(x):
-  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
+  """"""Returns True if x is array-like with integer dtype, False otherwise.""""""
   return (isinstance(x, int) and not isinstance(x, bool)
           or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
           or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 12a816edc..7da6fb38a 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -973,7 +973,7 @@ def _rewriting_take(arr, idx, axis=0):
 
 
 def _is_slice_none(idx):
-  """"""Return True if idx is equal to slice(None), falsey otherwise.""""""
+  """"""Return True if idx is equal to slice(None), False otherwise.""""""
   if isinstance(idx, slice):
     return idx.start is None and idx.stop is None and idx.step is None
 
@@ -1001,7 +1001,7 @@ def _is_advanced_int_indexer_without_slices(idx):
 
 
 def _is_int(x):
-  """"""Returns True if x is array-like with integer dtype, falsey otherwise.""""""
+  """"""Returns True if x is array-like with integer dtype, False otherwise.""""""
   return (isinstance(x, int) and not isinstance(x, bool)
           or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
           or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))",No
README.md,README.md,5a604d5b141c66fb415e0f0fce501e7f6ed44d1e,ae4d06f564db6f6b394f56adea016d07d51151f1,add more contributors to readme,"diff --git a/README.md b/README.md
index 8fbee9b7b..28e90f3e8 100644
--- a/README.md
+++ b/README.md
@@ -648,8 +648,11 @@ Some things we don't handle that might surprise NumPy users:
 So far, JAX includes lots of help and contributions from
 [Jamie Townsend](https://github.com/j-towns),
 [Peter Hawkins](https://github.com/hawkinsp),
+[Jonathan Ragan-Kelley](https://people.eecs.berkeley.edu/~jrk/),
 [Alex Wiltschko](http://github.com/alexbw),
 George Dahl,
+[Stephan Hoyer](http://stephanhoyer.com/),
+Sam Schoenholz,
 [Eli Bendersky](https://github.com/eliben),
 Zak Stone,
 [Alexey Radul](https://github.com/axch),","diff --git a/README.md b/README.md
index 8fbee9b7b..28e90f3e8 100644
--- a/README.md
+++ b/README.md
@@ -648,8 +648,11 @@ Some things we don't handle that might surprise NumPy users:
 So far, JAX includes lots of help and contributions from
 [Jamie Townsend](https://github.com/j-towns),
 [Peter Hawkins](https://github.com/hawkinsp),
+[Jonathan Ragan-Kelley](https://people.eecs.berkeley.edu/~jrk/),
 [Alex Wiltschko](http://github.com/alexbw),
 George Dahl,
+[Stephan Hoyer](http://stephanhoyer.com/),
+Sam Schoenholz,
 [Eli Bendersky](https://github.com/eliben),
 Zak Stone,
 [Alexey Radul](https://github.com/axch),",No
jax/lax.py,jax/lax.py,b7f6adaa90b0ed858b240619da6a12565efda61d,5a604d5b141c66fb415e0f0fce501e7f6ed44d1e,add dot_general batching rule,"diff --git a/jax/lax.py b/jax/lax.py
index fb57be271..5aa30b367 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1231,14 +1231,41 @@ def dot_general_transpose_rhs(g, x, dimension_numbers):
   return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)
 
 
-# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
-#   assert False  # TODO
+def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
+  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers
+  lhs, rhs = batched_args
+  lbd, rbd = batch_dims
+  assert lbd is not None or rbd is not None
+
+  if lbd is not None:
+    if lbd != 0:
+      lhs = batching.move_dim_to_front(lhs, lbd)
+      lbd = 0
+  else:
+    assert rbd is not None
+    lhs = broadcast(lhs, (rhs.shape[rbd],))
+  lhs_contract = tuple(onp.add(1, lhs_contract))
+  lhs_batch = (0,) + tuple(onp.add(1, lhs_batch))
+
+  if rbd is not None:
+    if rbd != 0:
+      rhs = batching.move_dim_to_front(rhs, rbd)
+      rbd = 0
+  else:
+    assert lbd is not None
+    rhs = broadcast(rhs, (lhs.shape[lbd],))
+  rhs_contract = tuple(onp.add(1, rhs_contract))
+  rhs_batch = (0,) + tuple(onp.add(1, rhs_batch))
+
+  new_dimension_numbers = [(lhs_contract, rhs_contract), (lhs_batch, rhs_batch)]
+  batched_out = dot_general(lhs, rhs, new_dimension_numbers)
+  return batched_out, 0
 
 dot_general_p = standard_primitive(dot_general_shape_rule,
                                    dot_general_dtype_rule, 'dot_general')
 ad.defbilinear(dot_general_p,
                dot_general_transpose_lhs, dot_general_transpose_rhs)
-# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule
+batching.primitive_batchers[dot_general_p] = dot_general_batch_rule
 
 
 def broadcast_shape_rule(operand, sizes):","diff --git a/jax/lax.py b/jax/lax.py
index fb57be271..5aa30b367 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1231,14 +1231,41 @@ def dot_general_transpose_rhs(g, x, dimension_numbers):
   return dot_general_transpose_lhs(g, x, swapped_dimension_numbers, True)
 
 
-# def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
-#   assert False  # TODO
+def dot_general_batch_rule(batched_args, batch_dims, dimension_numbers):
+  (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers
+  lhs, rhs = batched_args
+  lbd, rbd = batch_dims
+  assert lbd is not None or rbd is not None
+
+  if lbd is not None:
+    if lbd != 0:
+      lhs = batching.move_dim_to_front(lhs, lbd)
+      lbd = 0
+  else:
+    assert rbd is not None
+    lhs = broadcast(lhs, (rhs.shape[rbd],))
+  lhs_contract = tuple(onp.add(1, lhs_contract))
+  lhs_batch = (0,) + tuple(onp.add(1, lhs_batch))
+
+  if rbd is not None:
+    if rbd != 0:
+      rhs = batching.move_dim_to_front(rhs, rbd)
+      rbd = 0
+  else:
+    assert lbd is not None
+    rhs = broadcast(rhs, (lhs.shape[lbd],))
+  rhs_contract = tuple(onp.add(1, rhs_contract))
+  rhs_batch = (0,) + tuple(onp.add(1, rhs_batch))
+
+  new_dimension_numbers = [(lhs_contract, rhs_contract), (lhs_batch, rhs_batch)]
+  batched_out = dot_general(lhs, rhs, new_dimension_numbers)
+  return batched_out, 0
 
 dot_general_p = standard_primitive(dot_general_shape_rule,
                                    dot_general_dtype_rule, 'dot_general')
 ad.defbilinear(dot_general_p,
                dot_general_transpose_lhs, dot_general_transpose_rhs)
-# batching.primitive_batchers[dot_general_p] = dot_general_batch_rule
+batching.primitive_batchers[dot_general_p] = dot_general_batch_rule
 
 
 def broadcast_shape_rule(operand, sizes):",No
tests/batching_test.py,tests/batching_test.py,b7f6adaa90b0ed858b240619da6a12565efda61d,5a604d5b141c66fb415e0f0fce501e7f6ed44d1e,add dot_general batching rule,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 23d507279..80c4ea73f 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -194,6 +194,39 @@ class BatchingTest(jtu.JaxTestCase):
 
       self.assertAllClose(ans[i], expected_ans, check_dtypes=False)
 
+  def testDotGeneral(self):
+    R = onp.random.RandomState(0).randn
+ 
+    x = R(10, 3, 4, 5)
+    y = R(10, 3, 5, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
+    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+ 
+    x = R(3, 4, 10, 5)
+    y = R(3, 10, 5, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
+               in_axes=(2, 1))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+ 
+    x = R(3, 4, 5, 10)
+    y = R(3, 5, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
+               in_axes=(3, None))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    expected = onp.stack([fun(x[..., i], y) for i in range(10)])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+ 
+    x = R(3, 4, 5)
+    y = R(3, 5, 10, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
+               in_axes=(None, 2))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+
 
 if __name__ == '__main__':
   config.config_with_absl()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 23d507279..80c4ea73f 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -194,6 +194,39 @@ class BatchingTest(jtu.JaxTestCase):
 
       self.assertAllClose(ans[i], expected_ans, check_dtypes=False)
 
+  def testDotGeneral(self):
+    R = onp.random.RandomState(0).randn
+ 
+    x = R(10, 3, 4, 5)
+    y = R(10, 3, 5, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
+    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+ 
+    x = R(3, 4, 10, 5)
+    y = R(3, 10, 5, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
+               in_axes=(2, 1))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+ 
+    x = R(3, 4, 5, 10)
+    y = R(3, 5, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
+               in_axes=(3, None))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    expected = onp.stack([fun(x[..., i], y) for i in range(10)])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+ 
+    x = R(3, 4, 5)
+    y = R(3, 5, 10, 6)
+    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
+               in_axes=(None, 2))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+
 
 if __name__ == '__main__':
   config.config_with_absl()",No
setup.py,setup.py,1469e82361f6099850dbd3599df44f161c20b3c9,af6891399f07855f809edbc50725b4173619f674,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 3f9cbfa4d..38330c449 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.2',
+    version='0.1.4',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 3f9cbfa4d..38330c449 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.2',
+    version='0.1.4',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
jax/lax.py,jax/lax.py,499ea19e44ad68626d3ce10550f27feaf03812df,1469e82361f6099850dbd3599df44f161c20b3c9,fix 'unreachable' bug in dot batching rule,"diff --git a/jax/lax.py b/jax/lax.py
index 5aa30b367..4a2bc44fc 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1112,6 +1112,7 @@ def dot_batch_rule(batched_args, batch_dims):
   lbd, rbd = batch_dims
   T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])
 
+  # in some cases, we can call dot instead of dot_general
   if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
     if rbd is None:
       assert lbd in (0, 1)
@@ -1127,7 +1128,13 @@ def dot_batch_rule(batched_args, batch_dims):
       else:
         return dot(rhs, T(lhs)), 0
 
-    assert False  # unreachable
+    assert lbd is not None and rbd is not None
+    assert lhs.ndim == rhs.ndim == 2  # dot only supports rank 1 and above
+    if lbd != 0:
+      batching.move_dim_to_front(lhs, lbd)
+    if rbd != 0:
+      batching.move_dim_to_front(rhs, rbd)
+    return dot_general(lhs, rhs, [((1,), (1,)), ((0,), (0,))])
 
   if lbd is None:
     assert rbd is not None","diff --git a/jax/lax.py b/jax/lax.py
index 5aa30b367..4a2bc44fc 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1112,6 +1112,7 @@ def dot_batch_rule(batched_args, batch_dims):
   lbd, rbd = batch_dims
   T = lambda x: transpose(x, onp.arange(onp.ndim(x))[::-1])
 
+  # in some cases, we can call dot instead of dot_general
   if max(onp.ndim(lhs), onp.ndim(rhs)) <= 2:
     if rbd is None:
       assert lbd in (0, 1)
@@ -1127,7 +1128,13 @@ def dot_batch_rule(batched_args, batch_dims):
       else:
         return dot(rhs, T(lhs)), 0
 
-    assert False  # unreachable
+    assert lbd is not None and rbd is not None
+    assert lhs.ndim == rhs.ndim == 2  # dot only supports rank 1 and above
+    if lbd != 0:
+      batching.move_dim_to_front(lhs, lbd)
+    if rbd != 0:
+      batching.move_dim_to_front(rhs, rbd)
+    return dot_general(lhs, rhs, [((1,), (1,)), ((0,), (0,))])
 
   if lbd is None:
     assert rbd is not None",No
tests/batching_test.py,tests/batching_test.py,499ea19e44ad68626d3ce10550f27feaf03812df,1469e82361f6099850dbd3599df44f161c20b3c9,fix 'unreachable' bug in dot batching rule,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 80c4ea73f..f1ed960af 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -29,7 +29,7 @@ from jax.api import vmap
 from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
-from jax.util import partial
+from jax.util import partial, curry
 
 import functools as fn
 
@@ -196,13 +196,13 @@ class BatchingTest(jtu.JaxTestCase):
 
   def testDotGeneral(self):
     R = onp.random.RandomState(0).randn
- 
+
     x = R(10, 3, 4, 5)
     y = R(10, 3, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
     expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 10, 5)
     y = R(3, 10, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -210,7 +210,7 @@ class BatchingTest(jtu.JaxTestCase):
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 5, 10)
     y = R(3, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -218,7 +218,7 @@ class BatchingTest(jtu.JaxTestCase):
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i], y) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 5)
     y = R(3, 5, 10, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -227,6 +227,23 @@ class BatchingTest(jtu.JaxTestCase):
     expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  def testDot(self):
+    # these tests are based on @shoyer's notebook studying gufuncs
+    curried_vmap = curry(vmap)
+
+    def vecvec(a, b):
+      dot = np.dot
+      for ndim in range(1, max(a.ndim, b.ndim)):
+        a_ax = 0 if a.ndim > ndim else None
+        b_ax = 0 if b.ndim > ndim else None
+        dot = curried_vmap(dot, in_axes=(a_ax, b_ax))
+      return dot(a, b)
+
+    assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()
+    assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)
+    # TODO(mattjj): this fails due to an xla error in dot_general
+    # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
+
 
 if __name__ == '__main__':
   config.config_with_absl()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 80c4ea73f..f1ed960af 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -29,7 +29,7 @@ from jax.api import vmap
 from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
-from jax.util import partial
+from jax.util import partial, curry
 
 import functools as fn
 
@@ -196,13 +196,13 @@ class BatchingTest(jtu.JaxTestCase):
 
   def testDotGeneral(self):
     R = onp.random.RandomState(0).randn
- 
+
     x = R(10, 3, 4, 5)
     y = R(10, 3, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
     expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 10, 5)
     y = R(3, 10, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -210,7 +210,7 @@ class BatchingTest(jtu.JaxTestCase):
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 5, 10)
     y = R(3, 5, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -218,7 +218,7 @@ class BatchingTest(jtu.JaxTestCase):
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i], y) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
- 
+
     x = R(3, 4, 5)
     y = R(3, 5, 10, 6)
     ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
@@ -227,6 +227,23 @@ class BatchingTest(jtu.JaxTestCase):
     expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  def testDot(self):
+    # these tests are based on @shoyer's notebook studying gufuncs
+    curried_vmap = curry(vmap)
+
+    def vecvec(a, b):
+      dot = np.dot
+      for ndim in range(1, max(a.ndim, b.ndim)):
+        a_ax = 0 if a.ndim > ndim else None
+        b_ax = 0 if b.ndim > ndim else None
+        dot = curried_vmap(dot, in_axes=(a_ax, b_ax))
+      return dot(a, b)
+
+    assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()
+    assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)
+    # TODO(mattjj): this fails due to an xla error in dot_general
+    # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
+
 
 if __name__ == '__main__':
   config.config_with_absl()",No
setup.py,setup.py,4d6e7c8900fb4e24323c387d25d7114823d8bd33,499ea19e44ad68626d3ce10550f27feaf03812df,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 38330c449..96319427f 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.4',
+    version='0.1.5',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 38330c449..96319427f 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.4',
+    version='0.1.5',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,f50996df640ae7a8fb51c73b1cd8570ebdaecf6a,4d6e7c8900fb4e24323c387d25d7114823d8bd33,update gufunc notebook w/ dot_general batch rule,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 518953942..805ab23d0 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -435,17 +435,15 @@
         ""assert matmat(np.zeros((2, 3)), np.zeros((3, 4))).shape == (2, 4)\n"",
         ""assert matmat(np.zeros((2, 3)), np.zeros((1, 3, 4))).shape == (1, 2, 4)\n"",
         ""assert matmat(np.zeros((5, 2, 3)), np.zeros((1, 3, 4))).shape == (5, 2, 4)\n"",
-        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
-        ""# assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
+        ""assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
         ""\n"",
         ""assert matvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
         ""assert matvec(np.zeros((2, 3)), np.zeros((1, 3))).shape == (1, 2)\n"",
         ""assert matvec(np.zeros((4, 2, 3)), np.zeros((1, 3))).shape == (4, 2)\n"",
-        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
-        ""# assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
+        ""assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
         ""\n"",
         ""assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()\n"",
-        ""# these raise: AssertionError -- assert False  # unreachable\n"",
+        ""# these next two don't work yet\n"",
         ""# assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
         ""# assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2) ""
       ],
@@ -486,7 +484,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""assert magnitude(np.arange(3.0)).shape == ()\n"",
-        ""# these also raise (\""unreachable\"")\n"",
+        ""# these next two don't work yet\n"",
         ""# assert magnitude(np.arange(6.0).reshape(2, 3)).shape == (2,)\n"",
         ""# assert magnitude(np.arange(6.0).reshape(1, 2, 3)).shape == (1, 2,)""
       ],
@@ -563,10 +561,10 @@
       ""metadata"": {
         ""id"": ""MSyxOrUPixDI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""e8d05c0c-a337-4a9c-bdc8-8645616b4b93"",
+        ""outputId"": ""65ff7d75-6658-402b-85f7-558924ebe934"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 35
+          ""height"": 34
         }
       },
       ""cell_type"": ""code"",
@@ -589,10 +587,10 @@
       ""metadata"": {
         ""id"": ""7UZGOS8FGT_D"",
         ""colab_type"": ""code"",
-        ""outputId"": ""fdb7dad9-721c-439a-ffd6-c469445f371a"",
+        ""outputId"": ""28dd7afe-2bbe-485a-ac6e-f8ca6bac9f37"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -622,10 +620,10 @@
       ""metadata"": {
         ""id"": ""n2Fz91ptjM_7"",
         ""colab_type"": ""code"",
-        ""outputId"": ""0a75386a-8238-4fca-c0c8-1d8ac9573e48"",
+        ""outputId"": ""9ea07d50-7cbc-423c-bffb-92047dcca62c"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -650,10 +648,10 @@
       ""metadata"": {
         ""id"": ""G-AyCkAK4RKT"",
         ""colab_type"": ""code"",
-        ""outputId"": ""b4738597-d189-43b9-ad52-67a527b4c179"",
+        ""outputId"": ""dccce2b9-a7ec-4634-d45d-d440ddc1e1fe"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -678,10 +676,10 @@
       ""metadata"": {
         ""id"": ""_FhnjYMUjZgI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""b81b93ac-5f6a-43ce-ce85-17d6c237891a"",
+        ""outputId"": ""96c71fea-b30e-4057-9634-22f4d62218e2"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -717,4 +715,4 @@
       ""outputs"": []
     }
   ]
-}
+}
\ No newline at end of file","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 518953942..805ab23d0 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -435,17 +435,15 @@
         ""assert matmat(np.zeros((2, 3)), np.zeros((3, 4))).shape == (2, 4)\n"",
         ""assert matmat(np.zeros((2, 3)), np.zeros((1, 3, 4))).shape == (1, 2, 4)\n"",
         ""assert matmat(np.zeros((5, 2, 3)), np.zeros((1, 3, 4))).shape == (5, 2, 4)\n"",
-        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
-        ""# assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
+        ""assert matmat(np.zeros((6, 5, 2, 3)), np.zeros((3, 4))).shape == (6, 5, 2, 4)\n"",
         ""\n"",
         ""assert matvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
         ""assert matvec(np.zeros((2, 3)), np.zeros((1, 3))).shape == (1, 2)\n"",
         ""assert matvec(np.zeros((4, 2, 3)), np.zeros((1, 3))).shape == (4, 2)\n"",
-        ""# raises: NotImplementedError: Batching rule for 'dot_general' not implemented\n"",
-        ""# assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
+        ""assert matvec(np.zeros((5, 4, 2, 3)), np.zeros((1, 3))).shape == (5, 4, 2)\n"",
         ""\n"",
         ""assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()\n"",
-        ""# these raise: AssertionError -- assert False  # unreachable\n"",
+        ""# these next two don't work yet\n"",
         ""# assert vecvec(np.zeros((2, 3)), np.zeros((3,))).shape == (2,)\n"",
         ""# assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2) ""
       ],
@@ -486,7 +484,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""assert magnitude(np.arange(3.0)).shape == ()\n"",
-        ""# these also raise (\""unreachable\"")\n"",
+        ""# these next two don't work yet\n"",
         ""# assert magnitude(np.arange(6.0).reshape(2, 3)).shape == (2,)\n"",
         ""# assert magnitude(np.arange(6.0).reshape(1, 2, 3)).shape == (1, 2,)""
       ],
@@ -563,10 +561,10 @@
       ""metadata"": {
         ""id"": ""MSyxOrUPixDI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""e8d05c0c-a337-4a9c-bdc8-8645616b4b93"",
+        ""outputId"": ""65ff7d75-6658-402b-85f7-558924ebe934"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 35
+          ""height"": 34
         }
       },
       ""cell_type"": ""code"",
@@ -589,10 +587,10 @@
       ""metadata"": {
         ""id"": ""7UZGOS8FGT_D"",
         ""colab_type"": ""code"",
-        ""outputId"": ""fdb7dad9-721c-439a-ffd6-c469445f371a"",
+        ""outputId"": ""28dd7afe-2bbe-485a-ac6e-f8ca6bac9f37"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -622,10 +620,10 @@
       ""metadata"": {
         ""id"": ""n2Fz91ptjM_7"",
         ""colab_type"": ""code"",
-        ""outputId"": ""0a75386a-8238-4fca-c0c8-1d8ac9573e48"",
+        ""outputId"": ""9ea07d50-7cbc-423c-bffb-92047dcca62c"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -650,10 +648,10 @@
       ""metadata"": {
         ""id"": ""G-AyCkAK4RKT"",
         ""colab_type"": ""code"",
-        ""outputId"": ""b4738597-d189-43b9-ad52-67a527b4c179"",
+        ""outputId"": ""dccce2b9-a7ec-4634-d45d-d440ddc1e1fe"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -678,10 +676,10 @@
       ""metadata"": {
         ""id"": ""_FhnjYMUjZgI"",
         ""colab_type"": ""code"",
-        ""outputId"": ""b81b93ac-5f6a-43ce-ce85-17d6c237891a"",
+        ""outputId"": ""96c71fea-b30e-4057-9634-22f4d62218e2"",
         ""colab"": {
           ""base_uri"": ""https://localhost:8080/"",
-          ""height"": 71
+          ""height"": 68
         }
       },
       ""cell_type"": ""code"",
@@ -717,4 +715,4 @@
       ""outputs"": []
     }
   ]
-}
+}
\ No newline at end of file",No
build/build_jaxlib_wheels.sh,build/build_jaxlib_wheels.sh,b907bcd5ae8f0161303ca474d9d1e121f4cd6237,f50996df640ae7a8fb51c73b1cd8570ebdaecf6a,improve jaxlib wheel building script,"diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 31cbf27a9..00987deed 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -3,28 +3,31 @@ set -xev
 JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
 
 PYTHON_VERSIONS=""py2 py3""
-CUDA_VERSIONS=""9.2""  # ""9.2 10.0""
+CUDA_VERSIONS=""9.0 9.2 10.0""
 CUDA_VARIANTS=""cuda""  # ""cuda cuda-included""
 
 mkdir -p dist
+
+# build the pypi linux packages, tagging with manylinux1 for pypi reasons
+docker build -t jaxbuild jax/build/
+for PYTHON_VERSION in $PYTHON_VERSIONS
+do
+  mkdir -p dist/nocuda/
+  nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
+  mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+done
+
+# build the cuda linux packages, tagging with linux_x86_64
 for CUDA_VERSION in $CUDA_VERSIONS
 do
   docker build -t jaxbuild jax/build/ --build-arg CUDA_VERSION=$CUDA_VERSION
-
   for PYTHON_VERSION in $PYTHON_VERSIONS
   do
-    mkdir -p dist/nocuda/
-    nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
-    mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
-
     for CUDA_VARIANT in $CUDA_VARIANTS
     do
-      mkdir -p dist/cuda${CUDA_VERSION//.}
+      mkdir -p dist/${CUDA_VARIANT}${CUDA_VERSION//.}
       nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
-      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
+      mv dist/*.whl dist/${CUDA_VARIANT}${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
     done
   done
 done
-
-echo ""now you might want to run something like:""
-echo ""python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/nocuda/*.whl --verbose""","diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 31cbf27a9..00987deed 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -3,28 +3,31 @@ set -xev
 JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
 
 PYTHON_VERSIONS=""py2 py3""
-CUDA_VERSIONS=""9.2""  # ""9.2 10.0""
+CUDA_VERSIONS=""9.0 9.2 10.0""
 CUDA_VARIANTS=""cuda""  # ""cuda cuda-included""
 
 mkdir -p dist
+
+# build the pypi linux packages, tagging with manylinux1 for pypi reasons
+docker build -t jaxbuild jax/build/
+for PYTHON_VERSION in $PYTHON_VERSIONS
+do
+  mkdir -p dist/nocuda/
+  nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
+  mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+done
+
+# build the cuda linux packages, tagging with linux_x86_64
 for CUDA_VERSION in $CUDA_VERSIONS
 do
   docker build -t jaxbuild jax/build/ --build-arg CUDA_VERSION=$CUDA_VERSION
-
   for PYTHON_VERSION in $PYTHON_VERSIONS
   do
-    mkdir -p dist/nocuda/
-    nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
-    mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
-
     for CUDA_VARIANT in $CUDA_VARIANTS
     do
-      mkdir -p dist/cuda${CUDA_VERSION//.}
+      mkdir -p dist/${CUDA_VARIANT}${CUDA_VERSION//.}
       nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
-      mv dist/*.whl dist/cuda${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
+      mv dist/*.whl dist/${CUDA_VARIANT}${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
     done
   done
 done
-
-echo ""now you might want to run something like:""
-echo ""python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/nocuda/*.whl --verbose""",No
jax/api.py,jax/api.py,b907bcd5ae8f0161303ca474d9d1e121f4cd6237,f50996df640ae7a8fb51c73b1cd8570ebdaecf6a,improve jaxlib wheel building script,"diff --git a/jax/api.py b/jax/api.py
index f968bbfef..638f8811e 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -57,7 +57,6 @@ def grad(fun, argnums=0):
     g = vjp_py(onp.ones((), onp.result_type(ans)))
     return g[0] if isinstance(argnums, int) else g
 
-
   return grad_f
 
 @curry","diff --git a/jax/api.py b/jax/api.py
index f968bbfef..638f8811e 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -57,7 +57,6 @@ def grad(fun, argnums=0):
     g = vjp_py(onp.ones((), onp.result_type(ans)))
     return g[0] if isinstance(argnums, int) else g
 
-
   return grad_f
 
 @curry",No
build/setup.py,build/setup.py,2bbc046d9dfa2f282e0ef7912f4d6274eba15289,b907bcd5ae8f0161303ca474d9d1e121f4cd6237,Require protobuf 3.6.0 or later,"diff --git a/build/setup.py b/build/setup.py
index fb08bafad..a92031625 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -25,7 +25,7 @@ setup(
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=['jaxlib'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jaxlib': binary_libs},","diff --git a/build/setup.py b/build/setup.py
index fb08bafad..a92031625 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -25,7 +25,7 @@ setup(
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=['jaxlib'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jaxlib': binary_libs},",No
setup.py,setup.py,2bbc046d9dfa2f282e0ef7912f4d6274eba15289,b907bcd5ae8f0161303ca474d9d1e121f4cd6237,Require protobuf 3.6.0 or later,"diff --git a/setup.py b/setup.py
index 96319427f..c6c06b52e 100644
--- a/setup.py
+++ b/setup.py
@@ -22,7 +22,7 @@ setup(
     author_email='jax-dev@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.scipy.stats', 'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py',
+    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
                       'opt_einsum'],
     url='https://github.com/google/jax',
     license='Apache-2.0',","diff --git a/setup.py b/setup.py
index 96319427f..c6c06b52e 100644
--- a/setup.py
+++ b/setup.py
@@ -22,7 +22,7 @@ setup(
     author_email='jax-dev@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.scipy.stats', 'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py',
+    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
                       'opt_einsum'],
     url='https://github.com/google/jax',
     license='Apache-2.0',",No
examples/datasets.py,examples/datasets.py,38c7a07248fbccf5f9a8bde5cd88972c747c52aa,269de6010d0aaded0e9b3a3cfc320f02c5f5d71c,"download mnist example data w/ six.moves.request
fixes #28","diff --git a/examples/datasets.py b/examples/datasets.py
index 4178017fa..3197d41b1 100644
--- a/examples/datasets.py
+++ b/examples/datasets.py
@@ -23,7 +23,7 @@ import gzip
 import os
 from os import path
 import struct
-import urllib2
+from six.moves.urllib.request import urlretrieve
 
 import numpy as np
 
@@ -37,9 +37,8 @@ def _download(url, filename):
     os.makedirs(_DATA)
   out_file = path.join(_DATA, filename)
   if not path.isfile(out_file):
-    with open(out_file, ""wb"") as f:
-      f.write(urllib2.urlopen(url, out_file).read())
-      print(""downloaded {} to {}"".format(url, _DATA))
+    urlretrieve(url, out_file)
+    print(""downloaded {} to {}"".format(url, _DATA))
 
 
 def _partial_flatten(x):","diff --git a/examples/datasets.py b/examples/datasets.py
index 4178017fa..3197d41b1 100644
--- a/examples/datasets.py
+++ b/examples/datasets.py
@@ -23,7 +23,7 @@ import gzip
 import os
 from os import path
 import struct
-import urllib2
+from six.moves.urllib.request import urlretrieve
 
 import numpy as np
 
@@ -37,9 +37,8 @@ def _download(url, filename):
     os.makedirs(_DATA)
   out_file = path.join(_DATA, filename)
   if not path.isfile(out_file):
-    with open(out_file, ""wb"") as f:
-      f.write(urllib2.urlopen(url, out_file).read())
-      print(""downloaded {} to {}"".format(url, _DATA))
+    urlretrieve(url, out_file)
+    print(""downloaded {} to {}"".format(url, _DATA))
 
 
 def _partial_flatten(x):",No
jax/lax.py,jax/lax.py,38927153b1bb6c60a659c02cacf6771c3cbe4b14,38c7a07248fbccf5f9a8bde5cd88972c747c52aa,"Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.","diff --git a/jax/lax.py b/jax/lax.py
index 4a2bc44fc..3d83b755a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -692,7 +692,9 @@ def broadcasting_shape_rule(name, *avals):
   if len({len(shape) for shape in shapes}) != 1:
     msg = '{} got arrays of different rank: {}.'
     raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
-  result_shape = onp.max(shapes, axis=0)
+  min_shape = onp.min(shapes, axis=0)
+  max_shape = onp.max(shapes, axis=0)
+  result_shape = onp.where(min_shape == 0, 0, max_shape)
   if not onp.all((shapes == result_shape) | (shapes == 1)):
     msg = '{} got incompatible shapes for broadcasting: {}.'
     raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))","diff --git a/jax/lax.py b/jax/lax.py
index 4a2bc44fc..3d83b755a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -692,7 +692,9 @@ def broadcasting_shape_rule(name, *avals):
   if len({len(shape) for shape in shapes}) != 1:
     msg = '{} got arrays of different rank: {}.'
     raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))
-  result_shape = onp.max(shapes, axis=0)
+  min_shape = onp.min(shapes, axis=0)
+  max_shape = onp.max(shapes, axis=0)
+  result_shape = onp.where(min_shape == 0, 0, max_shape)
   if not onp.all((shapes == result_shape) | (shapes == 1)):
     msg = '{} got incompatible shapes for broadcasting: {}.'
     raise TypeError(msg.format(name, ', '.join(map(str, map(tuple, shapes)))))",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,38927153b1bb6c60a659c02cacf6771c3cbe4b14,38c7a07248fbccf5f9a8bde5cd88972c747c52aa,"Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7da6fb38a..15d6cbbf4 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -113,7 +113,9 @@ def _broadcast_shapes(*shapes):
     return shapes[0]
   ndim = _max(len(shape) for shape in shapes)
   shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
-  result_shape = onp.max(shapes, axis=0)
+  min_shape = onp.min(shapes, axis=0)
+  max_shape = onp.max(shapes, axis=0)
+  result_shape = onp.where(min_shape == 0, 0, max_shape)
   if not onp.all((shapes == result_shape) | (shapes == 1)):
     raise ValueError(""Incompatible shapes for broadcasting: {}""
                      .format(tuple(map(tuple, shapes))))","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7da6fb38a..15d6cbbf4 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -113,7 +113,9 @@ def _broadcast_shapes(*shapes):
     return shapes[0]
   ndim = _max(len(shape) for shape in shapes)
   shapes = onp.array([(1,) * (ndim - len(shape)) + shape for shape in shapes])
-  result_shape = onp.max(shapes, axis=0)
+  min_shape = onp.min(shapes, axis=0)
+  max_shape = onp.max(shapes, axis=0)
+  result_shape = onp.where(min_shape == 0, 0, max_shape)
   if not onp.all((shapes == result_shape) | (shapes == 1)):
     raise ValueError(""Incompatible shapes for broadcasting: {}""
                      .format(tuple(map(tuple, shapes))))",No
jax/test_util.py,jax/test_util.py,38927153b1bb6c60a659c02cacf6771c3cbe4b14,38c7a07248fbccf5f9a8bde5cd88972c747c52aa,"Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.","diff --git a/jax/test_util.py b/jax/test_util.py
index a605bb781..62189102a 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -274,8 +274,11 @@ def rand_some_equal():
   rng = npr.RandomState(0)
 
   def post(x):
+    x_ravel = x.ravel()
+    if len(x_ravel) == 0:
+      return x
     flips = rng.rand(*onp.shape(x)) < 0.5
-    return onp.where(flips, x.ravel()[0], x)
+    return onp.where(flips, x_ravel[0], x)
 
   return partial(_rand_dtype, randn, scale=100., post=post)
 ","diff --git a/jax/test_util.py b/jax/test_util.py
index a605bb781..62189102a 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -274,8 +274,11 @@ def rand_some_equal():
   rng = npr.RandomState(0)
 
   def post(x):
+    x_ravel = x.ravel()
+    if len(x_ravel) == 0:
+      return x
     flips = rng.rand(*onp.shape(x)) < 0.5
-    return onp.where(flips, x.ravel()[0], x)
+    return onp.where(flips, x_ravel[0], x)
 
   return partial(_rand_dtype, randn, scale=100., post=post)
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,38927153b1bb6c60a659c02cacf6771c3cbe4b14,38c7a07248fbccf5f9a8bde5cd88972c747c52aa,"Fix support for arrays with size-0 dimensions.

* Fix broadcasting rules to support size-0 dimensions.
* Add tests for size-0 dimensions. This required extending the test harness to support testing shapes that aren't necessarily broadcast compatible.
* Fix test utils to support size-0 dimensions.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 2f9b823f5..ea6ef11a6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -33,9 +33,13 @@ from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
-array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+empty_array_shapes = [(0,), (0, 4), (3, 0),]
 
-all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
+scalar_shapes = [jtu.NUMPY_SCALAR_SHAPE]
+array_shapes = nonempty_array_shapes + empty_array_shapes
+nonempty_shapes = scalar_shapes + nonempty_array_shapes
+all_shapes =  scalar_shapes + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -46,90 +50,91 @@ default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
 
 
-OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
-                                               ""diff_modes"", ""test_name""])
+OpRecord = collections.namedtuple(
+  ""OpRecord"",
+  [""name"", ""nargs"", ""dtypes"", ""shapes"", ""rng"", ""diff_modes"", ""test_name""])
 
 
-def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+def op_record(name, nargs, dtypes, shapes, rng, diff_modes, test_name=None):
   test_name = test_name or name
-  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+  return OpRecord(name, nargs, dtypes, shapes, rng, diff_modes, test_name)
 
 JAX_ONE_TO_ONE_OP_RECORDS = [
-    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
-    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
-    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
-    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
-    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
-    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""abs"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""ceil"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    #op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
+    #op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+    op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
-    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
-    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
-    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
-    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
-    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
-    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), all_shapes, jtu.rand_some_zero(), []),
 ]
 
 JAX_BITWISE_OP_RECORDS = [
-    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
 ]
 
 JAX_REDUCER_RECORDS = [
-    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
-    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
-    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    #op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    #op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    #op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
-    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmin"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
 ]
 
 CombosWithReplacement = itertools.combinations_with_replacement
@@ -150,6 +155,15 @@ def _dtypes_are_compatible_for_bitwise_ops(args):
       or (width(x) == 32 and width(y) == 32)
       or (width(x) == 32 and width(y) == 64 and is_signed(y)))
 
+def _shapes_are_broadcast_compatible(shapes):
+  accumulator = onp.zeros([])
+  for shape in shapes:
+    try:
+      accumulator = accumulator + onp.zeros(shape)
+    except ValueError:
+      return False
+  return True
+
 
 class LaxBackedNumpyTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed Numpy implementation.""""""
@@ -164,7 +178,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
       for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                  JAX_COMPOUND_OP_RECORDS)
-      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for shapes in filter(
+        _shapes_are_broadcast_compatible,
+        CombosWithReplacement(rec.shapes, rec.nargs))
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
@@ -177,10 +193,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
       for rec in JAX_BITWISE_OP_RECORDS
-      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for shapes in filter(
+        _shapes_are_broadcast_compatible,
+        CombosWithReplacement(rec.shapes, rec.nargs))
       for dtypes in filter(
-          _dtypes_are_compatible_for_bitwise_ops,
-          CombosWithReplacement(rec.dtypes, rec.nargs))))
+        _dtypes_are_compatible_for_bitwise_ops,
+        CombosWithReplacement(rec.dtypes, rec.nargs))))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     if not FLAGS.jax_enable_x64 and any(
         onp.iinfo(dtype).bits == 64 for dtype in dtypes):
@@ -197,7 +215,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis, ""keepdims"": keepdims}
       for rec in JAX_REDUCER_RECORDS
-      for shape in all_shapes for dtype in rec.dtypes
+      for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
@@ -215,7 +233,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis}
       for rec in JAX_ARGMINMAX_RECORDS
-      for shape in all_shapes for dtype in rec.dtypes
+      for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))))
   def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
 ","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 2f9b823f5..ea6ef11a6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -33,9 +33,13 @@ from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
-array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+empty_array_shapes = [(0,), (0, 4), (3, 0),]
 
-all_shapes = [jtu.NUMPY_SCALAR_SHAPE] + array_shapes
+scalar_shapes = [jtu.NUMPY_SCALAR_SHAPE]
+array_shapes = nonempty_array_shapes + empty_array_shapes
+nonempty_shapes = scalar_shapes + nonempty_array_shapes
+all_shapes =  scalar_shapes + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -46,90 +50,91 @@ default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
 
 
-OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
-                                               ""diff_modes"", ""test_name""])
+OpRecord = collections.namedtuple(
+  ""OpRecord"",
+  [""name"", ""nargs"", ""dtypes"", ""shapes"", ""rng"", ""diff_modes"", ""test_name""])
 
 
-def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+def op_record(name, nargs, dtypes, shapes, rng, diff_modes, test_name=None):
   test_name = test_name or name
-  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+  return OpRecord(name, nargs, dtypes, shapes, rng, diff_modes, test_name)
 
 JAX_ONE_TO_ONE_OP_RECORDS = [
-    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
-    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
-    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
-    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
-    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
-    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
-    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""abs"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""ceil"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    #op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
+    #op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+    op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
-    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
-    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
-    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
-    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
-    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
-    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
-    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
-    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), all_shapes, jtu.rand_some_zero(), []),
 ]
 
 JAX_BITWISE_OP_RECORDS = [
-    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_and"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_not"", 1, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_or"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
-    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes,
+    op_record(""bitwise_xor"", 2, int_dtypes + unsigned_dtypes, all_shapes,
               jtu.rand_bool(), []),
 ]
 
 JAX_REDUCER_RECORDS = [
-    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
-    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
-    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    #op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    #op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    #op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
-    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
-    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmin"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
 ]
 
 CombosWithReplacement = itertools.combinations_with_replacement
@@ -150,6 +155,15 @@ def _dtypes_are_compatible_for_bitwise_ops(args):
       or (width(x) == 32 and width(y) == 32)
       or (width(x) == 32 and width(y) == 64 and is_signed(y)))
 
+def _shapes_are_broadcast_compatible(shapes):
+  accumulator = onp.zeros([])
+  for shape in shapes:
+    try:
+      accumulator = accumulator + onp.zeros(shape)
+    except ValueError:
+      return False
+  return True
+
 
 class LaxBackedNumpyTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed Numpy implementation.""""""
@@ -164,7 +178,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
       for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
                                  JAX_COMPOUND_OP_RECORDS)
-      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for shapes in filter(
+        _shapes_are_broadcast_compatible,
+        CombosWithReplacement(rec.shapes, rec.nargs))
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
@@ -177,10 +193,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
       for rec in JAX_BITWISE_OP_RECORDS
-      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for shapes in filter(
+        _shapes_are_broadcast_compatible,
+        CombosWithReplacement(rec.shapes, rec.nargs))
       for dtypes in filter(
-          _dtypes_are_compatible_for_bitwise_ops,
-          CombosWithReplacement(rec.dtypes, rec.nargs))))
+        _dtypes_are_compatible_for_bitwise_ops,
+        CombosWithReplacement(rec.dtypes, rec.nargs))))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     if not FLAGS.jax_enable_x64 and any(
         onp.iinfo(dtype).bits == 64 for dtype in dtypes):
@@ -197,7 +215,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis, ""keepdims"": keepdims}
       for rec in JAX_REDUCER_RECORDS
-      for shape in all_shapes for dtype in rec.dtypes
+      for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
@@ -215,7 +233,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis}
       for rec in JAX_ARGMINMAX_RECORDS
-      for shape in all_shapes for dtype in rec.dtypes
+      for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))))
   def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
 ",No
build/build.py,build/build.py,5510ceaf55c3c4ec7ee5f179df37aefd0512ac2e,38c7a07248fbccf5f9a8bde5cd88972c747c52aa,"Set --distinct_host_configuration=false in the bazel options.

This makes initial builds cheaper (since we don't need to build some files in separate host and target configurations) but may make switching between build configurations more expensive (since we can share less work). The build script should optimize for the former.","diff --git a/build/build.py b/build/build.py
index dd2ba18ef..3b08c5f7e 100755
--- a/build/build.py
+++ b/build/build.py
@@ -163,6 +163,7 @@ build --python_path=""{python_bin_path}""
 build --action_env TF_NEED_CUDA=""{tf_need_cuda}""
 build --action_env CUDA_TOOLKIT_PATH=""{cuda_toolkit_path}""
 build --action_env CUDNN_INSTALL_PATH=""{cudnn_install_path}""
+build --distinct_host_configuration=false
 build:opt --copt=-march=native
 build:opt --copt=-Wno-sign-compare
 build:opt --host_copt=-march=native","diff --git a/build/build.py b/build/build.py
index dd2ba18ef..3b08c5f7e 100755
--- a/build/build.py
+++ b/build/build.py
@@ -163,6 +163,7 @@ build --python_path=""{python_bin_path}""
 build --action_env TF_NEED_CUDA=""{tf_need_cuda}""
 build --action_env CUDA_TOOLKIT_PATH=""{cuda_toolkit_path}""
 build --action_env CUDNN_INSTALL_PATH=""{cudnn_install_path}""
+build --distinct_host_configuration=false
 build:opt --copt=-march=native
 build:opt --copt=-Wno-sign-compare
 build:opt --host_copt=-march=native",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,6776bcf7bab0a0aaab1ff649531bf83be09550ac,38927153b1bb6c60a659c02cacf6771c3cbe4b14,Re-enable tests that were accidentally commented out.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index ea6ef11a6..ca09e3f12 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -66,7 +66,7 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    #op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
@@ -82,7 +82,7 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
-    #op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
@@ -125,11 +125,11 @@ JAX_REDUCER_RECORDS = [
     op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    #op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    #op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
-    #op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index ea6ef11a6..ca09e3f12 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -66,7 +66,7 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    #op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
@@ -82,7 +82,7 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
-    #op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
@@ -125,11 +125,11 @@ JAX_REDUCER_RECORDS = [
     op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    #op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    #op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
-    #op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [",No
README.md,README.md,0f8710a6e341f5e7b130867ce4e8d48091e12a76,55bb2e1df4d2997853eb5df7f3e5b6de2ec3c22a,add another link to Autograd in the readme,"diff --git a/README.md b/README.md
index 28e90f3e8..fd3f1f51c 100644
--- a/README.md
+++ b/README.md
@@ -8,7 +8,8 @@ JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
 brought together for high-performance machine learning research.
 
-With its updated version of Autograd, JAX can automatically differentiate native
+With its updated version of [Autograd](https://github.com/hips/autograd),
+JAX can automatically differentiate native
 Python and NumPy functions. It can differentiate through loops, branches,
 recursion, and closures, and it can take derivatives of derivatives of
 derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)","diff --git a/README.md b/README.md
index 28e90f3e8..fd3f1f51c 100644
--- a/README.md
+++ b/README.md
@@ -8,7 +8,8 @@ JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),
 brought together for high-performance machine learning research.
 
-With its updated version of Autograd, JAX can automatically differentiate native
+With its updated version of [Autograd](https://github.com/hips/autograd),
+JAX can automatically differentiate native
 Python and NumPy functions. It can differentiate through loops, branches,
 recursion, and closures, and it can take derivatives of derivatives of
 derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,72b77b5bbe3f181d0efb1cfd9cb88b477f4a4a84,0f8710a6e341f5e7b130867ce4e8d48091e12a76,fix typo in notebook text (closes #22),"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 805ab23d0..7b02d00b7 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -51,7 +51,7 @@
         ""\n"",
         ""1. Write the inner loops yourself in C.\n"",
         ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
-        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differention.\n"",
+        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differentiation.\n"",
         ""\n"",
         ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
         ""\n"",
@@ -715,4 +715,4 @@
       ""outputs"": []
     }
   ]
-}
\ No newline at end of file
+}","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 805ab23d0..7b02d00b7 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -51,7 +51,7 @@
         ""\n"",
         ""1. Write the inner loops yourself in C.\n"",
         ""2. [`np.vectorize`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html) creates something kind of like a gufunc, but it's painfully slow: the outer loop is performed in Python.\n"",
-        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differention.\n"",
+        ""3. [`numba.guvectorize`](https://numba.pydata.org/numba-doc/dev/user/vectorize.html) can work well, if you don't need further code transformations like automatic differentiation.\n"",
         ""\n"",
         ""JAX's `vmap` contains all the core functionality we need to write functions that work like gufuncs. JAX gufuncs play nicely with other transformations like `grad` and `jit`.\n"",
         ""\n"",
@@ -715,4 +715,4 @@
       ""outputs"": []
     }
   ]
-}
\ No newline at end of file
+}",No
build/WORKSPACE,build/WORKSPACE,c0e24f18b8f9fd528438805707826ed4fdc2b61c,6776bcf7bab0a0aaab1ff649531bf83be09550ac,"Update XLA to include Mac OS X fixes from
https://github.com/tensorflow/tensorflow/commit/c07297759059a953351f1d5e531b6e6af878365c","diff --git a/build/WORKSPACE b/build/WORKSPACE
index 0df920561..0f5c808e2 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -15,10 +15,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
-    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
+    sha256 = ""0fa744b8b806cd415e6a7747a0618306b816fa7329f45c94a865a2c763efdc47"",
+    strip_prefix = ""tensorflow-c07297759059a953351f1d5e531b6e6af878365c"",
     urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/c07297759059a953351f1d5e531b6e6af878365c.tar.gz"",
     ],
 )
 ","diff --git a/build/WORKSPACE b/build/WORKSPACE
index 0df920561..0f5c808e2 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -15,10 +15,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""dccd52030b173ee803191134215f712df7b18ee97a7c7d00437014002d29c26b"",
-    strip_prefix = ""tensorflow-8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd"",
+    sha256 = ""0fa744b8b806cd415e6a7747a0618306b816fa7329f45c94a865a2c763efdc47"",
+    strip_prefix = ""tensorflow-c07297759059a953351f1d5e531b6e6af878365c"",
     urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/8ccf1ebdbf4475bc7af6d79b2fa7e1fd8221e3fd.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/c07297759059a953351f1d5e531b6e6af878365c.tar.gz"",
     ],
 )
 ",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,c169cc6c053c9cf67a7146a07319d60f6008ed9b,f4212d47e5079ff90eefa83216f169fd436ce836,Update neural_network_and_data_loading.ipynb,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 08949104a..2532ba6f6 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -55,7 +55,7 @@
       },
       ""cell_type"": ""markdown"",
       ""source"": [
-        ""# Training a Simple Neural Network, with Torch Data Loading\n"",
+        ""# Training a Simple Neural Network, with PyTorch Data Loading\n"",
         ""\n"",
         ""_Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary_\n"",
         ""\n"",
@@ -452,4 +452,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 08949104a..2532ba6f6 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -55,7 +55,7 @@
       },
       ""cell_type"": ""markdown"",
       ""source"": [
-        ""# Training a Simple Neural Network, with Torch Data Loading\n"",
+        ""# Training a Simple Neural Network, with PyTorch Data Loading\n"",
         ""\n"",
         ""_Dougal Maclaurin, Peter Hawkins, Matthew Johnson, Roy Frostig, Alex Wiltschko, Chris Leary_\n"",
         ""\n"",
@@ -452,4 +452,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}",No
README.md,README.md,e425f244dbbe4293ee8a4ae7b50fbd3dc860642b,ad27af0aceab319e97fb2e7f010f2a4b2870b4a2,Update README.md,"diff --git a/README.md b/README.md
index fd3f1f51c..d7be0ba82 100644
--- a/README.md
+++ b/README.md
@@ -71,9 +71,10 @@ open](https://github.com/google/jax) by a growing number of
 * [Current gotchas](#current-gotchas)
 
 ## Quickstart: Colab in the Cloud
-Jump right in using [a notebook in your
-browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
-connected to a Google Cloud GPU.
+Jump right in using a notebook in your browser, connected to a Google Cloud GPU: 
+- [The basics: NumPy on accelerators, `grad` for automatic differentiation, `jit` for compilation and `vmap` for auto-vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
+- [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
+
 
 ## Installation
 JAX is written in pure Python, but it depends on XLA, which needs to be","diff --git a/README.md b/README.md
index fd3f1f51c..d7be0ba82 100644
--- a/README.md
+++ b/README.md
@@ -71,9 +71,10 @@ open](https://github.com/google/jax) by a growing number of
 * [Current gotchas](#current-gotchas)
 
 ## Quickstart: Colab in the Cloud
-Jump right in using [a notebook in your
-browser](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
-connected to a Google Cloud GPU.
+Jump right in using a notebook in your browser, connected to a Google Cloud GPU: 
+- [The basics: NumPy on accelerators, `grad` for automatic differentiation, `jit` for compilation and `vmap` for auto-vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
+- [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
+
 
 ## Installation
 JAX is written in pure Python, but it depends on XLA, which needs to be",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,7180eb031b4c7ddaf82e61b51b43ba6f6984f25d,fc4afb409b071a9d93738730b15c675662dec711,More informative error on trying to concatenate 0-dim arrays.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 15d6cbbf4..8a1361ab4 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -633,6 +633,8 @@ def stack(arrays):
 def concatenate(arrays, axis=0):
   if not arrays:
     raise ValueError(""Need at least one array to concatenate."")
+  if ndim(arrays[0]) == 0:
+    raise ValueError(""Zero-dimensional arrays cannot be concatenated."")
   return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 15d6cbbf4..8a1361ab4 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -633,6 +633,8 @@ def stack(arrays):
 def concatenate(arrays, axis=0):
   if not arrays:
     raise ValueError(""Need at least one array to concatenate."")
+  if ndim(arrays[0]) == 0:
+    raise ValueError(""Zero-dimensional arrays cannot be concatenated."")
   return lax.concatenate(_promote_dtypes(*arrays), axis % ndim(arrays[0]))
 
 ",No
jax/numpy/__init__.py,jax/numpy/__init__.py,763f8601350a9ef36d1b9b52719beabd6eb58654,f4212d47e5079ff90eefa83216f169fd436ce836,Automated detection of unimplemented functions,"diff --git a/jax/numpy/__init__.py b/jax/numpy/__init__.py
index 712eaeb07..c13abe3fa 100644
--- a/jax/numpy/__init__.py
+++ b/jax/numpy/__init__.py
@@ -13,4 +13,4 @@
 # limitations under the License.
 
 from __future__ import absolute_import
-from .lax_numpy import *
+from . lax_numpy import *","diff --git a/jax/numpy/__init__.py b/jax/numpy/__init__.py
index 712eaeb07..c13abe3fa 100644
--- a/jax/numpy/__init__.py
+++ b/jax/numpy/__init__.py
@@ -13,4 +13,4 @@
 # limitations under the License.
 
 from __future__ import absolute_import
-from .lax_numpy import *
+from . lax_numpy import *",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,763f8601350a9ef36d1b9b52719beabd6eb58654,f4212d47e5079ff90eefa83216f169fd436ce836,Automated detection of unimplemented functions,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 15d6cbbf4..3ce94f285 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -19,13 +19,15 @@ from __future__ import print_function
 from six.moves import builtins
 
 import six
+import math
 import numpy as onp
 
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
-import jax.lax as lax
+from .. import lax
 from ..util import memoize
+from ..util import get_module_functions
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -170,6 +172,8 @@ def _promote_args_like(op, *args):
 def _constant_like(x, const):
   return onp.array(const, dtype=_dtype(x))
 
+# A dictionary mapping implemented funcs to their lax equivalent.
+IMPLEMENTED_FUNCS = {}
 
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
@@ -183,6 +187,7 @@ def _wraps(fun):
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
+      IMPLEMENTED_FUNCS[fun] = op
       return op
   return wrap
 
@@ -856,21 +861,12 @@ def _not_implemented(fun):
     raise Exception(""Numpy function {} not yet implemented"".format(fun))
   return wrapped
 
-argpartition = _not_implemented(onp.argpartition)
-argsort = _not_implemented(onp.argsort)
-compress = _not_implemented(onp.compress)
-cumprod = _not_implemented(onp.cumprod)
-cumsum = _not_implemented(onp.cumsum)
-delete = _not_implemented(onp.delete)
-diagonal = _not_implemented(onp.diagonal)
-insert = _not_implemented(onp.insert)
-linspace = _not_implemented(onp.linspace)
-nonzero = _not_implemented(onp.nonzero)
-ptp = _not_implemented(onp.ptp)
-searchsorted = _not_implemented(onp.searchsorted)
-take = _not_implemented(onp.take)
-trace = _not_implemented(onp.trace)
-
+# Build a set of all unimplemented NumPy functions.
+# TODO(alexbw): deal with numpy.random
+UNIMPLEMENTED_FUNCS = get_module_functions(onp) - set(IMPLEMENTED_FUNCS)
+for func in UNIMPLEMENTED_FUNCS:
+  if func.__name__ not in globals():
+    globals()[func.__name__] = _not_implemented(func)
 
 ### Indexing
 
@@ -1129,3 +1125,4 @@ setattr(DeviceArray, ""T"", property(transpose))
 
 # Extra methods that are handy
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
+","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 15d6cbbf4..3ce94f285 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -19,13 +19,15 @@ from __future__ import print_function
 from six.moves import builtins
 
 import six
+import math
 import numpy as onp
 
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
-import jax.lax as lax
+from .. import lax
 from ..util import memoize
+from ..util import get_module_functions
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -170,6 +172,8 @@ def _promote_args_like(op, *args):
 def _constant_like(x, const):
   return onp.array(const, dtype=_dtype(x))
 
+# A dictionary mapping implemented funcs to their lax equivalent.
+IMPLEMENTED_FUNCS = {}
 
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
@@ -183,6 +187,7 @@ def _wraps(fun):
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
+      IMPLEMENTED_FUNCS[fun] = op
       return op
   return wrap
 
@@ -856,21 +861,12 @@ def _not_implemented(fun):
     raise Exception(""Numpy function {} not yet implemented"".format(fun))
   return wrapped
 
-argpartition = _not_implemented(onp.argpartition)
-argsort = _not_implemented(onp.argsort)
-compress = _not_implemented(onp.compress)
-cumprod = _not_implemented(onp.cumprod)
-cumsum = _not_implemented(onp.cumsum)
-delete = _not_implemented(onp.delete)
-diagonal = _not_implemented(onp.diagonal)
-insert = _not_implemented(onp.insert)
-linspace = _not_implemented(onp.linspace)
-nonzero = _not_implemented(onp.nonzero)
-ptp = _not_implemented(onp.ptp)
-searchsorted = _not_implemented(onp.searchsorted)
-take = _not_implemented(onp.take)
-trace = _not_implemented(onp.trace)
-
+# Build a set of all unimplemented NumPy functions.
+# TODO(alexbw): deal with numpy.random
+UNIMPLEMENTED_FUNCS = get_module_functions(onp) - set(IMPLEMENTED_FUNCS)
+for func in UNIMPLEMENTED_FUNCS:
+  if func.__name__ not in globals():
+    globals()[func.__name__] = _not_implemented(func)
 
 ### Indexing
 
@@ -1129,3 +1125,4 @@ setattr(DeviceArray, ""T"", property(transpose))
 
 # Extra methods that are handy
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
+",No
jax/util.py,jax/util.py,763f8601350a9ef36d1b9b52719beabd6eb58654,f4212d47e5079ff90eefa83216f169fd436ce836,Automated detection of unimplemented functions,"diff --git a/jax/util.py b/jax/util.py
index ea686229b..cfb8c3f8b 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -19,6 +19,8 @@ from __future__ import print_function
 import functools
 import itertools as it
 from operator import mul
+import types
+import numpy as onp
 
 allow_memoize_hash_failures = False
 
@@ -154,3 +156,21 @@ class WrapHashably(object):
 
   def __eq__(self, other):
     return self.val is other.val
+
+
+
+def get_module_functions(module):
+  """"""Finds functions in module.
+  Args:
+    module: A Python module.
+  Returns:
+    module_fns: A set of functions, builtins or ufuncs in `module`.
+  """"""
+
+  module_fns = set()
+  for key in dir(module):
+    attr = getattr(module, key)
+    if isinstance(
+        attr, (types.BuiltinFunctionType, types.FunctionType, onp.ufunc)):
+      module_fns.add(attr)
+  return module_fns","diff --git a/jax/util.py b/jax/util.py
index ea686229b..cfb8c3f8b 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -19,6 +19,8 @@ from __future__ import print_function
 import functools
 import itertools as it
 from operator import mul
+import types
+import numpy as onp
 
 allow_memoize_hash_failures = False
 
@@ -154,3 +156,21 @@ class WrapHashably(object):
 
   def __eq__(self, other):
     return self.val is other.val
+
+
+
+def get_module_functions(module):
+  """"""Finds functions in module.
+  Args:
+    module: A Python module.
+  Returns:
+    module_fns: A set of functions, builtins or ufuncs in `module`.
+  """"""
+
+  module_fns = set()
+  for key in dir(module):
+    attr = getattr(module, key)
+    if isinstance(
+        attr, (types.BuiltinFunctionType, types.FunctionType, onp.ufunc)):
+      module_fns.add(attr)
+  return module_fns",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6873760f4cd384ce29195b1396dae604d4f4082c,763f8601350a9ef36d1b9b52719beabd6eb58654,Remove unused lines,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 3ce94f285..43545fdfe 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -19,7 +19,6 @@ from __future__ import print_function
 from six.moves import builtins
 
 import six
-import math
 import numpy as onp
 
 from .. import core
@@ -862,7 +861,6 @@ def _not_implemented(fun):
   return wrapped
 
 # Build a set of all unimplemented NumPy functions.
-# TODO(alexbw): deal with numpy.random
 UNIMPLEMENTED_FUNCS = get_module_functions(onp) - set(IMPLEMENTED_FUNCS)
 for func in UNIMPLEMENTED_FUNCS:
   if func.__name__ not in globals():
@@ -1125,4 +1123,3 @@ setattr(DeviceArray, ""T"", property(transpose))
 
 # Extra methods that are handy
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
-","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 3ce94f285..43545fdfe 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -19,7 +19,6 @@ from __future__ import print_function
 from six.moves import builtins
 
 import six
-import math
 import numpy as onp
 
 from .. import core
@@ -862,7 +861,6 @@ def _not_implemented(fun):
   return wrapped
 
 # Build a set of all unimplemented NumPy functions.
-# TODO(alexbw): deal with numpy.random
 UNIMPLEMENTED_FUNCS = get_module_functions(onp) - set(IMPLEMENTED_FUNCS)
 for func in UNIMPLEMENTED_FUNCS:
   if func.__name__ not in globals():
@@ -1125,4 +1123,3 @@ setattr(DeviceArray, ""T"", property(transpose))
 
 # Extra methods that are handy
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
-",No
jax/scipy/stats/__init__.py,jax/scipy/stats/__init__.py,2d25a7430ad6d1b47badcaab492a4dd87a5d51d3,b5cc142befbeac29926fcd82b828b89db80c9c48,"Add a few popular distributions to jax.scipy.stats

- Beta
- Exponential
- Gamma
- Laplace
- Uniform","diff --git a/jax/scipy/stats/__init__.py b/jax/scipy/stats/__init__.py
index 9f3005f7b..6bae8b090 100644
--- a/jax/scipy/stats/__init__.py
+++ b/jax/scipy/stats/__init__.py
@@ -13,4 +13,9 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from . import beta
+from . import expon
+from . import gamma
+from . import laplace
 from . import norm
+from . import uniform","diff --git a/jax/scipy/stats/__init__.py b/jax/scipy/stats/__init__.py
index 9f3005f7b..6bae8b090 100644
--- a/jax/scipy/stats/__init__.py
+++ b/jax/scipy/stats/__init__.py
@@ -13,4 +13,9 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from . import beta
+from . import expon
+from . import gamma
+from . import laplace
 from . import norm
+from . import uniform",No
jax/numpy/__init__.py,jax/numpy/__init__.py,7b9a3077f2676008f3e8058dd8807d690ce14a4f,6873760f4cd384ce29195b1396dae604d4f4082c,Reverting unnecessary blank space addition,"diff --git a/jax/numpy/__init__.py b/jax/numpy/__init__.py
index c13abe3fa..712eaeb07 100644
--- a/jax/numpy/__init__.py
+++ b/jax/numpy/__init__.py
@@ -13,4 +13,4 @@
 # limitations under the License.
 
 from __future__ import absolute_import
-from . lax_numpy import *
+from .lax_numpy import *","diff --git a/jax/numpy/__init__.py b/jax/numpy/__init__.py
index c13abe3fa..712eaeb07 100644
--- a/jax/numpy/__init__.py
+++ b/jax/numpy/__init__.py
@@ -13,4 +13,4 @@
 # limitations under the License.
 
 from __future__ import absolute_import
-from . lax_numpy import *
+from .lax_numpy import *",No
jax/numpy/__init__.py,jax/numpy/__init__.py,0b7bed87781c787a67b86c00207115518cf02051,887c50036870205cfa63e20dd19bf47797bb6f5a,"Adding unimplemented functions to numpy.random, numpy.fft and numpy.linalg","diff --git a/jax/numpy/__init__.py b/jax/numpy/__init__.py
index 712eaeb07..2da1b5171 100644
--- a/jax/numpy/__init__.py
+++ b/jax/numpy/__init__.py
@@ -14,3 +14,5 @@
 
 from __future__ import absolute_import
 from .lax_numpy import *
+from . import fft
+from . import linalg","diff --git a/jax/numpy/__init__.py b/jax/numpy/__init__.py
index 712eaeb07..2da1b5171 100644
--- a/jax/numpy/__init__.py
+++ b/jax/numpy/__init__.py
@@ -14,3 +14,5 @@
 
 from __future__ import absolute_import
 from .lax_numpy import *
+from . import fft
+from . import linalg",No
jax/random.py,jax/random.py,0b7bed87781c787a67b86c00207115518cf02051,887c50036870205cfa63e20dd19bf47797bb6f5a,"Adding unimplemented functions to numpy.random, numpy.fft and numpy.linalg","diff --git a/jax/random.py b/jax/random.py
index a03599f97..26c8c10c9 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -27,7 +27,9 @@ from . import numpy as np
 from . import tree_util
 from .api import jit
 from jax.lib import xla_bridge
-
+from .util import get_module_functions
+from .numpy.lax_numpy import _not_implemented
+from .numpy.lax_numpy import IMPLEMENTED_FUNCS
 
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
@@ -359,3 +361,10 @@ def bernoulli(key, mean=onp.float32(0.5), shape=()):
   if onp.shape(mean) != shape:
     mean = lax.broadcast(mean, shape)
   return lax.lt(uniform(key, shape), mean)
+
+# TODO(alexbw): np.random.random is an alias of random_sample, and
+# doesn't show up after a call to `dir(module)`.
+UNIMPLEMENTED_FUNCS = get_module_functions(onp.random) - set(IMPLEMENTED_FUNCS)
+for func in UNIMPLEMENTED_FUNCS:
+  if func.__name__ not in globals():
+    globals()[func.__name__] = _not_implemented(func)","diff --git a/jax/random.py b/jax/random.py
index a03599f97..26c8c10c9 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -27,7 +27,9 @@ from . import numpy as np
 from . import tree_util
 from .api import jit
 from jax.lib import xla_bridge
-
+from .util import get_module_functions
+from .numpy.lax_numpy import _not_implemented
+from .numpy.lax_numpy import IMPLEMENTED_FUNCS
 
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
@@ -359,3 +361,10 @@ def bernoulli(key, mean=onp.float32(0.5), shape=()):
   if onp.shape(mean) != shape:
     mean = lax.broadcast(mean, shape)
   return lax.lt(uniform(key, shape), mean)
+
+# TODO(alexbw): np.random.random is an alias of random_sample, and
+# doesn't show up after a call to `dir(module)`.
+UNIMPLEMENTED_FUNCS = get_module_functions(onp.random) - set(IMPLEMENTED_FUNCS)
+for func in UNIMPLEMENTED_FUNCS:
+  if func.__name__ not in globals():
+    globals()[func.__name__] = _not_implemented(func)",No
WORKSPACE,WORKSPACE,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,887c50036870205cfa63e20dd19bf47797bb6f5a,Added higher-order differentiation checks in lax_test and fixed some bugs. Conv tests currently failing.,"diff --git a/WORKSPACE b/WORKSPACE
new file mode 100644
index 000000000..e69de29bb","diff --git a/WORKSPACE b/WORKSPACE
new file mode 100644
index 000000000..e69de29bb",No
jax/core.py,jax/core.py,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,887c50036870205cfa63e20dd19bf47797bb6f5a,Added higher-order differentiation checks in lax_test and fixed some bugs. Conv tests currently failing.,"diff --git a/jax/core.py b/jax/core.py
index 9564985f6..5cb42b912 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -28,9 +28,9 @@ from .util import unzip2, safe_zip, safe_map, partial
 from .pprint_util import pp, vcat, hcat, pp_kv_pairs
 
 # TODO(dougalm): the trace cache breaks the leak detector. Consisder solving.
-# TODO(mattjj): put each of these behind flags
 check_leaks = False
-skip_checks = True  # not __debug__  # google doesn't use -O
+# TODO(dougalm): put this behind a flag that's enabled during testing
+skip_checks = False  # not __debug__  # google doesn't use -O
 
 zip = safe_zip
 map = safe_map
@@ -435,7 +435,9 @@ pytype_aval_mappings = {}
 
 class JaxTuple(tuple):
   def __new__(cls, xs):
-    assert skip_checks or all(map(valid_jaxtype, xs)), xs
+    if not skip_checks:
+      xs = list(xs)
+      assert all(map(valid_jaxtype, xs)), xs
     return tuple.__new__(cls, xs)
 
   def __repr__(self):","diff --git a/jax/core.py b/jax/core.py
index 9564985f6..5cb42b912 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -28,9 +28,9 @@ from .util import unzip2, safe_zip, safe_map, partial
 from .pprint_util import pp, vcat, hcat, pp_kv_pairs
 
 # TODO(dougalm): the trace cache breaks the leak detector. Consisder solving.
-# TODO(mattjj): put each of these behind flags
 check_leaks = False
-skip_checks = True  # not __debug__  # google doesn't use -O
+# TODO(dougalm): put this behind a flag that's enabled during testing
+skip_checks = False  # not __debug__  # google doesn't use -O
 
 zip = safe_zip
 map = safe_map
@@ -435,7 +435,9 @@ pytype_aval_mappings = {}
 
 class JaxTuple(tuple):
   def __new__(cls, xs):
-    assert skip_checks or all(map(valid_jaxtype, xs)), xs
+    if not skip_checks:
+      xs = list(xs)
+      assert all(map(valid_jaxtype, xs)), xs
     return tuple.__new__(cls, xs)
 
   def __repr__(self):",No
jax/lax.py,jax/lax.py,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,887c50036870205cfa63e20dd19bf47797bb6f5a,Added higher-order differentiation checks in lax_test and fixed some bugs. Conv tests currently failing.,"diff --git a/jax/lax.py b/jax/lax.py
index 3d83b755a..41558e9d3 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1799,10 +1799,12 @@ def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
 
 def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
   axes = kwargs['axes']
+  t_src = t_dst = None
   if src is None:
     t_src = index_take(t, idxs, axes)
   if dst is None:
     t_dst = t
+
   return [t_src, t_dst] + [None] * len(idxs)
 
 index_untake_p = standard_primitive(
@@ -2123,7 +2125,7 @@ def sort_key_val_jvp(primals, tangents, dimension):
     values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
 
   tangents_out = keys_tangents_out, values_tangents_out
-  return core.pack(val_out), core.pack(tangents_out)
+  return core.pack(val_out), ad.TangentTuple(tangents_out)
 
 def sort_key_val_transpose_rule(t, keys, values, dimension):
   t_keys, t_values = t","diff --git a/jax/lax.py b/jax/lax.py
index 3d83b755a..41558e9d3 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1799,10 +1799,12 @@ def index_untake_jvp(primals, tangents, axes, jaxpr, consts):
 
 def index_untake_transpose_rule(t, src, dst, *idxs, **kwargs):
   axes = kwargs['axes']
+  t_src = t_dst = None
   if src is None:
     t_src = index_take(t, idxs, axes)
   if dst is None:
     t_dst = t
+
   return [t_src, t_dst] + [None] * len(idxs)
 
 index_untake_p = standard_primitive(
@@ -2123,7 +2125,7 @@ def sort_key_val_jvp(primals, tangents, dimension):
     values_tangents_out = sort_jvp_rule(values_tangents, keys, dimension)
 
   tangents_out = keys_tangents_out, values_tangents_out
-  return core.pack(val_out), core.pack(tangents_out)
+  return core.pack(val_out), ad.TangentTuple(tangents_out)
 
 def sort_key_val_transpose_rule(t, keys, values, dimension):
   t_keys, t_values = t",No
jax/test_util.py,jax/test_util.py,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,887c50036870205cfa63e20dd19bf47797bb6f5a,Added higher-order differentiation checks in lax_test and fixed some bugs. Conv tests currently failing.,"diff --git a/jax/test_util.py b/jax/test_util.py
index 62189102a..177357f39 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -37,7 +37,7 @@ from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce
 FLAGS = flags.FLAGS
 flags.DEFINE_enum(
     'jax_test_dut',
-    None,
+    'cpu',
     enum_values=['cpu', 'gpu', 'tpu'],
     help=
     'Describes the device under test in case special consideration is required.'","diff --git a/jax/test_util.py b/jax/test_util.py
index 62189102a..177357f39 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -37,7 +37,7 @@ from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce
 FLAGS = flags.FLAGS
 flags.DEFINE_enum(
     'jax_test_dut',
-    None,
+    'cpu',
     enum_values=['cpu', 'gpu', 'tpu'],
     help=
     'Describes the device under test in case special consideration is required.'",No
tests/build_defs.bzl,tests/build_defs.bzl,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,887c50036870205cfa63e20dd19bf47797bb6f5a,Added higher-order differentiation checks in lax_test and fixed some bugs. Conv tests currently failing.,"diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index 1e1052687..c51793a4f 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -31,9 +31,7 @@ def jax_test(
             fail(""Only one test source file is currently supported."")
 
     # Deps that are linked into all test target variants.
-    all_test_deps = [
-        "":libjax"",
-    ]
+    all_test_deps = []
     disabled_tags = [""manual"", ""notap"", ""disabled""]
     native.py_test(
         name = name + ""_cpu"",
@@ -45,33 +43,33 @@ def jax_test(
         args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
         tags = (disabled_tags if ""cpu"" in disable else []),
     )
-    native.py_test(
-        name = name + ""_cpu_x32"",
-        main = main,
-        srcs = srcs,
-        data = data,
-        deps = deps + all_test_deps,
-        shard_count = shard_count.get(""cpu"") if shard_count else None,
-        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
-        tags = (disabled_tags if ""cpu"" in disable else []),
-    )
-    native.py_test(
-        name = name + ""_gpu"",
-        main = main,
-        srcs = srcs,
-        data = data,
-        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
-        deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-        shard_count = shard_count.get(""gpu"") if shard_count else None,
-    )
-    native.py_test(
-        name = name + ""_gpu_x32"",
-        main = main,
-        srcs = srcs,
-        data = data,
-        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
-        deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-        shard_count = shard_count.get(""gpu"") if shard_count else None,
-    )
+    # native.py_test(
+    #     name = name + ""_cpu_x32"",
+    #     main = main,
+    #     srcs = srcs,
+    #     data = data,
+    #     deps = deps + all_test_deps,
+    #     shard_count = shard_count.get(""cpu"") if shard_count else None,
+    #     args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+    #     tags = (disabled_tags if ""cpu"" in disable else []),
+    # )
+    # native.py_test(
+    #     name = name + ""_gpu"",
+    #     main = main,
+    #     srcs = srcs,
+    #     data = data,
+    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+    #     deps = deps + all_test_deps,
+    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
+    # )
+    # native.py_test(
+    #     name = name + ""_gpu_x32"",
+    #     main = main,
+    #     srcs = srcs,
+    #     data = data,
+    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+    #     deps = deps + all_test_deps,
+    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
+    # )","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index 1e1052687..c51793a4f 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -31,9 +31,7 @@ def jax_test(
             fail(""Only one test source file is currently supported."")
 
     # Deps that are linked into all test target variants.
-    all_test_deps = [
-        "":libjax"",
-    ]
+    all_test_deps = []
     disabled_tags = [""manual"", ""notap"", ""disabled""]
     native.py_test(
         name = name + ""_cpu"",
@@ -45,33 +43,33 @@ def jax_test(
         args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
         tags = (disabled_tags if ""cpu"" in disable else []),
     )
-    native.py_test(
-        name = name + ""_cpu_x32"",
-        main = main,
-        srcs = srcs,
-        data = data,
-        deps = deps + all_test_deps,
-        shard_count = shard_count.get(""cpu"") if shard_count else None,
-        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
-        tags = (disabled_tags if ""cpu"" in disable else []),
-    )
-    native.py_test(
-        name = name + ""_gpu"",
-        main = main,
-        srcs = srcs,
-        data = data,
-        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
-        deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-        shard_count = shard_count.get(""gpu"") if shard_count else None,
-    )
-    native.py_test(
-        name = name + ""_gpu_x32"",
-        main = main,
-        srcs = srcs,
-        data = data,
-        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
-        deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-        shard_count = shard_count.get(""gpu"") if shard_count else None,
-    )
+    # native.py_test(
+    #     name = name + ""_cpu_x32"",
+    #     main = main,
+    #     srcs = srcs,
+    #     data = data,
+    #     deps = deps + all_test_deps,
+    #     shard_count = shard_count.get(""cpu"") if shard_count else None,
+    #     args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+    #     tags = (disabled_tags if ""cpu"" in disable else []),
+    # )
+    # native.py_test(
+    #     name = name + ""_gpu"",
+    #     main = main,
+    #     srcs = srcs,
+    #     data = data,
+    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+    #     deps = deps + all_test_deps,
+    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
+    # )
+    # native.py_test(
+    #     name = name + ""_gpu_x32"",
+    #     main = main,
+    #     srcs = srcs,
+    #     data = data,
+    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+    #     deps = deps + all_test_deps,
+    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
+    # )",No
tests/lax_test.py,tests/lax_test.py,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,887c50036870205cfa63e20dd19bf47797bb6f5a,Added higher-order differentiation checks in lax_test and fixed some bugs. Conv tests currently failing.,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index 561ebc0cb..47fc5b9ee 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1411,13 +1411,19 @@ LAX_GRAD_OPS = [
 
 
 def check_grads(f, args, order, atol=None, rtol=None, eps=None):
-  # TODO(mattjj,dougalm): add higher-order check
-  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
-  atol = atol or default_tol
-  rtol = rtol or default_tol
-  eps = eps or default_tol
-  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
-  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+  if order > 1:
+    def f_vjp(*args):
+      out_primal_py, vjp_py = api.vjp(f, *args)
+      return vjp_py(out_primal_py)
+
+    check_grads(f_vjp, args, order - 1, atol=atol, rtol=rtol, eps=eps)
+  else:
+    default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+    atol = atol or default_tol
+    rtol = rtol or default_tol
+    eps = eps or default_tol
+    jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+    jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
 
 
 def check_grads_bilinear(f, args, order, atol=None, rtol=None):","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 561ebc0cb..47fc5b9ee 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1411,13 +1411,19 @@ LAX_GRAD_OPS = [
 
 
 def check_grads(f, args, order, atol=None, rtol=None, eps=None):
-  # TODO(mattjj,dougalm): add higher-order check
-  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
-  atol = atol or default_tol
-  rtol = rtol or default_tol
-  eps = eps or default_tol
-  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
-  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+  if order > 1:
+    def f_vjp(*args):
+      out_primal_py, vjp_py = api.vjp(f, *args)
+      return vjp_py(out_primal_py)
+
+    check_grads(f_vjp, args, order - 1, atol=atol, rtol=rtol, eps=eps)
+  else:
+    default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+    atol = atol or default_tol
+    rtol = rtol or default_tol
+    eps = eps or default_tol
+    jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+    jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
 
 
 def check_grads_bilinear(f, args, order, atol=None, rtol=None):",No
jax/lax.py,jax/lax.py,0d64aea6bb0cd55a2abcc4e2958633764fdcdc74,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,clean up conv dimension_numbers handling,"diff --git a/jax/lax.py b/jax/lax.py
index 41558e9d3..5031d5e7a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -44,12 +44,6 @@ from .lib import xla_bridge
 _max = builtins.max
 _min = builtins.max
 
-if six.PY3:
-  def maketrans(s1, s2):
-    return s1.maketrans(s1, s2)
-else:
-  maketrans = string.maketrans
-
 ### traceables
 
 def neg(x): return neg_p.bind(x)
@@ -126,28 +120,20 @@ def concatenate(operands, dimension):
   return concatenate_p.bind(*operands, dimension=dimension,
                             operand_shapes=tuple(o.shape for o in operands))
 
-def conv(lhs, rhs, window_strides, padding):
-  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
-  return conv_general_dilated_p.bind(
-      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
-      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
-      lhs_shape=lhs.shape, rhs_shape=rhs.shape)
-
-def conv_with_general_padding(lhs, rhs, window_strides, padding,
-                              lhs_dilation, rhs_dilation):
-  return conv_general_dilated_p.bind(
-      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
-      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
-      lhs_shape=lhs.shape, rhs_shape=rhs.shape)
-
-def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
-                         rhs_dilation, dimension_numbers):
+def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation=None,
+                         rhs_dilation=None, dimension_numbers=None):
+  if type(dimension_numbers) is not ConvDimensionNumbers:
+    dimension_numbers = conv_dimension_numbers(
+        lhs.shape, rhs.shape, dimension_numbers)
   if isinstance(padding, str):
-    perms = conv_general_permutations(dimension_numbers)
-    lhs_perm, rhs_perm, _ = perms
-    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
-                              onp.take(rhs.shape, rhs_perm)[2:],
-                              window_strides, padding)
+    lhs_perm, rhs_perm, _ = dimension_numbers
+    padding = padtype_to_pads(
+        onp.take(lhs.shape, lhs_perm)[2:], onp.take(rhs.shape, rhs_perm)[2:],
+        window_strides, padding)
+  if lhs_dilation is None:
+    lhs_dilation = (1,) * (lhs.ndim - 2)
+  if rhs_dilation is None:
+    rhs_dilation = (1,) * (rhs.ndim - 2)
   return conv_general_dilated_p.bind(
       lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
       lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
@@ -405,6 +391,17 @@ opaque_param_ids = itertools.count()
 ### convenience wrappers around traceables
 
 
+def conv(lhs, rhs, window_strides, padding):
+  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
+  return conv_general_dilated(lhs, rhs, window_strides, padding)
+
+def conv_with_general_padding(lhs, rhs, window_strides, padding,
+                              lhs_dilation, rhs_dilation):
+  return conv_general_dilated(
+      lhs, rhs, window_strides, padding, lhs_dilation=lhs_dilation,
+      rhs_dilation=rhs_dilation)
+
+
 def full_like(x, fill_value, dtype=None, shape=None):
   """"""Create a full array like np.full based on the example array `x`.
 
@@ -952,35 +949,13 @@ batching.defvectorized(bitcast_convert_type_p)
 
 def conv_general_dilated_shape_rule(
     lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
-    dimension_numbers=None, **unused_kwargs):
-  if dimension_numbers is None:
-    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
-    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
-    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
-                       window_strides)
-    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
-  else:
-    if not isinstance(dimension_numbers, (tuple, list)):
-      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
-      raise TypeError(msg.format(type(dimension_numbers)))
-    if len(dimension_numbers) != 3:
-      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
-      raise TypeError(msg.format(len(dimension_numbers)))
-    if not all(isinstance(elt, str) for elt in dimension_numbers):
-      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
-            ""got {}."")
-      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
-    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
-           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
-    for i, elt in enumerate(dimension_numbers):
-      if len(elt) != lhs.ndim:
-        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))
-
-    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
-    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
-    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
-    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
-    return tuple(onp.take(out_trans, onp.argsort(out_perm)))
+    dimension_numbers, **unused_kwargs):
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  lhs_perm, rhs_perm, out_perm = dimension_numbers
+  lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
+  rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
+  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
+  return tuple(onp.take(out_trans, onp.argsort(out_perm)))
 
 def conv_general_dilated_dtype_rule(
     lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
@@ -988,19 +963,17 @@ def conv_general_dilated_dtype_rule(
   return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                           lhs, rhs)
 
+_conv_transpose = lambda spec: (spec[1], spec[0]) + spec[2:]
+_conv_sdims = lambda spec: spec[2:]
+
 def conv_general_dilated_transpose_lhs(
     g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
     dimension_numbers, lhs_shape, rhs_shape):
-  if dimension_numbers is None:
-    nd = len(lhs_shape)
-    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
-    trans_dimension_numbers = ConvolutionDimensionNumbers(
-        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
-  else:
-    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
-    lhs_spec, rhs_spec, out_spec = dimension_numbers
-    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec
-
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
+  lhs_spec, rhs_spec, out_spec = dimension_numbers
+  t_rhs_spec = _conv_transpose(rhs_spec)
+  trans_dimension_numbers = ConvDimensionNumbers(lhs_spec, t_rhs_spec, out_spec)
   padding = _conv_general_vjp_lhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
       window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
@@ -1011,24 +984,13 @@ def conv_general_dilated_transpose_lhs(
       lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
       dimension_numbers=trans_dimension_numbers)
 
-
 def conv_general_dilated_transpose_rhs(
     g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
     dimension_numbers, lhs_shape, rhs_shape):
-  if dimension_numbers is None:
-    nd = len(lhs_shape)
-    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
-    trans_dimension_numbers = ConvolutionDimensionNumbers(
-        (1, 0) + tuple(range(2, nd)),
-        (1, 0) + tuple(range(2, nd)),
-        (1, 0) + tuple(range(2, nd)))
-  else:
-    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
-    lhs_spec, rhs_spec, out_spec = dimension_numbers
-    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
-                               out_spec.translate(maketrans(""NC"", ""IO"")),
-                               rhs_spec.translate(maketrans(""IO"", ""NC"")))
-
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
+  transposed = map(_conv_transpose, dimension_numbers)
+  trans_dimension_numbers = ConvDimensionNumbers(*transposed)
   padding = _conv_general_vjp_rhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
       window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
@@ -1038,12 +1000,11 @@ def conv_general_dilated_transpose_rhs(
       lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
       dimension_numbers=trans_dimension_numbers)
 
-
 def conv_general_dilated_translation_rule(
     c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
     dimension_numbers, **unused_kwargs):
-  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
-    dimension_numbers = _conv_general_proto(dimension_numbers)
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  dimension_numbers = _conv_general_proto(dimension_numbers)
   return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                               rhs_dilation, dimension_numbers)
 
@@ -2285,35 +2246,6 @@ def _check_shapelike(fun_name, arg_name, obj):
     raise TypeError(msg.format(fun_name, arg_name, obj))
 
 
-def conv_general_permutations(dimension_numbers):
-  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
-  lhs_spec, rhs_spec, out_spec = dimension_numbers
-  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
-  for i, (a, b) in enumerate(charpairs):
-    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
-      msg = (""convolution dimension_numbers[{}] must contain the characters ""
-             ""'{}' and '{}' exatly once, got {}."")
-      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
-    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
-      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
-             ""characters, got {}."")
-      raise TypeError(msg.format(i, dimension_numbers[i]))
-  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
-          set(out_spec) - set(out_char)):
-    msg = (""convolution dimension_numbers elements must each have the same ""
-           ""set of spatial characters, got {}."")
-    raise TypeError(msg.format(dimension_numbers))
-
-  def getperm(spec, charpair):
-    spatial = (i for i, c in enumerate(spec) if c not in charpair)
-    if spec is not rhs_spec:
-      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
-    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)
-
-  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
-  return lhs_perm, rhs_perm, out_perm
-
-
 def _dynamic_slice_indices(operand, start_indices):
   if isinstance(start_indices, (tuple, list)):
     start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
@@ -2345,25 +2277,80 @@ def remaining(original, *removed_lists):
   return [i for i in original if i not in blacklist]
 
 
-def _charswap(a, b, s):
-  return s.translate(maketrans(a + b, b + a))
+ConvDimensionNumbers = collections.namedtuple(
+    ""ConvDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])
+
+def conv_dimension_numbers(lhs_shape, rhs_shape, dimension_numbers):
+  """"""Convert from user spec of dimension_numbers to ConvDimensionNumbers.
+
+  Args:
+    lhs_shape: tuple of nonnegative integers, shape of the convolution input.
+    rhs_shape: tuple of nonnegative integers, shape of the convolution kernel.
+    dimension_numbers: None or a tuple/list of strings, following the
+      convolution dimension number specification format in xla_client.py.
+
+  Returns:
+    A ConvDimensionNumbers namedtuple representing dimension_numbers in a
+    canonical form that is handled by internal lax functions.
+  """"""
+  if len(lhs_shape) != len(rhs_shape):
+    msg = ""convolution requires lhs and rhs ndim to be equal, got {} and {}.""
+    raise TypeError(msg.format(len(lhs_shape), len(rhs_shape)))
+
+  if dimension_numbers is None:
+    iota = tuple(range(len(lhs_shape)))
+    return ConvDimensionNumbers(iota, iota, iota)
+  elif isinstance(dimension_numbers, (list, tuple)):
+    if len(dimension_numbers) != 3:
+      msg = ""convolution dimension_numbers list/tuple must be length 3, got {}.""
+      raise TypeError(msg.format(len(dimension_numbers)))
+    if not all(isinstance(elt, str) for elt in dimension_numbers):
+      msg = ""convolution dimension_numbers elements must be strings, got {}.""
+      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
+    msg = (""convolution dimension_numbers[{}] must have len equal to the ndim ""
+           ""of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
+    for i, elt in enumerate(dimension_numbers):
+      if len(elt) != len(lhs_shape):
+        raise TypeError(msg.format(i, len(elt), lhs_shape, rhs_shape))
+
+    lhs_spec, rhs_spec, out_spec = conv_general_permutations(dimension_numbers)
+    return ConvDimensionNumbers(lhs_spec, rhs_spec, out_spec)
+  else:
+    msg = ""convolution dimension_numbers must be tuple/list or None, got {}.""
+    raise TypeError(msg.format(type(dimension_numbers)))
 
 
-def _get_sdims(dimension_numbers):
+def conv_general_permutations(dimension_numbers):
+  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
   lhs_spec, rhs_spec, out_spec = dimension_numbers
-  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
-  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
-                     key=lambda i: rhs_spec.index(lhs_spec[i]))
-  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
-                     key=lambda i: rhs_spec.index(out_spec[i]))
-  return lhs_sdims, rhs_sdims, out_sdims
+  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
+  for i, (a, b) in enumerate(charpairs):
+    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
+      msg = (""convolution dimension_numbers[{}] must contain the characters ""
+             ""'{}' and '{}' exatly once, got {}."")
+      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
+    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
+      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
+             ""characters, got {}."")
+      raise TypeError(msg.format(i, dimension_numbers[i]))
+  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
+          set(out_spec) - set(out_char)):
+    msg = (""convolution dimension_numbers elements must each have the same ""
+           ""set of spatial characters, got {}."")
+    raise TypeError(msg.format(dimension_numbers))
 
+  def getperm(spec, charpair):
+    spatial = (i for i, c in enumerate(spec) if c not in charpair)
+    if spec is not rhs_spec:
+      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
+    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)
+
+  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
+  return lhs_perm, rhs_perm, out_perm
 
-ConvolutionDimensionNumbers = collections.namedtuple(
-    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])
 
 def _conv_general_proto(dimension_numbers):
-  assert type(dimension_numbers) is ConvolutionDimensionNumbers
+  assert type(dimension_numbers) is ConvDimensionNumbers
   lhs_spec, rhs_spec, out_spec = dimension_numbers
   proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
   proto.input_batch_dimension = lhs_spec[0]","diff --git a/jax/lax.py b/jax/lax.py
index 41558e9d3..5031d5e7a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -44,12 +44,6 @@ from .lib import xla_bridge
 _max = builtins.max
 _min = builtins.max
 
-if six.PY3:
-  def maketrans(s1, s2):
-    return s1.maketrans(s1, s2)
-else:
-  maketrans = string.maketrans
-
 ### traceables
 
 def neg(x): return neg_p.bind(x)
@@ -126,28 +120,20 @@ def concatenate(operands, dimension):
   return concatenate_p.bind(*operands, dimension=dimension,
                             operand_shapes=tuple(o.shape for o in operands))
 
-def conv(lhs, rhs, window_strides, padding):
-  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
-  return conv_general_dilated_p.bind(
-      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(pads),
-      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
-      lhs_shape=lhs.shape, rhs_shape=rhs.shape)
-
-def conv_with_general_padding(lhs, rhs, window_strides, padding,
-                              lhs_dilation, rhs_dilation):
-  return conv_general_dilated_p.bind(
-      lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
-      lhs_dilation=(), rhs_dilation=(), dimension_numbers=None,
-      lhs_shape=lhs.shape, rhs_shape=rhs.shape)
-
-def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation,
-                         rhs_dilation, dimension_numbers):
+def conv_general_dilated(lhs, rhs, window_strides, padding, lhs_dilation=None,
+                         rhs_dilation=None, dimension_numbers=None):
+  if type(dimension_numbers) is not ConvDimensionNumbers:
+    dimension_numbers = conv_dimension_numbers(
+        lhs.shape, rhs.shape, dimension_numbers)
   if isinstance(padding, str):
-    perms = conv_general_permutations(dimension_numbers)
-    lhs_perm, rhs_perm, _ = perms
-    padding = padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:],
-                              onp.take(rhs.shape, rhs_perm)[2:],
-                              window_strides, padding)
+    lhs_perm, rhs_perm, _ = dimension_numbers
+    padding = padtype_to_pads(
+        onp.take(lhs.shape, lhs_perm)[2:], onp.take(rhs.shape, rhs_perm)[2:],
+        window_strides, padding)
+  if lhs_dilation is None:
+    lhs_dilation = (1,) * (lhs.ndim - 2)
+  if rhs_dilation is None:
+    rhs_dilation = (1,) * (rhs.ndim - 2)
   return conv_general_dilated_p.bind(
       lhs, rhs, window_strides=tuple(window_strides), padding=tuple(padding),
       lhs_dilation=tuple(lhs_dilation), rhs_dilation=tuple(rhs_dilation),
@@ -405,6 +391,17 @@ opaque_param_ids = itertools.count()
 ### convenience wrappers around traceables
 
 
+def conv(lhs, rhs, window_strides, padding):
+  pads = padtype_to_pads(lhs.shape[2:], rhs.shape[2:], window_strides, padding)
+  return conv_general_dilated(lhs, rhs, window_strides, padding)
+
+def conv_with_general_padding(lhs, rhs, window_strides, padding,
+                              lhs_dilation, rhs_dilation):
+  return conv_general_dilated(
+      lhs, rhs, window_strides, padding, lhs_dilation=lhs_dilation,
+      rhs_dilation=rhs_dilation)
+
+
 def full_like(x, fill_value, dtype=None, shape=None):
   """"""Create a full array like np.full based on the example array `x`.
 
@@ -952,35 +949,13 @@ batching.defvectorized(bitcast_convert_type_p)
 
 def conv_general_dilated_shape_rule(
     lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
-    dimension_numbers=None, **unused_kwargs):
-  if dimension_numbers is None:
-    lhs_dilated = _dilate_shape(lhs.shape, lhs_dilation)
-    rhs_dilated = _dilate_shape(rhs.shape, rhs_dilation)
-    _check_conv_shapes('conv_general_dilated', lhs_dilated, rhs_dilated,
-                       window_strides)
-    return conv_shape_tuple(lhs_dilated, rhs_dilated, window_strides, padding)
-  else:
-    if not isinstance(dimension_numbers, (tuple, list)):
-      msg = ""conv_general_dilated dimension_numbers must be tuple/list, got {}.""
-      raise TypeError(msg.format(type(dimension_numbers)))
-    if len(dimension_numbers) != 3:
-      msg = ""conv_general_dilated dimension_numbers must be length 3, got {}.""
-      raise TypeError(msg.format(len(dimension_numbers)))
-    if not all(isinstance(elt, str) for elt in dimension_numbers):
-      msg = (""conv_general_dilated dimension_numbers elements must be strings, ""
-            ""got {}."")
-      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
-    msg = (""conv_general_dilated dimension_numbers[{}] must have len equal to ""
-           ""the ndim of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
-    for i, elt in enumerate(dimension_numbers):
-      if len(elt) != lhs.ndim:
-        raise TypeError(msg.format(i, len(elt), lhs.shape, rhs.shape))
-
-    lhs_perm, rhs_perm, out_perm = conv_general_permutations(dimension_numbers)
-    lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
-    rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
-    out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
-    return tuple(onp.take(out_trans, onp.argsort(out_perm)))
+    dimension_numbers, **unused_kwargs):
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  lhs_perm, rhs_perm, out_perm = dimension_numbers
+  lhs_trans = _dilate_shape(onp.take(lhs.shape, lhs_perm), lhs_dilation)
+  rhs_trans = _dilate_shape(onp.take(rhs.shape, rhs_perm), rhs_dilation)
+  out_trans = conv_shape_tuple(lhs_trans, rhs_trans, window_strides, padding)
+  return tuple(onp.take(out_trans, onp.argsort(out_perm)))
 
 def conv_general_dilated_dtype_rule(
     lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
@@ -988,19 +963,17 @@ def conv_general_dilated_dtype_rule(
   return binop_dtype_rule(_input_dtype, [_f32, _f32], 'conv_general_dilated',
                           lhs, rhs)
 
+_conv_transpose = lambda spec: (spec[1], spec[0]) + spec[2:]
+_conv_sdims = lambda spec: spec[2:]
+
 def conv_general_dilated_transpose_lhs(
     g, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
     dimension_numbers, lhs_shape, rhs_shape):
-  if dimension_numbers is None:
-    nd = len(lhs_shape)
-    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
-    trans_dimension_numbers = ConvolutionDimensionNumbers(
-        tuple(range(nd)), (1, 0) + tuple(range(2, nd)), tuple(range(nd)))
-  else:
-    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
-    lhs_spec, rhs_spec, out_spec = dimension_numbers
-    trans_dimension_numbers = out_spec, _charswap(""I"", ""O"", rhs_spec), lhs_spec
-
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
+  lhs_spec, rhs_spec, out_spec = dimension_numbers
+  t_rhs_spec = _conv_transpose(rhs_spec)
+  trans_dimension_numbers = ConvDimensionNumbers(lhs_spec, t_rhs_spec, out_spec)
   padding = _conv_general_vjp_lhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
       window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
@@ -1011,24 +984,13 @@ def conv_general_dilated_transpose_lhs(
       lhs_dilation=window_strides, rhs_dilation=rhs_dilation,
       dimension_numbers=trans_dimension_numbers)
 
-
 def conv_general_dilated_transpose_rhs(
     g, lhs, window_strides, padding, lhs_dilation, rhs_dilation,
     dimension_numbers, lhs_shape, rhs_shape):
-  if dimension_numbers is None:
-    nd = len(lhs_shape)
-    lhs_sdims = rhs_sdims = out_sdims = list(range(2, nd))
-    trans_dimension_numbers = ConvolutionDimensionNumbers(
-        (1, 0) + tuple(range(2, nd)),
-        (1, 0) + tuple(range(2, nd)),
-        (1, 0) + tuple(range(2, nd)))
-  else:
-    lhs_sdims, rhs_sdims, out_sdims = _get_sdims(dimension_numbers)
-    lhs_spec, rhs_spec, out_spec = dimension_numbers
-    trans_dimension_numbers = (_charswap(""C"", ""N"", lhs_spec),
-                               out_spec.translate(maketrans(""NC"", ""IO"")),
-                               rhs_spec.translate(maketrans(""IO"", ""NC"")))
-
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
+  transposed = map(_conv_transpose, dimension_numbers)
+  trans_dimension_numbers = ConvDimensionNumbers(*transposed)
   padding = _conv_general_vjp_rhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
       window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,
@@ -1038,12 +1000,11 @@ def conv_general_dilated_transpose_rhs(
       lhs_dilation=lhs_dilation, rhs_dilation=window_strides,
       dimension_numbers=trans_dimension_numbers)
 
-
 def conv_general_dilated_translation_rule(
     c, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,
     dimension_numbers, **unused_kwargs):
-  if isinstance(dimension_numbers, ConvolutionDimensionNumbers):
-    dimension_numbers = _conv_general_proto(dimension_numbers)
+  assert type(dimension_numbers) is ConvDimensionNumbers
+  dimension_numbers = _conv_general_proto(dimension_numbers)
   return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                               rhs_dilation, dimension_numbers)
 
@@ -2285,35 +2246,6 @@ def _check_shapelike(fun_name, arg_name, obj):
     raise TypeError(msg.format(fun_name, arg_name, obj))
 
 
-def conv_general_permutations(dimension_numbers):
-  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
-  lhs_spec, rhs_spec, out_spec = dimension_numbers
-  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
-  for i, (a, b) in enumerate(charpairs):
-    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
-      msg = (""convolution dimension_numbers[{}] must contain the characters ""
-             ""'{}' and '{}' exatly once, got {}."")
-      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
-    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
-      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
-             ""characters, got {}."")
-      raise TypeError(msg.format(i, dimension_numbers[i]))
-  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
-          set(out_spec) - set(out_char)):
-    msg = (""convolution dimension_numbers elements must each have the same ""
-           ""set of spatial characters, got {}."")
-    raise TypeError(msg.format(dimension_numbers))
-
-  def getperm(spec, charpair):
-    spatial = (i for i, c in enumerate(spec) if c not in charpair)
-    if spec is not rhs_spec:
-      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
-    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)
-
-  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
-  return lhs_perm, rhs_perm, out_perm
-
-
 def _dynamic_slice_indices(operand, start_indices):
   if isinstance(start_indices, (tuple, list)):
     start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
@@ -2345,25 +2277,80 @@ def remaining(original, *removed_lists):
   return [i for i in original if i not in blacklist]
 
 
-def _charswap(a, b, s):
-  return s.translate(maketrans(a + b, b + a))
+ConvDimensionNumbers = collections.namedtuple(
+    ""ConvDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])
+
+def conv_dimension_numbers(lhs_shape, rhs_shape, dimension_numbers):
+  """"""Convert from user spec of dimension_numbers to ConvDimensionNumbers.
+
+  Args:
+    lhs_shape: tuple of nonnegative integers, shape of the convolution input.
+    rhs_shape: tuple of nonnegative integers, shape of the convolution kernel.
+    dimension_numbers: None or a tuple/list of strings, following the
+      convolution dimension number specification format in xla_client.py.
+
+  Returns:
+    A ConvDimensionNumbers namedtuple representing dimension_numbers in a
+    canonical form that is handled by internal lax functions.
+  """"""
+  if len(lhs_shape) != len(rhs_shape):
+    msg = ""convolution requires lhs and rhs ndim to be equal, got {} and {}.""
+    raise TypeError(msg.format(len(lhs_shape), len(rhs_shape)))
+
+  if dimension_numbers is None:
+    iota = tuple(range(len(lhs_shape)))
+    return ConvDimensionNumbers(iota, iota, iota)
+  elif isinstance(dimension_numbers, (list, tuple)):
+    if len(dimension_numbers) != 3:
+      msg = ""convolution dimension_numbers list/tuple must be length 3, got {}.""
+      raise TypeError(msg.format(len(dimension_numbers)))
+    if not all(isinstance(elt, str) for elt in dimension_numbers):
+      msg = ""convolution dimension_numbers elements must be strings, got {}.""
+      raise TypeError(msg.format(tuple(map(type, dimension_numbers))))
+    msg = (""convolution dimension_numbers[{}] must have len equal to the ndim ""
+           ""of lhs and rhs, got {} for lhs and rhs shapes {} and {}."")
+    for i, elt in enumerate(dimension_numbers):
+      if len(elt) != len(lhs_shape):
+        raise TypeError(msg.format(i, len(elt), lhs_shape, rhs_shape))
+
+    lhs_spec, rhs_spec, out_spec = conv_general_permutations(dimension_numbers)
+    return ConvDimensionNumbers(lhs_spec, rhs_spec, out_spec)
+  else:
+    msg = ""convolution dimension_numbers must be tuple/list or None, got {}.""
+    raise TypeError(msg.format(type(dimension_numbers)))
 
 
-def _get_sdims(dimension_numbers):
+def conv_general_permutations(dimension_numbers):
+  """"""Utility for convolution dimension permutations relative to Conv HLO.""""""
   lhs_spec, rhs_spec, out_spec = dimension_numbers
-  rhs_sdims = [i for i, c in enumerate(rhs_spec) if c not in {""I"", ""O""}]
-  lhs_sdims = sorted((i for i, c in enumerate(lhs_spec) if c not in {""N"", ""C""}),
-                     key=lambda i: rhs_spec.index(lhs_spec[i]))
-  out_sdims = sorted((i for i, c in enumerate(out_spec) if c not in {""N"", ""C""}),
-                     key=lambda i: rhs_spec.index(out_spec[i]))
-  return lhs_sdims, rhs_sdims, out_sdims
+  lhs_char, rhs_char, out_char = charpairs = (""N"", ""C""), (""O"", ""I""), (""N"", ""C"")
+  for i, (a, b) in enumerate(charpairs):
+    if not dimension_numbers[i].count(a) == dimension_numbers[i].count(b) == 1:
+      msg = (""convolution dimension_numbers[{}] must contain the characters ""
+             ""'{}' and '{}' exatly once, got {}."")
+      raise TypeError(msg.format(i, a, b, dimension_numbers[i]))
+    if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):
+      msg = (""convolution dimension_numbers[{}] cannot have duplicate ""
+             ""characters, got {}."")
+      raise TypeError(msg.format(i, dimension_numbers[i]))
+  if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==
+          set(out_spec) - set(out_char)):
+    msg = (""convolution dimension_numbers elements must each have the same ""
+           ""set of spatial characters, got {}."")
+    raise TypeError(msg.format(dimension_numbers))
 
+  def getperm(spec, charpair):
+    spatial = (i for i, c in enumerate(spec) if c not in charpair)
+    if spec is not rhs_spec:
+      spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
+    return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)
+
+  lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)
+  return lhs_perm, rhs_perm, out_perm
 
-ConvolutionDimensionNumbers = collections.namedtuple(
-    ""ConvolutionDimensionNumbers"", [""lhs_spec"", ""rhs_spec"", ""out_spec""])
 
 def _conv_general_proto(dimension_numbers):
-  assert type(dimension_numbers) is ConvolutionDimensionNumbers
+  assert type(dimension_numbers) is ConvDimensionNumbers
   lhs_spec, rhs_spec, out_spec = dimension_numbers
   proto = xla_bridge.xla_data_pb2.ConvolutionDimensionNumbers()
   proto.input_batch_dimension = lhs_spec[0]",No
jax/test_util.py,jax/test_util.py,0d64aea6bb0cd55a2abcc4e2958633764fdcdc74,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,clean up conv dimension_numbers handling,"diff --git a/jax/test_util.py b/jax/test_util.py
index 177357f39..b0288d836 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -70,7 +70,8 @@ def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
   if testing_tpu or testing_x32:
     atol = max(atol, 1e-1)
     rtol = max(rtol, 1e-1)
-  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)
+  return onp.allclose(a, b, atol=atol * a.size, rtol=rtol * b.size,
+                      equal_nan=equal_nan)
 
 
 def check_eq(xs, ys):","diff --git a/jax/test_util.py b/jax/test_util.py
index 177357f39..b0288d836 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -70,7 +70,8 @@ def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
   if testing_tpu or testing_x32:
     atol = max(atol, 1e-1)
     rtol = max(rtol, 1e-1)
-  return onp.allclose(a, b, atol=atol, rtol=rtol, equal_nan=equal_nan)
+  return onp.allclose(a, b, atol=atol * a.size, rtol=rtol * b.size,
+                      equal_nan=equal_nan)
 
 
 def check_eq(xs, ys):",No
tests/lax_test.py,tests/lax_test.py,0d64aea6bb0cd55a2abcc4e2958633764fdcdc74,1350db2b799430e55a73d84c1bc551ecaa9ab4f7,clean up conv dimension_numbers handling,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index 47fc5b9ee..8342c1b8b 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1576,10 +1576,10 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
-          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
-          [(1, 1), (2, 1)], [(1, 1)])
-          for b, i, j in itertools.product([1, 2], repeat=3)]
+          ((b, i, 5, 6), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+           [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],
+           [(1, 1), (2, 1)], [(1, 1)])
+          for b, i, j in itertools.product([2, 3], repeat=3)]
       for strides in all_strides
       for rhs_dil in rhs_dils
       for lhs_dil in lhs_dils
@@ -1588,7 +1588,8 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]
       for dim_nums, perms in [
           ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
-          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))]))
+          # ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))
+      ]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                  padding, lhs_dil, rhs_dil, dimension_numbers,","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 47fc5b9ee..8342c1b8b 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1576,10 +1576,10 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
-          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
-          [(1, 1), (2, 1)], [(1, 1)])
-          for b, i, j in itertools.product([1, 2], repeat=3)]
+          ((b, i, 5, 6), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+           [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],
+           [(1, 1), (2, 1)], [(1, 1)])
+          for b, i, j in itertools.product([2, 3], repeat=3)]
       for strides in all_strides
       for rhs_dil in rhs_dils
       for lhs_dil in lhs_dils
@@ -1588,7 +1588,8 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]
       for dim_nums, perms in [
           ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
-          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))]))
+          # ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))
+      ]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
                                  padding, lhs_dil, rhs_dil, dimension_numbers,",No
jax/interpreters/ad.py,jax/interpreters/ad.py,ffecf2ccc5a6ea01a2bbe1e11079b3b13b380770,0d64aea6bb0cd55a2abcc4e2958633764fdcdc74,fix bugs in zeros_like and complex transpose,"diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 595248d9e..e7a97b6d8 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -344,7 +344,7 @@ def zero_jvp(primitive, primals, tangents, **params):
   return primitive.bind(*primals, **params), zero
 
 
-deflinear(zeros_like_p, lambda t: (zeros_like_jaxval(t),))
+deflinear(zeros_like_p, lambda t: [zero])
 deflinear(core.identity_p, lambda t: (t,))
 deflinear(core.pack_p, lambda t: list(t) if t is not zero else zero)
 deflinear(add_jaxvals_p, lambda t: (t, t))","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 595248d9e..e7a97b6d8 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -344,7 +344,7 @@ def zero_jvp(primitive, primals, tangents, **params):
   return primitive.bind(*primals, **params), zero
 
 
-deflinear(zeros_like_p, lambda t: (zeros_like_jaxval(t),))
+deflinear(zeros_like_p, lambda t: [zero])
 deflinear(core.identity_p, lambda t: (t,))
 deflinear(core.pack_p, lambda t: list(t) if t is not zero else zero)
 deflinear(add_jaxvals_p, lambda t: (t, t))",No
jax/lax.py,jax/lax.py,ffecf2ccc5a6ea01a2bbe1e11079b3b13b380770,0d64aea6bb0cd55a2abcc4e2958633764fdcdc74,fix bugs in zeros_like and complex transpose,"diff --git a/jax/lax.py b/jax/lax.py
index 5031d5e7a..19df63a41 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -808,7 +808,7 @@ real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
 ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])
 
 imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
-ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])
+ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), t)])
 
 complex_p = standard_binop([_f32, _f32], 'complex')
 ad.deflinear(complex_p, lambda t: [real(t), imag(t)])","diff --git a/jax/lax.py b/jax/lax.py
index 5031d5e7a..19df63a41 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -808,7 +808,7 @@ real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
 ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])
 
 imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
-ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), neg(t))])
+ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), t)])
 
 complex_p = standard_binop([_f32, _f32], 'complex')
 ad.deflinear(complex_p, lambda t: [real(t), imag(t)])",No
jax/core.py,jax/core.py,7198e09465fefbe59e1b9ff736c9a32c8ce6b1e5,ffecf2ccc5a6ea01a2bbe1e11079b3b13b380770,enable skip_checks for merging to master,"diff --git a/jax/core.py b/jax/core.py
index 5cb42b912..827ed0cda 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -30,7 +30,7 @@ from .pprint_util import pp, vcat, hcat, pp_kv_pairs
 # TODO(dougalm): the trace cache breaks the leak detector. Consisder solving.
 check_leaks = False
 # TODO(dougalm): put this behind a flag that's enabled during testing
-skip_checks = False  # not __debug__  # google doesn't use -O
+skip_checks = True  # not __debug__  # google doesn't use -O
 
 zip = safe_zip
 map = safe_map","diff --git a/jax/core.py b/jax/core.py
index 5cb42b912..827ed0cda 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -30,7 +30,7 @@ from .pprint_util import pp, vcat, hcat, pp_kv_pairs
 # TODO(dougalm): the trace cache breaks the leak detector. Consisder solving.
 check_leaks = False
 # TODO(dougalm): put this behind a flag that's enabled during testing
-skip_checks = False  # not __debug__  # google doesn't use -O
+skip_checks = True  # not __debug__  # google doesn't use -O
 
 zip = safe_zip
 map = safe_map",No
tests/build_defs.bzl,tests/build_defs.bzl,567caecdea7995a33fc42d3f6c3f6bf1e312b09f,7198e09465fefbe59e1b9ff736c9a32c8ce6b1e5,revert build_defs.bzl for now,"diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index c51793a4f..1e1052687 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -31,7 +31,9 @@ def jax_test(
             fail(""Only one test source file is currently supported."")
 
     # Deps that are linked into all test target variants.
-    all_test_deps = []
+    all_test_deps = [
+        "":libjax"",
+    ]
     disabled_tags = [""manual"", ""notap"", ""disabled""]
     native.py_test(
         name = name + ""_cpu"",
@@ -43,33 +45,33 @@ def jax_test(
         args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
         tags = (disabled_tags if ""cpu"" in disable else []),
     )
-    # native.py_test(
-    #     name = name + ""_cpu_x32"",
-    #     main = main,
-    #     srcs = srcs,
-    #     data = data,
-    #     deps = deps + all_test_deps,
-    #     shard_count = shard_count.get(""cpu"") if shard_count else None,
-    #     args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
-    #     tags = (disabled_tags if ""cpu"" in disable else []),
-    # )
-    # native.py_test(
-    #     name = name + ""_gpu"",
-    #     main = main,
-    #     srcs = srcs,
-    #     data = data,
-    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
-    #     deps = deps + all_test_deps,
-    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
-    # )
-    # native.py_test(
-    #     name = name + ""_gpu_x32"",
-    #     main = main,
-    #     srcs = srcs,
-    #     data = data,
-    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
-    #     deps = deps + all_test_deps,
-    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
-    # )
+    native.py_test(
+        name = name + ""_cpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_gpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )
+    native.py_test(
+        name = name + ""_gpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index c51793a4f..1e1052687 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -31,7 +31,9 @@ def jax_test(
             fail(""Only one test source file is currently supported."")
 
     # Deps that are linked into all test target variants.
-    all_test_deps = []
+    all_test_deps = [
+        "":libjax"",
+    ]
     disabled_tags = [""manual"", ""notap"", ""disabled""]
     native.py_test(
         name = name + ""_cpu"",
@@ -43,33 +45,33 @@ def jax_test(
         args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
         tags = (disabled_tags if ""cpu"" in disable else []),
     )
-    # native.py_test(
-    #     name = name + ""_cpu_x32"",
-    #     main = main,
-    #     srcs = srcs,
-    #     data = data,
-    #     deps = deps + all_test_deps,
-    #     shard_count = shard_count.get(""cpu"") if shard_count else None,
-    #     args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
-    #     tags = (disabled_tags if ""cpu"" in disable else []),
-    # )
-    # native.py_test(
-    #     name = name + ""_gpu"",
-    #     main = main,
-    #     srcs = srcs,
-    #     data = data,
-    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
-    #     deps = deps + all_test_deps,
-    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
-    # )
-    # native.py_test(
-    #     name = name + ""_gpu_x32"",
-    #     main = main,
-    #     srcs = srcs,
-    #     data = data,
-    #     args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
-    #     deps = deps + all_test_deps,
-    #     tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
-    #     shard_count = shard_count.get(""gpu"") if shard_count else None,
-    # )
+    native.py_test(
+        name = name + ""_cpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_gpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )
+    native.py_test(
+        name = name + ""_gpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,5a1aeca96cb8cb24e90b1fbfea25fa40d202b8fa,567caecdea7995a33fc42d3f6c3f6bf1e312b09f,"add rot90 and flip, adjust testOp test selection
closes #55","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ed26fdf50..fbd673d03 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -327,6 +327,30 @@ def transpose(x, axis=None):
   return lax.transpose(x, axis)
 
 
+@_wraps(onp.rot90)
+def rot90(m, k=1, axes=(0, 1)):
+  ax1, ax2 = axes
+  if ax1 % m.ndim == ax2 % m.ndim:
+    raise ValueError(""Axes must be different"")  # same as numpy error
+  k = k % 4
+  if k == 0:
+    return m
+  elif k == 2:
+    return flip(flip(m, ax1), ax2)
+  else:
+    perm = list(range(m.ndim))
+    perm[ax1], perm[ax2] = perm[ax2], perm[ax1]
+    if k == 1:
+      return transpose(flip(m, ax2), perm)
+    else:
+      return flip(transpose(m, perm), ax2)
+
+
+@_wraps(onp.flip)
+def flip(m, axis):
+  return lax.rev(m, [axis])
+
+
 @_wraps(onp.sinh)
 def sinh(x):
   x, = _promote_to_result_dtype(onp.sinh, x)
@@ -454,7 +478,10 @@ def where(condition, x=None, y=None):
   if not onp.issubdtype(_dtype(condition), onp.bool_):
     condition = lax.ne(condition, zeros_like(condition))
   condition, x, y = broadcast_arrays(condition, x, y)
-  return lax.select(condition, *_promote_dtypes(x, y))
+  if not x.size:
+    return x
+  else:
+    return lax.select(condition, *_promote_dtypes(x, y))
 
 
 def broadcast_arrays(*args):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ed26fdf50..fbd673d03 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -327,6 +327,30 @@ def transpose(x, axis=None):
   return lax.transpose(x, axis)
 
 
+@_wraps(onp.rot90)
+def rot90(m, k=1, axes=(0, 1)):
+  ax1, ax2 = axes
+  if ax1 % m.ndim == ax2 % m.ndim:
+    raise ValueError(""Axes must be different"")  # same as numpy error
+  k = k % 4
+  if k == 0:
+    return m
+  elif k == 2:
+    return flip(flip(m, ax1), ax2)
+  else:
+    perm = list(range(m.ndim))
+    perm[ax1], perm[ax2] = perm[ax2], perm[ax1]
+    if k == 1:
+      return transpose(flip(m, ax2), perm)
+    else:
+      return flip(transpose(m, perm), ax2)
+
+
+@_wraps(onp.flip)
+def flip(m, axis):
+  return lax.rev(m, [axis])
+
+
 @_wraps(onp.sinh)
 def sinh(x):
   x, = _promote_to_result_dtype(onp.sinh, x)
@@ -454,7 +478,10 @@ def where(condition, x=None, y=None):
   if not onp.issubdtype(_dtype(condition), onp.bool_):
     condition = lax.ne(condition, zeros_like(condition))
   condition, x, y = broadcast_arrays(condition, x, y)
-  return lax.select(condition, *_promote_dtypes(x, y))
+  if not x.size:
+    return x
+  else:
+    return lax.select(condition, *_promote_dtypes(x, y))
 
 
 def broadcast_arrays(*args):",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,5a1aeca96cb8cb24e90b1fbfea25fa40d202b8fa,567caecdea7995a33fc42d3f6c3f6bf1e312b09f,"add rot90 and flip, adjust testOp test selection
closes #55","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index ca09e3f12..6a73548a6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -171,34 +171,36 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
   def _GetArgsMaker(self, rng, shapes, dtypes):
     return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
-                                                    dtypes),
-       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
-       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
-      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
-                                 JAX_COMPOUND_OP_RECORDS)
-      for shapes in filter(
-        _shapes_are_broadcast_compatible,
-        CombosWithReplacement(rec.shapes, rec.nargs))
-      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                      dtypes),
+         ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+         ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+        for shapes in filter(
+          _shapes_are_broadcast_compatible,
+          CombosWithReplacement(rec.shapes, rec.nargs))
+        for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS, JAX_COMPOUND_OP_RECORDS)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
-                                                    dtypes),
-       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
-       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
-      for rec in JAX_BITWISE_OP_RECORDS
-      for shapes in filter(
-        _shapes_are_broadcast_compatible,
-        CombosWithReplacement(rec.shapes, rec.nargs))
-      for dtypes in filter(
-        _dtypes_are_compatible_for_bitwise_ops,
-        CombosWithReplacement(rec.dtypes, rec.nargs))))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                      dtypes),
+         ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+         ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+        for rec in JAX_BITWISE_OP_RECORDS
+        for shapes in filter(
+          _shapes_are_broadcast_compatible,
+          CombosWithReplacement(rec.shapes, rec.nargs))
+        for dtypes in filter(
+          _dtypes_are_compatible_for_bitwise_ops,
+          CombosWithReplacement(rec.dtypes, rec.nargs)))
+      for rec in JAX_BITWISE_OP_RECORDS))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     if not FLAGS.jax_enable_x64 and any(
         onp.iinfo(dtype).bits == 64 for dtype in dtypes):
@@ -622,6 +624,41 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     cfoo = api.jit(foo)
     self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for shape in [(3,), (2, 3)]
+      for dtype in default_dtypes
+      for axis in range(len(shape))
+      for rng in [jtu.rand_default()]))
+  def testFlip(self, shape, dtype, axis, rng):
+    args_maker = self._GetArgsMaker(rng, [shape], [dtype])
+    lnp_op = lambda x: lnp.flip(x, axis)
+    onp_op = lambda x: onp.flip(x, axis)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_k={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), k, axes),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""k"": k, ""axes"": axes}
+      for shape, axes in [
+          [(2, 3), (0, 1)],
+          [(2, 3), (1, 0)],
+          [(4, 3, 2), (0, 2)],
+          [(4, 3, 2), (2, 1)],
+      ]
+      for k in range(-3, 4)
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()]))
+  def testRot90(self, shape, dtype, k, axes, rng):
+    args_maker = self._GetArgsMaker(rng, [shape], [dtype])
+    lnp_op = lambda x: lnp.rot90(x, k, axes)
+    onp_op = lambda x: onp.rot90(x, k, axes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
   # TODO(mattjj): test infix operator overrides
 
   def DISABLED_testRavel(self):","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index ca09e3f12..6a73548a6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -171,34 +171,36 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
   def _GetArgsMaker(self, rng, shapes, dtypes):
     return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
-                                                    dtypes),
-       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
-       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
-      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
-                                 JAX_COMPOUND_OP_RECORDS)
-      for shapes in filter(
-        _shapes_are_broadcast_compatible,
-        CombosWithReplacement(rec.shapes, rec.nargs))
-      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs)))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                      dtypes),
+         ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+         ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+        for shapes in filter(
+          _shapes_are_broadcast_compatible,
+          CombosWithReplacement(rec.shapes, rec.nargs))
+        for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS, JAX_COMPOUND_OP_RECORDS)))
   def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     args_maker = self._GetArgsMaker(rng, shapes, dtypes)
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
-                                                    dtypes),
-       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
-       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
-      for rec in JAX_BITWISE_OP_RECORDS
-      for shapes in filter(
-        _shapes_are_broadcast_compatible,
-        CombosWithReplacement(rec.shapes, rec.nargs))
-      for dtypes in filter(
-        _dtypes_are_compatible_for_bitwise_ops,
-        CombosWithReplacement(rec.dtypes, rec.nargs))))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                      dtypes),
+         ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+         ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+        for rec in JAX_BITWISE_OP_RECORDS
+        for shapes in filter(
+          _shapes_are_broadcast_compatible,
+          CombosWithReplacement(rec.shapes, rec.nargs))
+        for dtypes in filter(
+          _dtypes_are_compatible_for_bitwise_ops,
+          CombosWithReplacement(rec.dtypes, rec.nargs)))
+      for rec in JAX_BITWISE_OP_RECORDS))
   def testBitwiseOp(self, onp_op, lnp_op, rng, shapes, dtypes):
     if not FLAGS.jax_enable_x64 and any(
         onp.iinfo(dtype).bits == 64 for dtype in dtypes):
@@ -622,6 +624,41 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     cfoo = api.jit(foo)
     self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for shape in [(3,), (2, 3)]
+      for dtype in default_dtypes
+      for axis in range(len(shape))
+      for rng in [jtu.rand_default()]))
+  def testFlip(self, shape, dtype, axis, rng):
+    args_maker = self._GetArgsMaker(rng, [shape], [dtype])
+    lnp_op = lambda x: lnp.flip(x, axis)
+    onp_op = lambda x: onp.flip(x, axis)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_k={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), k, axes),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""k"": k, ""axes"": axes}
+      for shape, axes in [
+          [(2, 3), (0, 1)],
+          [(2, 3), (1, 0)],
+          [(4, 3, 2), (0, 2)],
+          [(4, 3, 2), (2, 1)],
+      ]
+      for k in range(-3, 4)
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()]))
+  def testRot90(self, shape, dtype, k, axes, rng):
+    args_maker = self._GetArgsMaker(rng, [shape], [dtype])
+    lnp_op = lambda x: lnp.rot90(x, k, axes)
+    onp_op = lambda x: onp.rot90(x, k, axes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
   # TODO(mattjj): test infix operator overrides
 
   def DISABLED_testRavel(self):",No
tests/lax_test.py,tests/lax_test.py,5a1aeca96cb8cb24e90b1fbfea25fa40d202b8fa,567caecdea7995a33fc42d3f6c3f6bf1e312b09f,"add rot90 and flip, adjust testOp test selection
closes #55","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 8342c1b8b..09c5d9c57 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -137,27 +137,29 @@ CombosWithReplacement = itertools.combinations_with_replacement
 class LaxTest(jtu.JaxTestCase):
   """"""Numerical tests for LAX operations.""""""
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          rec.op.__name__, shapes, itertools.repeat(dtype)),
-       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
-      for rec in LAX_OPS
-      for shape_group in compatible_shapes
-      for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.op.__name__, shapes, itertools.repeat(dtype)),
+         ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
+        for shape_group in compatible_shapes
+        for shapes in CombosWithReplacement(shape_group, rec.nargs)
+        for dtype in rec.dtypes)
+      for rec in LAX_OPS))
   def testOp(self, op, rng, shapes, dtype):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          rec.op.__name__, shapes, itertools.repeat(dtype)),
-       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
-       ""tol"": rec.tol}
-      for rec in LAX_OPS
-      for shape_group in compatible_shapes
-      for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.op.__name__, shapes, itertools.repeat(dtype)),
+         ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+         ""tol"": rec.tol}
+        for shape_group in compatible_shapes
+        for shapes in CombosWithReplacement(shape_group, rec.nargs)
+        for dtype in rec.dtypes)
+      for rec in LAX_OPS))
   def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     numpy_op = getattr(lax_reference, op.__name__)
@@ -1436,16 +1438,16 @@ def check_grads_bilinear(f, args, order, atol=None, rtol=None):
 
 class LaxAutodiffTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          rec.op.__name__, shapes, itertools.repeat(dtype)),
-       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
-       ""order"": rec.order}
-      for rec in LAX_GRAD_OPS
-      for shape_group in compatible_shapes
-      for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes
-  ))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.op.__name__, shapes, itertools.repeat(dtype)),
+         ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+         ""order"": rec.order}
+        for shape_group in compatible_shapes
+        for shapes in CombosWithReplacement(shape_group, rec.nargs)
+        for dtype in rec.dtypes)
+      for rec in LAX_GRAD_OPS))
   def testOpGrad(self, op, rng, shapes, dtype, order):
     if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
       if dtype is onp.complex64:","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 8342c1b8b..09c5d9c57 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -137,27 +137,29 @@ CombosWithReplacement = itertools.combinations_with_replacement
 class LaxTest(jtu.JaxTestCase):
   """"""Numerical tests for LAX operations.""""""
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          rec.op.__name__, shapes, itertools.repeat(dtype)),
-       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
-      for rec in LAX_OPS
-      for shape_group in compatible_shapes
-      for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.op.__name__, shapes, itertools.repeat(dtype)),
+         ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
+        for shape_group in compatible_shapes
+        for shapes in CombosWithReplacement(shape_group, rec.nargs)
+        for dtype in rec.dtypes)
+      for rec in LAX_OPS))
   def testOp(self, op, rng, shapes, dtype):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     self._CompileAndCheck(op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          rec.op.__name__, shapes, itertools.repeat(dtype)),
-       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
-       ""tol"": rec.tol}
-      for rec in LAX_OPS
-      for shape_group in compatible_shapes
-      for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.op.__name__, shapes, itertools.repeat(dtype)),
+         ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+         ""tol"": rec.tol}
+        for shape_group in compatible_shapes
+        for shapes in CombosWithReplacement(shape_group, rec.nargs)
+        for dtype in rec.dtypes)
+      for rec in LAX_OPS))
   def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
     args_maker = lambda: [rng(shape, dtype) for shape in shapes]
     numpy_op = getattr(lax_reference, op.__name__)
@@ -1436,16 +1438,16 @@ def check_grads_bilinear(f, args, order, atol=None, rtol=None):
 
 class LaxAutodiffTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          rec.op.__name__, shapes, itertools.repeat(dtype)),
-       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
-       ""order"": rec.order}
-      for rec in LAX_GRAD_OPS
-      for shape_group in compatible_shapes
-      for shapes in CombosWithReplacement(shape_group, rec.nargs)
-      for dtype in rec.dtypes
-  ))
+  @parameterized.named_parameters(itertools.chain.from_iterable(
+      jtu.cases_from_list(
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.op.__name__, shapes, itertools.repeat(dtype)),
+         ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+         ""order"": rec.order}
+        for shape_group in compatible_shapes
+        for shapes in CombosWithReplacement(shape_group, rec.nargs)
+        for dtype in rec.dtypes)
+      for rec in LAX_GRAD_OPS))
   def testOpGrad(self, op, rng, shapes, dtype, order):
     if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
       if dtype is onp.complex64:",No
jax/abstract_arrays.py,jax/abstract_arrays.py,9cd60279791b78550f22f1a4e09ae414e63bba94,2b9a8bc13ba38943f6cf3dc408508123522dfdec,"add python built-in complex type to array types
fixes #74","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 66d913c9e..7e2c4df33 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -153,8 +153,8 @@ def make_shaped_array(x):
   return ShapedArray(onp.shape(x), dtype)
 
 array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
-               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32, float,
-               int, bool]
+               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32,
+               complex, float, int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 66d913c9e..7e2c4df33 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -153,8 +153,8 @@ def make_shaped_array(x):
   return ShapedArray(onp.shape(x), dtype)
 
 array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
-               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32, float,
-               int, bool]
+               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32,
+               complex, float, int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray",No
README.md,README.md,cbd9ace4250a7a9bc382663b9eacba591cf2c402,9cd60279791b78550f22f1a4e09ae414e63bba94,change vmap api to be curried (closes #78),"diff --git a/README.md b/README.md
index d7be0ba82..b9552983d 100644
--- a/README.md
+++ b/README.md
@@ -47,8 +47,7 @@ def logprob_fun(params, inputs, targets):
   return np.sum((preds - targets)**2)
 
 grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function
-perex_grads = jit(lambda params, inputs, targets:  # fast per-example gradients
-                  vmap(partial(grad_fun, params), inputs, targets))
+perex_grads = jit(vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads
 ```
 
 JAX started as a research project by [Matt Johnson](https://github.com/mattjj),
@@ -400,7 +399,9 @@ The `vmap` function does that transformation for us. That is, if we write
 
 ```python
 from jax import vmap
-predictions = vmap(partial(predict, params), input_batch)
+predictions = vmap(partial(predict, params))(input_batch)
+# or, alternatively
+predictions = vmap(predict, in_axes=(None, 0))(input_batch)
 ```
 
 then the `vmap` function will push the outer loop inside the function, and our
@@ -414,7 +415,7 @@ of parameters, we want to compute the gradient of our loss function evaluated
 separately at each example in a batch. With `vmap`, its easy:
 
 ```python
-per_example_gradients = vmap(partial(grad(loss), params), inputs, targets)
+per_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)
 ```
 
 Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other","diff --git a/README.md b/README.md
index d7be0ba82..b9552983d 100644
--- a/README.md
+++ b/README.md
@@ -47,8 +47,7 @@ def logprob_fun(params, inputs, targets):
   return np.sum((preds - targets)**2)
 
 grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function
-perex_grads = jit(lambda params, inputs, targets:  # fast per-example gradients
-                  vmap(partial(grad_fun, params), inputs, targets))
+perex_grads = jit(vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads
 ```
 
 JAX started as a research project by [Matt Johnson](https://github.com/mattjj),
@@ -400,7 +399,9 @@ The `vmap` function does that transformation for us. That is, if we write
 
 ```python
 from jax import vmap
-predictions = vmap(partial(predict, params), input_batch)
+predictions = vmap(partial(predict, params))(input_batch)
+# or, alternatively
+predictions = vmap(predict, in_axes=(None, 0))(input_batch)
 ```
 
 then the `vmap` function will push the outer loop inside the function, and our
@@ -414,7 +415,7 @@ of parameters, we want to compute the gradient of our loss function evaluated
 separately at each example in a batch. With `vmap`, its easy:
 
 ```python
-per_example_gradients = vmap(partial(grad(loss), params), inputs, targets)
+per_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)
 ```
 
 Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other",No
jax/api.py,jax/api.py,cbd9ace4250a7a9bc382663b9eacba591cf2c402,9cd60279791b78550f22f1a4e09ae414e63bba94,change vmap api to be curried (closes #78),"diff --git a/jax/api.py b/jax/api.py
index 638f8811e..6e54ef882 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -78,21 +78,18 @@ def jacrev(fun, x):
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
-def vmap(fun, *args, **kwargs):
-  in_axes = kwargs.pop(""in_axes"", 0)
-  out_axes = kwargs.pop(""out_axes"", 0)
-  if kwargs:
-    msg = ""vmap keyword args must be 'in_axes' and/or 'out_axes', got {}.""
-    raise TypeError(msg.format(', '.join(kwargs)))
-
-  if type(in_axes) is int:
-    in_axes = (in_axes,) * len(args)
-  if not isinstance(fun, lu.WrappedFun):
-    fun = lu.wrap_init(fun)
-  in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_flat = batching.batch(flat_fun, in_flat, in_axes, out_axes)
-  return build_tree(out_tree(), out_flat)
+def vmap(fun, in_axes=0, out_axes=0):
+
+  def batched_fun(*args, **kwargs):
+    if not isinstance(fun, lu.WrappedFun):
+      f = lu.wrap_init(fun)
+    in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
+    in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
+    flat_fun, out_tree = flatten_fun(f, in_trees)
+    out_flat = batching.batch(flat_fun, in_flat, in_axes_, out_axes)
+    return build_tree(out_tree(), out_flat)
+
+  return batched_fun
 
 def jvp(fun, primals, tangents):
   def flatten_arg(primal, tangent):","diff --git a/jax/api.py b/jax/api.py
index 638f8811e..6e54ef882 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -78,21 +78,18 @@ def jacrev(fun, x):
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
-def vmap(fun, *args, **kwargs):
-  in_axes = kwargs.pop(""in_axes"", 0)
-  out_axes = kwargs.pop(""out_axes"", 0)
-  if kwargs:
-    msg = ""vmap keyword args must be 'in_axes' and/or 'out_axes', got {}.""
-    raise TypeError(msg.format(', '.join(kwargs)))
+def vmap(fun, in_axes=0, out_axes=0):
 
-  if type(in_axes) is int:
-    in_axes = (in_axes,) * len(args)
-  if not isinstance(fun, lu.WrappedFun):
-    fun = lu.wrap_init(fun)
-  in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_flat = batching.batch(flat_fun, in_flat, in_axes, out_axes)
-  return build_tree(out_tree(), out_flat)
+  def batched_fun(*args, **kwargs):
+    if not isinstance(fun, lu.WrappedFun):
+      f = lu.wrap_init(fun)
+    in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
+    in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
+    flat_fun, out_tree = flatten_fun(f, in_trees)
+    out_flat = batching.batch(flat_fun, in_flat, in_axes_, out_axes)
+    return build_tree(out_tree(), out_flat)
+
+  return batched_fun
 
 def jvp(fun, primals, tangents):
   def flatten_arg(primal, tangent):",Yes
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,cbd9ace4250a7a9bc382663b9eacba591cf2c402,9cd60279791b78550f22f1a4e09ae414e63bba94,change vmap api to be curried (closes #78),"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 7b02d00b7..f9920811d 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -350,12 +350,6 @@
       ""source"": [
         ""import functools\n"",
         ""\n"",
-        ""def curried_vmap(func):\n"",
-        ""  @functools.wraps(func)\n"",
-        ""  def wrapper(*args):\n"",
-        ""    return vmap(func, *args)\n"",
-        ""  return wrapper\n"",
-        ""\n"",
         ""\n"",
         ""def vectorize(signature):\n"",
         ""  \""\""\""Vectorize a function using JAX.\""\""\""\n"",
@@ -375,7 +369,7 @@
         ""\n"",
         ""      vectorized_func = func\n"",
         ""      for _ in range(num_batch_dims):\n"",
-        ""        vectorized_func = curried_vmap(vectorized_func)\n"",
+        ""        vectorized_func = vmap(vectorized_func)\n"",
         ""      result = vectorized_func(*boardcast_args)\n"",
         ""\n"",
         ""      if axis is not None:\n"",","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 7b02d00b7..f9920811d 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -350,12 +350,6 @@
       ""source"": [
         ""import functools\n"",
         ""\n"",
-        ""def curried_vmap(func):\n"",
-        ""  @functools.wraps(func)\n"",
-        ""  def wrapper(*args):\n"",
-        ""    return vmap(func, *args)\n"",
-        ""  return wrapper\n"",
-        ""\n"",
         ""\n"",
         ""def vectorize(signature):\n"",
         ""  \""\""\""Vectorize a function using JAX.\""\""\""\n"",
@@ -375,7 +369,7 @@
         ""\n"",
         ""      vectorized_func = func\n"",
         ""      for _ in range(num_batch_dims):\n"",
-        ""        vectorized_func = curried_vmap(vectorized_func)\n"",
+        ""        vectorized_func = vmap(vectorized_func)\n"",
         ""      result = vectorized_func(*boardcast_args)\n"",
         ""\n"",
         ""      if axis is not None:\n"",",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,cbd9ace4250a7a9bc382663b9eacba591cf2c402,9cd60279791b78550f22f1a4e09ae414e63bba94,change vmap api to be curried (closes #78),"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 2532ba6f6..868366910 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -229,16 +229,9 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""# Let's upgrade it to handle batches using `vmap`\n"",
-        ""from jax.util import curry\n"",
-        ""\n"",
-        ""# The `vmap` function alone works like `map`, in that it will\n"",
-        ""# apply a function across inputs.\n"",
-        ""# However, let's curry vmap so that it returns a function\n"",
-        ""# with the original function signature.\n"",
-        ""curried_vmap = curry(vmap)\n"",
         ""\n"",
         ""# Make a batched version of the `predict` function\n"",
-        ""batched_predict = curried_vmap(predict, in_axes=(None, 0))\n"",
+        ""batched_predict = vmap(predict, in_axes=(None, 0))\n"",
         ""\n"",
         ""# `batched_predict` has the same call signature as `predict`\n"",
         ""batched_preds = batched_predict(params, random_flattened_images)\n"",","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 2532ba6f6..868366910 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -229,16 +229,9 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""# Let's upgrade it to handle batches using `vmap`\n"",
-        ""from jax.util import curry\n"",
-        ""\n"",
-        ""# The `vmap` function alone works like `map`, in that it will\n"",
-        ""# apply a function across inputs.\n"",
-        ""# However, let's curry vmap so that it returns a function\n"",
-        ""# with the original function signature.\n"",
-        ""curried_vmap = curry(vmap)\n"",
         ""\n"",
         ""# Make a batched version of the `predict` function\n"",
-        ""batched_predict = curried_vmap(predict, in_axes=(None, 0))\n"",
+        ""batched_predict = vmap(predict, in_axes=(None, 0))\n"",
         ""\n"",
         ""# `batched_predict` has the same call signature as `predict`\n"",
         ""batched_preds = batched_predict(params, random_flattened_images)\n"",",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,cbd9ace4250a7a9bc382663b9eacba591cf2c402,9cd60279791b78550f22f1a4e09ae414e63bba94,change vmap api to be curried (closes #78),"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 962ddb45b..340343c25 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -563,7 +563,7 @@
       ""source"": [
         ""@jit\n"",
         ""def vmap_batched_apply_matrix(batched_x):\n"",
-        ""  return vmap(apply_matrix, batched_x)\n"",
+        ""  return vmap(apply_matrix)(batched_x)\n"",
         ""\n"",
         ""print('Auto-vectorized with vmap')\n"",
         ""%timeit vmap_batched_apply_matrix(batched_x)""","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 962ddb45b..340343c25 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -563,7 +563,7 @@
       ""source"": [
         ""@jit\n"",
         ""def vmap_batched_apply_matrix(batched_x):\n"",
-        ""  return vmap(apply_matrix, batched_x)\n"",
+        ""  return vmap(apply_matrix)(batched_x)\n"",
         ""\n"",
         ""print('Auto-vectorized with vmap')\n"",
         ""%timeit vmap_batched_apply_matrix(batched_x)""",No
tests/batching_test.py,tests/batching_test.py,cbd9ace4250a7a9bc382663b9eacba591cf2c402,9cd60279791b78550f22f1a4e09ae414e63bba94,change vmap api to be curried (closes #78),"diff --git a/tests/batching_test.py b/tests/batching_test.py
index f1ed960af..7751de30b 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -31,21 +31,16 @@ from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial, curry
 
-import functools as fn
-
 class BatchingTest(jtu.JaxTestCase):
 
   def testConstantFunction(self):
-    ans = vmap(lambda x: 3, onp.ones(4))
+    ans = vmap(lambda x: 3)(onp.ones(4))
     expected = 3 * onp.ones(4)
     self.assertAllClose(ans, expected, check_dtypes=False)
 
   def testNestedBatchingMatMat(self):
-    def matvec(A, b):
-      return vmap(np.vdot, A, b, in_axes=(0, None))
-
-    def matmat(A, B):
-      return vmap(matvec, A, B, in_axes=(None, 1), out_axes=1)
+    matvec = vmap(np.vdot, in_axes=(0, None))
+    matmat = vmap(matvec, in_axes=(None, 1), out_axes=1)
 
     R = onp.random.RandomState(0).randn
     A = R(4, 3)
@@ -94,7 +89,7 @@ class BatchingTest(jtu.JaxTestCase):
     target_batch = R(5, 4)
     batch = (input_batch, target_batch)
 
-    ans = vmap(partial(grad(loss), params), batch)
+    ans = vmap(partial(grad(loss), params))(batch)
 
     for ans_pair, param_pair in zip(ans, params):
       dW, db = ans_pair
@@ -107,13 +102,13 @@ class BatchingTest(jtu.JaxTestCase):
     def jacbwd(f, x):
       y, pullback = vjp(f, x)
       std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-      jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
+      jac_flat, = vmap(pullback, out_axes=onp.ndim(y))(std_basis)
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     def jacfwd(f, x):
       pushfwd = lambda v: jvp(f, (x,), (v,))
       std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
-      y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
+      y, jac_flat = vmap(pushfwd, out_axes=(None, 0))(std_basis)
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     R = onp.random.RandomState(0).randn
@@ -133,7 +128,7 @@ class BatchingTest(jtu.JaxTestCase):
       side.append(None)
       return x + x
 
-    g = jit(lambda x: vmap(f, x))
+    g = jit(vmap(f))
     self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
     self.assertEqual(len(side), 1)
     self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
@@ -145,7 +140,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(5, 10)
 
-    ans = vmap(fun, x)
+    ans = vmap(fun)(x)
     expected_ans = x[:, 2:4]
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
@@ -154,7 +149,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(10, 5, 3, 7)
 
-    ans = vmap(fun, x)
+    ans = vmap(fun)(x)
     expected_ans = x[:, :, 2]
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
@@ -163,7 +158,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(10, 5, 3, 7)
 
-    ans = vmap(fun, x)
+    ans = vmap(fun)(x)
     expected_ans = onp.maximum(x, 0.0)
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
@@ -171,7 +166,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(10, 5, 3, 7)
 
-    ans = vmap(lambda x: x > 1.0, x)
+    ans = vmap(lambda x: x > 1.0)(x)
     expected_ans = x > 1.0
     self.assertAllClose(ans, expected_ans, check_dtypes=True)
 
@@ -182,7 +177,7 @@ class BatchingTest(jtu.JaxTestCase):
 
     fun = lambda W, x: np.sum(np.maximum(np.dot(x, W), 0.0) ** 2)
 
-    ans = vmap(fn.partial(grad(fun), W), x)
+    ans = vmap(partial(grad(fun), W))(x)
 
     W_t = np.transpose(W)
     for i in range(10):
@@ -199,44 +194,44 @@ class BatchingTest(jtu.JaxTestCase):
 
     x = R(10, 3, 4, 5)
     y = R(10, 3, 5, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun)(x, y)
     expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
     x = R(3, 4, 10, 5)
     y = R(3, 10, 5, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
-               in_axes=(2, 1))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun, in_axes=(2, 1))(x, y)
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
     x = R(3, 4, 5, 10)
     y = R(3, 5, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
-               in_axes=(3, None))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun, in_axes=(3, None))(x, y)
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i], y) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
     x = R(3, 4, 5)
     y = R(3, 5, 10, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
-               in_axes=(None, 2))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun, in_axes=(None, 2))(x, y)
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
   def testDot(self):
     # these tests are based on @shoyer's notebook studying gufuncs
-    curried_vmap = curry(vmap)
 
     def vecvec(a, b):
       dot = np.dot
       for ndim in range(1, max(a.ndim, b.ndim)):
         a_ax = 0 if a.ndim > ndim else None
         b_ax = 0 if b.ndim > ndim else None
-        dot = curried_vmap(dot, in_axes=(a_ax, b_ax))
+        dot = vmap(dot, in_axes=(a_ax, b_ax))
       return dot(a, b)
 
     assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index f1ed960af..7751de30b 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -31,21 +31,16 @@ from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial, curry
 
-import functools as fn
-
 class BatchingTest(jtu.JaxTestCase):
 
   def testConstantFunction(self):
-    ans = vmap(lambda x: 3, onp.ones(4))
+    ans = vmap(lambda x: 3)(onp.ones(4))
     expected = 3 * onp.ones(4)
     self.assertAllClose(ans, expected, check_dtypes=False)
 
   def testNestedBatchingMatMat(self):
-    def matvec(A, b):
-      return vmap(np.vdot, A, b, in_axes=(0, None))
-
-    def matmat(A, B):
-      return vmap(matvec, A, B, in_axes=(None, 1), out_axes=1)
+    matvec = vmap(np.vdot, in_axes=(0, None))
+    matmat = vmap(matvec, in_axes=(None, 1), out_axes=1)
 
     R = onp.random.RandomState(0).randn
     A = R(4, 3)
@@ -94,7 +89,7 @@ class BatchingTest(jtu.JaxTestCase):
     target_batch = R(5, 4)
     batch = (input_batch, target_batch)
 
-    ans = vmap(partial(grad(loss), params), batch)
+    ans = vmap(partial(grad(loss), params))(batch)
 
     for ans_pair, param_pair in zip(ans, params):
       dW, db = ans_pair
@@ -107,13 +102,13 @@ class BatchingTest(jtu.JaxTestCase):
     def jacbwd(f, x):
       y, pullback = vjp(f, x)
       std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-      jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
+      jac_flat, = vmap(pullback, out_axes=onp.ndim(y))(std_basis)
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     def jacfwd(f, x):
       pushfwd = lambda v: jvp(f, (x,), (v,))
       std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
-      y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
+      y, jac_flat = vmap(pushfwd, out_axes=(None, 0))(std_basis)
       return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
     R = onp.random.RandomState(0).randn
@@ -133,7 +128,7 @@ class BatchingTest(jtu.JaxTestCase):
       side.append(None)
       return x + x
 
-    g = jit(lambda x: vmap(f, x))
+    g = jit(vmap(f))
     self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
     self.assertEqual(len(side), 1)
     self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
@@ -145,7 +140,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(5, 10)
 
-    ans = vmap(fun, x)
+    ans = vmap(fun)(x)
     expected_ans = x[:, 2:4]
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
@@ -154,7 +149,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(10, 5, 3, 7)
 
-    ans = vmap(fun, x)
+    ans = vmap(fun)(x)
     expected_ans = x[:, :, 2]
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
@@ -163,7 +158,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(10, 5, 3, 7)
 
-    ans = vmap(fun, x)
+    ans = vmap(fun)(x)
     expected_ans = onp.maximum(x, 0.0)
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
@@ -171,7 +166,7 @@ class BatchingTest(jtu.JaxTestCase):
     R = onp.random.RandomState(0).randn
     x = R(10, 5, 3, 7)
 
-    ans = vmap(lambda x: x > 1.0, x)
+    ans = vmap(lambda x: x > 1.0)(x)
     expected_ans = x > 1.0
     self.assertAllClose(ans, expected_ans, check_dtypes=True)
 
@@ -182,7 +177,7 @@ class BatchingTest(jtu.JaxTestCase):
 
     fun = lambda W, x: np.sum(np.maximum(np.dot(x, W), 0.0) ** 2)
 
-    ans = vmap(fn.partial(grad(fun), W), x)
+    ans = vmap(partial(grad(fun), W))(x)
 
     W_t = np.transpose(W)
     for i in range(10):
@@ -199,44 +194,44 @@ class BatchingTest(jtu.JaxTestCase):
 
     x = R(10, 3, 4, 5)
     y = R(10, 3, 5, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y)
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun)(x, y)
     expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
     x = R(3, 4, 10, 5)
     y = R(3, 10, 5, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
-               in_axes=(2, 1))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun, in_axes=(2, 1))(x, y)
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
     x = R(3, 4, 5, 10)
     y = R(3, 5, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
-               in_axes=(3, None))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun, in_axes=(3, None))(x, y)
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x[..., i], y) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
     x = R(3, 4, 5)
     y = R(3, 5, 10, 6)
-    ans = vmap(lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))]), x, y,
-               in_axes=(None, 2))
+    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
+    ans = vmap(fun, in_axes=(None, 2))(x, y)
     fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])
     expected = onp.stack([fun(x, y[..., i, :]) for i in range(10)])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
   def testDot(self):
     # these tests are based on @shoyer's notebook studying gufuncs
-    curried_vmap = curry(vmap)
 
     def vecvec(a, b):
       dot = np.dot
       for ndim in range(1, max(a.ndim, b.ndim)):
         a_ax = 0 if a.ndim > ndim else None
         b_ax = 0 if b.ndim > ndim else None
-        dot = curried_vmap(dot, in_axes=(a_ax, b_ax))
+        dot = vmap(dot, in_axes=(a_ax, b_ax))
       return dot(a, b)
 
     assert vecvec(np.zeros((3,)), np.zeros((3,))).shape == ()",No
README.md,README.md,9099272e72a0243e3218c17c2cdab6d57876cf87,cbd9ace4250a7a9bc382663b9eacba591cf2c402,fix typo in readme,"diff --git a/README.md b/README.md
index b9552983d..bd6b83608 100644
--- a/README.md
+++ b/README.md
@@ -401,7 +401,7 @@ The `vmap` function does that transformation for us. That is, if we write
 from jax import vmap
 predictions = vmap(partial(predict, params))(input_batch)
 # or, alternatively
-predictions = vmap(predict, in_axes=(None, 0))(input_batch)
+predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)
 ```
 
 then the `vmap` function will push the outer loop inside the function, and our","diff --git a/README.md b/README.md
index b9552983d..bd6b83608 100644
--- a/README.md
+++ b/README.md
@@ -401,7 +401,7 @@ The `vmap` function does that transformation for us. That is, if we write
 from jax import vmap
 predictions = vmap(partial(predict, params))(input_batch)
 # or, alternatively
-predictions = vmap(predict, in_axes=(None, 0))(input_batch)
+predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)
 ```
 
 then the `vmap` function will push the outer loop inside the function, and our",No
setup.py,setup.py,4039aa480d5df1c50959e878aca40364aeb27f2a,72d3bd54be1003f6f3a43089e76ea5d20fbe81b3,bump version number for pypi,"diff --git a/setup.py b/setup.py
index c6c06b52e..33dd77df4 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.5',
+    version='0.1.6',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index c6c06b52e..33dd77df4 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup
 
 setup(
     name='jax',
-    version='0.1.5',
+    version='0.1.6',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,8fac32e505154391e074a78af2e180101698b452,4039aa480d5df1c50959e878aca40364aeb27f2a,"colab notebooks now do 'pip install --upgrade jax'
(it seems the vm can persist in some cases)","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index f9920811d..db5c5e5b6 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -126,7 +126,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install -q jax""
+        ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index f9920811d..db5c5e5b6 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -126,7 +126,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install -q jax""
+        ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,8fac32e505154391e074a78af2e180101698b452,4039aa480d5df1c50959e878aca40364aeb27f2a,"colab notebooks now do 'pip install --upgrade jax'
(it seems the vm can persist in some cases)","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 868366910..d6951b895 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -75,7 +75,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax""
+        ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 868366910..d6951b895 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -75,7 +75,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax""
+        ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,8fac32e505154391e074a78af2e180101698b452,4039aa480d5df1c50959e878aca40364aeb27f2a,"colab notebooks now do 'pip install --upgrade jax'
(it seems the vm can persist in some cases)","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 340343c25..5a8b08d0c 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -90,7 +90,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax""
+        ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 340343c25..5a8b08d0c 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -90,7 +90,7 @@
       ""cell_type"": ""code"",
       ""source"": [
         ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
-        ""!pip install jax""
+        ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,
       ""outputs"": []",No
setup.py,setup.py,3ac1001c4929d9fb69c05a34ce35b4f91d7778ea,8fac32e505154391e074a78af2e180101698b452,use find_packages() in setup.py,"diff --git a/setup.py b/setup.py
index 33dd77df4..7d2dbf88e 100644
--- a/setup.py
+++ b/setup.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from setuptools import setup
+from setuptools import setup, find_packages
 
 setup(
     name='jax',
@@ -20,8 +20,7 @@ setup(
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',
-    packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
-              'jax.scipy.stats', 'jax.experimental'],
+    packages=find_packages(),
     install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
                       'opt_einsum'],
     url='https://github.com/google/jax',","diff --git a/setup.py b/setup.py
index 33dd77df4..7d2dbf88e 100644
--- a/setup.py
+++ b/setup.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from setuptools import setup
+from setuptools import setup, find_packages
 
 setup(
     name='jax',
@@ -20,8 +20,7 @@ setup(
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',
-    packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
-              'jax.scipy.stats', 'jax.experimental'],
+    packages=find_packages(),
     install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
                       'opt_einsum'],
     url='https://github.com/google/jax',",No
jax/lax.py,jax/lax.py,97589a3d03a2d29ec1c01a1dd6ff76e818d45461,3ac1001c4929d9fb69c05a34ce35b4f91d7778ea,add batching rule for lax.pad (c.f. #54),"diff --git a/jax/lax.py b/jax/lax.py
index 19df63a41..719f97b06 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1398,9 +1398,21 @@ def pad_transpose(t, operand, padding_value, padding_config):
 
   return [t_operand, t_padv]
 
+def pad_batch_rule(batched_args, batch_dims, padding_config):
+  operand, padding_value = batched_args
+  operand_bdim, padding_value_bdim = batch_dims
+  if padding_value_bdim is None:
+    assert operand_bdim is not None
+    padding_config = list(padding_config)
+    padding_config.insert(operand_bdim, (0, 0, 0))
+    return pad(operand, padding_value, padding_config), operand_bdim
+  else:
+    raise NotImplementedError
+
 pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
 ad.deflinear(pad_p, pad_transpose)
 ad.primitive_transposes[pad_p] = pad_transpose
+batching.primitive_batchers[pad_p] = pad_batch_rule
 
 
 def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):","diff --git a/jax/lax.py b/jax/lax.py
index 19df63a41..719f97b06 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1398,9 +1398,21 @@ def pad_transpose(t, operand, padding_value, padding_config):
 
   return [t_operand, t_padv]
 
+def pad_batch_rule(batched_args, batch_dims, padding_config):
+  operand, padding_value = batched_args
+  operand_bdim, padding_value_bdim = batch_dims
+  if padding_value_bdim is None:
+    assert operand_bdim is not None
+    padding_config = list(padding_config)
+    padding_config.insert(operand_bdim, (0, 0, 0))
+    return pad(operand, padding_value, padding_config), operand_bdim
+  else:
+    raise NotImplementedError
+
 pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
 ad.deflinear(pad_p, pad_transpose)
 ad.primitive_transposes[pad_p] = pad_transpose
+batching.primitive_batchers[pad_p] = pad_batch_rule
 
 
 def reshape_shape_rule(operand, new_sizes, dimensions, **unused_kwargs):",No
tests/batching_test.py,tests/batching_test.py,97589a3d03a2d29ec1c01a1dd6ff76e818d45461,3ac1001c4929d9fb69c05a34ce35b4f91d7778ea,add batching rule for lax.pad (c.f. #54),"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 7751de30b..8683aa956 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -239,6 +239,24 @@ class BatchingTest(jtu.JaxTestCase):
     # TODO(mattjj): this fails due to an xla error in dot_general
     # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
 
+  def testPad(self):
+    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1)])
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10).astype(onp.float32)
+
+    ans = vmap(fun)(x)
+    expected_ans = np.stack(list(map(fun, x)))
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+
+    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1), (0, 1, 0)])
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10, 3).astype(onp.float32)
+
+    ans = vmap(fun)(x)
+    expected_ans = np.stack(list(map(fun, x)))
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   config.config_with_absl()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 7751de30b..8683aa956 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -239,6 +239,24 @@ class BatchingTest(jtu.JaxTestCase):
     # TODO(mattjj): this fails due to an xla error in dot_general
     # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
 
+  def testPad(self):
+    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1)])
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10).astype(onp.float32)
+
+    ans = vmap(fun)(x)
+    expected_ans = np.stack(list(map(fun, x)))
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+
+    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1), (0, 1, 0)])
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10, 3).astype(onp.float32)
+
+    ans = vmap(fun)(x)
+    expected_ans = np.stack(list(map(fun, x)))
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   config.config_with_absl()",No
jax/lax.py,jax/lax.py,e788539e0a642599352cccc488386b3c7be52e98,97589a3d03a2d29ec1c01a1dd6ff76e818d45461,add concatenate batching rule (c.f. #54),"diff --git a/jax/lax.py b/jax/lax.py
index 719f97b06..233814324 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1362,11 +1362,20 @@ def concatenate_transpose_rule(t, *operands, **kwargs):
     return [slice(t, start, limit) if o is None else None
             for o, start, limit in zip(operands, starts, limits)]
 
+def concatenate_batch_rule(batched_args, batch_dims, dimension, operand_shapes):
+  size = next(op.shape[bdim] for op, bdim in zip(batched_args, batch_dims)
+              if bdim is not None)
+  operands = [batching.move_dim_to_front(op, bdim) if bdim is not None
+              else broadcast(op, (size,))
+              for op, bdim in zip(batched_args, batch_dims)]
+  return concatenate(operands, dimension + 1), 0
+
 concatenate_p = standard_primitive(
     concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
     concatenate_translation_rule)
 ad.deflinear(concatenate_p, concatenate_transpose_rule)
 ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule
+batching.primitive_batchers[concatenate_p] = concatenate_batch_rule
 
 
 def pad_shape_rule(operand, padding_value, padding_config):","diff --git a/jax/lax.py b/jax/lax.py
index 719f97b06..233814324 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1362,11 +1362,20 @@ def concatenate_transpose_rule(t, *operands, **kwargs):
     return [slice(t, start, limit) if o is None else None
             for o, start, limit in zip(operands, starts, limits)]
 
+def concatenate_batch_rule(batched_args, batch_dims, dimension, operand_shapes):
+  size = next(op.shape[bdim] for op, bdim in zip(batched_args, batch_dims)
+              if bdim is not None)
+  operands = [batching.move_dim_to_front(op, bdim) if bdim is not None
+              else broadcast(op, (size,))
+              for op, bdim in zip(batched_args, batch_dims)]
+  return concatenate(operands, dimension + 1), 0
+
 concatenate_p = standard_primitive(
     concatenate_shape_rule, concatenate_dtype_rule, 'concatenate',
     concatenate_translation_rule)
 ad.deflinear(concatenate_p, concatenate_transpose_rule)
 ad.primitive_transposes[concatenate_p] = concatenate_transpose_rule
+batching.primitive_batchers[concatenate_p] = concatenate_batch_rule
 
 
 def pad_shape_rule(operand, padding_value, padding_config):",No
tests/batching_test.py,tests/batching_test.py,e788539e0a642599352cccc488386b3c7be52e98,97589a3d03a2d29ec1c01a1dd6ff76e818d45461,add concatenate batching rule (c.f. #54),"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 8683aa956..d2076202e 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -240,23 +240,38 @@ class BatchingTest(jtu.JaxTestCase):
     # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
 
   def testPad(self):
-    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1)])
     R = onp.random.RandomState(0).randn
-    x = R(5, 10).astype(onp.float32)
 
+    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1)])
+    x = R(5, 10).astype(onp.float32)
     ans = vmap(fun)(x)
     expected_ans = np.stack(list(map(fun, x)))
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
 
     fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1), (0, 1, 0)])
-    R = onp.random.RandomState(0).randn
     x = R(5, 10, 3).astype(onp.float32)
-
     ans = vmap(fun)(x)
     expected_ans = np.stack(list(map(fun, x)))
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
+  def testConcatenate(self):
+    R = lambda *shape: onp.random.RandomState(0).randn(*shape).astype(onp.float32)
+
+    fun = lambda *args: lax.concatenate(args, dimension=0)
+    x, y, z = R(10, 2, 3), R(1, 10, 3), R(4, 3)
+    ans = vmap(fun, in_axes=(0, 1, None))(x, y, z)
+    expected_ans = onp.concatenate([x, onp.swapaxes(y, 0, 1),
+                                    onp.broadcast_to(z, (10, 4, 3))], 1)
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+    fun = lambda *args: lax.concatenate(args, dimension=1)
+    x, y, z = R(10, 2, 1), R(2, 3), R(2, 4, 10)
+    ans = vmap(fun, in_axes=(0, None, 2))(x, y, z)
+    expected_ans = onp.concatenate([x, onp.broadcast_to(y, (10, 2, 3)),
+                                    onp.moveaxis(z, 2, 0)], 2)
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   config.config_with_absl()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 8683aa956..d2076202e 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -240,23 +240,38 @@ class BatchingTest(jtu.JaxTestCase):
     # assert vecvec(np.zeros((4, 2, 3)), np.zeros((3,))).shape == (4, 2)
 
   def testPad(self):
-    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1)])
     R = onp.random.RandomState(0).randn
-    x = R(5, 10).astype(onp.float32)
 
+    fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1)])
+    x = R(5, 10).astype(onp.float32)
     ans = vmap(fun)(x)
     expected_ans = np.stack(list(map(fun, x)))
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
 
     fun = lambda x: lax.pad(x, onp.float32(0), [(1, 2, 1), (0, 1, 0)])
-    R = onp.random.RandomState(0).randn
     x = R(5, 10, 3).astype(onp.float32)
-
     ans = vmap(fun)(x)
     expected_ans = np.stack(list(map(fun, x)))
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
+  def testConcatenate(self):
+    R = lambda *shape: onp.random.RandomState(0).randn(*shape).astype(onp.float32)
+
+    fun = lambda *args: lax.concatenate(args, dimension=0)
+    x, y, z = R(10, 2, 3), R(1, 10, 3), R(4, 3)
+    ans = vmap(fun, in_axes=(0, 1, None))(x, y, z)
+    expected_ans = onp.concatenate([x, onp.swapaxes(y, 0, 1),
+                                    onp.broadcast_to(z, (10, 4, 3))], 1)
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+    fun = lambda *args: lax.concatenate(args, dimension=1)
+    x, y, z = R(10, 2, 1), R(2, 3), R(2, 4, 10)
+    ans = vmap(fun, in_axes=(0, None, 2))(x, y, z)
+    expected_ans = onp.concatenate([x, onp.broadcast_to(y, (10, 2, 3)),
+                                    onp.moveaxis(z, 2, 0)], 2)
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   config.config_with_absl()",No
jax/api.py,jax/api.py,89349e5e6df4a82eb07278102f5caa9511e73ade,e788539e0a642599352cccc488386b3c7be52e98,fix transpose issue in jacfwd and jacrev,"diff --git a/jax/api.py b/jax/api.py
index 6e54ef882..8902c46fd 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -64,7 +64,7 @@ def jacfwd(fun, x):
   fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
-  y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
+  y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 @curry
@@ -72,7 +72,7 @@ def jacrev(fun, x):
   fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-  jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
+  jac_flat, = vmap(pullback, out_axes=0)(std_basis)
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 def hessian(fun):","diff --git a/jax/api.py b/jax/api.py
index 6e54ef882..8902c46fd 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -64,7 +64,7 @@ def jacfwd(fun, x):
   fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
-  y, jac_flat = vmap(pushfwd, std_basis, out_axes=(None, 0))
+  y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 @curry
@@ -72,7 +72,7 @@ def jacrev(fun, x):
   fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-  jac_flat, = vmap(pullback, std_basis, out_axes=onp.ndim(y))
+  jac_flat, = vmap(pullback, out_axes=0)(std_basis)
   return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
 def hessian(fun):",No
jax/interpreters/batching.py,jax/interpreters/batching.py,89349e5e6df4a82eb07278102f5caa9511e73ade,e788539e0a642599352cccc488386b3c7be52e98,fix transpose issue in jacfwd and jacrev,"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 56e764616..725e5b038 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -281,6 +281,7 @@ def moveaxis(sz, dst, src, x):
     else:
       return pack(map(partial(moveaxis, sz, dst, src), x))
   elif isinstance(aval, ShapedArray):
+    dst = (dst % aval.ndim) if dst is not None and aval.ndim else dst
     if src == dst:
       return x
     else:","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 56e764616..725e5b038 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -281,6 +281,7 @@ def moveaxis(sz, dst, src, x):
     else:
       return pack(map(partial(moveaxis, sz, dst, src), x))
   elif isinstance(aval, ShapedArray):
+    dst = (dst % aval.ndim) if dst is not None and aval.ndim else dst
     if src == dst:
       return x
     else:",No
tests/api_test.py,tests/api_test.py,89349e5e6df4a82eb07278102f5caa9511e73ade,e788539e0a642599352cccc488386b3c7be52e98,fix transpose issue in jacfwd and jacrev,"diff --git a/tests/api_test.py b/tests/api_test.py
index ff3fbb7e6..8a7982646 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -24,7 +24,7 @@ from jax import test_util as jtu
 
 import jax.numpy as np
 from jax.config import config
-from jax import jit, grad, device_get, device_put
+from jax import jit, grad, device_get, device_put, jacfwd, jacrev
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
 from jax.interpreters.ad import defjvp
@@ -235,6 +235,18 @@ class APITest(jtu.JaxTestCase):
     assert isinstance(y2[1][1], onp.ndarray)
     assert onp.all(y2[1][1] == 3 * x)
 
+  def test_jacobian(self):
+    R = onp.random.RandomState(0).randn
+    A = R(4, 3)
+    x = R(3)
+
+    f = lambda x: np.dot(A, x)
+    assert onp.allclose(jacfwd(f)(x), A)
+    assert onp.allclose(jacrev(f)(x), A)
+
+    f = lambda x: np.tanh(np.dot(A, x))
+    assert onp.allclose(jacfwd(f)(x), jacrev(f)(x))
+
 
 if __name__ == '__main__':
   config.config_with_absl()","diff --git a/tests/api_test.py b/tests/api_test.py
index ff3fbb7e6..8a7982646 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -24,7 +24,7 @@ from jax import test_util as jtu
 
 import jax.numpy as np
 from jax.config import config
-from jax import jit, grad, device_get, device_put
+from jax import jit, grad, device_get, device_put, jacfwd, jacrev
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
 from jax.interpreters.ad import defjvp
@@ -235,6 +235,18 @@ class APITest(jtu.JaxTestCase):
     assert isinstance(y2[1][1], onp.ndarray)
     assert onp.all(y2[1][1] == 3 * x)
 
+  def test_jacobian(self):
+    R = onp.random.RandomState(0).randn
+    A = R(4, 3)
+    x = R(3)
+
+    f = lambda x: np.dot(A, x)
+    assert onp.allclose(jacfwd(f)(x), A)
+    assert onp.allclose(jacrev(f)(x), A)
+
+    f = lambda x: np.tanh(np.dot(A, x))
+    assert onp.allclose(jacfwd(f)(x), jacrev(f)(x))
+
 
 if __name__ == '__main__':
   config.config_with_absl()",No
tests/batching_test.py,tests/batching_test.py,89349e5e6df4a82eb07278102f5caa9511e73ade,e788539e0a642599352cccc488386b3c7be52e98,fix transpose issue in jacfwd and jacrev,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index d2076202e..4e9ca8daf 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -24,7 +24,7 @@ import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
 from jax import lax
-from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev
 from jax.api import vmap
 from jax.config import config
 from jax.core import unit
@@ -272,6 +272,16 @@ class BatchingTest(jtu.JaxTestCase):
                                     onp.moveaxis(z, 2, 0)], 2)
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
+  def testJacobianIssue54(self):
+    # test modeling the code in https://github.com/google/jax/issues/54
+
+    def func(xs):
+      return np.array([x for x in xs])
+
+    xs = np.ones((5, 1))
+    jacrev(func)(xs)  # don't crash
+    jacfwd(func)(xs)  # don't crash
+
 
 if __name__ == '__main__':
   config.config_with_absl()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index d2076202e..4e9ca8daf 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -24,7 +24,7 @@ import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
 from jax import lax
-from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev
 from jax.api import vmap
 from jax.config import config
 from jax.core import unit
@@ -272,6 +272,16 @@ class BatchingTest(jtu.JaxTestCase):
                                     onp.moveaxis(z, 2, 0)], 2)
     self.assertAllClose(ans, expected_ans, check_dtypes=False)
 
+  def testJacobianIssue54(self):
+    # test modeling the code in https://github.com/google/jax/issues/54
+
+    def func(xs):
+      return np.array([x for x in xs])
+
+    xs = np.ones((5, 1))
+    jacrev(func)(xs)  # don't crash
+    jacfwd(func)(xs)  # don't crash
+
 
 if __name__ == '__main__':
   config.config_with_absl()",No
setup.py,setup.py,42302fc8682078e6fcec7bc7ca9d9949efec30da,c52f8822faf9be9998231705aa2553a2d75a6a56,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 7d2dbf88e..541da4f1d 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.6',
+    version='0.1.7',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 7d2dbf88e..541da4f1d 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.6',
+    version='0.1.7',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,650db099ef0a6b875507a8a46240f05deda18a00,42302fc8682078e6fcec7bc7ca9d9949efec30da,tweak readme to make colab bullet fit on one line,"diff --git a/README.md b/README.md
index bd6b83608..bcde8b728 100644
--- a/README.md
+++ b/README.md
@@ -71,7 +71,7 @@ open](https://github.com/google/jax) by a growing number of
 
 ## Quickstart: Colab in the Cloud
 Jump right in using a notebook in your browser, connected to a Google Cloud GPU: 
-- [The basics: NumPy on accelerators, `grad` for automatic differentiation, `jit` for compilation and `vmap` for auto-vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
+- [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
 - [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
 
 ","diff --git a/README.md b/README.md
index bd6b83608..bcde8b728 100644
--- a/README.md
+++ b/README.md
@@ -71,7 +71,7 @@ open](https://github.com/google/jax) by a growing number of
 
 ## Quickstart: Colab in the Cloud
 Jump right in using a notebook in your browser, connected to a Google Cloud GPU: 
-- [The basics: NumPy on accelerators, `grad` for automatic differentiation, `jit` for compilation and `vmap` for auto-vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
+- [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
 - [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
 
 ",No
jax/numpy/fft.py,jax/numpy/fft.py,dad72f94da8f719b992b7e5aee3f798b0de04ef8,650db099ef0a6b875507a8a46240f05deda18a00,Add missing license header to Python files.,"diff --git a/jax/numpy/fft.py b/jax/numpy/fft.py
index 12ebcea98..7a3fc2056 100644
--- a/jax/numpy/fft.py
+++ b/jax/numpy/fft.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function","diff --git a/jax/numpy/fft.py b/jax/numpy/fft.py
index 12ebcea98..7a3fc2056 100644
--- a/jax/numpy/fft.py
+++ b/jax/numpy/fft.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function",No
jax/numpy/linalg.py,jax/numpy/linalg.py,dad72f94da8f719b992b7e5aee3f798b0de04ef8,650db099ef0a6b875507a8a46240f05deda18a00,Add missing license header to Python files.,"diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index b70ef8c01..f017b6d9a 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index b70ef8c01..f017b6d9a 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function",No
build/WORKSPACE,build/WORKSPACE,314056edb642315c402bae255a8cfe7a20c4b9b2,dad72f94da8f719b992b7e5aee3f798b0de04ef8,"Make Jax buildable with the latest Bazel

Without this change build script fails with the following message: 
```
Encountered error while reading extension file 'tensorflow/workspace.bzl': no such package '@org_tensorflow//tensorflow': The native http_archive rule is deprecated. load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") for a drop-in replacement.
```

This PR simply implements the suggestion in the error. I was able to successfully build after applying the change.","diff --git a/build/WORKSPACE b/build/WORKSPACE
index 0f5c808e2..6c52e88d6 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -1,3 +1,5 @@
+load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
+
 http_archive(
     name = ""io_bazel_rules_closure"",
     sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",","diff --git a/build/WORKSPACE b/build/WORKSPACE
index 0f5c808e2..6c52e88d6 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -1,3 +1,5 @@
+load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
+
 http_archive(
     name = ""io_bazel_rules_closure"",
     sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",",No
build/WORKSPACE,build/WORKSPACE,c1a42617a3a5039e95d81fddc3d895d4d96085e3,314056edb642315c402bae255a8cfe7a20c4b9b2,Update XLA to include Python linear algebra changes.,"diff --git a/build/WORKSPACE b/build/WORKSPACE
index 6c52e88d6..56cbad6fc 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""0fa744b8b806cd415e6a7747a0618306b816fa7329f45c94a865a2c763efdc47"",
-    strip_prefix = ""tensorflow-c07297759059a953351f1d5e531b6e6af878365c"",
+    sha256 = ""413b2a398c51d5764eb7fb7ce12ce9b095c42de4287ee26e24e13f8b68892f9d"",
+    strip_prefix = ""tensorflow-25337d2065bd3ef79b9018714c0cb5af46ca06dc"",
     urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/c07297759059a953351f1d5e531b6e6af878365c.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/25337d2065bd3ef79b9018714c0cb5af46ca06dc.tar.gz"",
     ],
 )
 ","diff --git a/build/WORKSPACE b/build/WORKSPACE
index 6c52e88d6..56cbad6fc 100644
--- a/build/WORKSPACE
+++ b/build/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""0fa744b8b806cd415e6a7747a0618306b816fa7329f45c94a865a2c763efdc47"",
-    strip_prefix = ""tensorflow-c07297759059a953351f1d5e531b6e6af878365c"",
+    sha256 = ""413b2a398c51d5764eb7fb7ce12ce9b095c42de4287ee26e24e13f8b68892f9d"",
+    strip_prefix = ""tensorflow-25337d2065bd3ef79b9018714c0cb5af46ca06dc"",
     urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/c07297759059a953351f1d5e531b6e6af878365c.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/25337d2065bd3ef79b9018714c0cb5af46ca06dc.tar.gz"",
     ],
 )
 ",No
WORKSPACE,WORKSPACE,11720fd0aa7aa95494cb78dac9eda83359807ad2,c1a42617a3a5039e95d81fddc3d895d4d96085e3,"Consolidate build/WORKSPACE and WORKSPACE. We don't need to have two WORKSPACEs in the same project; if we later choose to split C++ and Python parts of the project into separate repositories, we can split the WORKSPACE at that time.","diff --git a/WORKSPACE b/WORKSPACE
index e69de29bb..56cbad6fc 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -0,0 +1,38 @@
+load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""413b2a398c51d5764eb7fb7ce12ce9b095c42de4287ee26e24e13f8b68892f9d"",
+    strip_prefix = ""tensorflow-25337d2065bd3ef79b9018714c0cb5af46ca06dc"",
+    urls = [
+        ""https://github.com/tensorflow/tensorflow/archive/25337d2065bd3ef79b9018714c0cb5af46ca06dc.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/WORKSPACE b/WORKSPACE
index e69de29bb..56cbad6fc 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -0,0 +1,38 @@
+load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+# To update TensorFlow to a new revision,
+# a) update URL and strip_prefix to the new git commit hash
+# b) get the sha256 hash of the commit by running:
+#    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
+#    and update the sha256 with the result.
+http_archive(
+    name = ""org_tensorflow"",
+    sha256 = ""413b2a398c51d5764eb7fb7ce12ce9b095c42de4287ee26e24e13f8b68892f9d"",
+    strip_prefix = ""tensorflow-25337d2065bd3ef79b9018714c0cb5af46ca06dc"",
+    urls = [
+        ""https://github.com/tensorflow/tensorflow/archive/25337d2065bd3ef79b9018714c0cb5af46ca06dc.tar.gz"",
+    ],
+)
+
+# For development, one can use a local TF repository instead.
+# local_repository(
+#    name = ""org_tensorflow"",
+#    path = ""tensorflow"",
+# )
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
jax/api.py,jax/api.py,9b835a180f0bf72b368337b47e1ecd2f6796ac3f,7b781a5d0e06afcc56242a849ba595963c26ab5a,Added some docstrings to top-level transformations,"diff --git a/jax/api.py b/jax/api.py
index 8902c46fd..67ccd4339 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -12,6 +12,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+""""""
+User-facing transformations.
+
+These mostly wrap internal transformations, providing convenience flags to
+control behavior and handling Python containers (tuples/lists/dicts) of
+arguments and outputs.
+""""""
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
@@ -36,6 +44,21 @@ from .interpreters import batching
 map = safe_map
 
 def jit(fun, static_argnums=()):
+  """"""Sets up `fun` for just-in-time compilation with XLA.
+
+  Args:
+    fun: Function to be jitted. Should be a pure function, as side-effects may
+        only be executed once. Its positional arguments and return value should
+        be arrays, scalars, or standard Python containers (tuple/list/dict)
+        thereof. Keyword arguments and positional arguments specified by
+        `static_argnums` can be anything at all. These are treated as static
+        (see below).
+    static_argnums: A tuple of ints. Specifies which arguments to treat as
+        static (compile-time constant). Operations that only depend on static
+        arguments will be constant-folded. Calling the jitted function with
+        different values for these constants will trigger recompilation.
+   Returns: A wrapped version of `fun`, set up for just-in-time compilation.
+  """"""
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
@@ -49,6 +72,21 @@ def jit(fun, static_argnums=()):
   return f_jitted
 
 def grad(fun, argnums=0):
+  """"""Creates a function which evaluates the gradient of `fun`.
+
+  Args:
+    fun: Function to be differentiated. Its arguments at positions specified by
+        `argnums` should be arrays, scalars, or standard Python containers. It
+        should return a scalar (which includes arrays with shape `()` but not
+        arrays with shape `(1,)` etc.)
+    argnums: Integer or tuple of integers. Specifies which positional
+        argument(s) to differentiate with respect to.
+  Returns: A function with the same arguments as `fun`, that evaluates the
+      gradient of `fun`. If `argnums` is an integer then the gradient has the
+      same shape and type as the positional argument indicated by that integer.
+      If argnums is a tuple of integers, the gradient is a tuple of values with
+      the same shapes and types as the corresponding arguments.
+  """"""
   def grad_f(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     f_partial, dyn_args = argnums_partial(f, argnums, args)
@@ -61,6 +99,7 @@ def grad(fun, argnums=0):
 
 @curry
 def jacfwd(fun, x):
+  """"""Jacobian of `fun`, evaluated column-by-column using forward-mode AD""""""
   fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
@@ -69,6 +108,7 @@ def jacfwd(fun, x):
 
 @curry
 def jacrev(fun, x):
+  """"""Jacobian of `fun`, evaluated row-by-row using reverse-mode AD""""""
   fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
@@ -79,7 +119,27 @@ def hessian(fun):
   return jacfwd(jacrev(fun))
 
 def vmap(fun, in_axes=0, out_axes=0):
+  """"""Vectorizing map. Creates a function which maps `fun` over additional axes.
+
+  Args:
+    fun: Function to be mapped over additional axes.
+    in_axes, out_axes: Specifies which axes to map over. These may be integers,
+        None, or (possibly nested) tuples of integers or None.
+  Returns: Batched/vectorized version of `fun` with arguments that correspond to
+      those of `fun`, but with extra array axes at positions indicated by
+      `in_axes`, and a return value that corresponds to that of `fun`, but with
+      extra array axes at positions indicated by `out_axes`.
+
+  For example, we can implement a matrix-matrix product using a vector dot
+  product:
+
+    vv = lambda x, y: np.vdot(x, y)  #  ([a], [a]) -> []
+    mv = vmap(vv, (0, None), 0)      #  ([a,b], [b]) -> [a]
+    mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
+
+  (`[a,b]` indicates an array with shape (a,b))
 
+  """"""
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun)","diff --git a/jax/api.py b/jax/api.py
index 8902c46fd..67ccd4339 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -12,6 +12,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+""""""
+User-facing transformations.
+
+These mostly wrap internal transformations, providing convenience flags to
+control behavior and handling Python containers (tuples/lists/dicts) of
+arguments and outputs.
+""""""
+
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
@@ -36,6 +44,21 @@ from .interpreters import batching
 map = safe_map
 
 def jit(fun, static_argnums=()):
+  """"""Sets up `fun` for just-in-time compilation with XLA.
+
+  Args:
+    fun: Function to be jitted. Should be a pure function, as side-effects may
+        only be executed once. Its positional arguments and return value should
+        be arrays, scalars, or standard Python containers (tuple/list/dict)
+        thereof. Keyword arguments and positional arguments specified by
+        `static_argnums` can be anything at all. These are treated as static
+        (see below).
+    static_argnums: A tuple of ints. Specifies which arguments to treat as
+        static (compile-time constant). Operations that only depend on static
+        arguments will be constant-folded. Calling the jitted function with
+        different values for these constants will trigger recompilation.
+   Returns: A wrapped version of `fun`, set up for just-in-time compilation.
+  """"""
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
@@ -49,6 +72,21 @@ def jit(fun, static_argnums=()):
   return f_jitted
 
 def grad(fun, argnums=0):
+  """"""Creates a function which evaluates the gradient of `fun`.
+
+  Args:
+    fun: Function to be differentiated. Its arguments at positions specified by
+        `argnums` should be arrays, scalars, or standard Python containers. It
+        should return a scalar (which includes arrays with shape `()` but not
+        arrays with shape `(1,)` etc.)
+    argnums: Integer or tuple of integers. Specifies which positional
+        argument(s) to differentiate with respect to.
+  Returns: A function with the same arguments as `fun`, that evaluates the
+      gradient of `fun`. If `argnums` is an integer then the gradient has the
+      same shape and type as the positional argument indicated by that integer.
+      If argnums is a tuple of integers, the gradient is a tuple of values with
+      the same shapes and types as the corresponding arguments.
+  """"""
   def grad_f(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     f_partial, dyn_args = argnums_partial(f, argnums, args)
@@ -61,6 +99,7 @@ def grad(fun, argnums=0):
 
 @curry
 def jacfwd(fun, x):
+  """"""Jacobian of `fun`, evaluated column-by-column using forward-mode AD""""""
   fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
@@ -69,6 +108,7 @@ def jacfwd(fun, x):
 
 @curry
 def jacrev(fun, x):
+  """"""Jacobian of `fun`, evaluated row-by-row using reverse-mode AD""""""
   fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
@@ -79,7 +119,27 @@ def hessian(fun):
   return jacfwd(jacrev(fun))
 
 def vmap(fun, in_axes=0, out_axes=0):
+  """"""Vectorizing map. Creates a function which maps `fun` over additional axes.
 
+  Args:
+    fun: Function to be mapped over additional axes.
+    in_axes, out_axes: Specifies which axes to map over. These may be integers,
+        None, or (possibly nested) tuples of integers or None.
+  Returns: Batched/vectorized version of `fun` with arguments that correspond to
+      those of `fun`, but with extra array axes at positions indicated by
+      `in_axes`, and a return value that corresponds to that of `fun`, but with
+      extra array axes at positions indicated by `out_axes`.
+
+  For example, we can implement a matrix-matrix product using a vector dot
+  product:
+
+    vv = lambda x, y: np.vdot(x, y)  #  ([a], [a]) -> []
+    mv = vmap(vv, (0, None), 0)      #  ([a,b], [b]) -> [a]
+    mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
+
+  (`[a,b]` indicates an array with shape (a,b))
+
+  """"""
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun)",Yes
jax/scipy/stats/beta.py,jax/scipy/stats/beta.py,0e6a44f1d239903012a7b91437cecabb115e8d14,2d25a7430ad6d1b47badcaab492a4dd87a5d51d3,"Add tests, fix small bugs in the logpdf functions","diff --git a/jax/scipy/stats/beta.py b/jax/scipy/stats/beta.py
index 15adca343..6af0d5996 100644
--- a/jax/scipy/stats/beta.py
+++ b/jax/scipy/stats/beta.py
@@ -22,7 +22,7 @@ import scipy.stats as osp_stats
 from ... import lax
 from ...numpy.lax_numpy import (_promote_args_like, _constant_like, _wraps,
                                 where, inf, logical_or)
-from ..scipy.special import gammaln
+from ..special import gammaln
 
 
 @_wraps(osp_stats.beta.logpdf)
@@ -33,7 +33,7 @@ def logpdf(x, a, b, loc=0, scale=1):
   shape_term = lax.sub(gammaln(lax.add(a, b)), shape_term_tmp)
   y = lax.div(lax.sub(x, loc), scale)
   log_linear_term = lax.add(lax.mul(lax.sub(a, one), lax.log(y)),
-                            lax.mul(lax.sub(b, one), lax.log(lax.sub(1, y))))
+                            lax.mul(lax.sub(b, one), lax.log(lax.sub(one, y))))
   log_probs = lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
-  return where(logical_or(lax.ge(log_probs, lax.add(loc, scale)),
-                          lax.le(log_probs, lax.add(loc))), -inf, log_probs)
+  return where(logical_or(lax.ge(x, lax.add(loc, scale)),
+                          lax.le(x, loc)), -inf, log_probs)","diff --git a/jax/scipy/stats/beta.py b/jax/scipy/stats/beta.py
index 15adca343..6af0d5996 100644
--- a/jax/scipy/stats/beta.py
+++ b/jax/scipy/stats/beta.py
@@ -22,7 +22,7 @@ import scipy.stats as osp_stats
 from ... import lax
 from ...numpy.lax_numpy import (_promote_args_like, _constant_like, _wraps,
                                 where, inf, logical_or)
-from ..scipy.special import gammaln
+from ..special import gammaln
 
 
 @_wraps(osp_stats.beta.logpdf)
@@ -33,7 +33,7 @@ def logpdf(x, a, b, loc=0, scale=1):
   shape_term = lax.sub(gammaln(lax.add(a, b)), shape_term_tmp)
   y = lax.div(lax.sub(x, loc), scale)
   log_linear_term = lax.add(lax.mul(lax.sub(a, one), lax.log(y)),
-                            lax.mul(lax.sub(b, one), lax.log(lax.sub(1, y))))
+                            lax.mul(lax.sub(b, one), lax.log(lax.sub(one, y))))
   log_probs = lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
-  return where(logical_or(lax.ge(log_probs, lax.add(loc, scale)),
-                          lax.le(log_probs, lax.add(loc))), -inf, log_probs)
+  return where(logical_or(lax.ge(x, lax.add(loc, scale)),
+                          lax.le(x, loc)), -inf, log_probs)",No
jax/scipy/stats/gamma.py,jax/scipy/stats/gamma.py,0e6a44f1d239903012a7b91437cecabb115e8d14,2d25a7430ad6d1b47badcaab492a4dd87a5d51d3,"Add tests, fix small bugs in the logpdf functions","diff --git a/jax/scipy/stats/gamma.py b/jax/scipy/stats/gamma.py
index ea06e63b5..9607380f2 100644
--- a/jax/scipy/stats/gamma.py
+++ b/jax/scipy/stats/gamma.py
@@ -22,7 +22,7 @@ import scipy.stats as osp_stats
 from ... import lax
 from ...numpy.lax_numpy import (_promote_args_like, _constant_like, _wraps,
                                 where, inf)
-from ..scipy.special import gammaln
+from ..special import gammaln
 
 
 @_wraps(osp_stats.gamma.logpdf)
@@ -33,4 +33,4 @@ def logpdf(x, a, loc=0, scale=1):
   log_linear_term = lax.sub(lax.mul(lax.sub(a, one), lax.log(y)), y)
   shape_terms = lax.add(gammaln(a), lax.log(scale))
   log_probs = lax.sub(log_linear_term, shape_terms)
-  return where(lax.le(log_probs), -inf, log_probs)
+  return where(lax.le(x, loc), -inf, log_probs)","diff --git a/jax/scipy/stats/gamma.py b/jax/scipy/stats/gamma.py
index ea06e63b5..9607380f2 100644
--- a/jax/scipy/stats/gamma.py
+++ b/jax/scipy/stats/gamma.py
@@ -22,7 +22,7 @@ import scipy.stats as osp_stats
 from ... import lax
 from ...numpy.lax_numpy import (_promote_args_like, _constant_like, _wraps,
                                 where, inf)
-from ..scipy.special import gammaln
+from ..special import gammaln
 
 
 @_wraps(osp_stats.gamma.logpdf)
@@ -33,4 +33,4 @@ def logpdf(x, a, loc=0, scale=1):
   log_linear_term = lax.sub(lax.mul(lax.sub(a, one), lax.log(y)), y)
   shape_terms = lax.add(gammaln(a), lax.log(scale))
   log_probs = lax.sub(log_linear_term, shape_terms)
-  return where(lax.le(log_probs), -inf, log_probs)
+  return where(lax.le(x, loc), -inf, log_probs)",No
jax/scipy/stats/laplace.py,jax/scipy/stats/laplace.py,0e6a44f1d239903012a7b91437cecabb115e8d14,2d25a7430ad6d1b47badcaab492a4dd87a5d51d3,"Add tests, fix small bugs in the logpdf functions","diff --git a/jax/scipy/stats/laplace.py b/jax/scipy/stats/laplace.py
index 0e2659ff6..5dcdd512c 100644
--- a/jax/scipy/stats/laplace.py
+++ b/jax/scipy/stats/laplace.py
@@ -23,7 +23,7 @@ from ... import lax
 from ...numpy.lax_numpy import _promote_args_like, _constant_like, _wraps
 
 
-@_wraps(osp_stats.laplace.logpdf):
+@_wraps(osp_stats.laplace.logpdf)
 def logpdf(x, loc=0, scale=1):
   x, loc, scale = _promote_args_like(osp_stats.laplace.logpdf, x, loc, scale)
   two = _constant_like(x, 2)","diff --git a/jax/scipy/stats/laplace.py b/jax/scipy/stats/laplace.py
index 0e2659ff6..5dcdd512c 100644
--- a/jax/scipy/stats/laplace.py
+++ b/jax/scipy/stats/laplace.py
@@ -23,7 +23,7 @@ from ... import lax
 from ...numpy.lax_numpy import _promote_args_like, _constant_like, _wraps
 
 
-@_wraps(osp_stats.laplace.logpdf):
+@_wraps(osp_stats.laplace.logpdf)
 def logpdf(x, loc=0, scale=1):
   x, loc, scale = _promote_args_like(osp_stats.laplace.logpdf, x, loc, scale)
   two = _constant_like(x, 2)",No
jax/scipy/stats/uniform.py,jax/scipy/stats/uniform.py,0e6a44f1d239903012a7b91437cecabb115e8d14,2d25a7430ad6d1b47badcaab492a4dd87a5d51d3,"Add tests, fix small bugs in the logpdf functions","diff --git a/jax/scipy/stats/uniform.py b/jax/scipy/stats/uniform.py
index 276c41d33..3ac7bc86b 100644
--- a/jax/scipy/stats/uniform.py
+++ b/jax/scipy/stats/uniform.py
@@ -20,12 +20,13 @@ import numpy as onp
 import scipy.stats as osp_stats
 
 from ... import lax
-from ...numpy.lax_numpy import _promote_args_like, _wraps, where, inf
+from ...numpy.lax_numpy import _promote_args_like, _wraps, where, inf, logical_or
 
 
 @_wraps(osp_stats.uniform.logpdf)
 def logpdf(x, loc=0, scale=1):
   x, loc, scale = _promote_args_like(osp_stats.uniform.logpdf, x, loc, scale)
-  log_probs = lax.full_like(x, lax.log(scale))
-  return where(logical_or(lax.ge(log_probs, lax.add(loc, scale)),
-                          lax.le(log_probs, lax.add(loc))), -inf, log_probs)
+  fill_value = lax.neg(lax.log(scale))
+  log_probs = lax.broadcast(fill_value, onp.shape(x))
+  return where(logical_or(lax.ge(x, lax.add(loc, scale)),
+                          lax.le(x, loc)), -inf, log_probs)","diff --git a/jax/scipy/stats/uniform.py b/jax/scipy/stats/uniform.py
index 276c41d33..3ac7bc86b 100644
--- a/jax/scipy/stats/uniform.py
+++ b/jax/scipy/stats/uniform.py
@@ -20,12 +20,13 @@ import numpy as onp
 import scipy.stats as osp_stats
 
 from ... import lax
-from ...numpy.lax_numpy import _promote_args_like, _wraps, where, inf
+from ...numpy.lax_numpy import _promote_args_like, _wraps, where, inf, logical_or
 
 
 @_wraps(osp_stats.uniform.logpdf)
 def logpdf(x, loc=0, scale=1):
   x, loc, scale = _promote_args_like(osp_stats.uniform.logpdf, x, loc, scale)
-  log_probs = lax.full_like(x, lax.log(scale))
-  return where(logical_or(lax.ge(log_probs, lax.add(loc, scale)),
-                          lax.le(log_probs, lax.add(loc))), -inf, log_probs)
+  fill_value = lax.neg(lax.log(scale))
+  log_probs = lax.broadcast(fill_value, onp.shape(x))
+  return where(logical_or(lax.ge(x, lax.add(loc, scale)),
+                          lax.le(x, loc)), -inf, log_probs)",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,0e6a44f1d239903012a7b91437cecabb115e8d14,2d25a7430ad6d1b47badcaab492a4dd87a5d51d3,"Add tests, fix small bugs in the logpdf functions","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index dd658799e..1ec17186a 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -117,42 +117,5 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          """", shapes, dtypes),
-       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
-      for shapes in CombosWithReplacement(all_shapes, 3)
-      for dtypes in CombosWithReplacement(default_dtypes, 3)
-      for rng in [jtu.rand_default()]))
-  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
-  def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
-    # TODO(mattjj): test autodiff
-    scipy_fun = osp_stats.norm.logpdf
-    lax_fun = lsp_stats.norm.logpdf
-    def args_maker():
-      x, loc, scale = map(rng, shapes, dtypes)
-      scale = 0.5 + onp.abs(scale)
-      return [x, loc, scale]
-    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
-    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
-
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          """", shapes, dtypes),
-       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
-      for shapes in CombosWithReplacement(all_shapes, 2)
-      for dtypes in CombosWithReplacement(default_dtypes, 2)
-      for rng in [jtu.rand_default()]))
-  def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
-    # TODO(mattjj): test autodiff
-    scale = 0.5
-    scipy_fun = functools.partial(osp_stats.norm.logpdf, scale=scale)
-    lax_fun = functools.partial(lsp_stats.norm.logpdf, scale=scale)
-    def args_maker():
-      return list(map(rng, shapes, dtypes))
-    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
-    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
-
-
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index dd658799e..1ec17186a 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -117,42 +117,5 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          """", shapes, dtypes),
-       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
-      for shapes in CombosWithReplacement(all_shapes, 3)
-      for dtypes in CombosWithReplacement(default_dtypes, 3)
-      for rng in [jtu.rand_default()]))
-  @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
-  def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
-    # TODO(mattjj): test autodiff
-    scipy_fun = osp_stats.norm.logpdf
-    lax_fun = lsp_stats.norm.logpdf
-    def args_maker():
-      x, loc, scale = map(rng, shapes, dtypes)
-      scale = 0.5 + onp.abs(scale)
-      return [x, loc, scale]
-    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
-    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
-
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": jtu.format_test_name_suffix(
-          """", shapes, dtypes),
-       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
-      for shapes in CombosWithReplacement(all_shapes, 2)
-      for dtypes in CombosWithReplacement(default_dtypes, 2)
-      for rng in [jtu.rand_default()]))
-  def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
-    # TODO(mattjj): test autodiff
-    scale = 0.5
-    scipy_fun = functools.partial(osp_stats.norm.logpdf, scale=scale)
-    lax_fun = functools.partial(lsp_stats.norm.logpdf, scale=scale)
-    def args_maker():
-      return list(map(rng, shapes, dtypes))
-    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
-    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
-
-
 if __name__ == ""__main__"":
   absltest.main()",No
.gitignore,.gitignore,46f7c6a8a3666baad635491e510260610cf6b781,9b835a180f0bf72b368337b47e1ecd2f6796ac3f,"Add build/ and dist/ to .gitignore

An alternative to installing `jax` using `pip install -e .` is to use `python setup.py`. This creates build/ and dist/ directories that should not be included in diffs since they are meant to be local.

This PR adds those entries to the .gitignore file","diff --git a/.gitignore b/.gitignore
index 7a4f3c259..db88dce20 100644
--- a/.gitignore
+++ b/.gitignore
@@ -8,4 +8,6 @@
 /bazel-*
 .bazelrc
 /tensorflow
-.DS_Store
\ No newline at end of file
+.DS_Store
+build/
+dist/","diff --git a/.gitignore b/.gitignore
index 7a4f3c259..db88dce20 100644
--- a/.gitignore
+++ b/.gitignore
@@ -8,4 +8,6 @@
 /bazel-*
 .bazelrc
 /tensorflow
-.DS_Store
\ No newline at end of file
+.DS_Store
+build/
+dist/",No
jax/interpreters/ad.py,jax/interpreters/ad.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index e7a97b6d8..92777b885 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -237,6 +237,8 @@ class JVPTracer(Tracer):
     self.trace = trace
     self.primal = primal
     self.tangent = tangent
+    # TODO(mattjj,dougalm): behind skip_checks, check primal/tangent shapes and
+    # dtypes agree (up to jax_enable_x64 flag)
 
   @property
   def aval(self):","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index e7a97b6d8..92777b885 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -237,6 +237,8 @@ class JVPTracer(Tracer):
     self.trace = trace
     self.primal = primal
     self.tangent = tangent
+    # TODO(mattjj,dougalm): behind skip_checks, check primal/tangent shapes and
+    # dtypes agree (up to jax_enable_x64 flag)
 
   @property
   def aval(self):",No
jax/lax.py,jax/lax.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/jax/lax.py b/jax/lax.py
index 233814324..a9243ff97 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -72,7 +72,7 @@ def erf_inv(x): return erf_inv_p.bind(x)
 def real(x): return real_p.bind(x)
 def imag(x): return imag_p.bind(x)
 def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
-def conj(x): return conj_p.bind(x)
+def conj(x): return conj_p.bind(x, input_dtype=_dtype(x))
 def abs(x): return abs_p.bind(x)
 def pow(x, y): return pow_p.bind(x, y)
 
@@ -652,7 +652,7 @@ def standard_translate(name, c, *args, **kwargs):
   return getattr(c, xla_opname)(*args, **kwargs)
 
 
-def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
+def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):
   if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
     msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
     typename = str(onp.dtype(aval.dtype).name)
@@ -663,10 +663,11 @@ def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
 
 def unop(result_dtype, accepted_dtypes, name):
   dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
-  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
+  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)
   batching.defvectorized(prim)
   return prim
 standard_unop = partial(unop, identity)
+_attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)
 
 
 def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
@@ -810,17 +811,25 @@ ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])
 imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
 ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), t)])
 
-complex_p = standard_binop([_f32, _f32], 'complex')
+complex_p = binop(_fixed_dtype(onp.complex64), [_f32, _f32], 'complex')
 ad.deflinear(complex_p, lambda t: [real(t), imag(t)])
 
-# TODO promotes dtypes, need to remember whether we came from float or not
 conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
-ad.deflinear(conj_p, lambda t: [conj(t)])
+
+def conj_transpose_rule(t, x, input_dtype):
+  assert x is None
+  if onp.issubdtype(input_dtype, onp.complexfloating):
+    return [conj(t)]
+  else:
+    return [real(t)]
+
+ad.primitive_jvps[conj_p] = partial(ad.linear_jvp, conj_p)
+ad.primitive_transposes[conj_p] = conj_transpose_rule
 
 abs_p = unop(_complex_basetype, _num, 'abs')
 ad.defjvp2(abs_p,
-           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
-                                 _replace_zero(ans)))
+           lambda g, ans, x:
+           div(_maybe_real(mul(g, _maybe_conj(x))), _replace_zero(ans)))
 _maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
 _maybe_real = lambda x: real(x) if _iscomplex(x) else x
 ","diff --git a/jax/lax.py b/jax/lax.py
index 233814324..a9243ff97 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -72,7 +72,7 @@ def erf_inv(x): return erf_inv_p.bind(x)
 def real(x): return real_p.bind(x)
 def imag(x): return imag_p.bind(x)
 def complex(x, y): return complex_p.bind(_brcast(x, y), _brcast(y, x))
-def conj(x): return conj_p.bind(x)
+def conj(x): return conj_p.bind(x, input_dtype=_dtype(x))
 def abs(x): return abs_p.bind(x)
 def pow(x, y): return pow_p.bind(x, y)
 
@@ -652,7 +652,7 @@ def standard_translate(name, c, *args, **kwargs):
   return getattr(c, xla_opname)(*args, **kwargs)
 
 
-def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
+def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval, **kwargs):
   if not any(onp.issubdtype(aval.dtype, t) for t in accepted_dtypes):
     msg = '{} does not accept dtype {}. Accepted dtypes are subtypes of {}.'
     typename = str(onp.dtype(aval.dtype).name)
@@ -663,10 +663,11 @@ def unop_dtype_rule(result_dtype, accepted_dtypes, name, aval):
 
 def unop(result_dtype, accepted_dtypes, name):
   dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
-  prim = standard_primitive(operator.attrgetter('shape'), dtype_rule, name)
+  prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)
   batching.defvectorized(prim)
   return prim
 standard_unop = partial(unop, identity)
+_attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)
 
 
 def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
@@ -810,17 +811,25 @@ ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])
 imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
 ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), t)])
 
-complex_p = standard_binop([_f32, _f32], 'complex')
+complex_p = binop(_fixed_dtype(onp.complex64), [_f32, _f32], 'complex')
 ad.deflinear(complex_p, lambda t: [real(t), imag(t)])
 
-# TODO promotes dtypes, need to remember whether we came from float or not
 conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
-ad.deflinear(conj_p, lambda t: [conj(t)])
+
+def conj_transpose_rule(t, x, input_dtype):
+  assert x is None
+  if onp.issubdtype(input_dtype, onp.complexfloating):
+    return [conj(t)]
+  else:
+    return [real(t)]
+
+ad.primitive_jvps[conj_p] = partial(ad.linear_jvp, conj_p)
+ad.primitive_transposes[conj_p] = conj_transpose_rule
 
 abs_p = unop(_complex_basetype, _num, 'abs')
 ad.defjvp2(abs_p,
-           lambda g, ans, x: div(_maybe_real(mul(g, _maybe_conj(x))),
-                                 _replace_zero(ans)))
+           lambda g, ans, x:
+           div(_maybe_real(mul(g, _maybe_conj(x))), _replace_zero(ans)))
 _maybe_conj = lambda x: conj(x) if _iscomplex(x) else x
 _maybe_real = lambda x: real(x) if _iscomplex(x) else x
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index fbd673d03..d348131d9 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -479,7 +479,8 @@ def where(condition, x=None, y=None):
     condition = lax.ne(condition, zeros_like(condition))
   condition, x, y = broadcast_arrays(condition, x, y)
   if not x.size:
-    return x
+    empty, _ = _promote_dtypes(x, y)
+    return empty
   else:
     return lax.select(condition, *_promote_dtypes(x, y))
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index fbd673d03..d348131d9 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -479,7 +479,8 @@ def where(condition, x=None, y=None):
     condition = lax.ne(condition, zeros_like(condition))
   condition, x, y = broadcast_arrays(condition, x, y)
   if not x.size:
-    return x
+    empty, _ = _promote_dtypes(x, y)
+    return empty
   else:
     return lax.select(condition, *_promote_dtypes(x, y))
 ",No
tests/api_test.py,tests/api_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/api_test.py b/tests/api_test.py
index 8a7982646..17c5e9a04 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,7 +23,6 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
-from jax.config import config
 from jax import jit, grad, device_get, device_put, jacfwd, jacrev
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
@@ -31,6 +30,9 @@ from jax.interpreters.ad import defjvp
 from jax.interpreters.xla import DeviceArray
 from jax.abstract_arrays import concretization_err_msg
 
+from jax.config import config
+config.parse_flags_with_absl()
+
 class APITest(jtu.JaxTestCase):
 
   def test_grad_argnums(self):
@@ -235,6 +237,7 @@ class APITest(jtu.JaxTestCase):
     assert isinstance(y2[1][1], onp.ndarray)
     assert onp.all(y2[1][1] == 3 * x)
 
+  @jtu.skip_on_devices(""tpu"")
   def test_jacobian(self):
     R = onp.random.RandomState(0).randn
     A = R(4, 3)
@@ -249,5 +252,4 @@ class APITest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
index 8a7982646..17c5e9a04 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,7 +23,6 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
-from jax.config import config
 from jax import jit, grad, device_get, device_put, jacfwd, jacrev
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
@@ -31,6 +30,9 @@ from jax.interpreters.ad import defjvp
 from jax.interpreters.xla import DeviceArray
 from jax.abstract_arrays import concretization_err_msg
 
+from jax.config import config
+config.parse_flags_with_absl()
+
 class APITest(jtu.JaxTestCase):
 
   def test_grad_argnums(self):
@@ -235,6 +237,7 @@ class APITest(jtu.JaxTestCase):
     assert isinstance(y2[1][1], onp.ndarray)
     assert onp.all(y2[1][1] == 3 * x)
 
+  @jtu.skip_on_devices(""tpu"")
   def test_jacobian(self):
     R = onp.random.RandomState(0).randn
     A = R(4, 3)
@@ -249,5 +252,4 @@ class APITest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()",No
tests/batching_test.py,tests/batching_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 4e9ca8daf..31f8a6cdb 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -26,11 +26,13 @@ from jax.abstract_arrays import ShapedArray
 from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev
 from jax.api import vmap
-from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial, curry
 
+from jax.config import config
+config.parse_flags_with_absl()
+
 class BatchingTest(jtu.JaxTestCase):
 
   def testConstantFunction(self):
@@ -284,5 +286,4 @@ class BatchingTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 4e9ca8daf..31f8a6cdb 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -26,11 +26,13 @@ from jax.abstract_arrays import ShapedArray
 from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev
 from jax.api import vmap
-from jax.config import config
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
 from jax.util import partial, curry
 
+from jax.config import config
+config.parse_flags_with_absl()
+
 class BatchingTest(jtu.JaxTestCase):
 
   def testConstantFunction(self):
@@ -284,5 +286,4 @@ class BatchingTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()",No
tests/core_test.py,tests/core_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/core_test.py b/tests/core_test.py
index fce93167e..8de1e880a 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -28,13 +28,15 @@ from jax import core
 from jax import numpy as np
 from jax import test_util as jtu
 from jax.api import jvp, linearize, vjp, jit
-from jax.config import config
 from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
 from jax.tree_util import tree_flatten, tree_unflatten, tree_multimap, tree_reduce
 from jax.util import partial
 from jax.interpreters import partial_eval as pe
 from jax.interpreters import xla
 
+from jax.config import config
+config.parse_flags_with_absl()
+
 _ = pe.PartialVal((UnshapedArray(onp.float32), core.unit))
 __ = pe.PartialVal((ShapedArray((), onp.float32), core.unit))
 
@@ -329,5 +331,4 @@ class CoreTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/core_test.py b/tests/core_test.py
index fce93167e..8de1e880a 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -28,13 +28,15 @@ from jax import core
 from jax import numpy as np
 from jax import test_util as jtu
 from jax.api import jvp, linearize, vjp, jit
-from jax.config import config
 from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
 from jax.tree_util import tree_flatten, tree_unflatten, tree_multimap, tree_reduce
 from jax.util import partial
 from jax.interpreters import partial_eval as pe
 from jax.interpreters import xla
 
+from jax.config import config
+config.parse_flags_with_absl()
+
 _ = pe.PartialVal((UnshapedArray(onp.float32), core.unit))
 __ = pe.PartialVal((ShapedArray((), onp.float32), core.unit))
 
@@ -329,5 +331,4 @@ class CoreTest(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()",No
tests/generated_fun_test.py,tests/generated_fun_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index dec320953..470078db7 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -27,8 +27,8 @@ import itertools as it
 import jax.numpy as np
 from jax import jit, jvp, vjp
 import jax.test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 
 npr.seed(0)","diff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py
index dec320953..470078db7 100644
--- a/tests/generated_fun_test.py
+++ b/tests/generated_fun_test.py
@@ -27,8 +27,8 @@ import itertools as it
 import jax.numpy as np
 from jax import jit, jvp, vjp
 import jax.test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 
 npr.seed(0)",No
tests/lapax_test.py,tests/lapax_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
index e63fb24b3..12a8ddc85 100644
--- a/tests/lapax_test.py
+++ b/tests/lapax_test.py
@@ -25,9 +25,9 @@ from absl.testing import parameterized
 
 from jax import jit
 from jax import test_util as jtu
-from jax.config import config
 from jax.experimental import lapax
 
+from jax.config import config
 config.parse_flags_with_absl()
 
 class LapaxTest(jtu.JaxTestCase):","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
index e63fb24b3..12a8ddc85 100644
--- a/tests/lapax_test.py
+++ b/tests/lapax_test.py
@@ -25,9 +25,9 @@ from absl.testing import parameterized
 
 from jax import jit
 from jax import test_util as jtu
-from jax.config import config
 from jax.experimental import lapax
 
+from jax.config import config
 config.parse_flags_with_absl()
 
 class LapaxTest(jtu.JaxTestCase):",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 92fa80a4d..8944d5afe 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -29,8 +29,8 @@ from jax import api
 from jax import lax
 from jax import numpy as lnp
 from jax import test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 92fa80a4d..8944d5afe 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -29,8 +29,8 @@ from jax import api
 from jax import lax
 from jax import numpy as lnp
 from jax import test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 6a73548a6..0f8cec036 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -28,8 +28,8 @@ import numpy as onp
 from jax import api
 from jax import numpy as lnp
 from jax import test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
@@ -189,11 +189,10 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   @parameterized.named_parameters(itertools.chain.from_iterable(
       jtu.cases_from_list(
-        {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
-                                                      dtypes),
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.test_name, shapes, dtypes),
          ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
          ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
-        for rec in JAX_BITWISE_OP_RECORDS
         for shapes in filter(
           _shapes_are_broadcast_compatible,
           CombosWithReplacement(rec.shapes, rec.nargs))","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 6a73548a6..0f8cec036 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -28,8 +28,8 @@ import numpy as onp
 from jax import api
 from jax import numpy as lnp
 from jax import test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
@@ -189,11 +189,10 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   @parameterized.named_parameters(itertools.chain.from_iterable(
       jtu.cases_from_list(
-        {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
-                                                      dtypes),
+        {""testcase_name"": jtu.format_test_name_suffix(
+            rec.test_name, shapes, dtypes),
          ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
          ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
-        for rec in JAX_BITWISE_OP_RECORDS
         for shapes in filter(
           _shapes_are_broadcast_compatible,
           CombosWithReplacement(rec.shapes, rec.nargs))",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index dd658799e..107d2b872 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -30,11 +30,11 @@ import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
-from jax.config import config
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
 from jax.scipy import stats as lsp_stats
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index dd658799e..107d2b872 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -30,11 +30,11 @@ import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
-from jax.config import config
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
 from jax.scipy import stats as lsp_stats
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ",No
tests/lax_test.py,tests/lax_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 09c5d9c57..06346cfcb 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -32,10 +32,10 @@ from jax import core
 from jax import lax
 from jax import test_util as jtu
 from jax import lax_reference
-from jax.config import config
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 09c5d9c57..06346cfcb 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -32,10 +32,10 @@ from jax import core
 from jax import lax
 from jax import test_util as jtu
 from jax import lax_reference
-from jax.config import config
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ",No
tests/minmax_test.py,tests/minmax_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index a548d1362..f15c4091f 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -20,13 +20,14 @@ from __future__ import print_function
 import functools
 
 from absl.testing import absltest
-from jax.config import config
 import jax.numpy as np
 import jax.test_util as jtu
 from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
+from jax.config import config
+config.parse_flags_with_absl()
 
 class OptimizerTests(jtu.JaxTestCase):
 
@@ -170,5 +171,4 @@ class OptimizerTests(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index a548d1362..f15c4091f 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -20,13 +20,14 @@ from __future__ import print_function
 import functools
 
 from absl.testing import absltest
-from jax.config import config
 import jax.numpy as np
 import jax.test_util as jtu
 from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
+from jax.config import config
+config.parse_flags_with_absl()
 
 class OptimizerTests(jtu.JaxTestCase):
 
@@ -170,5 +171,4 @@ class OptimizerTests(jtu.JaxTestCase):
 
 
 if __name__ == '__main__':
-  config.config_with_absl()
   absltest.main()",No
tests/random_test.py,tests/random_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/random_test.py b/tests/random_test.py
index 39a6046b0..46963f076 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -27,8 +27,8 @@ from jax import api
 from jax import lax
 from jax import random
 from jax import test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ","diff --git a/tests/random_test.py b/tests/random_test.py
index 39a6046b0..46963f076 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -27,8 +27,8 @@ from jax import api
 from jax import lax
 from jax import random
 from jax import test_util as jtu
-from jax.config import config
 
+from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 ",No
tests/stax_test.py,tests/stax_test.py,d3ec0b23c50e383ba2559a6b18a220a64725fc0d,13e4233e143f75c3166ba0a049270efef26ad3cd,"fix miscellaneous bugs, incl. complex abs grad","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 5d5a38fb1..3be51aa77 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -24,9 +24,9 @@ import numpy as onp
 
 from jax import test_util as jtu
 from jax import random
-from jax.config import config
 from jax.experimental import stax
 
+from jax.config import config
 config.parse_flags_with_absl()
 
 ","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 5d5a38fb1..3be51aa77 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -24,9 +24,9 @@ import numpy as onp
 
 from jax import test_util as jtu
 from jax import random
-from jax.config import config
 from jax.experimental import stax
 
+from jax.config import config
 config.parse_flags_with_absl()
 
 ",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,3c11d4f78021d0fb3c2a418b932a638cdebe2829,13e4233e143f75c3166ba0a049270efef26ad3cd,"Add implementations of np.{tri,tril,triu}.","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 96db8a1b2..7f13190ec 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -349,7 +349,7 @@ def _ndarray_constant_handler(c, val):
     An XLA ComputationDataHandle / XlaOp representing the constant ndarray
     staged into the XLA Computation.
   """"""
-  if onp.any(onp.equal(0, val.strides)):
+  if onp.any(onp.equal(0, val.strides)) and val.size > 0:
     zero_stride_axes, = onp.where(onp.equal(0, val.strides))
     other_axes, = onp.where(onp.not_equal(0, val.strides))
     collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None)","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 96db8a1b2..7f13190ec 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -349,7 +349,7 @@ def _ndarray_constant_handler(c, val):
     An XLA ComputationDataHandle / XlaOp representing the constant ndarray
     staged into the XLA Computation.
   """"""
-  if onp.any(onp.equal(0, val.strides)):
+  if onp.any(onp.equal(0, val.strides)) and val.size > 0:
     zero_stride_axes, = onp.where(onp.equal(0, val.strides))
     other_axes, = onp.where(onp.not_equal(0, val.strides))
     collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None)",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,3c11d4f78021d0fb3c2a418b932a638cdebe2829,13e4233e143f75c3166ba0a049270efef26ad3cd,"Add implementations of np.{tri,tril,triu}.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index fbd673d03..f92a5b7de 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -803,6 +803,31 @@ def repeat(a, repeats, axis=None):
       a_shape)
 
 
+@_wraps(onp.tri)
+def tri(N, M=None, k=0, dtype=None):
+  M = M if M is not None else N
+  dtype = dtype or float32
+  x = arange(N, dtype=int32)
+  y = arange(M, dtype=int32)
+  mask = lax.ge(
+      (lax.broadcast_in_dim(x, shape=(N, M), broadcast_dimensions=(0,)) +
+       int32(k)),
+      lax.broadcast(y, [N]))
+  return lax.convert_element_type(mask, dtype)
+
+
+@_wraps(onp.tril)
+def tril(m, k=0):
+  mask = tri(*shape(m)[-2:], k=k, dtype=bool)
+  return where(mask, m, zeros_like(m))
+
+
+@_wraps(onp.triu)
+def triu(m, k=0):
+  mask = tri(*shape(m)[-2:], k=k - 1, dtype=bool)
+  return where(mask, zeros_like(m), m)
+
+
 ### Tensor contraction operations
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index fbd673d03..f92a5b7de 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -803,6 +803,31 @@ def repeat(a, repeats, axis=None):
       a_shape)
 
 
+@_wraps(onp.tri)
+def tri(N, M=None, k=0, dtype=None):
+  M = M if M is not None else N
+  dtype = dtype or float32
+  x = arange(N, dtype=int32)
+  y = arange(M, dtype=int32)
+  mask = lax.ge(
+      (lax.broadcast_in_dim(x, shape=(N, M), broadcast_dimensions=(0,)) +
+       int32(k)),
+      lax.broadcast(y, [N]))
+  return lax.convert_element_type(mask, dtype)
+
+
+@_wraps(onp.tril)
+def tril(m, k=0):
+  mask = tri(*shape(m)[-2:], k=k, dtype=bool)
+  return where(mask, m, zeros_like(m))
+
+
+@_wraps(onp.triu)
+def triu(m, k=0):
+  mask = tri(*shape(m)[-2:], k=k - 1, dtype=bool)
+  return where(mask, zeros_like(m), m)
+
+
 ### Tensor contraction operations
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,3c11d4f78021d0fb3c2a418b932a638cdebe2829,13e4233e143f75c3166ba0a049270efef26ad3cd,"Add implementations of np.{tri,tril,triu}.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 6a73548a6..6e86dda32 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -371,6 +371,37 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_dtype={}_m={}_n={}_k={}"".format(
+          onp.dtype(dtype).name, m, n, k),
+       ""m"": m, ""n"": n, ""k"": k, ""dtype"": dtype, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for n in [0, 4]
+      for m in [None, 0, 1, 3, 4]
+      for k in list(range(-4, 4))))
+  def testTri(self, m, n, k, dtype, rng):
+    onp_fun = lambda: onp.tri(n, M=m, k=k, dtype=dtype)
+    lnp_fun = lambda: lnp.tri(n, M=m, k=k, dtype=dtype)
+    args_maker = lambda: []
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_op={}_shape={}_k={}"".format(
+          op, jtu.format_shape_dtype_string(shape, dtype), k),
+       ""dtype"": dtype, ""shape"": shape, ""op"": op, ""k"": k,
+       ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) >= 1]
+      for op in [""tril"", ""triu""]
+      for k in list(range(-3, 3))))
+  def testTriLU(self, dtype, shape, op, k, rng):
+    onp_fun = lambda arg: getattr(onp, op)(arg, k=k)
+    lnp_fun = lambda arg: getattr(lnp, op)(arg, k=k)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 6a73548a6..6e86dda32 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -371,6 +371,37 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_dtype={}_m={}_n={}_k={}"".format(
+          onp.dtype(dtype).name, m, n, k),
+       ""m"": m, ""n"": n, ""k"": k, ""dtype"": dtype, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for n in [0, 4]
+      for m in [None, 0, 1, 3, 4]
+      for k in list(range(-4, 4))))
+  def testTri(self, m, n, k, dtype, rng):
+    onp_fun = lambda: onp.tri(n, M=m, k=k, dtype=dtype)
+    lnp_fun = lambda: lnp.tri(n, M=m, k=k, dtype=dtype)
+    args_maker = lambda: []
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_op={}_shape={}_k={}"".format(
+          op, jtu.format_shape_dtype_string(shape, dtype), k),
+       ""dtype"": dtype, ""shape"": shape, ""op"": op, ""k"": k,
+       ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) >= 1]
+      for op in [""tril"", ""triu""]
+      for k in list(range(-3, 3))))
+  def testTriLU(self, dtype, shape, op, k, rng):
+    onp_fun = lambda arg: getattr(onp, op)(arg, k=k)
+    lnp_fun = lambda arg: getattr(lnp, op)(arg, k=k)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),",No
setup.py,setup.py,a7b339295ea75d3a63cd171cbb6877064bf40d34,af8223a2ad57e25f00b6aa4adf27fe340e2b57fa,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 541da4f1d..6b2a5427e 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.7',
+    version='0.1.8',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 541da4f1d..6b2a5427e 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.7',
+    version='0.1.8',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
tests/test_scipy_stats.py,tests/scipy_stats_test.py,9cfc6499c5a939dbce145f4cafeab4229025c9c5,0e6a44f1d239903012a7b91437cecabb115e8d14,Rename test_scipy_stats to scipy_stats_test,"diff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py
new file mode 100644
index 000000000..af393be71
--- /dev/null
+++ b/tests/scipy_stats_test.py
@@ -0,0 +1,117 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl.testing import absltest, parameterized
+
+import numpy as onp
+import scipy.stats as osp_stats
+
+from jax import test_util as jtu
+from jax.scipy import stats as lsp_stats
+from lax_scipy_test import CombosWithReplacement, float_dtypes
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
+
+def genNamedParametersNArgs(n):
+    return parameterized.named_parameters(
+            jtu.cases_from_list(
+              {""testcase_name"": jtu.format_test_name_suffix("""", shapes, dtypes),
+               ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+              for shapes in CombosWithReplacement(all_shapes, n)
+              for dtypes in CombosWithReplacement(float_dtypes, n)
+              for rng in [jtu.rand_default()]
+            ))
+
+class LaxBackedScipyStatsTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed scipy.stats implementations""""""
+
+  beta_decorator = genNamedParametersNArgs(5)
+  expon_decorator = genNamedParametersNArgs(3)
+  laplace_decorator = genNamedParametersNArgs(3)
+  norm_decorator = genNamedParametersNArgs(3)
+  uniform_decorator = genNamedParametersNArgs(3)
+
+  @beta_decorator
+  def testBetaLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.beta.logpdf
+    lax_fun = lsp_stats.beta.logpdf
+
+    def args_maker():
+      x, a, b, loc, scale = map(rng, shapes, dtypes)
+      return [x, onp.abs(a), onp.abs(b), loc, onp.abs(scale)]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @norm_decorator
+  def testNormLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.norm.logpdf
+    lax_fun = lsp_stats.norm.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      return [x, loc, scale]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @expon_decorator
+  def testExponLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.expon.logpdf
+    lax_fun = lsp_stats.expon.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      return [x, loc, onp.abs(scale)]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @laplace_decorator
+  def testLaplaceLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.laplace.logpdf
+    lax_fun = lsp_stats.laplace.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      return [x, loc, scale]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @uniform_decorator
+  def testUniformLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.uniform.logpdf
+    lax_fun = lsp_stats.uniform.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = onp.abs(scale)  # clipping to ensure that scale is not too low
+      return [x, loc, scale]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+if __name__ == ""__main__"":
+    absltest.main()","diff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py
new file mode 100644
index 000000000..af393be71
--- /dev/null
+++ b/tests/scipy_stats_test.py
@@ -0,0 +1,117 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl.testing import absltest, parameterized
+
+import numpy as onp
+import scipy.stats as osp_stats
+
+from jax import test_util as jtu
+from jax.scipy import stats as lsp_stats
+from lax_scipy_test import CombosWithReplacement, float_dtypes
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
+
+def genNamedParametersNArgs(n):
+    return parameterized.named_parameters(
+            jtu.cases_from_list(
+              {""testcase_name"": jtu.format_test_name_suffix("""", shapes, dtypes),
+               ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+              for shapes in CombosWithReplacement(all_shapes, n)
+              for dtypes in CombosWithReplacement(float_dtypes, n)
+              for rng in [jtu.rand_default()]
+            ))
+
+class LaxBackedScipyStatsTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed scipy.stats implementations""""""
+
+  beta_decorator = genNamedParametersNArgs(5)
+  expon_decorator = genNamedParametersNArgs(3)
+  laplace_decorator = genNamedParametersNArgs(3)
+  norm_decorator = genNamedParametersNArgs(3)
+  uniform_decorator = genNamedParametersNArgs(3)
+
+  @beta_decorator
+  def testBetaLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.beta.logpdf
+    lax_fun = lsp_stats.beta.logpdf
+
+    def args_maker():
+      x, a, b, loc, scale = map(rng, shapes, dtypes)
+      return [x, onp.abs(a), onp.abs(b), loc, onp.abs(scale)]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @norm_decorator
+  def testNormLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.norm.logpdf
+    lax_fun = lsp_stats.norm.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      return [x, loc, scale]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @expon_decorator
+  def testExponLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.expon.logpdf
+    lax_fun = lsp_stats.expon.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      return [x, loc, onp.abs(scale)]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @laplace_decorator
+  def testLaplaceLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.laplace.logpdf
+    lax_fun = lsp_stats.laplace.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      return [x, loc, scale]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @uniform_decorator
+  def testUniformLogPdf(self, rng, shapes, dtypes):
+    scipy_fun = osp_stats.uniform.logpdf
+    lax_fun = lsp_stats.uniform.logpdf
+
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = onp.abs(scale)  # clipping to ensure that scale is not too low
+      return [x, loc, scale]
+
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+if __name__ == ""__main__"":
+    absltest.main()",No
build/build.py,build/build.py,cad36945a2040aaa842a62f57649ec6b12850196,986514eeda4ede3a8c83b4e724023ccd4a8ad299,"Update build.py to write its .bazelrc in the root directory of the repository, next to the workspace.

Fixes build breakage caused by workspace move.","diff --git a/build/build.py b/build/build.py
index 3b08c5f7e..7bb0f599c 100755
--- a/build/build.py
+++ b/build/build.py
@@ -171,7 +171,7 @@ build:opt --host_copt=-march=native
 
 
 def write_bazelrc(**kwargs):
-  f = open("".bazelrc"", ""w"")
+  f = open(""../.bazelrc"", ""w"")
   f.write(BAZELRC_TEMPLATE.format(**kwargs))
   f.close()
 ","diff --git a/build/build.py b/build/build.py
index 3b08c5f7e..7bb0f599c 100755
--- a/build/build.py
+++ b/build/build.py
@@ -171,7 +171,7 @@ build:opt --host_copt=-march=native
 
 
 def write_bazelrc(**kwargs):
-  f = open("".bazelrc"", ""w"")
+  f = open(""../.bazelrc"", ""w"")
   f.write(BAZELRC_TEMPLATE.format(**kwargs))
   f.close()
 ",No
examples/onnx2xla.py,examples/onnx2xla.py,d95fa733a8752a95aeb8a2d74281a9c7023723f4,9ddd30c23c762abb1dd0be90b23a1a99d44605c3,add newline to end of file,"diff --git a/examples/onnx2xla.py b/examples/onnx2xla.py
index b993f67b4..e3c1350bc 100644
--- a/examples/onnx2xla.py
+++ b/examples/onnx2xla.py
@@ -129,3 +129,4 @@ if __name__ == ""__main__"":
   fun = lambda inputs: np.sum(compiled_predict(inputs))
   print(""a derivative with respect to inputs:"")
   print(grad(fun)(np.ones((1, 1, 28, 28)))[..., :3, :3])
+","diff --git a/examples/onnx2xla.py b/examples/onnx2xla.py
index b993f67b4..e3c1350bc 100644
--- a/examples/onnx2xla.py
+++ b/examples/onnx2xla.py
@@ -129,3 +129,4 @@ if __name__ == ""__main__"":
   fun = lambda inputs: np.sum(compiled_predict(inputs))
   print(""a derivative with respect to inputs:"")
   print(grad(fun)(np.ones((1, 1, 28, 28)))[..., :3, :3])
+",No
examples/onnx2xla.py,examples/onnx2xla.py,88986e4d9e233be5c9558706ba920123302c5684,d95fa733a8752a95aeb8a2d74281a9c7023723f4,add md5 check for downloaded onnx file,"diff --git a/examples/onnx2xla.py b/examples/onnx2xla.py
index e3c1350bc..47a1bd0f6 100644
--- a/examples/onnx2xla.py
+++ b/examples/onnx2xla.py
@@ -17,12 +17,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from cStringIO import StringIO
 from functools import partial
-from six.moves.urllib.request import urlopen
+import hashlib
+import sys
 
 import onnx
 from onnx import numpy_helper
 from onnx import onnx_pb2
+from six.moves.urllib.request import urlopen
 
 import jax.numpy as np
 from jax import jit, grad
@@ -112,7 +115,11 @@ if __name__ == ""__main__"":
   url = ('https://github.com/onnx/models/blob/'
          '81c4779096d1205edd0b809e191a924c58c38fef/'
          'mnist/model.onnx?raw=true')
-  model = onnx.load(urlopen(url))
+  download = urlopen(url).read()
+  if hashlib.md5(download).hexdigest() != 'bc8ad9bd19c5a058055dc18d0f089dad':
+    print(""onnx file checksum mismatch"")
+    sys.exit(1)
+  model = onnx.load(StringIO(download))
 
   predict = lambda inputs: interpret_onnx(model.graph, inputs)[0]
 ","diff --git a/examples/onnx2xla.py b/examples/onnx2xla.py
index e3c1350bc..47a1bd0f6 100644
--- a/examples/onnx2xla.py
+++ b/examples/onnx2xla.py
@@ -17,12 +17,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from cStringIO import StringIO
 from functools import partial
-from six.moves.urllib.request import urlopen
+import hashlib
+import sys
 
 import onnx
 from onnx import numpy_helper
 from onnx import onnx_pb2
+from six.moves.urllib.request import urlopen
 
 import jax.numpy as np
 from jax import jit, grad
@@ -112,7 +115,11 @@ if __name__ == ""__main__"":
   url = ('https://github.com/onnx/models/blob/'
          '81c4779096d1205edd0b809e191a924c58c38fef/'
          'mnist/model.onnx?raw=true')
-  model = onnx.load(urlopen(url))
+  download = urlopen(url).read()
+  if hashlib.md5(download).hexdigest() != 'bc8ad9bd19c5a058055dc18d0f089dad':
+    print(""onnx file checksum mismatch"")
+    sys.exit(1)
+  model = onnx.load(StringIO(download))
 
   predict = lambda inputs: interpret_onnx(model.graph, inputs)[0]
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,7530f56809ad2cafd1767c8055e24d3a6cac8d9b,cad36945a2040aaa842a62f57649ec6b12850196,"Implement np.{diag,diagonal}.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 8962c8ada..af2060793 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -41,6 +41,7 @@ from ..lib import xla_bridge
 
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
+_abs = builtins.abs
 _all = builtins.all
 _any = builtins.any
 _max = builtins.max
@@ -829,6 +830,41 @@ def triu(m, k=0):
   return where(mask, zeros_like(m), m)
 
 
+@_wraps(onp.diagonal)
+def diagonal(a, offset=0, axis1=0, axis2=1):
+  a_shape = shape(a)
+
+  # Move the two dimensions to the end.
+  perm = [i for i in range(len(a_shape)) if i != axis1 and i != axis2]
+  perm = perm + [axis1, axis2]
+  a = lax.transpose(a, perm)
+
+  # Mask out the diagonal and reduce over one of the axes
+  a = where(eye(a_shape[axis1], a_shape[axis2], k=offset, dtype=bool),
+            a, zeros_like(a))
+  reduce_axis = -2 if offset < 0 else -1
+  d = sum(a, axis=reduce_axis, dtype=_dtype(a))
+
+  # Slice out the correct diagonal size.
+  diag_size = _max(0, _min(a_shape[axis1] + _min(offset, 0),
+                           a_shape[axis2] - _max(offset, 0)))
+  return lax.slice_in_dim(d, 0, diag_size, axis=-1)
+
+
+@_wraps(onp.diag)
+def diag(v, k=0):
+  v_shape = shape(v)
+  if len(v_shape) == 1:
+    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
+    n = v_shape[0] + _abs(k)
+    v = lax.pad(v, zero(v), ((_max(0, k), _max(0, -k), 0),))
+    return where(eye(n, k=k, dtype=bool), v, zeros_like(v))
+  elif len(v_shape) == 2:
+    return diagonal(v, offset=k)
+  else:
+    raise ValueError(""diag input must be 1d or 2d"")
+
+
 ### Tensor contraction operations
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 8962c8ada..af2060793 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -41,6 +41,7 @@ from ..lib import xla_bridge
 
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
+_abs = builtins.abs
 _all = builtins.all
 _any = builtins.any
 _max = builtins.max
@@ -829,6 +830,41 @@ def triu(m, k=0):
   return where(mask, zeros_like(m), m)
 
 
+@_wraps(onp.diagonal)
+def diagonal(a, offset=0, axis1=0, axis2=1):
+  a_shape = shape(a)
+
+  # Move the two dimensions to the end.
+  perm = [i for i in range(len(a_shape)) if i != axis1 and i != axis2]
+  perm = perm + [axis1, axis2]
+  a = lax.transpose(a, perm)
+
+  # Mask out the diagonal and reduce over one of the axes
+  a = where(eye(a_shape[axis1], a_shape[axis2], k=offset, dtype=bool),
+            a, zeros_like(a))
+  reduce_axis = -2 if offset < 0 else -1
+  d = sum(a, axis=reduce_axis, dtype=_dtype(a))
+
+  # Slice out the correct diagonal size.
+  diag_size = _max(0, _min(a_shape[axis1] + _min(offset, 0),
+                           a_shape[axis2] - _max(offset, 0)))
+  return lax.slice_in_dim(d, 0, diag_size, axis=-1)
+
+
+@_wraps(onp.diag)
+def diag(v, k=0):
+  v_shape = shape(v)
+  if len(v_shape) == 1:
+    zero = lambda x: lax.full_like(x, shape=(), fill_value=0)
+    n = v_shape[0] + _abs(k)
+    v = lax.pad(v, zero(v), ((_max(0, k), _max(0, -k), 0),))
+    return where(eye(n, k=k, dtype=bool), v, zeros_like(v))
+  elif len(v_shape) == 2:
+    return diagonal(v, offset=k)
+  else:
+    raise ValueError(""diag input must be 1d or 2d"")
+
+
 ### Tensor contraction operations
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,7530f56809ad2cafd1767c8055e24d3a6cac8d9b,cad36945a2040aaa842a62f57649ec6b12850196,"Implement np.{diag,diagonal}.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 27d5b0e7d..9cc1d9b80 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -401,6 +401,36 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_k={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), k),
+       ""dtype"": dtype, ""shape"": shape, ""k"": k, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) in (1, 2)]
+      for k in list(range(-4, 4))))
+  def testDiag(self, shape, dtype, k, rng):
+    onp_fun = lambda arg: onp.diag(arg, k)
+    lnp_fun = lambda arg: lnp.diag(arg, k)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_offset={}_axis1={}_axis2={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), offset, axis1, axis2),
+       ""dtype"": dtype, ""shape"": shape, ""offset"": offset, ""axis1"": axis1,
+       ""axis2"": axis2, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) >= 2]
+      for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
+      for offset in list(range(-4, 4))))
+  def testDiagonal(self, shape, dtype, offset, axis1, axis2, rng):
+    onp_fun = lambda arg: onp.diagonal(arg, offset, axis1, axis2)
+    lnp_fun = lambda arg: lnp.diagonal(arg, offset, axis1, axis2)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 27d5b0e7d..9cc1d9b80 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -401,6 +401,36 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_k={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), k),
+       ""dtype"": dtype, ""shape"": shape, ""k"": k, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) in (1, 2)]
+      for k in list(range(-4, 4))))
+  def testDiag(self, shape, dtype, k, rng):
+    onp_fun = lambda arg: onp.diag(arg, k)
+    lnp_fun = lambda arg: lnp.diag(arg, k)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_offset={}_axis1={}_axis2={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), offset, axis1, axis2),
+       ""dtype"": dtype, ""shape"": shape, ""offset"": offset, ""axis1"": axis1,
+       ""axis2"": axis2, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) >= 2]
+      for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
+      for offset in list(range(-4, 4))))
+  def testDiagonal(self, shape, dtype, offset, axis1, axis2, rng):
+    onp_fun = lambda arg: onp.diagonal(arg, offset, axis1, axis2)
+    lnp_fun = lambda arg: lnp.diagonal(arg, offset, axis1, axis2)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),",No
tests/build_defs.bzl,tests/build_defs.bzl,5fe781e26ec7ee0ca05e42b53c2a4de2117db89a,e29b41ee59ee25029a80855205a9fff7c5416c0f,"Add tags to bazel test rules for filtering.

For example:

  bazel test tests/... --test_tag_filters=-jax_test_gpu

builds and runs all targets except those generated to test jax compilation to GPU.","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index 1e1052687..03cb563c1 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -32,8 +32,10 @@ def jax_test(
 
     # Deps that are linked into all test target variants.
     all_test_deps = [
-        "":libjax"",
+        ""//jax:libjax"",
     ]
+    cpu_tags = [""jax_test_cpu""]
+    gpu_tags = [""jax_test_gpu""]
     disabled_tags = [""manual"", ""notap"", ""disabled""]
     native.py_test(
         name = name + ""_cpu"",
@@ -43,7 +45,7 @@ def jax_test(
         deps = deps + all_test_deps,
         shard_count = shard_count.get(""cpu"") if shard_count else None,
         args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
-        tags = (disabled_tags if ""cpu"" in disable else []),
+        tags = cpu_tags + (disabled_tags if ""cpu"" in disable else []),
     )
     native.py_test(
         name = name + ""_cpu_x32"",
@@ -53,7 +55,7 @@ def jax_test(
         deps = deps + all_test_deps,
         shard_count = shard_count.get(""cpu"") if shard_count else None,
         args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
-        tags = (disabled_tags if ""cpu"" in disable else []),
+        tags = cpu_tags + (disabled_tags if ""cpu"" in disable else []),
     )
     native.py_test(
         name = name + ""_gpu"",
@@ -62,7 +64,7 @@ def jax_test(
         data = data,
         args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
         deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        tags = gpu_tags + [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
         shard_count = shard_count.get(""gpu"") if shard_count else None,
     )
     native.py_test(
@@ -72,6 +74,6 @@ def jax_test(
         data = data,
         args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
         deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        tags = gpu_tags + [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
         shard_count = shard_count.get(""gpu"") if shard_count else None,
     )","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index 1e1052687..03cb563c1 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -32,8 +32,10 @@ def jax_test(
 
     # Deps that are linked into all test target variants.
     all_test_deps = [
-        "":libjax"",
+        ""//jax:libjax"",
     ]
+    cpu_tags = [""jax_test_cpu""]
+    gpu_tags = [""jax_test_gpu""]
     disabled_tags = [""manual"", ""notap"", ""disabled""]
     native.py_test(
         name = name + ""_cpu"",
@@ -43,7 +45,7 @@ def jax_test(
         deps = deps + all_test_deps,
         shard_count = shard_count.get(""cpu"") if shard_count else None,
         args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
-        tags = (disabled_tags if ""cpu"" in disable else []),
+        tags = cpu_tags + (disabled_tags if ""cpu"" in disable else []),
     )
     native.py_test(
         name = name + ""_cpu_x32"",
@@ -53,7 +55,7 @@ def jax_test(
         deps = deps + all_test_deps,
         shard_count = shard_count.get(""cpu"") if shard_count else None,
         args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
-        tags = (disabled_tags if ""cpu"" in disable else []),
+        tags = cpu_tags + (disabled_tags if ""cpu"" in disable else []),
     )
     native.py_test(
         name = name + ""_gpu"",
@@ -62,7 +64,7 @@ def jax_test(
         data = data,
         args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
         deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        tags = gpu_tags + [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
         shard_count = shard_count.get(""gpu"") if shard_count else None,
     )
     native.py_test(
@@ -72,6 +74,6 @@ def jax_test(
         data = data,
         args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
         deps = deps + all_test_deps,
-        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        tags = gpu_tags + [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
         shard_count = shard_count.get(""gpu"") if shard_count else None,
     )",No
tests/BUILD,tests/BUILD,9c690143182dce8d477e014c97972a54aed9007d,5fe781e26ec7ee0ca05e42b53c2a4de2117db89a,Only build-depend on libjax optionally (via bazel CLI flag) in the generated bazel test targets.,"diff --git a/tests/BUILD b/tests/BUILD
index 9c65b16b0..4c967ca83 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -16,6 +16,11 @@ licenses([""notice""])  # Apache 2
 
 load("":build_defs.bzl"", ""jax_test"")
 
+config_setting(
+    name = ""libjax_depend"",
+    values = {""define"": ""libjax_depend=true""},
+)
+
 jax_test(
     name = ""core_test"",
     srcs = [""core_test.py""],","diff --git a/tests/BUILD b/tests/BUILD
index 9c65b16b0..4c967ca83 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -16,6 +16,11 @@ licenses([""notice""])  # Apache 2
 
 load("":build_defs.bzl"", ""jax_test"")
 
+config_setting(
+    name = ""libjax_depend"",
+    values = {""define"": ""libjax_depend=true""},
+)
+
 jax_test(
     name = ""core_test"",
     srcs = [""core_test.py""],",No
tests/build_defs.bzl,tests/build_defs.bzl,9c690143182dce8d477e014c97972a54aed9007d,5fe781e26ec7ee0ca05e42b53c2a4de2117db89a,Only build-depend on libjax optionally (via bazel CLI flag) in the generated bazel test targets.,"diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index 03cb563c1..4323c1dd5 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -31,9 +31,10 @@ def jax_test(
             fail(""Only one test source file is currently supported."")
 
     # Deps that are linked into all test target variants.
-    all_test_deps = [
-        ""//jax:libjax"",
-    ]
+    all_test_deps = select({
+        "":libjax_depend"": [""//jax:libjax""],
+        ""//conditions:default"": [],
+    })
     cpu_tags = [""jax_test_cpu""]
     gpu_tags = [""jax_test_gpu""]
     disabled_tags = [""manual"", ""notap"", ""disabled""]","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
index 03cb563c1..4323c1dd5 100644
--- a/tests/build_defs.bzl
+++ b/tests/build_defs.bzl
@@ -31,9 +31,10 @@ def jax_test(
             fail(""Only one test source file is currently supported."")
 
     # Deps that are linked into all test target variants.
-    all_test_deps = [
-        ""//jax:libjax"",
-    ]
+    all_test_deps = select({
+        "":libjax_depend"": [""//jax:libjax""],
+        ""//conditions:default"": [],
+    })
     cpu_tags = [""jax_test_cpu""]
     gpu_tags = [""jax_test_gpu""]
     disabled_tags = [""manual"", ""notap"", ""disabled""]",No
tests/BUILD,tests/BUILD,cfc37eb0be0757937f3530e3d403070d3caa2949,fce7fe6040ae85c1785fe19449ae6eca86dbf8ba,add a test of the resnet50 example,"diff --git a/tests/BUILD b/tests/BUILD
index 4c967ca83..b94447eb2 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -101,3 +101,8 @@ jax_test(
     name = ""generated_fun_test"",
     srcs = [""generated_fun_test.py""],
 )
+
+jax_test(
+    name = ""examples_test"",
+    srcs = [""examples_test.py""],
+)","diff --git a/tests/BUILD b/tests/BUILD
index 4c967ca83..b94447eb2 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -101,3 +101,8 @@ jax_test(
     name = ""generated_fun_test"",
     srcs = [""generated_fun_test.py""],
 )
+
+jax_test(
+    name = ""examples_test"",
+    srcs = [""examples_test.py""],
+)",No
tests/examples_test.py,tests/examples_test.py,91fe4a1bcc704da76c5d2ba9bde62ad9ff68143c,cfc37eb0be0757937f3530e3d403070d3caa2949,skip resnet50 example tests in x64 mode,"diff --git a/tests/examples_test.py b/tests/examples_test.py
index b0b18a08b..e4614fd74 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -30,6 +30,10 @@ sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 from examples import resnet50
 sys.path.pop()
 
+from jax.config import config
+config.parse_flags_with_absl()
+FLAGS = config.FLAGS
+
 
 def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   result_shape, params = init_fun(input_shape)
@@ -44,6 +48,7 @@ class ResNet50Test(jtu.JaxTestCase):
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
       for input_shape in [(2, 20, 25, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
   def testIdentityBlockShape(self, input_shape):
     init_fun, apply_fun = resnet50.IdentityBlock(2, (4, 3))
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
@@ -52,6 +57,7 @@ class ResNet50Test(jtu.JaxTestCase):
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
       for input_shape in [(2, 20, 25, 3)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
   def testConvBlockShape(self, input_shape):
     init_fun, apply_fun = resnet50.ConvBlock(3, (2, 3, 4))
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
@@ -62,6 +68,7 @@ class ResNet50Test(jtu.JaxTestCase):
        ""num_classes"": num_classes, ""input_shape"": input_shape}
       for num_classes in [5, 10]
       for input_shape in [(224, 224, 3, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
   def testResNet50Shape(self, num_classes, input_shape):
     init_fun, apply_fun = resnet50.ResNet50(num_classes)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)","diff --git a/tests/examples_test.py b/tests/examples_test.py
index b0b18a08b..e4614fd74 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -30,6 +30,10 @@ sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 from examples import resnet50
 sys.path.pop()
 
+from jax.config import config
+config.parse_flags_with_absl()
+FLAGS = config.FLAGS
+
 
 def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   result_shape, params = init_fun(input_shape)
@@ -44,6 +48,7 @@ class ResNet50Test(jtu.JaxTestCase):
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
       for input_shape in [(2, 20, 25, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
   def testIdentityBlockShape(self, input_shape):
     init_fun, apply_fun = resnet50.IdentityBlock(2, (4, 3))
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
@@ -52,6 +57,7 @@ class ResNet50Test(jtu.JaxTestCase):
       {""testcase_name"": ""_input_shape={}"".format(input_shape),
        ""input_shape"": input_shape}
       for input_shape in [(2, 20, 25, 3)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
   def testConvBlockShape(self, input_shape):
     init_fun, apply_fun = resnet50.ConvBlock(3, (2, 3, 4))
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
@@ -62,6 +68,7 @@ class ResNet50Test(jtu.JaxTestCase):
        ""num_classes"": num_classes, ""input_shape"": input_shape}
       for num_classes in [5, 10]
       for input_shape in [(224, 224, 3, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
   def testResNet50Shape(self, num_classes, input_shape):
     init_fun, apply_fun = resnet50.ResNet50(num_classes)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)",No
examples/mnist_classifier.py,examples/mnist_classifier.py,bec24999a82b1eaa4b508058963ac58bad152a5f,91fe4a1bcc704da76c5d2ba9bde62ad9ff68143c,"import more examples in examples_test, fix resulting errors","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 6336ac675..be063a29f 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
-import datasets
+from examples import datasets
 
 
 def loss(params, batch):
@@ -94,4 +94,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 6336ac675..be063a29f 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
-import datasets
+from examples import datasets
 
 
 def loss(params, batch):
@@ -94,4 +94,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,bec24999a82b1eaa4b508058963ac58bad152a5f,91fe4a1bcc704da76c5d2ba9bde62ad9ff68143c,"import more examples in examples_test, fix resulting errors","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index b0dfbb5cd..d9b1b9a98 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -29,7 +29,7 @@ from jax.api import jit, grad
 from jax.config import config
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
-import datasets
+from examples import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -93,4 +93,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index b0dfbb5cd..d9b1b9a98 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -29,7 +29,7 @@ from jax.api import jit, grad
 from jax.config import config
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
-import datasets
+from examples import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -93,4 +93,3 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-",No
examples/mnist_vae.py,examples/mnist_vae.py,bec24999a82b1eaa4b508058963ac58bad152a5f,91fe4a1bcc704da76c5d2ba9bde62ad9ff68143c,"import more examples in examples_test, fix resulting errors","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index bda0a2bc2..9f0449bcc 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -33,7 +33,7 @@ from jax import jit, grad, lax, random
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import datasets
+from examples import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -139,4 +139,3 @@ if __name__ == ""__main__"":
     test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
-","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index bda0a2bc2..9f0449bcc 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -33,7 +33,7 @@ from jax import jit, grad, lax, random
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import datasets
+from examples import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -139,4 +139,3 @@ if __name__ == ""__main__"":
     test_elbo, sampled_images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), sampled_images, cmap=plt.cm.gray)
-",No
tests/examples_test.py,tests/examples_test.py,bec24999a82b1eaa4b508058963ac58bad152a5f,91fe4a1bcc704da76c5d2ba9bde62ad9ff68143c,"import more examples in examples_test, fix resulting errors","diff --git a/tests/examples_test.py b/tests/examples_test.py
index e4614fd74..50093f4e4 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -27,6 +27,9 @@ import numpy as onp
 from jax import test_util as jtu
 
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+from examples import mnist_classifier
+from examples import mnist_classifier_fromscratch
+from examples import mnist_vae
 from examples import resnet50
 sys.path.pop()
 ","diff --git a/tests/examples_test.py b/tests/examples_test.py
index e4614fd74..50093f4e4 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -27,6 +27,9 @@ import numpy as onp
 from jax import test_util as jtu
 
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+from examples import mnist_classifier
+from examples import mnist_classifier_fromscratch
+from examples import mnist_vae
 from examples import resnet50
 sys.path.pop()
 ",No
examples/BUILD,examples/BUILD,693e51073e37fb141f3980d8ccd65f2cab4abc29,74d62fd998a2219efedb50ddef6f08df7c1ec89e,add kernel least-squares regression example,"diff --git a/examples/BUILD b/examples/BUILD
index e8d0bf985..7742ceac1 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -60,3 +60,11 @@ py_binary(
         ""//jax:stax"",
     ],
 )
+
+py_binary(
+    name = ""kernel_lsq"",
+    srcs = [""kernel_lsq.py""],
+    deps = [
+        ""//jax:libjax"",
+    ],
+)","diff --git a/examples/BUILD b/examples/BUILD
index e8d0bf985..7742ceac1 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -60,3 +60,11 @@ py_binary(
         ""//jax:stax"",
     ],
 )
+
+py_binary(
+    name = ""kernel_lsq"",
+    srcs = [""kernel_lsq.py""],
+    deps = [
+        ""//jax:libjax"",
+    ],
+)",No
tests/examples_test.py,tests/examples_test.py,437f780a32c786142b3831bebd3487ab26ce8cbb,693e51073e37fb141f3980d8ccd65f2cab4abc29,test the kernel least-squares example,"diff --git a/tests/examples_test.py b/tests/examples_test.py
index 50093f4e4..f4398eacb 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -25,8 +25,10 @@ from absl.testing import parameterized
 import numpy as onp
 
 from jax import test_util as jtu
+import jax.numpy as np
 
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+from examples import kernel_lsq
 from examples import mnist_classifier
 from examples import mnist_classifier_fromscratch
 from examples import mnist_vae
@@ -76,6 +78,25 @@ class ResNet50Test(jtu.JaxTestCase):
     init_fun, apply_fun = resnet50.ResNet50(num_classes)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
+  def testKernelRegressionGram(self):
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    assert np.all(kernel_lsq.gram(kernel, xs) == np.dot(xs, xs.T))
+
+  def testKernelRegressionTrainAndPredict(self):
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    predict = kernel_lsq.train(kernel, xs, ys)
+    assert np.allclose(predict(xs), ys, atol=1e-3)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/examples_test.py b/tests/examples_test.py
index 50093f4e4..f4398eacb 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -25,8 +25,10 @@ from absl.testing import parameterized
 import numpy as onp
 
 from jax import test_util as jtu
+import jax.numpy as np
 
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+from examples import kernel_lsq
 from examples import mnist_classifier
 from examples import mnist_classifier_fromscratch
 from examples import mnist_vae
@@ -76,6 +78,25 @@ class ResNet50Test(jtu.JaxTestCase):
     init_fun, apply_fun = resnet50.ResNet50(num_classes)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
+  def testKernelRegressionGram(self):
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    assert np.all(kernel_lsq.gram(kernel, xs) == np.dot(xs, xs.T))
+
+  def testKernelRegressionTrainAndPredict(self):
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    predict = kernel_lsq.train(kernel, xs, ys)
+    assert np.allclose(predict(xs), ys, atol=1e-3)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,9c63c0ff0a7e25735a0dd6331997749a0862f3f7,45444f02ff8aa94c2f10d97bfd479429bda44649,"Fix dtype semantics of numpy reductions to more closely match that of numpy.

Add tests for the dtype argument to reductions.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index af2060793..f5e5e9418 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -572,7 +572,7 @@ def _make_reduction(np_fun, op, init_val):
 
     a = a if isinstance(a, ndarray) else asarray(a)
     dims = _reduction_dims(a, axis)
-    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
+    result_dtype = _dtype(np_fun(onp.ones((), dtype=dtype or _dtype(a))))
     if _dtype(a) != result_dtype:
       a = lax.convert_element_type(a, result_dtype)
     result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
@@ -616,25 +616,36 @@ any = _make_reduction(onp.any, logical_or, False)
 
 
 @_wraps(onp.mean)
-def mean(a, axis=None, keepdims=False):
+def mean(a, axis=None, dtype=None, keepdims=False):
   if axis is None:
     normalizer = size(a)
   else:
     normalizer = onp.prod(onp.take(shape(a), axis))
-  if onp.issubdtype(_dtype(a), onp.bool_):
-    a = lax.convert_element_type(a, onp.int32)
-  return true_divide(sum(a, axis, keepdims=keepdims),
-                     _constant_like(a, normalizer))
+  if dtype is None:
+    if (onp.issubdtype(_dtype(a), onp.bool_) or
+        onp.issubdtype(_dtype(a), onp.integer)):
+      dtype = xla_bridge.canonicalize_dtype(onp.float64)
+    else:
+      dtype = _dtype(a)
+
+  td = true_divide(
+      sum(a, axis, dtype=dtype, keepdims=keepdims),
+      lax.convert_element_type(normalizer, dtype))
+  return lax.convert_element_type(td, dtype)
 
 
 @_wraps(onp.var)
-def var(a, axis=None, keepdims=False, ddof=0):
+def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
   if ddof != 0:
     raise NotImplementedError(""Only implemented for ddof=0."")
-  centered = subtract(a, mean(a, axis, keepdims=True))
+  if dtype is None:
+    if (onp.issubdtype(_dtype(a), onp.bool_) or
+        onp.issubdtype(_dtype(a), onp.integer)):
+      dtype = xla_bridge.canonicalize_dtype(onp.float64)
+  centered = subtract(a, mean(a, axis, dtype=dtype, keepdims=True))
   if iscomplexobj(centered):
     centered = lax.abs(centered)
-  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)
+  return mean(lax.mul(centered, centered), axis, dtype=dtype, keepdims=keepdims)
 
 
 @_wraps(onp.std)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index af2060793..f5e5e9418 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -572,7 +572,7 @@ def _make_reduction(np_fun, op, init_val):
 
     a = a if isinstance(a, ndarray) else asarray(a)
     dims = _reduction_dims(a, axis)
-    result_dtype = _dtype(np_fun(onp.ones((), dtype=_dtype(a))))
+    result_dtype = _dtype(np_fun(onp.ones((), dtype=dtype or _dtype(a))))
     if _dtype(a) != result_dtype:
       a = lax.convert_element_type(a, result_dtype)
     result = lax.reduce(a, _reduction_init_val(a, init_val), op, dims)
@@ -616,25 +616,36 @@ any = _make_reduction(onp.any, logical_or, False)
 
 
 @_wraps(onp.mean)
-def mean(a, axis=None, keepdims=False):
+def mean(a, axis=None, dtype=None, keepdims=False):
   if axis is None:
     normalizer = size(a)
   else:
     normalizer = onp.prod(onp.take(shape(a), axis))
-  if onp.issubdtype(_dtype(a), onp.bool_):
-    a = lax.convert_element_type(a, onp.int32)
-  return true_divide(sum(a, axis, keepdims=keepdims),
-                     _constant_like(a, normalizer))
+  if dtype is None:
+    if (onp.issubdtype(_dtype(a), onp.bool_) or
+        onp.issubdtype(_dtype(a), onp.integer)):
+      dtype = xla_bridge.canonicalize_dtype(onp.float64)
+    else:
+      dtype = _dtype(a)
+
+  td = true_divide(
+      sum(a, axis, dtype=dtype, keepdims=keepdims),
+      lax.convert_element_type(normalizer, dtype))
+  return lax.convert_element_type(td, dtype)
 
 
 @_wraps(onp.var)
-def var(a, axis=None, keepdims=False, ddof=0):
+def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
   if ddof != 0:
     raise NotImplementedError(""Only implemented for ddof=0."")
-  centered = subtract(a, mean(a, axis, keepdims=True))
+  if dtype is None:
+    if (onp.issubdtype(_dtype(a), onp.bool_) or
+        onp.issubdtype(_dtype(a), onp.integer)):
+      dtype = xla_bridge.canonicalize_dtype(onp.float64)
+  centered = subtract(a, mean(a, axis, dtype=dtype, keepdims=True))
   if iscomplexobj(centered):
     centered = lax.abs(centered)
-  return mean(lax.mul(centered, centered), axis, keepdims=keepdims)
+  return mean(lax.mul(centered, centered), axis, dtype=dtype, keepdims=keepdims)
 
 
 @_wraps(onp.std)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,9c63c0ff0a7e25735a0dd6331997749a0862f3f7,45444f02ff8aa94c2f10d97bfd479429bda44649,"Fix dtype semantics of numpy reductions to more closely match that of numpy.

Add tests for the dtype argument to reductions.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 9cc1d9b80..217702482 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -122,14 +122,17 @@ JAX_BITWISE_OP_RECORDS = [
 ]
 
 JAX_REDUCER_RECORDS = [
+    op_record(""mean"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+]
+
+JAX_REDUCER_NO_DTYPE_RECORDS = [
     op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
@@ -208,6 +211,26 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""{}_inshape={}_axis={}_dtype={}_keepdims={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis,
+          ""None"" if out_dtype is None else onp.dtype(out_dtype).name, keepdims),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype, ""out_dtype"": out_dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis, ""keepdims"": keepdims}
+      for rec in JAX_REDUCER_RECORDS
+      for shape in rec.shapes for dtype in rec.dtypes
+      for out_dtype in [None] + rec.dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True]))
+  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, out_dtype, axis, keepdims):
+    onp_fun = lambda x: onp_op(x, axis, dtype=out_dtype, keepdims=keepdims)
+    lnp_fun = lambda x: lnp_op(x, axis, dtype=out_dtype, keepdims=keepdims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),
@@ -215,11 +238,11 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis, ""keepdims"": keepdims}
-      for rec in JAX_REDUCER_RECORDS
+      for rec in JAX_REDUCER_NO_DTYPE_RECORDS
       for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True]))
-  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
+  def testReducerNoDtype(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
     lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
     args_maker = lambda: [rng(shape, dtype)]","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 9cc1d9b80..217702482 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -122,14 +122,17 @@ JAX_BITWISE_OP_RECORDS = [
 ]
 
 JAX_REDUCER_RECORDS = [
+    op_record(""mean"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+]
+
+JAX_REDUCER_NO_DTYPE_RECORDS = [
     op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""mean"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
@@ -208,6 +211,26 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""{}_inshape={}_axis={}_dtype={}_keepdims={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis,
+          ""None"" if out_dtype is None else onp.dtype(out_dtype).name, keepdims),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype, ""out_dtype"": out_dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis, ""keepdims"": keepdims}
+      for rec in JAX_REDUCER_RECORDS
+      for shape in rec.shapes for dtype in rec.dtypes
+      for out_dtype in [None] + rec.dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True]))
+  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, out_dtype, axis, keepdims):
+    onp_fun = lambda x: onp_op(x, axis, dtype=out_dtype, keepdims=keepdims)
+    lnp_fun = lambda x: lnp_op(x, axis, dtype=out_dtype, keepdims=keepdims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
           rec.test_name.capitalize(),
@@ -215,11 +238,11 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
        ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
        ""axis"": axis, ""keepdims"": keepdims}
-      for rec in JAX_REDUCER_RECORDS
+      for rec in JAX_REDUCER_NO_DTYPE_RECORDS
       for shape in rec.shapes for dtype in rec.dtypes
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True]))
-  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
+  def testReducerNoDtype(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
     lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
     args_maker = lambda: [rng(shape, dtype)]",No
README.md,README.md,3307553dbe7e98a564ce07e4f1805240a6e976bb,45444f02ff8aa94c2f10d97bfd479429bda44649,readme: JAX is about composable transformations,"diff --git a/README.md b/README.md
index bcde8b728..428a6e5d8 100644
--- a/README.md
+++ b/README.md
@@ -13,8 +13,8 @@ JAX can automatically differentiate native
 Python and NumPy functions. It can differentiate through loops, branches,
 recursion, and closures, and it can take derivatives of derivatives of
 derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)
-as well as forward-mode differentiation, and the two can be composed arbitrarily
-to any order.
+via [`grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,
+and the two can be composed arbitrarily to any order.
 
 Whats new is that JAX uses
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
@@ -26,6 +26,12 @@ into XLA-optimized kernels using a one-function API,
 composed arbitrarily, so you can express sophisticated algorithms and get
 maximal performance without leaving Python.
 
+Dig a little deeper, and you'll see that JAX is really an extensible system for
+[composable transformations of functions](#transformations). Both
+[`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)
+are instances of such transformations. Another is [`vmap`](#auto-vectorization-with-vmap)
+for automatic vectorization, with more to come.
+
 This is a research project, not an official Google product. Expect bugs and
 sharp edges. Please help by trying it out, [reporting
 bugs](https://github.com/google/jax/issues), and letting us know what you","diff --git a/README.md b/README.md
index bcde8b728..428a6e5d8 100644
--- a/README.md
+++ b/README.md
@@ -13,8 +13,8 @@ JAX can automatically differentiate native
 Python and NumPy functions. It can differentiate through loops, branches,
 recursion, and closures, and it can take derivatives of derivatives of
 derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)
-as well as forward-mode differentiation, and the two can be composed arbitrarily
-to any order.
+via [`grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,
+and the two can be composed arbitrarily to any order.
 
 Whats new is that JAX uses
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md)
@@ -26,6 +26,12 @@ into XLA-optimized kernels using a one-function API,
 composed arbitrarily, so you can express sophisticated algorithms and get
 maximal performance without leaving Python.
 
+Dig a little deeper, and you'll see that JAX is really an extensible system for
+[composable transformations of functions](#transformations). Both
+[`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)
+are instances of such transformations. Another is [`vmap`](#auto-vectorization-with-vmap)
+for automatic vectorization, with more to come.
+
 This is a research project, not an official Google product. Expect bugs and
 sharp edges. Please help by trying it out, [reporting
 bugs](https://github.com/google/jax/issues), and letting us know what you",No
jax/lax.py,jax/lax.py,538850e271e43a7edc1d03000ff2ef11ee6c86bb,8f019d8910dda6f27addd79b6593dcb1a5e27287,add misc numpy ops (c.f. #70),"diff --git a/jax/lax.py b/jax/lax.py
index a9243ff97..cd6f30569 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -561,14 +561,12 @@ def tan(x):
   return div(sin(x), cos(x))
 
 def asin(x):
-  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
-  return mul(_const(x, 2.),
-             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))
+  return mul(_const(x, 2),
+             atan2(x, add(_const(x, 1), sqrt(sub(_const(x, 1), square(x))))))
 
 def acos(x):
-  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
-  return mul(_const(x, 2.),
-             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))
+  return mul(_const(x, 2),
+             atan2(sqrt(sub(_const(x, 1), square(x))), add(_const(x, 1), x)))
 
 def atan(x):
   return atan2(x, _const(x, 1.))
@@ -588,6 +586,11 @@ def acosh(x):
   return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                         sqrt(sub(x, _const(x, 1.))))))
 
+def atanh(x):
+  # atanh(x) = 0.5 * log((1 + x) / (1 - x))
+  return mul(_const(x, 0.5), log(div(add(_const(x, 1.), x),
+                                     sub(_const(x, 1.), x))))
+
 
 # Add some methods to ShapedArray that rely on lax primitives
 ","diff --git a/jax/lax.py b/jax/lax.py
index a9243ff97..cd6f30569 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -561,14 +561,12 @@ def tan(x):
   return div(sin(x), cos(x))
 
 def asin(x):
-  # asin(x) = 2 * atan(x / (1 + sqrt(1 - x**2)))
-  return mul(_const(x, 2.),
-             atan2(x, add(_const(x, 1.), sqrt(add(_const(x, 1.), square(x))))))
+  return mul(_const(x, 2),
+             atan2(x, add(_const(x, 1), sqrt(sub(_const(x, 1), square(x))))))
 
 def acos(x):
-  # acos(x) = 2 * atan(sqrt(1 - x**2) / (1 + x))
-  return mul(_const(x, 2.),
-             atan2(sqrt(sub(_const(x, 1.), square(x))), add(_const(x, 1.), x)))
+  return mul(_const(x, 2),
+             atan2(sqrt(sub(_const(x, 1), square(x))), add(_const(x, 1), x)))
 
 def atan(x):
   return atan2(x, _const(x, 1.))
@@ -588,6 +586,11 @@ def acosh(x):
   return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
                         sqrt(sub(x, _const(x, 1.))))))
 
+def atanh(x):
+  # atanh(x) = 0.5 * log((1 + x) / (1 - x))
+  return mul(_const(x, 0.5), log(div(add(_const(x, 1.), x),
+                                     sub(_const(x, 1.), x))))
+
 
 # Add some methods to ShapedArray that rely on lax primitives
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,538850e271e43a7edc1d03000ff2ef11ee6c86bb,8f019d8910dda6f27addd79b6593dcb1a5e27287,add misc numpy ops (c.f. #70),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f5e5e9418..1026eb607 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -195,44 +195,65 @@ def _wraps(fun):
 ### implementations of numpy functions in terms of lax
 
 
-def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
-  if promote_to_result_dtype:
-    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
+def _one_to_one_unop(numpy_fn, lax_fn, promote_like=False):
+  if promote_like:
+    fn = lambda x: lax_fn(lax.convert_element_type(x, _result_dtype(numpy_fn, x)))
   else:
-    name = numpy_fn.__name__
-    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
-  return _wraps(numpy_fn)(promoted_lax_fn)
-
-absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
-add = _one_to_one_op(onp.add, lax.add)
-bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
-bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
-bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
-bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
-right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
-left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
-ceil = _one_to_one_op(onp.ceil, lax.ceil)
-equal = _one_to_one_op(onp.equal, lax.eq)
-expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
-exp = _one_to_one_op(onp.exp, lax.exp, True)
-floor = _one_to_one_op(onp.floor, lax.floor)
-greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
-greater = _one_to_one_op(onp.greater, lax.gt)
-isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
-less_equal = _one_to_one_op(onp.less_equal, lax.le)
-less = _one_to_one_op(onp.less, lax.lt)
-log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
-log = _one_to_one_op(onp.log, lax.log, True)
-maximum = _one_to_one_op(onp.maximum, lax.max)
-minimum = _one_to_one_op(onp.minimum, lax.min)
-multiply = _one_to_one_op(onp.multiply, lax.mul)
-negative = _one_to_one_op(onp.negative, lax.neg)
-not_equal = _one_to_one_op(onp.not_equal, lax.ne)
-power = _one_to_one_op(onp.power, lax.pow, True)
-sign = _one_to_one_op(onp.sign, lax.sign)
-subtract = _one_to_one_op(onp.subtract, lax.sub)
-tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
-sort = _one_to_one_op(onp.sort, lax.sort)
+    fn = lambda x: lax_fn(x)
+  return _wraps(numpy_fn)(fn)
+
+def _one_to_one_binop(numpy_fn, lax_fn, promote_like=False):
+  if promote_like:
+    fn = lambda x, y: lax_fn(*_promote_args_like(numpy_fn, x, y))
+  else:
+    fn = lambda x, y: lax_fn(*_promote_args(numpy_fn.__name__, x, y))
+  return _wraps(numpy_fn)(fn)
+
+
+absolute = abs = _one_to_one_unop(onp.absolute, lax.abs)
+bitwise_not = _one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
+negative = _one_to_one_unop(onp.negative, lax.neg)
+sort = _one_to_one_unop(onp.sort, lax.sort)
+sign = _one_to_one_unop(onp.sign, lax.sign)
+
+floor = _one_to_one_unop(onp.floor, lax.floor, True)
+ceil = _one_to_one_unop(onp.ceil, lax.ceil, True)
+exp = _one_to_one_unop(onp.exp, lax.exp, True)
+log = _one_to_one_unop(onp.log, lax.log, True)
+expm1 = _one_to_one_unop(onp.expm1, lax.expm1, True)
+log1p = _one_to_one_unop(onp.log1p, lax.log1p, True)
+sin = _one_to_one_unop(onp.sin, lax.sin, True)
+cos = _one_to_one_unop(onp.cos, lax.cos, True)
+tan = _one_to_one_unop(onp.tan, lax.tan, True)
+arcsin = _one_to_one_unop(onp.arcsin, lax.asin, True)
+arccos = _one_to_one_unop(onp.arccos, lax.acos, True)
+arctan = _one_to_one_unop(onp.arctan, lax.atan, True)
+sinh = _one_to_one_unop(onp.sinh, lax.sinh, True)
+cosh = _one_to_one_unop(onp.cosh, lax.cosh, True)
+tanh = _one_to_one_unop(onp.tanh, lax.tanh, True)
+arcsinh = _one_to_one_unop(onp.arcsinh, lax.asinh, True)
+arccosh = _one_to_one_unop(onp.arccosh, lax.acosh, True)
+arctanh = _one_to_one_unop(onp.arctanh, lax.atanh, True)
+
+add = _one_to_one_binop(onp.add, lax.add)
+bitwise_and = _one_to_one_binop(onp.bitwise_and, lax.bitwise_and)
+bitwise_or = _one_to_one_binop(onp.bitwise_or, lax.bitwise_or)
+bitwise_xor = _one_to_one_binop(onp.bitwise_xor, lax.bitwise_xor)
+right_shift = _one_to_one_binop(onp.right_shift, lax.shift_right_arithmetic)
+left_shift = _one_to_one_binop(onp.left_shift, lax.shift_left)
+equal = _one_to_one_binop(onp.equal, lax.eq)
+greater_equal = _one_to_one_binop(onp.greater_equal, lax.ge)
+greater = _one_to_one_binop(onp.greater, lax.gt)
+isfinite = _one_to_one_binop(onp.isfinite, lax.is_finite)
+less_equal = _one_to_one_binop(onp.less_equal, lax.le)
+less = _one_to_one_binop(onp.less, lax.lt)
+maximum = _one_to_one_binop(onp.maximum, lax.max)
+minimum = _one_to_one_binop(onp.minimum, lax.min)
+multiply = _one_to_one_binop(onp.multiply, lax.mul)
+not_equal = _one_to_one_binop(onp.not_equal, lax.ne)
+subtract = _one_to_one_binop(onp.subtract, lax.sub)
+power = _one_to_one_binop(onp.power, lax.pow, True)
+arctan2 = _one_to_one_binop(onp.arctan2, lax.atan2, True)
 
 
 def _logical_op(np_op, bitwise_op):
@@ -352,30 +373,6 @@ def flip(m, axis):
   return lax.rev(m, [axis])
 
 
-@_wraps(onp.sinh)
-def sinh(x):
-  x, = _promote_to_result_dtype(onp.sinh, x)
-  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))
-
-
-@_wraps(onp.cosh)
-def cosh(x):
-  x, = _promote_to_result_dtype(onp.cosh, x)
-  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))
-
-
-@_wraps(onp.sin)
-def sin(x):
-  x, = _promote_to_result_dtype(onp.sin, x)
-  return lax.sin(x)
-
-
-@_wraps(onp.cos)
-def cos(x):
-  x, = _promote_to_result_dtype(onp.sin, x)
-  return lax.cos(x)
-
-
 @_wraps(onp.conjugate)
 def conjugate(x):
   return lax.conj(x) if iscomplexobj(x) else x
@@ -611,8 +608,8 @@ sum = _make_reduction(onp.sum, lax.add, 0)
 prod = _make_reduction(onp.prod, lax.mul, 1)
 max = _make_reduction(onp.max, lax.max, -onp.inf)
 min = _make_reduction(onp.min, lax.min, onp.inf)
-all = _make_reduction(onp.all, logical_and, True)
-any = _make_reduction(onp.any, logical_or, False)
+all = alltrue = _make_reduction(onp.all, logical_and, True)
+any = sometrue = _make_reduction(onp.any, logical_or, False)
 
 
 @_wraps(onp.mean)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f5e5e9418..1026eb607 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -195,44 +195,65 @@ def _wraps(fun):
 ### implementations of numpy functions in terms of lax
 
 
-def _one_to_one_op(numpy_fn, lax_fn, promote_to_result_dtype=False):
-  if promote_to_result_dtype:
-    promoted_lax_fn = lambda *args: lax_fn(*_promote_args_like(numpy_fn, *args))
+def _one_to_one_unop(numpy_fn, lax_fn, promote_like=False):
+  if promote_like:
+    fn = lambda x: lax_fn(lax.convert_element_type(x, _result_dtype(numpy_fn, x)))
   else:
-    name = numpy_fn.__name__
-    promoted_lax_fn = lambda *args: lax_fn(*_promote_args(name, *args))
-  return _wraps(numpy_fn)(promoted_lax_fn)
+    fn = lambda x: lax_fn(x)
+  return _wraps(numpy_fn)(fn)
 
-absolute = abs = _one_to_one_op(onp.absolute, lax.abs)
-add = _one_to_one_op(onp.add, lax.add)
-bitwise_and = _one_to_one_op(onp.bitwise_and, lax.bitwise_and)
-bitwise_not = _one_to_one_op(onp.bitwise_not, lax.bitwise_not)
-bitwise_or = _one_to_one_op(onp.bitwise_or, lax.bitwise_or)
-bitwise_xor = _one_to_one_op(onp.bitwise_xor, lax.bitwise_xor)
-right_shift = _one_to_one_op(onp.right_shift, lax.shift_right_arithmetic)
-left_shift = _one_to_one_op(onp.left_shift, lax.shift_left)
-ceil = _one_to_one_op(onp.ceil, lax.ceil)
-equal = _one_to_one_op(onp.equal, lax.eq)
-expm1 = _one_to_one_op(onp.expm1, lax.expm1, True)
-exp = _one_to_one_op(onp.exp, lax.exp, True)
-floor = _one_to_one_op(onp.floor, lax.floor)
-greater_equal = _one_to_one_op(onp.greater_equal, lax.ge)
-greater = _one_to_one_op(onp.greater, lax.gt)
-isfinite = _one_to_one_op(onp.isfinite, lax.is_finite)
-less_equal = _one_to_one_op(onp.less_equal, lax.le)
-less = _one_to_one_op(onp.less, lax.lt)
-log1p = _one_to_one_op(onp.log1p, lax.log1p, True)
-log = _one_to_one_op(onp.log, lax.log, True)
-maximum = _one_to_one_op(onp.maximum, lax.max)
-minimum = _one_to_one_op(onp.minimum, lax.min)
-multiply = _one_to_one_op(onp.multiply, lax.mul)
-negative = _one_to_one_op(onp.negative, lax.neg)
-not_equal = _one_to_one_op(onp.not_equal, lax.ne)
-power = _one_to_one_op(onp.power, lax.pow, True)
-sign = _one_to_one_op(onp.sign, lax.sign)
-subtract = _one_to_one_op(onp.subtract, lax.sub)
-tanh = _one_to_one_op(onp.tanh, lax.tanh, True)
-sort = _one_to_one_op(onp.sort, lax.sort)
+def _one_to_one_binop(numpy_fn, lax_fn, promote_like=False):
+  if promote_like:
+    fn = lambda x, y: lax_fn(*_promote_args_like(numpy_fn, x, y))
+  else:
+    fn = lambda x, y: lax_fn(*_promote_args(numpy_fn.__name__, x, y))
+  return _wraps(numpy_fn)(fn)
+
+
+absolute = abs = _one_to_one_unop(onp.absolute, lax.abs)
+bitwise_not = _one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
+negative = _one_to_one_unop(onp.negative, lax.neg)
+sort = _one_to_one_unop(onp.sort, lax.sort)
+sign = _one_to_one_unop(onp.sign, lax.sign)
+
+floor = _one_to_one_unop(onp.floor, lax.floor, True)
+ceil = _one_to_one_unop(onp.ceil, lax.ceil, True)
+exp = _one_to_one_unop(onp.exp, lax.exp, True)
+log = _one_to_one_unop(onp.log, lax.log, True)
+expm1 = _one_to_one_unop(onp.expm1, lax.expm1, True)
+log1p = _one_to_one_unop(onp.log1p, lax.log1p, True)
+sin = _one_to_one_unop(onp.sin, lax.sin, True)
+cos = _one_to_one_unop(onp.cos, lax.cos, True)
+tan = _one_to_one_unop(onp.tan, lax.tan, True)
+arcsin = _one_to_one_unop(onp.arcsin, lax.asin, True)
+arccos = _one_to_one_unop(onp.arccos, lax.acos, True)
+arctan = _one_to_one_unop(onp.arctan, lax.atan, True)
+sinh = _one_to_one_unop(onp.sinh, lax.sinh, True)
+cosh = _one_to_one_unop(onp.cosh, lax.cosh, True)
+tanh = _one_to_one_unop(onp.tanh, lax.tanh, True)
+arcsinh = _one_to_one_unop(onp.arcsinh, lax.asinh, True)
+arccosh = _one_to_one_unop(onp.arccosh, lax.acosh, True)
+arctanh = _one_to_one_unop(onp.arctanh, lax.atanh, True)
+
+add = _one_to_one_binop(onp.add, lax.add)
+bitwise_and = _one_to_one_binop(onp.bitwise_and, lax.bitwise_and)
+bitwise_or = _one_to_one_binop(onp.bitwise_or, lax.bitwise_or)
+bitwise_xor = _one_to_one_binop(onp.bitwise_xor, lax.bitwise_xor)
+right_shift = _one_to_one_binop(onp.right_shift, lax.shift_right_arithmetic)
+left_shift = _one_to_one_binop(onp.left_shift, lax.shift_left)
+equal = _one_to_one_binop(onp.equal, lax.eq)
+greater_equal = _one_to_one_binop(onp.greater_equal, lax.ge)
+greater = _one_to_one_binop(onp.greater, lax.gt)
+isfinite = _one_to_one_binop(onp.isfinite, lax.is_finite)
+less_equal = _one_to_one_binop(onp.less_equal, lax.le)
+less = _one_to_one_binop(onp.less, lax.lt)
+maximum = _one_to_one_binop(onp.maximum, lax.max)
+minimum = _one_to_one_binop(onp.minimum, lax.min)
+multiply = _one_to_one_binop(onp.multiply, lax.mul)
+not_equal = _one_to_one_binop(onp.not_equal, lax.ne)
+subtract = _one_to_one_binop(onp.subtract, lax.sub)
+power = _one_to_one_binop(onp.power, lax.pow, True)
+arctan2 = _one_to_one_binop(onp.arctan2, lax.atan2, True)
 
 
 def _logical_op(np_op, bitwise_op):
@@ -352,30 +373,6 @@ def flip(m, axis):
   return lax.rev(m, [axis])
 
 
-@_wraps(onp.sinh)
-def sinh(x):
-  x, = _promote_to_result_dtype(onp.sinh, x)
-  return lax.div(lax.sub(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))
-
-
-@_wraps(onp.cosh)
-def cosh(x):
-  x, = _promote_to_result_dtype(onp.cosh, x)
-  return lax.div(lax.add(lax.exp(x), lax.exp(lax.neg(x))), _constant_like(x, 2))
-
-
-@_wraps(onp.sin)
-def sin(x):
-  x, = _promote_to_result_dtype(onp.sin, x)
-  return lax.sin(x)
-
-
-@_wraps(onp.cos)
-def cos(x):
-  x, = _promote_to_result_dtype(onp.sin, x)
-  return lax.cos(x)
-
-
 @_wraps(onp.conjugate)
 def conjugate(x):
   return lax.conj(x) if iscomplexobj(x) else x
@@ -611,8 +608,8 @@ sum = _make_reduction(onp.sum, lax.add, 0)
 prod = _make_reduction(onp.prod, lax.mul, 1)
 max = _make_reduction(onp.max, lax.max, -onp.inf)
 min = _make_reduction(onp.min, lax.min, onp.inf)
-all = _make_reduction(onp.all, logical_and, True)
-any = _make_reduction(onp.any, logical_or, False)
+all = alltrue = _make_reduction(onp.all, logical_and, True)
+any = sometrue = _make_reduction(onp.any, logical_or, False)
 
 
 @_wraps(onp.mean)",Yes
tests/lax_numpy_test.py,tests/lax_numpy_test.py,538850e271e43a7edc1d03000ff2ef11ee6c86bb,8f019d8910dda6f27addd79b6593dcb1a5e27287,add misc numpy ops (c.f. #70),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 217702482..196063985 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -84,13 +84,22 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
     op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tan"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sinh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cosh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""arcsin"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccos"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan2"", 2, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arcsinh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccosh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctanh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
@@ -103,7 +112,6 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
-    op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 217702482..196063985 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -84,13 +84,22 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
     op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tan"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sinh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cosh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""arcsin"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccos"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan2"", 2, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arcsinh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccosh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctanh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
@@ -103,7 +112,6 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
-    op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),",No
jax/lax.py,jax/lax.py,77d6fb4c012a3ea3c6e727b94fa69459561a2525,538850e271e43a7edc1d03000ff2ef11ee6c86bb,transpose shouldn't transpose with identity perm,"diff --git a/jax/lax.py b/jax/lax.py
index cd6f30569..918a1eaa7 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -244,7 +244,11 @@ def _index_untake(axes, src, dst, *idxs):
   return dst
 
 def transpose(operand, permutation):
-  return transpose_p.bind(operand, permutation=tuple(permutation))
+  permutation = tuple(permutation)
+  if permutation == tuple(range(len(permutation))):
+    return operand
+  else:
+    return transpose_p.bind(operand, permutation=permutation)
 
 def reduce(operand, init_value, computation, dimensions):
   monoid_reducer = _get_monoid_reducer(computation, init_value)","diff --git a/jax/lax.py b/jax/lax.py
index cd6f30569..918a1eaa7 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -244,7 +244,11 @@ def _index_untake(axes, src, dst, *idxs):
   return dst
 
 def transpose(operand, permutation):
-  return transpose_p.bind(operand, permutation=tuple(permutation))
+  permutation = tuple(permutation)
+  if permutation == tuple(range(len(permutation))):
+    return operand
+  else:
+    return transpose_p.bind(operand, permutation=permutation)
 
 def reduce(operand, init_value, computation, dimensions):
   monoid_reducer = _get_monoid_reducer(computation, init_value)",No
jax/lax.py,jax/lax.py,1f42d980b86a68a4ac0529e750023ca9b2361e34,77d6fb4c012a3ea3c6e727b94fa69459561a2525,"rename ResNet50Test -> ExamplesTest, remove some dots","diff --git a/jax/lax.py b/jax/lax.py
index 918a1eaa7..42a927b18 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -559,7 +559,7 @@ def square(x):
   return mul(x, x)
 
 def reciprocal(x):
-  return div(_const(x, 1.), x)
+  return div(_const(x, 1), x)
 
 def tan(x):
   return div(sin(x), cos(x))
@@ -573,7 +573,7 @@ def acos(x):
              atan2(sqrt(sub(_const(x, 1), square(x))), add(_const(x, 1), x)))
 
 def atan(x):
-  return atan2(x, _const(x, 1.))
+  return atan2(x, _const(x, 1))
 
 def sinh(x):
   return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))
@@ -583,17 +583,17 @@ def cosh(x):
 
 def asinh(x):
   # asinh(x) = log(x + sqrt(x**2 + 1))
-  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))
+  return log(add(x, sqrt(add(mul(x, x), _const(x, 1)))))
 
 def acosh(x):
   # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
-  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
-                        sqrt(sub(x, _const(x, 1.))))))
+  return log(add(x, mul(sqrt(add(x, _const(x, 1))),
+                        sqrt(sub(x, _const(x, 1))))))
 
 def atanh(x):
   # atanh(x) = 0.5 * log((1 + x) / (1 - x))
-  return mul(_const(x, 0.5), log(div(add(_const(x, 1.), x),
-                                     sub(_const(x, 1.), x))))
+  return mul(_const(x, 0.5), log(div(add(_const(x, 1), x),
+                                     sub(_const(x, 1), x))))
 
 
 # Add some methods to ShapedArray that rely on lax primitives","diff --git a/jax/lax.py b/jax/lax.py
index 918a1eaa7..42a927b18 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -559,7 +559,7 @@ def square(x):
   return mul(x, x)
 
 def reciprocal(x):
-  return div(_const(x, 1.), x)
+  return div(_const(x, 1), x)
 
 def tan(x):
   return div(sin(x), cos(x))
@@ -573,7 +573,7 @@ def acos(x):
              atan2(sqrt(sub(_const(x, 1), square(x))), add(_const(x, 1), x)))
 
 def atan(x):
-  return atan2(x, _const(x, 1.))
+  return atan2(x, _const(x, 1))
 
 def sinh(x):
   return mul(_const(x, 0.5), sub(exp(x), exp(neg(x))))
@@ -583,17 +583,17 @@ def cosh(x):
 
 def asinh(x):
   # asinh(x) = log(x + sqrt(x**2 + 1))
-  return log(add(x, sqrt(add(mul(x, x), _const(x, 1.)))))
+  return log(add(x, sqrt(add(mul(x, x), _const(x, 1)))))
 
 def acosh(x):
   # acosh(x) = log(x + sqrt((x + 1) * (x - 1)))
-  return log(add(x, mul(sqrt(add(x, _const(x, 1.))),
-                        sqrt(sub(x, _const(x, 1.))))))
+  return log(add(x, mul(sqrt(add(x, _const(x, 1))),
+                        sqrt(sub(x, _const(x, 1))))))
 
 def atanh(x):
   # atanh(x) = 0.5 * log((1 + x) / (1 - x))
-  return mul(_const(x, 0.5), log(div(add(_const(x, 1.), x),
-                                     sub(_const(x, 1.), x))))
+  return mul(_const(x, 0.5), log(div(add(_const(x, 1), x),
+                                     sub(_const(x, 1), x))))
 
 
 # Add some methods to ShapedArray that rely on lax primitives",No
tests/examples_test.py,tests/examples_test.py,1f42d980b86a68a4ac0529e750023ca9b2361e34,77d6fb4c012a3ea3c6e727b94fa69459561a2525,"rename ResNet50Test -> ExamplesTest, remove some dots","diff --git a/tests/examples_test.py b/tests/examples_test.py
index f4398eacb..aa53509f4 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -47,7 +47,7 @@ def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   test_case.assertEqual(result.shape, result_shape)
 
 
-class ResNet50Test(jtu.JaxTestCase):
+class ExamplesTest(jtu.JaxTestCase):
 
   @parameterized.named_parameters(
       {""testcase_name"": ""_input_shape={}"".format(input_shape),","diff --git a/tests/examples_test.py b/tests/examples_test.py
index f4398eacb..aa53509f4 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -47,7 +47,7 @@ def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   test_case.assertEqual(result.shape, result_shape)
 
 
-class ResNet50Test(jtu.JaxTestCase):
+class ExamplesTest(jtu.JaxTestCase):
 
   @parameterized.named_parameters(
       {""testcase_name"": ""_input_shape={}"".format(input_shape),",No
setup.py,setup.py,756bc8bb34f1615065fbbf9c788f7cf7486b0947,5c3e918bfd2a6ce56413b8a23973bb351f9ce1c4,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 6b2a5427e..061bd0768 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.8',
+    version='0.1.9',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 6b2a5427e..061bd0768 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.8',
+    version='0.1.9',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,51d89a332c48210dc7d832e42358857529336eb2,756bc8bb34f1615065fbbf9c788f7cf7486b0947,tweak readme text,"diff --git a/README.md b/README.md
index 428a6e5d8..a498c623b 100644
--- a/README.md
+++ b/README.md
@@ -27,7 +27,7 @@ composed arbitrarily, so you can express sophisticated algorithms and get
 maximal performance without leaving Python.
 
 Dig a little deeper, and you'll see that JAX is really an extensible system for
-[composable transformations of functions](#transformations). Both
+[composable function transformations](#transformations). Both
 [`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)
 are instances of such transformations. Another is [`vmap`](#auto-vectorization-with-vmap)
 for automatic vectorization, with more to come.","diff --git a/README.md b/README.md
index 428a6e5d8..a498c623b 100644
--- a/README.md
+++ b/README.md
@@ -27,7 +27,7 @@ composed arbitrarily, so you can express sophisticated algorithms and get
 maximal performance without leaving Python.
 
 Dig a little deeper, and you'll see that JAX is really an extensible system for
-[composable transformations of functions](#transformations). Both
+[composable function transformations](#transformations). Both
 [`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)
 are instances of such transformations. Another is [`vmap`](#auto-vectorization-with-vmap)
 for automatic vectorization, with more to come.",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,52d329c62176d1ccd0b789c0de3b8c35e130b680,986514eeda4ede3a8c83b4e724023ccd4a8ad299,"Add log2, log10, logaddexp2, exp2 to lax_numpy

I also took this opportunity to add some missing decorators","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 8962c8ada..4e060814d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -301,6 +301,7 @@ def _float_divmod(x1, x2):
   return lax.round(div), mod
 
 
+@_wraps(onp.logaddexp)
 def logaddexp(x1, x2):
   x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
   amax = lax.max(x1, x2)
@@ -308,6 +309,32 @@ def logaddexp(x1, x2):
                                        lax.exp(lax.sub(x2, amax)))))
 
 
+@_wraps(onp.logaddexp2)
+def logaddexp2(x1, x2):
+  x1, x2 = _promote_to_result_dtype(onp.logaddexp2, *_promote_shapes(x1, x2))
+  amax = lax.max(x1, x2)
+  return lax.add(amax, log2(lax.add(exp2(lax.sub(x1, amax)),
+                                    exp2(lax.sub(x2, amax)))))
+
+
+@_wraps(onp.log2)
+def log2(x):
+  x, = _promote_to_result_dtype(onp.log2, x)
+  return lax.div(lax.log(x), lax.log(_constant_like(x, 2)))
+
+
+@_wraps(onp.log10)
+def log10(x):
+  x, = _promote_to_result_dtype(onp.log10, x)
+  return lax.div(lax.log(x), lax.log(_constant_like(x, 10)))
+
+
+@_wraps(onp.exp2)
+def exp2(x):
+  x, = _promote_to_result_dtype(onp.exp2, x)
+  return lax.exp(lax.mul(lax.log(_constant_like(x, 2)), x))
+
+
 @_wraps(onp.remainder)
 def remainder(x1, x2):
   x1, x2 = _promote_args(""remainder"", x1, x2)
@@ -316,6 +343,7 @@ mod = remainder
 fmod = lax.rem
 
 
+@_wraps(onp.sqrt)
 def sqrt(x):
   x, = _promote_to_result_dtype(onp.sqrt, x)
   return power(x, _constant_like(x, 0.5))","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 8962c8ada..4e060814d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -301,6 +301,7 @@ def _float_divmod(x1, x2):
   return lax.round(div), mod
 
 
+@_wraps(onp.logaddexp)
 def logaddexp(x1, x2):
   x1, x2 = _promote_to_result_dtype(onp.logaddexp, *_promote_shapes(x1, x2))
   amax = lax.max(x1, x2)
@@ -308,6 +309,32 @@ def logaddexp(x1, x2):
                                        lax.exp(lax.sub(x2, amax)))))
 
 
+@_wraps(onp.logaddexp2)
+def logaddexp2(x1, x2):
+  x1, x2 = _promote_to_result_dtype(onp.logaddexp2, *_promote_shapes(x1, x2))
+  amax = lax.max(x1, x2)
+  return lax.add(amax, log2(lax.add(exp2(lax.sub(x1, amax)),
+                                    exp2(lax.sub(x2, amax)))))
+
+
+@_wraps(onp.log2)
+def log2(x):
+  x, = _promote_to_result_dtype(onp.log2, x)
+  return lax.div(lax.log(x), lax.log(_constant_like(x, 2)))
+
+
+@_wraps(onp.log10)
+def log10(x):
+  x, = _promote_to_result_dtype(onp.log10, x)
+  return lax.div(lax.log(x), lax.log(_constant_like(x, 10)))
+
+
+@_wraps(onp.exp2)
+def exp2(x):
+  x, = _promote_to_result_dtype(onp.exp2, x)
+  return lax.exp(lax.mul(lax.log(_constant_like(x, 2)), x))
+
+
 @_wraps(onp.remainder)
 def remainder(x1, x2):
   x1, x2 = _promote_args(""remainder"", x1, x2)
@@ -316,6 +343,7 @@ mod = remainder
 fmod = lax.rem
 
 
+@_wraps(onp.sqrt)
 def sqrt(x):
   x, = _promote_to_result_dtype(onp.sqrt, x)
   return power(x, _constant_like(x, 0.5))",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,cdff3b9b55a71dbaa2071ebbd32167440355ece3,52d329c62176d1ccd0b789c0de3b8c35e130b680,Add tests for implemented functions,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 27d5b0e7d..31dd15474 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -92,15 +92,19 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
 JAX_COMPOUND_OP_RECORDS = [
     op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""exp2"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
     op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""log10"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
     op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 27d5b0e7d..31dd15474 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -92,15 +92,19 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
 JAX_COMPOUND_OP_RECORDS = [
     op_record(""cosh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""exp2"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
     op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""log10"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
     op_record(""sinh"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,22f9d50be0882890d3a5eacf39932c141e65d05a,3fba60fcd58ec00537af9f5e4b09a6cc5997229b,"Implement np.trace.

Also add code to forward np.{iinfo,finfo,issubdtype,integer} to their original numpy equivalents.

Fixes #29.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 91b053afe..47a510e08 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -93,6 +93,12 @@ float32 = onp.float32
 float64 = onp.float64
 complex64 = onp.complex64
 
+integer = onp.integer
+
+iinfo = onp.iinfo
+finfo = onp.finfo
+
+issubdtype = onp.issubdtype
 
 ### utility functions
 
@@ -866,6 +872,30 @@ def triu(m, k=0):
   return where(mask, zeros_like(m), m)
 
 
+@_wraps(onp.trace)
+def trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):
+  if out:
+    raise NotImplementedError(""The 'out' argument to trace is not supported."")
+
+  a_shape = shape(a)
+  if dtype is None:
+    dtype = _dtype(a)
+    if issubdtype(dtype, integer):
+      default_int = xla_bridge.canonicalize_dtype(onp.int_)
+      if iinfo(dtype).bits < iinfo(default_int).bits:
+        dtype = default_int
+
+  # Move the axis? dimensions to the end.
+  perm = [i for i in range(len(a_shape)) if i != axis1 and i != axis2]
+  perm = perm + [axis1, axis2]
+  a = lax.transpose(a, perm)
+
+  # Mask out the diagonal and reduce.
+  a = where(eye(a_shape[axis1], a_shape[axis2], k=offset, dtype=bool),
+            a, zeros_like(a))
+  return sum(a, axis=(-2, -1), dtype=dtype)
+
+
 @_wraps(onp.diagonal)
 def diagonal(a, offset=0, axis1=0, axis2=1):
   a_shape = shape(a)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 91b053afe..47a510e08 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -93,6 +93,12 @@ float32 = onp.float32
 float64 = onp.float64
 complex64 = onp.complex64
 
+integer = onp.integer
+
+iinfo = onp.iinfo
+finfo = onp.finfo
+
+issubdtype = onp.issubdtype
 
 ### utility functions
 
@@ -866,6 +872,30 @@ def triu(m, k=0):
   return where(mask, zeros_like(m), m)
 
 
+@_wraps(onp.trace)
+def trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):
+  if out:
+    raise NotImplementedError(""The 'out' argument to trace is not supported."")
+
+  a_shape = shape(a)
+  if dtype is None:
+    dtype = _dtype(a)
+    if issubdtype(dtype, integer):
+      default_int = xla_bridge.canonicalize_dtype(onp.int_)
+      if iinfo(dtype).bits < iinfo(default_int).bits:
+        dtype = default_int
+
+  # Move the axis? dimensions to the end.
+  perm = [i for i in range(len(a_shape)) if i != axis1 and i != axis2]
+  perm = perm + [axis1, axis2]
+  a = lax.transpose(a, perm)
+
+  # Mask out the diagonal and reduce.
+  a = where(eye(a_shape[axis1], a_shape[axis2], k=offset, dtype=bool),
+            a, zeros_like(a))
+  return sum(a, axis=(-2, -1), dtype=dtype)
+
+
 @_wraps(onp.diagonal)
 def diagonal(a, offset=0, axis1=0, axis2=1):
   a_shape = shape(a)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,22f9d50be0882890d3a5eacf39932c141e65d05a,3fba60fcd58ec00537af9f5e4b09a6cc5997229b,"Implement np.trace.

Also add code to forward np.{iinfo,finfo,issubdtype,integer} to their original numpy equivalents.

Fixes #29.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 31266ecec..83a35af5c 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -466,6 +466,24 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_dtype_{}_offset={}_axis1={}_axis2={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          out_dtype, offset, axis1, axis2),
+       ""dtype"": dtype, ""out_dtype"": out_dtype, ""shape"": shape, ""offset"": offset,
+       ""axis1"": axis1, ""axis2"": axis2, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for out_dtype in [None] + default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) >= 2]
+      for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
+      for offset in list(range(-4, 4))))
+  def testTrace(self, shape, dtype, out_dtype, offset, axis1, axis2, rng):
+    onp_fun = lambda arg: onp.trace(arg, offset, axis1, axis2, out_dtype)
+    lnp_fun = lambda arg: lnp.trace(arg, offset, axis1, axis2, out_dtype)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 31266ecec..83a35af5c 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -466,6 +466,24 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_dtype_{}_offset={}_axis1={}_axis2={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          out_dtype, offset, axis1, axis2),
+       ""dtype"": dtype, ""out_dtype"": out_dtype, ""shape"": shape, ""offset"": offset,
+       ""axis1"": axis1, ""axis2"": axis2, ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for out_dtype in [None] + default_dtypes
+      for shape in [shape for shape in all_shapes if len(shape) >= 2]
+      for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
+      for offset in list(range(-4, 4))))
+  def testTrace(self, shape, dtype, out_dtype, offset, axis1, axis2, rng):
+    onp_fun = lambda arg: onp.trace(arg, offset, axis1, axis2, out_dtype)
+    lnp_fun = lambda arg: lnp.trace(arg, offset, axis1, axis2, out_dtype)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}"".format(
           jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,579665b1b0ee8caa0f154d6677cd4b13be6adc3f,9ce67993ce6b5d588f99af43cc83e6375d540678,add set_printoptions (same as onp version),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 47a510e08..0b2d468a5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -49,12 +49,14 @@ _min = builtins.min
 _sum = builtins.sum
 
 # We need some numpy scalars
-# TODO(mattjj): handle constants in an indirected, less explicit way?
 pi = onp.pi
 e = onp.e
 inf = onp.inf
 nan = onp.nan
 
+# And some numpy utility functions
+set_printoptions = onp.set_printoptions
+
 # We want isinstance(x, np.ndarray) checks in user code to work with the our
 # array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
 # array base class). We can override the isinstance behavior directly, without
@@ -1013,6 +1015,7 @@ def _argminmax(op, a, axis):
 
 
 def _not_implemented(fun):
+  @_wraps(fun)
   def wrapped(*args, **kwargs):
     raise Exception(""Numpy function {} not yet implemented"".format(fun))
   return wrapped","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 47a510e08..0b2d468a5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -49,12 +49,14 @@ _min = builtins.min
 _sum = builtins.sum
 
 # We need some numpy scalars
-# TODO(mattjj): handle constants in an indirected, less explicit way?
 pi = onp.pi
 e = onp.e
 inf = onp.inf
 nan = onp.nan
 
+# And some numpy utility functions
+set_printoptions = onp.set_printoptions
+
 # We want isinstance(x, np.ndarray) checks in user code to work with the our
 # array-like types, including DeviceArray and UnshapedArray (i.e. the abstract
 # array base class). We can override the isinstance behavior directly, without
@@ -1013,6 +1015,7 @@ def _argminmax(op, a, axis):
 
 
 def _not_implemented(fun):
+  @_wraps(fun)
   def wrapped(*args, **kwargs):
     raise Exception(""Numpy function {} not yet implemented"".format(fun))
   return wrapped",No
jax/test_util.py,jax/test_util.py,54bceee9e184039ebdc69f065b0550b5859fb737,579665b1b0ee8caa0f154d6677cd4b13be6adc3f,make num_generated_cases also settable by env var,"diff --git a/jax/test_util.py b/jax/test_util.py
index b0288d836..bed532c27 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import functools
 import re
 import itertools as it
+import os
 import random
 
 from absl.testing import absltest
@@ -45,7 +46,7 @@ flags.DEFINE_enum(
 
 flags.DEFINE_integer(
   'num_generated_cases',
-  100,
+  os.getenv('JAX_NUM_GENERATED_CASES', 100),
   help='Number of generated cases to test')
 
 EPS = 1e-4","diff --git a/jax/test_util.py b/jax/test_util.py
index b0288d836..bed532c27 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import functools
 import re
 import itertools as it
+import os
 import random
 
 from absl.testing import absltest
@@ -45,7 +46,7 @@ flags.DEFINE_enum(
 
 flags.DEFINE_integer(
   'num_generated_cases',
-  100,
+  os.getenv('JAX_NUM_GENERATED_CASES', 100),
   help='Number of generated cases to test')
 
 EPS = 1e-4",No
jax/lax.py,jax/lax.py,3561b432c25952d2d52ba56e3beeb064c5750fcb,579665b1b0ee8caa0f154d6677cd4b13be6adc3f,"Add Cholesky, QR, and Triangular solve implementations.

* Adds lax.{cholesky,triangular_solve,qr}. Adds a JVP for Cholesky.
* Adds a transpose rule for add_p, needed by the Cholesky JVP.
* Adds np.linalg.{cholesky,qr,dot,matmul,trace}.
* Adds scipy.linalg.{cholesky,qr,solve_triangular,tril,triu}.

Pair programmed with mattjj.","diff --git a/jax/lax.py b/jax/lax.py
index 42a927b18..4ec36c10d 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -358,7 +358,7 @@ def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
       window_strides=tuple(window_strides), padding=padding)
 
 def sort(operand, dimension=-1):
-  return sort_p.bind(operand, dimension=-1)
+  return sort_p.bind(operand, dimension=dimension)
 
 def sort_key_val(keys, values, dimension=-1):
   # TODO new sort_key_val is variadic
@@ -595,7 +595,6 @@ def atanh(x):
   return mul(_const(x, 0.5), log(div(add(_const(x, 1), x),
                                      sub(_const(x, 1), x))))
 
-
 # Add some methods to ShapedArray that rely on lax primitives
 
 ShapedArray.broadcast = core.aval_method(broadcast)
@@ -677,7 +676,7 @@ standard_unop = partial(unop, identity)
 _attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)
 
 
-def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
+def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals, **kwargs):
   aval_dtypes = [aval.dtype for aval in avals]
   for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
     if not any(onp.issubdtype(aval_dtype, t) for t in types):
@@ -752,7 +751,6 @@ _bool = {onp.bool_}
 _num = _int | _float | _complex
 _any = _int | _float | _complex | _bool
 
-
 neg_p = standard_unop(_num, 'neg')
 ad.deflinear(neg_p, lambda t: [neg(t)])
 batching.defvectorized(neg_p)
@@ -860,8 +858,13 @@ ad.defjvp_zero(or_p)
 xor_p = standard_binop([_any, _any], 'xor')
 ad.defjvp_zero(xor_p)
 
+def add_transpose(t, x, y):
+  assert x is None and y is None  # computation must be linear, not affine
+  return [t, t]
+
 add_p = standard_binop([_num, _num], 'add')
 ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))
+ad.primitive_transposes[add_p] = add_transpose
 
 sub_p = standard_binop([_num, _num], 'sub')
 ad.defjvp(sub_p,
@@ -873,7 +876,7 @@ ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO
 
 
 def div_transpose_rule(cotangent, x, y):
-  assert x is None
+  assert x is None and y is not None
   res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
   return res, None
 div_p = standard_binop([_num, _num], 'div')","diff --git a/jax/lax.py b/jax/lax.py
index 42a927b18..4ec36c10d 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -358,7 +358,7 @@ def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
       window_strides=tuple(window_strides), padding=padding)
 
 def sort(operand, dimension=-1):
-  return sort_p.bind(operand, dimension=-1)
+  return sort_p.bind(operand, dimension=dimension)
 
 def sort_key_val(keys, values, dimension=-1):
   # TODO new sort_key_val is variadic
@@ -595,7 +595,6 @@ def atanh(x):
   return mul(_const(x, 0.5), log(div(add(_const(x, 1), x),
                                      sub(_const(x, 1), x))))
 
-
 # Add some methods to ShapedArray that rely on lax primitives
 
 ShapedArray.broadcast = core.aval_method(broadcast)
@@ -677,7 +676,7 @@ standard_unop = partial(unop, identity)
 _attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)
 
 
-def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals):
+def binop_dtype_rule(result_dtype, accepted_dtypes, name, *avals, **kwargs):
   aval_dtypes = [aval.dtype for aval in avals]
   for i, (aval_dtype, types) in enumerate(zip(aval_dtypes, accepted_dtypes)):
     if not any(onp.issubdtype(aval_dtype, t) for t in types):
@@ -752,7 +751,6 @@ _bool = {onp.bool_}
 _num = _int | _float | _complex
 _any = _int | _float | _complex | _bool
 
-
 neg_p = standard_unop(_num, 'neg')
 ad.deflinear(neg_p, lambda t: [neg(t)])
 batching.defvectorized(neg_p)
@@ -860,8 +858,13 @@ ad.defjvp_zero(or_p)
 xor_p = standard_binop([_any, _any], 'xor')
 ad.defjvp_zero(xor_p)
 
+def add_transpose(t, x, y):
+  assert x is None and y is None  # computation must be linear, not affine
+  return [t, t]
+
 add_p = standard_binop([_num, _num], 'add')
 ad.defjvp(add_p, lambda g, x, y: _brcast(g, y), lambda g, x, y: _brcast(g, x))
+ad.primitive_transposes[add_p] = add_transpose
 
 sub_p = standard_binop([_num, _num], 'sub')
 ad.defjvp(sub_p,
@@ -873,7 +876,7 @@ ad.defbilinear_broadcasting(_brcast, mul_p, mul, mul)  # TODO
 
 
 def div_transpose_rule(cotangent, x, y):
-  assert x is None
+  assert x is None and y is not None
   res = ad_util.zero if cotangent is ad_util.zero else div(cotangent, y)
   return res, None
 div_p = standard_binop([_num, _num], 'div')",No
jax/numpy/linalg.py,jax/numpy/linalg.py,3561b432c25952d2d52ba56e3beeb064c5750fcb,579665b1b0ee8caa0f154d6677cd4b13be6adc3f,"Add Cholesky, QR, and Triangular solve implementations.

* Adds lax.{cholesky,triangular_solve,qr}. Adds a JVP for Cholesky.
* Adds a transpose rule for add_p, needed by the Cholesky JVP.
* Adds np.linalg.{cholesky,qr,dot,matmul,trace}.
* Adds scipy.linalg.{cholesky,qr,solve_triangular,tril,triu}.

Pair programmed with mattjj.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index f017b6d9a..01431a91d 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -16,11 +16,40 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+
 import numpy as onp
 
-from ..util import get_module_functions
+from .. import lax_linalg
 from .lax_numpy import _not_implemented
+from .lax_numpy import _wraps
 from .lax_numpy import IMPLEMENTED_FUNCS
+from . import lax_numpy as np
+from ..util import get_module_functions
+
+
+dot = np.dot
+matmul = np.matmul
+trace = np.trace
+
+
+@_wraps(onp.linalg.cholesky)
+def cholesky(a):
+  return lax_linalg.cholesky(a)
+
+
+@_wraps(onp.linalg.qr)
+def qr(a, mode=""reduced""):
+  if mode in (""reduced"", ""r"", ""full""):
+    full_matrices = False
+  elif mode == ""complete"":
+    full_matrices = True
+  else:
+    raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
+  q, r = lax_linalg.qr(a, full_matrices)
+  if mode == ""r"":
+    return r
+  return q, r
+
 
 UNIMPLEMENTED_FUNCS = get_module_functions(onp.linalg) - set(IMPLEMENTED_FUNCS)
 for func in UNIMPLEMENTED_FUNCS:","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index f017b6d9a..01431a91d 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -16,11 +16,40 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+
 import numpy as onp
 
-from ..util import get_module_functions
+from .. import lax_linalg
 from .lax_numpy import _not_implemented
+from .lax_numpy import _wraps
 from .lax_numpy import IMPLEMENTED_FUNCS
+from . import lax_numpy as np
+from ..util import get_module_functions
+
+
+dot = np.dot
+matmul = np.matmul
+trace = np.trace
+
+
+@_wraps(onp.linalg.cholesky)
+def cholesky(a):
+  return lax_linalg.cholesky(a)
+
+
+@_wraps(onp.linalg.qr)
+def qr(a, mode=""reduced""):
+  if mode in (""reduced"", ""r"", ""full""):
+    full_matrices = False
+  elif mode == ""complete"":
+    full_matrices = True
+  else:
+    raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
+  q, r = lax_linalg.qr(a, full_matrices)
+  if mode == ""r"":
+    return r
+  return q, r
+
 
 UNIMPLEMENTED_FUNCS = get_module_functions(onp.linalg) - set(IMPLEMENTED_FUNCS)
 for func in UNIMPLEMENTED_FUNCS:",No
jax/scipy/__init__.py,jax/scipy/__init__.py,3561b432c25952d2d52ba56e3beeb064c5750fcb,579665b1b0ee8caa0f154d6677cd4b13be6adc3f,"Add Cholesky, QR, and Triangular solve implementations.

* Adds lax.{cholesky,triangular_solve,qr}. Adds a JVP for Cholesky.
* Adds a transpose rule for add_p, needed by the Cholesky JVP.
* Adds np.linalg.{cholesky,qr,dot,matmul,trace}.
* Adds scipy.linalg.{cholesky,qr,solve_triangular,tril,triu}.

Pair programmed with mattjj.","diff --git a/jax/scipy/__init__.py b/jax/scipy/__init__.py
index 210eb4594..6452512bd 100644
--- a/jax/scipy/__init__.py
+++ b/jax/scipy/__init__.py
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from . import linalg
 from . import misc
 from . import special
 from . import stats","diff --git a/jax/scipy/__init__.py b/jax/scipy/__init__.py
index 210eb4594..6452512bd 100644
--- a/jax/scipy/__init__.py
+++ b/jax/scipy/__init__.py
@@ -13,6 +13,7 @@
 # limitations under the License.
 
 from __future__ import absolute_import
+from . import linalg
 from . import misc
 from . import special
 from . import stats",No
tests/BUILD,tests/BUILD,3561b432c25952d2d52ba56e3beeb064c5750fcb,579665b1b0ee8caa0f154d6677cd4b13be6adc3f,"Add Cholesky, QR, and Triangular solve implementations.

* Adds lax.{cholesky,triangular_solve,qr}. Adds a JVP for Cholesky.
* Adds a transpose rule for add_p, needed by the Cholesky JVP.
* Adds np.linalg.{cholesky,qr,dot,matmul,trace}.
* Adds scipy.linalg.{cholesky,qr,solve_triangular,tril,triu}.

Pair programmed with mattjj.","diff --git a/tests/BUILD b/tests/BUILD
index b94447eb2..e523c1619 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -58,6 +58,14 @@ jax_test(
     },
 )
 
+jax_test(
+    name = ""linalg_test"",
+    srcs = [""linalg_test.py""],
+    shard_count = {
+        ""cpu"": 2,
+    },
+)
+
 jax_test(
     name = ""lax_scipy_test"",
     srcs = [""lax_scipy_test.py""],","diff --git a/tests/BUILD b/tests/BUILD
index b94447eb2..e523c1619 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -58,6 +58,14 @@ jax_test(
     },
 )
 
+jax_test(
+    name = ""linalg_test"",
+    srcs = [""linalg_test.py""],
+    shard_count = {
+        ""cpu"": 2,
+    },
+)
+
 jax_test(
     name = ""lax_scipy_test"",
     srcs = [""lax_scipy_test.py""],",No
jax/config.py,jax/config.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/config.py b/jax/config.py
index 6f5bb11c9..f0bb94888 100644
--- a/jax/config.py
+++ b/jax/config.py
@@ -78,9 +78,13 @@ class Config(object):
       self.update(name, getattr(absl_flags.FLAGS, name))
 
   def parse_flags_with_absl(self):
-    import absl.flags
-    self.config_with_absl()
-    absl.flags.FLAGS(sys.argv)
+    global already_configured_with_absl
+    if not already_configured_with_absl:
+      import absl.flags
+      self.config_with_absl()
+      absl.flags.FLAGS(sys.argv)
+      already_configured_with_absl = True
+
 
 class NameSpace(object):
   def __init__(self, getter):
@@ -92,3 +96,4 @@ class NameSpace(object):
 
 config = Config()
 flags = config
+already_configured_with_absl = False","diff --git a/jax/config.py b/jax/config.py
index 6f5bb11c9..f0bb94888 100644
--- a/jax/config.py
+++ b/jax/config.py
@@ -78,9 +78,13 @@ class Config(object):
       self.update(name, getattr(absl_flags.FLAGS, name))
 
   def parse_flags_with_absl(self):
-    import absl.flags
-    self.config_with_absl()
-    absl.flags.FLAGS(sys.argv)
+    global already_configured_with_absl
+    if not already_configured_with_absl:
+      import absl.flags
+      self.config_with_absl()
+      absl.flags.FLAGS(sys.argv)
+      already_configured_with_absl = True
+
 
 class NameSpace(object):
   def __init__(self, getter):
@@ -92,3 +96,4 @@ class NameSpace(object):
 
 config = Config()
 flags = config
+already_configured_with_absl = False",No
jax/experimental/stax.py,jax/experimental/stax.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 99546da16..eab5c2c1d 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -49,7 +49,7 @@ def logsoftmax(x, axis=-1):
 
 def fastvar(x, axis, keepdims):
   """"""A fast but less numerically-stable variance calculation than np.var.""""""
-  return np.mean(x**2, axis, keepdims) - np.mean(x, axis, keepdims)**2
+  return np.mean(x**2, axis, keepdims=keepdims) - np.mean(x, axis, keepdims=keepdims)**2
 
 
 # Initializers","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 99546da16..eab5c2c1d 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -49,7 +49,7 @@ def logsoftmax(x, axis=-1):
 
 def fastvar(x, axis, keepdims):
   """"""A fast but less numerically-stable variance calculation than np.var.""""""
-  return np.mean(x**2, axis, keepdims) - np.mean(x, axis, keepdims)**2
+  return np.mean(x**2, axis, keepdims=keepdims) - np.mean(x, axis, keepdims=keepdims)**2
 
 
 # Initializers",No
jax/numpy/fft.py,jax/numpy/fft.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/numpy/fft.py b/jax/numpy/fft.py
index 7a3fc2056..a03dd9ac2 100644
--- a/jax/numpy/fft.py
+++ b/jax/numpy/fft.py
@@ -20,9 +20,7 @@ import numpy as onp
 
 from ..util import get_module_functions
 from .lax_numpy import _not_implemented
-from .lax_numpy import IMPLEMENTED_FUNCS
 
-UNIMPLEMENTED_FUNCS = get_module_functions(onp.fft) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
+for func in get_module_functions(onp.fft):
   if func.__name__ not in globals():
     globals()[func.__name__] = _not_implemented(func)","diff --git a/jax/numpy/fft.py b/jax/numpy/fft.py
index 7a3fc2056..a03dd9ac2 100644
--- a/jax/numpy/fft.py
+++ b/jax/numpy/fft.py
@@ -20,9 +20,7 @@ import numpy as onp
 
 from ..util import get_module_functions
 from .lax_numpy import _not_implemented
-from .lax_numpy import IMPLEMENTED_FUNCS
 
-UNIMPLEMENTED_FUNCS = get_module_functions(onp.fft) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
+for func in get_module_functions(onp.fft):
   if func.__name__ not in globals():
     globals()[func.__name__] = _not_implemented(func)",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 0b2d468a5..b15448927 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -180,9 +180,6 @@ def _promote_args_like(op, *args):
 def _constant_like(x, const):
   return onp.array(const, dtype=_dtype(x))
 
-# A dictionary mapping implemented funcs to their lax equivalent.
-IMPLEMENTED_FUNCS = {}
-
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   docstr = """"""
@@ -195,7 +192,6 @@ def _wraps(fun):
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
-      IMPLEMENTED_FUNCS[fun] = op
       return op
   return wrap
 
@@ -1013,19 +1009,6 @@ def _argminmax(op, a, axis):
   mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
   return min(mask_idxs, axis)
 
-
-def _not_implemented(fun):
-  @_wraps(fun)
-  def wrapped(*args, **kwargs):
-    raise Exception(""Numpy function {} not yet implemented"".format(fun))
-  return wrapped
-
-# Build a set of all unimplemented NumPy functions.
-UNIMPLEMENTED_FUNCS = get_module_functions(onp) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
-  if func.__name__ not in globals():
-    globals()[func.__name__] = _not_implemented(func)
-
 ### Indexing
 
 
@@ -1198,6 +1181,20 @@ def _static_idx(idx, size):
     return stop_inclusive, end, -step, True
 
 
+### track unimplemented functions
+
+def _not_implemented(fun):
+  @_wraps(fun)
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
+
+# Build a set of all unimplemented NumPy functions.
+for func in get_module_functions(onp):
+  if func.__name__ not in globals():
+    globals()[func.__name__] = _not_implemented(func)
+
+
 ### add method and operator overloads to arraylike classes
 
 # We add operator overloads to DeviceArray and ShapedArray. These method and","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 0b2d468a5..b15448927 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -180,9 +180,6 @@ def _promote_args_like(op, *args):
 def _constant_like(x, const):
   return onp.array(const, dtype=_dtype(x))
 
-# A dictionary mapping implemented funcs to their lax equivalent.
-IMPLEMENTED_FUNCS = {}
-
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   docstr = """"""
@@ -195,7 +192,6 @@ def _wraps(fun):
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
-      IMPLEMENTED_FUNCS[fun] = op
       return op
   return wrap
 
@@ -1013,19 +1009,6 @@ def _argminmax(op, a, axis):
   mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
   return min(mask_idxs, axis)
 
-
-def _not_implemented(fun):
-  @_wraps(fun)
-  def wrapped(*args, **kwargs):
-    raise Exception(""Numpy function {} not yet implemented"".format(fun))
-  return wrapped
-
-# Build a set of all unimplemented NumPy functions.
-UNIMPLEMENTED_FUNCS = get_module_functions(onp) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
-  if func.__name__ not in globals():
-    globals()[func.__name__] = _not_implemented(func)
-
 ### Indexing
 
 
@@ -1198,6 +1181,20 @@ def _static_idx(idx, size):
     return stop_inclusive, end, -step, True
 
 
+### track unimplemented functions
+
+def _not_implemented(fun):
+  @_wraps(fun)
+  def wrapped(*args, **kwargs):
+    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+  return wrapped
+
+# Build a set of all unimplemented NumPy functions.
+for func in get_module_functions(onp):
+  if func.__name__ not in globals():
+    globals()[func.__name__] = _not_implemented(func)
+
+
 ### add method and operator overloads to arraylike classes
 
 # We add operator overloads to DeviceArray and ShapedArray. These method and",No
jax/numpy/linalg.py,jax/numpy/linalg.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index f017b6d9a..2b819a54a 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -20,9 +20,7 @@ import numpy as onp
 
 from ..util import get_module_functions
 from .lax_numpy import _not_implemented
-from .lax_numpy import IMPLEMENTED_FUNCS
 
-UNIMPLEMENTED_FUNCS = get_module_functions(onp.linalg) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
+for func in get_module_functions(onp.linalg):
   if func.__name__ not in globals():
     globals()[func.__name__] = _not_implemented(func)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index f017b6d9a..2b819a54a 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -20,9 +20,7 @@ import numpy as onp
 
 from ..util import get_module_functions
 from .lax_numpy import _not_implemented
-from .lax_numpy import IMPLEMENTED_FUNCS
 
-UNIMPLEMENTED_FUNCS = get_module_functions(onp.linalg) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
+for func in get_module_functions(onp.linalg):
   if func.__name__ not in globals():
     globals()[func.__name__] = _not_implemented(func)",No
jax/random.py,jax/random.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/random.py b/jax/random.py
index 26c8c10c9..d2fcd1837 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -27,9 +27,6 @@ from . import numpy as np
 from . import tree_util
 from .api import jit
 from jax.lib import xla_bridge
-from .util import get_module_functions
-from .numpy.lax_numpy import _not_implemented
-from .numpy.lax_numpy import IMPLEMENTED_FUNCS
 
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
@@ -361,10 +358,3 @@ def bernoulli(key, mean=onp.float32(0.5), shape=()):
   if onp.shape(mean) != shape:
     mean = lax.broadcast(mean, shape)
   return lax.lt(uniform(key, shape), mean)
-
-# TODO(alexbw): np.random.random is an alias of random_sample, and
-# doesn't show up after a call to `dir(module)`.
-UNIMPLEMENTED_FUNCS = get_module_functions(onp.random) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
-  if func.__name__ not in globals():
-    globals()[func.__name__] = _not_implemented(func)","diff --git a/jax/random.py b/jax/random.py
index 26c8c10c9..d2fcd1837 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -27,9 +27,6 @@ from . import numpy as np
 from . import tree_util
 from .api import jit
 from jax.lib import xla_bridge
-from .util import get_module_functions
-from .numpy.lax_numpy import _not_implemented
-from .numpy.lax_numpy import IMPLEMENTED_FUNCS
 
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
@@ -361,10 +358,3 @@ def bernoulli(key, mean=onp.float32(0.5), shape=()):
   if onp.shape(mean) != shape:
     mean = lax.broadcast(mean, shape)
   return lax.lt(uniform(key, shape), mean)
-
-# TODO(alexbw): np.random.random is an alias of random_sample, and
-# doesn't show up after a call to `dir(module)`.
-UNIMPLEMENTED_FUNCS = get_module_functions(onp.random) - set(IMPLEMENTED_FUNCS)
-for func in UNIMPLEMENTED_FUNCS:
-  if func.__name__ not in globals():
-    globals()[func.__name__] = _not_implemented(func)",No
jax/scipy/special.py,jax/scipy/special.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/jax/scipy/special.py b/jax/scipy/special.py
index 66cd9defa..30f4b50ff 100644
--- a/jax/scipy/special.py
+++ b/jax/scipy/special.py
@@ -22,8 +22,9 @@ from .. import lax
 from ..numpy.lax_numpy import _wraps
 
 
-gammaln = _wraps(osp_special.gammaln)(lax.lgamma)
-digamma = _wraps(osp_special.digamma)(lax.digamma)
-erf = _wraps(osp_special.erf)(lax.erf)
-erfc = _wraps(osp_special.erfc)(lax.erfc)
-erfinv = _wraps(osp_special.erfinv)(lax.erf_inv)
+# need to create new functions because _wraps sets the __name__ attribute
+gammaln = _wraps(osp_special.gammaln)(lambda x: lax.lgamma(x))
+digamma = _wraps(osp_special.digamma)(lambda x: lax.digamma(x))
+erf = _wraps(osp_special.erf)(lambda x: lax.erf(x))
+erfc = _wraps(osp_special.erfc)(lambda x: lax.erfc(x))
+erfinv = _wraps(osp_special.erfinv)(lambda x: lax.erf_inv(x))","diff --git a/jax/scipy/special.py b/jax/scipy/special.py
index 66cd9defa..30f4b50ff 100644
--- a/jax/scipy/special.py
+++ b/jax/scipy/special.py
@@ -22,8 +22,9 @@ from .. import lax
 from ..numpy.lax_numpy import _wraps
 
 
-gammaln = _wraps(osp_special.gammaln)(lax.lgamma)
-digamma = _wraps(osp_special.digamma)(lax.digamma)
-erf = _wraps(osp_special.erf)(lax.erf)
-erfc = _wraps(osp_special.erfc)(lax.erfc)
-erfinv = _wraps(osp_special.erfinv)(lax.erf_inv)
+# need to create new functions because _wraps sets the __name__ attribute
+gammaln = _wraps(osp_special.gammaln)(lambda x: lax.lgamma(x))
+digamma = _wraps(osp_special.digamma)(lambda x: lax.digamma(x))
+erf = _wraps(osp_special.erf)(lambda x: lax.erf(x))
+erfc = _wraps(osp_special.erfc)(lambda x: lax.erfc(x))
+erfinv = _wraps(osp_special.erfinv)(lambda x: lax.erf_inv(x))",No
tests/core_test.py,tests/core_test.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/tests/core_test.py b/tests/core_test.py
index 8de1e880a..024b00268 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 import operator
 from collections import namedtuple
+from unittest import skip
 
 import numpy as onp
 from absl.testing import absltest
@@ -152,7 +153,8 @@ def check_trace_eval(f, pvals, vals, expected_out_pval):
 
 class CoreTest(jtu.JaxTestCase):
 
-  def DISABLED_test_pack_unpack(self):
+  @skip
+  def test_pack_unpack(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
     y = onp.array(1.0)
     def foo(x):
@@ -162,7 +164,8 @@ class CoreTest(jtu.JaxTestCase):
 
     pe.trace_to_jaxpr(foo, (_,))
 
-  def DISABLED_test_tup_add(self):
+  @skip
+  def test_tup_add(self):
     # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
     y = onp.array(1.0)
     def foo(x):
@@ -184,7 +187,8 @@ class CoreTest(jtu.JaxTestCase):
     except TypeError:
       pass
 
-  def DIABLED_test_print_jaxpr_compound(self):
+  @skip
+  def test_print_jaxpr_compound(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
     pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
     print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
@@ -226,7 +230,8 @@ class CoreTest(jtu.JaxTestCase):
 
     api.trace_to_jaxpr(foo, (__,))
 
-  def DISABLED_test_nested_grad(self):
+  @skip
+  def test_nested_grad(self):
     def foo(x):
       print(type(x), x)
       def bar(y):","diff --git a/tests/core_test.py b/tests/core_test.py
index 8de1e880a..024b00268 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 import operator
 from collections import namedtuple
+from unittest import skip
 
 import numpy as onp
 from absl.testing import absltest
@@ -152,7 +153,8 @@ def check_trace_eval(f, pvals, vals, expected_out_pval):
 
 class CoreTest(jtu.JaxTestCase):
 
-  def DISABLED_test_pack_unpack(self):
+  @skip
+  def test_pack_unpack(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
     y = onp.array(1.0)
     def foo(x):
@@ -162,7 +164,8 @@ class CoreTest(jtu.JaxTestCase):
 
     pe.trace_to_jaxpr(foo, (_,))
 
-  def DISABLED_test_tup_add(self):
+  @skip
+  def test_tup_add(self):
     # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
     y = onp.array(1.0)
     def foo(x):
@@ -184,7 +187,8 @@ class CoreTest(jtu.JaxTestCase):
     except TypeError:
       pass
 
-  def DIABLED_test_print_jaxpr_compound(self):
+  @skip
+  def test_print_jaxpr_compound(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
     pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
     print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
@@ -226,7 +230,8 @@ class CoreTest(jtu.JaxTestCase):
 
     api.trace_to_jaxpr(foo, (__,))
 
-  def DISABLED_test_nested_grad(self):
+  @skip
+  def test_nested_grad(self):
     def foo(x):
       print(type(x), x)
       def bar(y):",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 8944d5afe..c6f034ed0 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import collections
 from functools import partial
 import itertools
+from unittest import skip
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -322,6 +323,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
+  @skip
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 8944d5afe..c6f034ed0 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import collections
 from functools import partial
 import itertools
+from unittest import skip
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -322,6 +323,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
+  @skip
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 83a35af5c..996d2c846 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import collections
 import functools
 import itertools
+from unittest import skip
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -647,7 +648,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self.assertFalse(a3)
 
   @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
-  def DISABLED_testOnesBroadcastingConstantHandler(self):
+  @skip
+  def testOnesBroadcastingConstantHandler(self):
     # TODO(mattjj): update this test for jax3
 
     def fun(x):
@@ -727,7 +729,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
     self.assertRaises(TypeError, lambda: g(3.))
 
-  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
+  @skip
+  def testTracingPrimitiveWithNoTranslationErrorMessage(self):
     # TODO(mattjj): update this for jax3
     foo = lnp._not_implemented(lambda x: x)
 
@@ -774,7 +777,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   # TODO(mattjj): test infix operator overrides
 
-  def DISABLED_testRavel(self):
+  @skip
+  def testRavel(self):
     # TODO(mattjj): support this method-based syntax?
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 83a35af5c..996d2c846 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import collections
 import functools
 import itertools
+from unittest import skip
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -647,7 +648,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self.assertFalse(a3)
 
   @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
-  def DISABLED_testOnesBroadcastingConstantHandler(self):
+  @skip
+  def testOnesBroadcastingConstantHandler(self):
     # TODO(mattjj): update this test for jax3
 
     def fun(x):
@@ -727,7 +729,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
     self.assertRaises(TypeError, lambda: g(3.))
 
-  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
+  @skip
+  def testTracingPrimitiveWithNoTranslationErrorMessage(self):
     # TODO(mattjj): update this for jax3
     foo = lnp._not_implemented(lambda x: x)
 
@@ -774,7 +777,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   # TODO(mattjj): test infix operator overrides
 
-  def DISABLED_testRavel(self):
+  @skip
+  def testRavel(self):
     # TODO(mattjj): support this method-based syntax?
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]",No
tests/lax_test.py,tests/lax_test.py,a28501711018bac59791797ad6f1404c1656e294,54bceee9e184039ebdc69f065b0550b5859fb737,fix failing tests (misc small bugs),"diff --git a/tests/lax_test.py b/tests/lax_test.py
index 06346cfcb..17fa7f554 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -20,6 +20,7 @@ import collections
 import functools
 from functools import partial
 import itertools
+from unittest import skip
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -365,6 +366,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
+  @skip
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}"".format(","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 06346cfcb..17fa7f554 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -20,6 +20,7 @@ import collections
 import functools
 from functools import partial
 import itertools
+from unittest import skip
 
 from absl.testing import absltest
 from absl.testing import parameterized
@@ -365,6 +366,7 @@ class LaxTest(jtu.JaxTestCase):
 
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
+  @skip
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
        ""_lhs_dilation={}_rhs_dilation={}"".format(",No
jax/test_util.py,jax/test_util.py,bf5a30cdbd4df455b9a4a11b93ebb147cc567bba,a28501711018bac59791797ad6f1404c1656e294,fix warning,"diff --git a/jax/test_util.py b/jax/test_util.py
index bed532c27..6831da56f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -293,7 +293,7 @@ def rand_some_inf():
 
   def rand(shape, dtype):
     """"""The random sampler function.""""""
-    if not onp.issubdtype(dtype, onp.float):
+    if not onp.issubdtype(dtype, onp.floating):
       # only float types have inf
       return base_rand(shape, dtype)
 ","diff --git a/jax/test_util.py b/jax/test_util.py
index bed532c27..6831da56f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -293,7 +293,7 @@ def rand_some_inf():
 
   def rand(shape, dtype):
     """"""The random sampler function.""""""
-    if not onp.issubdtype(dtype, onp.float):
+    if not onp.issubdtype(dtype, onp.floating):
       # only float types have inf
       return base_rand(shape, dtype)
 ",No
WORKSPACE,WORKSPACE,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,f88b799e57095507807e23c9ff5c8446b3432496,Update XLA version in JAX to pick up fix for Mac OS __bzero intrinsic crash.,"diff --git a/WORKSPACE b/WORKSPACE
index 56cbad6fc..ddacdf819 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""413b2a398c51d5764eb7fb7ce12ce9b095c42de4287ee26e24e13f8b68892f9d"",
-    strip_prefix = ""tensorflow-25337d2065bd3ef79b9018714c0cb5af46ca06dc"",
+    sha256 = ""346f7d3a4248cfb4c37d5c8eed168c3cad35c54c4da6d4bf36294a144c6953c0"",
+    strip_prefix = ""tensorflow-27cffd795981f6d86e3b09b6d82c384d8b4e117a"",
     urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/25337d2065bd3ef79b9018714c0cb5af46ca06dc.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/27cffd795981f6d86e3b09b6d82c384d8b4e117a.tar.gz"",
     ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 56cbad6fc..ddacdf819 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
     name = ""org_tensorflow"",
-    sha256 = ""413b2a398c51d5764eb7fb7ce12ce9b095c42de4287ee26e24e13f8b68892f9d"",
-    strip_prefix = ""tensorflow-25337d2065bd3ef79b9018714c0cb5af46ca06dc"",
+    sha256 = ""346f7d3a4248cfb4c37d5c8eed168c3cad35c54c4da6d4bf36294a144c6953c0"",
+    strip_prefix = ""tensorflow-27cffd795981f6d86e3b09b6d82c384d8b4e117a"",
     urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/25337d2065bd3ef79b9018714c0cb5af46ca06dc.tar.gz"",
+        ""https://github.com/tensorflow/tensorflow/archive/27cffd795981f6d86e3b09b6d82c384d8b4e117a.tar.gz"",
     ],
 )
 ",No
examples/kernel_lsq.py,examples/kernel_lsq.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/examples/kernel_lsq.py b/examples/kernel_lsq.py
index 9f2e741f3..4d5c6e083 100644
--- a/examples/kernel_lsq.py
+++ b/examples/kernel_lsq.py
@@ -18,6 +18,8 @@ from __future__ import print_function
 
 from functools import partial
 
+from six.moves import xrange
+
 import numpy.random as npr
 
 import jax.numpy as np","diff --git a/examples/kernel_lsq.py b/examples/kernel_lsq.py
index 9f2e741f3..4d5c6e083 100644
--- a/examples/kernel_lsq.py
+++ b/examples/kernel_lsq.py
@@ -18,6 +18,8 @@ from __future__ import print_function
 
 from functools import partial
 
+from six.moves import xrange
+
 import numpy.random as npr
 
 import jax.numpy as np",No
jax/abstract_arrays.py,jax/abstract_arrays.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 7e2c4df33..4348c025f 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -57,7 +57,7 @@ class UnshapedArray(core.AbstractValue):
   _float   = concretization_function_error(float)
   _int     = concretization_function_error(int)
   if six.PY2:
-    _long    = concretization_function_error(long)
+    _long    = concretization_function_error(long)  # noqa: F821
   _complex = concretization_function_error(complex)
   _hex     = concretization_function_error(hex)
   _oct     = concretization_function_error(oct)","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 7e2c4df33..4348c025f 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -57,7 +57,7 @@ class UnshapedArray(core.AbstractValue):
   _float   = concretization_function_error(float)
   _int     = concretization_function_error(int)
   if six.PY2:
-    _long    = concretization_function_error(long)
+    _long    = concretization_function_error(long)  # noqa: F821
   _complex = concretization_function_error(complex)
   _hex     = concretization_function_error(hex)
   _oct     = concretization_function_error(oct)",No
jax/core.py,jax/core.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/jax/core.py b/jax/core.py
index 827ed0cda..2685d0deb 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -160,7 +160,6 @@ class Trace(object):
                         .format(val.trace, (level, sublevel)))
       return self.lift(val)
     elif val.trace.level > level:
-      print_trace_stack()
       raise Exception(""Can't lift {} to {}"".format(val, self))
     elif val.trace.level == self.level:
       raise Exception(""Different traces at same level: {}, {}"".format(val, self))
@@ -368,7 +367,6 @@ def new_sublevel():
     t = ref(sublevel)
     del sublevel
     if t() is not None:
-      print_trace_stack()
       raise Exception('Leaked sublevel {}'.format(t()))
 
 # -------------------- abstract values --------------------","diff --git a/jax/core.py b/jax/core.py
index 827ed0cda..2685d0deb 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -160,7 +160,6 @@ class Trace(object):
                         .format(val.trace, (level, sublevel)))
       return self.lift(val)
     elif val.trace.level > level:
-      print_trace_stack()
       raise Exception(""Can't lift {} to {}"".format(val, self))
     elif val.trace.level == self.level:
       raise Exception(""Different traces at same level: {}, {}"".format(val, self))
@@ -368,7 +367,6 @@ def new_sublevel():
     t = ref(sublevel)
     del sublevel
     if t() is not None:
-      print_trace_stack()
       raise Exception('Leaked sublevel {}'.format(t()))
 
 # -------------------- abstract values --------------------",No
jax/interpreters/xla.py,jax/interpreters/xla.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 296b4757e..6bdf01d4e 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -291,7 +291,7 @@ class DeviceArray(DeviceValue):
   __float__ = partialmethod(forward_to_value, float)
   __int__ = partialmethod(forward_to_value, int)
   if six.PY2:
-    __long__ = partialmethod(forward_to_value, long)
+    __long__ = partialmethod(forward_to_value, long)  # noqa: F821
   __complex__ = partialmethod(forward_to_value, complex)
   __hex__ = partialmethod(forward_to_value, hex)
   __oct__ = partialmethod(forward_to_value, oct)","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 296b4757e..6bdf01d4e 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -291,7 +291,7 @@ class DeviceArray(DeviceValue):
   __float__ = partialmethod(forward_to_value, float)
   __int__ = partialmethod(forward_to_value, int)
   if six.PY2:
-    __long__ = partialmethod(forward_to_value, long)
+    __long__ = partialmethod(forward_to_value, long)  # noqa: F821
   __complex__ = partialmethod(forward_to_value, complex)
   __hex__ = partialmethod(forward_to_value, hex)
   __oct__ = partialmethod(forward_to_value, oct)",No
jax/lax.py,jax/lax.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/jax/lax.py b/jax/lax.py
index 4ec36c10d..00d3f480f 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2013,7 +2013,7 @@ def select_and_scatter_shape_rule(
     raise TypeError(msg.format(window_strides, window_dimensions))
   return operand.shape
 
-def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
+def select_and_scatter_translation(c, operand, source, init_value, select_jaxpr,
                                    select_consts, scatter_jaxpr, scatter_consts,
                                    window_dimensions, window_strides, padding):
   select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
@@ -2165,6 +2165,9 @@ xla.translations[while_p] = while_loop_translation_rule
 
 ### util
 
+def _ndim(x):
+  return x.ndim
+
 
 def _dilate_shape(shape, dilation):
   """"""Utility function for computing the shape resulting from a dilation.""""""","diff --git a/jax/lax.py b/jax/lax.py
index 4ec36c10d..00d3f480f 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2013,7 +2013,7 @@ def select_and_scatter_shape_rule(
     raise TypeError(msg.format(window_strides, window_dimensions))
   return operand.shape
 
-def select_and_scatter_translation(operand, source, init_value, select_jaxpr,
+def select_and_scatter_translation(c, operand, source, init_value, select_jaxpr,
                                    select_consts, scatter_jaxpr, scatter_consts,
                                    window_dimensions, window_strides, padding):
   select = _reduction_computation(c, select_jaxpr, select_consts, init_value)
@@ -2165,6 +2165,9 @@ xla.translations[while_p] = while_loop_translation_rule
 
 ### util
 
+def _ndim(x):
+  return x.ndim
+
 
 def _dilate_shape(shape, dilation):
   """"""Utility function for computing the shape resulting from a dilation.""""""",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 0b2d468a5..80e30e0dd 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -440,7 +440,7 @@ def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
   elif order == ""F"":
     dims = onp.arange(ndim(a))[::-1]
   elif order == ""A"":
-    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
+    raise NotImplementedError(""np.reshape order=A is not implemented."")
   else:
     raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 0b2d468a5..80e30e0dd 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -440,7 +440,7 @@ def reshape(a, newshape, order=""C""):  # pylint: disable=missing-docstring
   elif order == ""F"":
     dims = onp.arange(ndim(a))[::-1]
   elif order == ""A"":
-    dims = onp.arange(ndim(a))[::-1] if isfortran(a) else onp.arange(ndim(a))
+    raise NotImplementedError(""np.reshape order=A is not implemented."")
   else:
     raise ValueError(""Unexpected value for 'order' argument: {}."".format(order))
 ",No
tests/minmax_test.py,tests/minmax_test.py,0d4eb6c1e1916305b99fb48c8a22920becf92f47,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,"Make JAX flake8-clean.

Fixes #1.","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index f15c4091f..ab8b6abc1 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -43,22 +43,22 @@ class OptimizerTests(jtu.JaxTestCase):
   @jtu.skip_on_devices('gpu')
   def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):
     return # TODO(mattjj): bring back fax!
-    num_repl = xla.get_replica_count()
-    infeeder = fax.make_infeed_from_sequence(
-        [np.ones(1, dtype='float32')] * num_steps * num_repl,
-        with_pyvals=True)
-
-    def op(infeed, x0):
-      opt_init, opt_update = optimizer(*args, **kwargs)
-      return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
-    cop = jit(op)
-
-    a1, _ = op(infeeder(), x0)
-    a2, _ = cop(infeeder(), x0)
-
-    assert loss(a1, None) < 1e-3
-    assert loss(a2, None) < 1e-3
-    self.assertAllClose(a1, a2, check_dtypes=False)
+    # num_repl = xla.get_replica_count()
+    # infeeder = fax.make_infeed_from_sequence(
+    #     [np.ones(1, dtype='float32')] * num_steps * num_repl,
+    #     with_pyvals=True)
+
+    # def op(infeed, x0):
+    #   opt_init, opt_update = optimizer(*args, **kwargs)
+    #   return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
+    # cop = jit(op)
+
+    # a1, _ = op(infeeder(), x0)
+    # a2, _ = cop(infeeder(), x0)
+
+    # assert loss(a1, None) < 1e-3
+    # assert loss(a2, None) < 1e-3
+    # self.assertAllClose(a1, a2, check_dtypes=False)
 
   def testSgdScalar(self):
     def loss(x, _): return x**2","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index f15c4091f..ab8b6abc1 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -43,22 +43,22 @@ class OptimizerTests(jtu.JaxTestCase):
   @jtu.skip_on_devices('gpu')
   def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):
     return # TODO(mattjj): bring back fax!
-    num_repl = xla.get_replica_count()
-    infeeder = fax.make_infeed_from_sequence(
-        [np.ones(1, dtype='float32')] * num_steps * num_repl,
-        with_pyvals=True)
+    # num_repl = xla.get_replica_count()
+    # infeeder = fax.make_infeed_from_sequence(
+    #     [np.ones(1, dtype='float32')] * num_steps * num_repl,
+    #     with_pyvals=True)
 
-    def op(infeed, x0):
-      opt_init, opt_update = optimizer(*args, **kwargs)
-      return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
-    cop = jit(op)
+    # def op(infeed, x0):
+    #   opt_init, opt_update = optimizer(*args, **kwargs)
+    #   return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
+    # cop = jit(op)
 
-    a1, _ = op(infeeder(), x0)
-    a2, _ = cop(infeeder(), x0)
+    # a1, _ = op(infeeder(), x0)
+    # a2, _ = cop(infeeder(), x0)
 
-    assert loss(a1, None) < 1e-3
-    assert loss(a2, None) < 1e-3
-    self.assertAllClose(a1, a2, check_dtypes=False)
+    # assert loss(a1, None) < 1e-3
+    # assert loss(a2, None) < 1e-3
+    # self.assertAllClose(a1, a2, check_dtypes=False)
 
   def testSgdScalar(self):
     def loss(x, _): return x**2",Yes
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,a1ba39adf1cb3b015d730ce3bcefd35f2cbc3159,1eccd744f1aaabefff422e64f88aa1a463e7a6e3,Replace instance of go/jax with GitHub URL.,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 5a8b08d0c..62d805f67 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -61,7 +61,7 @@
         ""\n"",
         ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
         ""\n"",
-        ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
+        ""#### [JAX](https://github.com/google/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
         ""\n"",
         ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
         ""can automatically differentiate native Python and NumPy code. It can\n"",","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 5a8b08d0c..62d805f67 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -61,7 +61,7 @@
         ""\n"",
         ""![](https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png)\n"",
         ""\n"",
-        ""#### [JAX](http://go/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
+        ""#### [JAX](https://github.com/google/jax) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n"",
         ""\n"",
         ""With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n"",
         ""can automatically differentiate native Python and NumPy code. It can\n"",",No
examples/BUILD,examples/BUILD,b6a07acc91b8e5777a3337d8692f560facfb6a71,a1ba39adf1cb3b015d730ce3bcefd35f2cbc3159,Mark examples as public visibility in BUILD file.,"diff --git a/examples/BUILD b/examples/BUILD
index 7742ceac1..145b14e7f 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -14,6 +14,8 @@
 
 licenses([""notice""])  # Apache 2
 
+package(default_visibility = [""//visibility:public""])
+
 py_library(
     name = ""datasets"",
     srcs = [""datasets.py""],","diff --git a/examples/BUILD b/examples/BUILD
index 7742ceac1..145b14e7f 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -14,6 +14,8 @@
 
 licenses([""notice""])  # Apache 2
 
+package(default_visibility = [""//visibility:public""])
+
 py_library(
     name = ""datasets"",
     srcs = [""datasets.py""],",No
.travis.yml,.travis.yml,b7728d42393660b7f15493f96ad4afdee1d6505a,32f89e6c5a0830f7ad5a7ac7213344826a72e883,add jaxlib install to travis file,"diff --git a/.travis.yml b/.travis.yml
index 551916aac..a883d0427 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -20,6 +20,7 @@ before_install:
   - conda config --add channels conda-forge
 install:
   - conda install --yes python=$TRAVIS_PYTHON_VERSION $DEPS
+  - pip install jaxlib
   - pip install -v .
 script:
   - cd tests","diff --git a/.travis.yml b/.travis.yml
index 551916aac..a883d0427 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -20,6 +20,7 @@ before_install:
   - conda config --add channels conda-forge
 install:
   - conda install --yes python=$TRAVIS_PYTHON_VERSION $DEPS
+  - pip install jaxlib
   - pip install -v .
 script:
   - cd tests",No
tests/core_test.py,tests/core_test.py,e77599f31afbece62a04ac31bc8bc345e9f91725,b7728d42393660b7f15493f96ad4afdee1d6505a,"Disable tests that don't pass on all backends.

Avoid use of @skip decorator since it breaks in Python 2 in some cases.","diff --git a/tests/core_test.py b/tests/core_test.py
index 024b00268..3b706034a 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -153,9 +153,9 @@ def check_trace_eval(f, pvals, vals, expected_out_pval):
 
 class CoreTest(jtu.JaxTestCase):
 
-  @skip
   def test_pack_unpack(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    self.skipTest(""disabled"")
     y = onp.array(1.0)
     def foo(x):
       x1, y1 = core.pack((x, y))
@@ -164,9 +164,9 @@ class CoreTest(jtu.JaxTestCase):
 
     pe.trace_to_jaxpr(foo, (_,))
 
-  @skip
   def test_tup_add(self):
     # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
+    self.skipTest(""disabled"")
     y = onp.array(1.0)
     def foo(x):
       return np.tup_add(core.pack((x, y)))
@@ -187,9 +187,9 @@ class CoreTest(jtu.JaxTestCase):
     except TypeError:
       pass
 
-  @skip
   def test_print_jaxpr_compound(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    self.skipTest(""disabled"")
     pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
     print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
 
@@ -230,8 +230,8 @@ class CoreTest(jtu.JaxTestCase):
 
     api.trace_to_jaxpr(foo, (__,))
 
-  @skip
   def test_nested_grad(self):
+    self.skipTest(""disabled"")  # TODO: re-enable this test.
     def foo(x):
       print(type(x), x)
       def bar(y):","diff --git a/tests/core_test.py b/tests/core_test.py
index 024b00268..3b706034a 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -153,9 +153,9 @@ def check_trace_eval(f, pvals, vals, expected_out_pval):
 
 class CoreTest(jtu.JaxTestCase):
 
-  @skip
   def test_pack_unpack(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    self.skipTest(""disabled"")
     y = onp.array(1.0)
     def foo(x):
       x1, y1 = core.pack((x, y))
@@ -164,9 +164,9 @@ class CoreTest(jtu.JaxTestCase):
 
     pe.trace_to_jaxpr(foo, (_,))
 
-  @skip
   def test_tup_add(self):
     # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
+    self.skipTest(""disabled"")
     y = onp.array(1.0)
     def foo(x):
       return np.tup_add(core.pack((x, y)))
@@ -187,9 +187,9 @@ class CoreTest(jtu.JaxTestCase):
     except TypeError:
       pass
 
-  @skip
   def test_print_jaxpr_compound(self):
     # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    self.skipTest(""disabled"")
     pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
     print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
 
@@ -230,8 +230,8 @@ class CoreTest(jtu.JaxTestCase):
 
     api.trace_to_jaxpr(foo, (__,))
 
-  @skip
   def test_nested_grad(self):
+    self.skipTest(""disabled"")  # TODO: re-enable this test.
     def foo(x):
       print(type(x), x)
       def bar(y):",No
tests/examples_test.py,tests/examples_test.py,e77599f31afbece62a04ac31bc8bc345e9f91725,b7728d42393660b7f15493f96ad4afdee1d6505a,"Disable tests that don't pass on all backends.

Avoid use of @skip decorator since it breaks in Python 2 in some cases.","diff --git a/tests/examples_test.py b/tests/examples_test.py
index aa53509f4..1cec97acc 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -88,6 +88,8 @@ class ExamplesTest(jtu.JaxTestCase):
     assert np.all(kernel_lsq.gram(kernel, xs) == np.dot(xs, xs.T))
 
   def testKernelRegressionTrainAndPredict(self):
+    # TODO(frostig): reenable this test.
+    self.skipTest(""Test is broken"")
     n, d = 100, 20
     rng = onp.random.RandomState(0)
     truth = rng.randn(d)","diff --git a/tests/examples_test.py b/tests/examples_test.py
index aa53509f4..1cec97acc 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -88,6 +88,8 @@ class ExamplesTest(jtu.JaxTestCase):
     assert np.all(kernel_lsq.gram(kernel, xs) == np.dot(xs, xs.T))
 
   def testKernelRegressionTrainAndPredict(self):
+    # TODO(frostig): reenable this test.
+    self.skipTest(""Test is broken"")
     n, d = 100, 20
     rng = onp.random.RandomState(0)
     truth = rng.randn(d)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,e77599f31afbece62a04ac31bc8bc345e9f91725,b7728d42393660b7f15493f96ad4afdee1d6505a,"Disable tests that don't pass on all backends.

Avoid use of @skip decorator since it breaks in Python 2 in some cases.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 996d2c846..04800e712 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -648,9 +648,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self.assertFalse(a3)
 
   @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
-  @skip
   def testOnesBroadcastingConstantHandler(self):
     # TODO(mattjj): update this test for jax3
+    self.skipTest(""test needs jax3 update"")
 
     def fun(x):
       ones = lnp.ones((3, 4))
@@ -729,9 +729,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
     self.assertRaises(TypeError, lambda: g(3.))
 
-  @skip
   def testTracingPrimitiveWithNoTranslationErrorMessage(self):
     # TODO(mattjj): update this for jax3
+    self.skipTest(""test needs jax3 update"")
     foo = lnp._not_implemented(lambda x: x)
 
     # No error if there's no tracing.
@@ -777,9 +777,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   # TODO(mattjj): test infix operator overrides
 
-  @skip
   def testRavel(self):
     # TODO(mattjj): support this method-based syntax?
+    self.skipTest(""test disabled"")
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
     self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 996d2c846..04800e712 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -648,9 +648,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self.assertFalse(a3)
 
   @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
-  @skip
   def testOnesBroadcastingConstantHandler(self):
     # TODO(mattjj): update this test for jax3
+    self.skipTest(""test needs jax3 update"")
 
     def fun(x):
       ones = lnp.ones((3, 4))
@@ -729,9 +729,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
     self.assertRaises(TypeError, lambda: g(3.))
 
-  @skip
   def testTracingPrimitiveWithNoTranslationErrorMessage(self):
     # TODO(mattjj): update this for jax3
+    self.skipTest(""test needs jax3 update"")
     foo = lnp._not_implemented(lambda x: x)
 
     # No error if there's no tracing.
@@ -777,9 +777,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   # TODO(mattjj): test infix operator overrides
 
-  @skip
   def testRavel(self):
     # TODO(mattjj): support this method-based syntax?
+    self.skipTest(""test disabled"")
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
     self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)",No
.travis.yml,.travis.yml,5d7f356bf571f70c6a8547f14f91b4b33b9a333b,32b339a582dc7a0e88725f8d3bcca9b78f806872,use 'dist: xenial' in travis.yml to get updated toolchain,"diff --git a/.travis.yml b/.travis.yml
index a883d0427..b50031c62 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -1,6 +1,7 @@
 sudo: false
 notifications:
   email: false
+dist: xenial
 language: python
 python:
   - ""2.7""","diff --git a/.travis.yml b/.travis.yml
index a883d0427..b50031c62 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -1,6 +1,7 @@
 sudo: false
 notifications:
   email: false
+dist: xenial
 language: python
 python:
   - ""2.7""",No
tests/examples_test.py,tests/examples_test.py,b64df0b759aa4e328bc211532686cd19fb368524,5d7f356bf571f70c6a8547f14f91b4b33b9a333b,"comment out tests (temp), remove examples imports
examples imports add a new test dependency on matplotlib","diff --git a/tests/examples_test.py b/tests/examples_test.py
index 1cec97acc..9d5810e4d 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -29,9 +29,6 @@ import jax.numpy as np
 
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 from examples import kernel_lsq
-from examples import mnist_classifier
-from examples import mnist_classifier_fromscratch
-from examples import mnist_vae
 from examples import resnet50
 sys.path.pop()
 ","diff --git a/tests/examples_test.py b/tests/examples_test.py
index 1cec97acc..9d5810e4d 100644
--- a/tests/examples_test.py
+++ b/tests/examples_test.py
@@ -29,9 +29,6 @@ import jax.numpy as np
 
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 from examples import kernel_lsq
-from examples import mnist_classifier
-from examples import mnist_classifier_fromscratch
-from examples import mnist_vae
 from examples import resnet50
 sys.path.pop()
 ",No
tests/linalg_test.py,tests/linalg_test.py,b64df0b759aa4e328bc211532686cd19fb368524,5d7f356bf571f70c6a8547f14f91b4b33b9a333b,"comment out tests (temp), remove examples imports
examples imports add a new test dependency on matplotlib","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index dad5d103e..c8871afdc 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -42,114 +42,117 @@ def float_types():
 
 class NumpyLinalgTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"":
-       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
-       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
-      for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
-      for dtype in float_types()
-      for rng in [jtu.rand_default()]))
-  def testCholesky(self, shape, dtype, rng):
-    def args_maker():
-      a = rng(shape, dtype)
-      return [onp.matmul(a, T(a))]
-
-    self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
-                            check_dtypes=True, tol=1e-3)
-    self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
-
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
-          jtu.format_shape_dtype_string(shape, dtype), full_matrices),
-       ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
-       ""rng"": rng}
-      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
-      for dtype in float_types()
-      for full_matrices in [False, True]
-      for rng in [jtu.rand_default()]))
-  def testQr(self, shape, dtype, full_matrices, rng):
-    m, n = shape[-2:]
-
-    if full_matrices:
-      mode, k = ""complete"", m
-    else:
-      mode, k = ""reduced"", min(m, n)
-
-    a = rng(shape, dtype)
-    lq, lr = np.linalg.qr(a, mode=mode)
-
-    # onp.linalg.qr doesn't support broadcasting. But it seems like an
-    # inevitable extension so we support it in our version.
-    nq = onp.zeros(shape[:-2] + (m, k), dtype)
-    nr = onp.zeros(shape[:-2] + (k, n), dtype)
-    for index in onp.ndindex(*shape[:-2]):
-      nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
-
-    max_rank = max(m, n)
-
-    # Norm, adjusted for dimension and type.
-    def norm(x):
-      n = onp.linalg.norm(x, axis=(-2, -1))
-      return n / (max_rank * onp.finfo(dtype).eps)
-
-    def compare_orthogonal(q1, q2):
-      # Q is unique up to sign, so normalize the sign first.
-      sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
-      phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
-      q1 *= phases
-      self.assertTrue(onp.all(norm(q1 - q2) < 30))
-
-    # Check a ~= qr
-    self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
-
-    # Compare the first 'k' vectors of Q; the remainder form an arbitrary
-    # orthonormal basis for the null space.
-    compare_orthogonal(nq[..., :k], lq[..., :k])
-
-    # Check that q is close to unitary.
-    self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
-
-
-class ScipyLinalgTest(jtu.JaxTestCase):
-
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"":
-       ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
-           jtu.format_shape_dtype_string(lhs_shape, dtype),
-           jtu.format_shape_dtype_string(rhs_shape, dtype),
-           lower, transpose_a),
-       ""lower"": lower, ""transpose_a"": transpose_a,
-       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
-       ""rng"": rng}
-      for lower, transpose_a in itertools.product([False, True], repeat=2)
-      for lhs_shape, rhs_shape in [
-          ((4, 4), (4,)),
-          ((4, 4), (4, 3)),
-          ((2, 8, 8), (2, 8, 10)),
-      ]
-      for dtype in float_types()
-      for rng in [jtu.rand_default()]))
-  def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
-                                 rhs_shape, dtype, rng):
-    k = rng(lhs_shape, dtype)
-    l = onp.linalg.cholesky(onp.matmul(k, T(k))
-                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
-    l = l.astype(k.dtype)
-    b = rng(rhs_shape, dtype)
-
-    a = l if lower else T(l)
-    inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
-    if len(lhs_shape) == len(rhs_shape):
-      onp_ans = onp.matmul(inv, b)
-    else:
-      onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
-
-    # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
-    # But it seems like an inevitable extension so we support it.
-    ans = scipy.linalg.solve_triangular(
-        l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
-
-    self.assertAllClose(onp_ans, ans, check_dtypes=True)
+  # TODO(mattjj): put these tests back when we update jaxlib
+  pass
+
+  # @parameterized.named_parameters(jtu.cases_from_list(
+  #     {""testcase_name"":
+  #      ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+  #      ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+  #     for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
+  #     for dtype in float_types()
+  #     for rng in [jtu.rand_default()]))
+  # def testCholesky(self, shape, dtype, rng):
+  #   def args_maker():
+  #     a = rng(shape, dtype)
+  #     return [onp.matmul(a, T(a))]
+
+  #   self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
+  #                           check_dtypes=True, tol=1e-3)
+  #   self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
+
+  # @parameterized.named_parameters(jtu.cases_from_list(
+  #     {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
+  #         jtu.format_shape_dtype_string(shape, dtype), full_matrices),
+  #      ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
+  #      ""rng"": rng}
+  #     for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
+  #     for dtype in float_types()
+  #     for full_matrices in [False, True]
+  #     for rng in [jtu.rand_default()]))
+  # def testQr(self, shape, dtype, full_matrices, rng):
+  #   m, n = shape[-2:]
+
+  #   if full_matrices:
+  #     mode, k = ""complete"", m
+  #   else:
+  #     mode, k = ""reduced"", min(m, n)
+
+  #   a = rng(shape, dtype)
+  #   lq, lr = np.linalg.qr(a, mode=mode)
+
+  #   # onp.linalg.qr doesn't support broadcasting. But it seems like an
+  #   # inevitable extension so we support it in our version.
+  #   nq = onp.zeros(shape[:-2] + (m, k), dtype)
+  #   nr = onp.zeros(shape[:-2] + (k, n), dtype)
+  #   for index in onp.ndindex(*shape[:-2]):
+  #     nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
+
+  #   max_rank = max(m, n)
+
+  #   # Norm, adjusted for dimension and type.
+  #   def norm(x):
+  #     n = onp.linalg.norm(x, axis=(-2, -1))
+  #     return n / (max_rank * onp.finfo(dtype).eps)
+
+  #   def compare_orthogonal(q1, q2):
+  #     # Q is unique up to sign, so normalize the sign first.
+  #     sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
+  #     phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
+  #     q1 *= phases
+  #     self.assertTrue(onp.all(norm(q1 - q2) < 30))
+
+  #   # Check a ~= qr
+  #   self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
+
+  #   # Compare the first 'k' vectors of Q; the remainder form an arbitrary
+  #   # orthonormal basis for the null space.
+  #   compare_orthogonal(nq[..., :k], lq[..., :k])
+
+  #   # Check that q is close to unitary.
+  #   self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
+
+
+# class ScipyLinalgTest(jtu.JaxTestCase):
+
+  # @parameterized.named_parameters(jtu.cases_from_list(
+  #     {""testcase_name"":
+  #      ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
+  #          jtu.format_shape_dtype_string(lhs_shape, dtype),
+  #          jtu.format_shape_dtype_string(rhs_shape, dtype),
+  #          lower, transpose_a),
+  #      ""lower"": lower, ""transpose_a"": transpose_a,
+  #      ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+  #      ""rng"": rng}
+  #     for lower, transpose_a in itertools.product([False, True], repeat=2)
+  #     for lhs_shape, rhs_shape in [
+  #         ((4, 4), (4,)),
+  #         ((4, 4), (4, 3)),
+  #         ((2, 8, 8), (2, 8, 10)),
+  #     ]
+  #     for dtype in float_types()
+  #     for rng in [jtu.rand_default()]))
+  # def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
+  #                                rhs_shape, dtype, rng):
+  #   k = rng(lhs_shape, dtype)
+  #   l = onp.linalg.cholesky(onp.matmul(k, T(k))
+  #                           + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+  #   l = l.astype(k.dtype)
+  #   b = rng(rhs_shape, dtype)
+
+  #   a = l if lower else T(l)
+  #   inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
+  #   if len(lhs_shape) == len(rhs_shape):
+  #     onp_ans = onp.matmul(inv, b)
+  #   else:
+  #     onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
+
+  #   # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
+  #   # But it seems like an inevitable extension so we support it.
+  #   ans = scipy.linalg.solve_triangular(
+  #       l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
+
+  #   self.assertAllClose(onp_ans, ans, check_dtypes=True)
 
 
 if __name__ == ""__main__"":","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index dad5d103e..c8871afdc 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -42,114 +42,117 @@ def float_types():
 
 class NumpyLinalgTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"":
-       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
-       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
-      for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
-      for dtype in float_types()
-      for rng in [jtu.rand_default()]))
-  def testCholesky(self, shape, dtype, rng):
-    def args_maker():
-      a = rng(shape, dtype)
-      return [onp.matmul(a, T(a))]
+  # TODO(mattjj): put these tests back when we update jaxlib
+  pass
 
-    self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
-                            check_dtypes=True, tol=1e-3)
-    self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
+  # @parameterized.named_parameters(jtu.cases_from_list(
+  #     {""testcase_name"":
+  #      ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+  #      ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+  #     for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
+  #     for dtype in float_types()
+  #     for rng in [jtu.rand_default()]))
+  # def testCholesky(self, shape, dtype, rng):
+  #   def args_maker():
+  #     a = rng(shape, dtype)
+  #     return [onp.matmul(a, T(a))]
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
-          jtu.format_shape_dtype_string(shape, dtype), full_matrices),
-       ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
-       ""rng"": rng}
-      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
-      for dtype in float_types()
-      for full_matrices in [False, True]
-      for rng in [jtu.rand_default()]))
-  def testQr(self, shape, dtype, full_matrices, rng):
-    m, n = shape[-2:]
+  #   self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
+  #                           check_dtypes=True, tol=1e-3)
+  #   self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
-    if full_matrices:
-      mode, k = ""complete"", m
-    else:
-      mode, k = ""reduced"", min(m, n)
+  # @parameterized.named_parameters(jtu.cases_from_list(
+  #     {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
+  #         jtu.format_shape_dtype_string(shape, dtype), full_matrices),
+  #      ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
+  #      ""rng"": rng}
+  #     for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
+  #     for dtype in float_types()
+  #     for full_matrices in [False, True]
+  #     for rng in [jtu.rand_default()]))
+  # def testQr(self, shape, dtype, full_matrices, rng):
+  #   m, n = shape[-2:]
 
-    a = rng(shape, dtype)
-    lq, lr = np.linalg.qr(a, mode=mode)
+  #   if full_matrices:
+  #     mode, k = ""complete"", m
+  #   else:
+  #     mode, k = ""reduced"", min(m, n)
 
-    # onp.linalg.qr doesn't support broadcasting. But it seems like an
-    # inevitable extension so we support it in our version.
-    nq = onp.zeros(shape[:-2] + (m, k), dtype)
-    nr = onp.zeros(shape[:-2] + (k, n), dtype)
-    for index in onp.ndindex(*shape[:-2]):
-      nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
+  #   a = rng(shape, dtype)
+  #   lq, lr = np.linalg.qr(a, mode=mode)
 
-    max_rank = max(m, n)
+  #   # onp.linalg.qr doesn't support broadcasting. But it seems like an
+  #   # inevitable extension so we support it in our version.
+  #   nq = onp.zeros(shape[:-2] + (m, k), dtype)
+  #   nr = onp.zeros(shape[:-2] + (k, n), dtype)
+  #   for index in onp.ndindex(*shape[:-2]):
+  #     nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
 
-    # Norm, adjusted for dimension and type.
-    def norm(x):
-      n = onp.linalg.norm(x, axis=(-2, -1))
-      return n / (max_rank * onp.finfo(dtype).eps)
+  #   max_rank = max(m, n)
 
-    def compare_orthogonal(q1, q2):
-      # Q is unique up to sign, so normalize the sign first.
-      sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
-      phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
-      q1 *= phases
-      self.assertTrue(onp.all(norm(q1 - q2) < 30))
+  #   # Norm, adjusted for dimension and type.
+  #   def norm(x):
+  #     n = onp.linalg.norm(x, axis=(-2, -1))
+  #     return n / (max_rank * onp.finfo(dtype).eps)
 
-    # Check a ~= qr
-    self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
+  #   def compare_orthogonal(q1, q2):
+  #     # Q is unique up to sign, so normalize the sign first.
+  #     sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
+  #     phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
+  #     q1 *= phases
+  #     self.assertTrue(onp.all(norm(q1 - q2) < 30))
 
-    # Compare the first 'k' vectors of Q; the remainder form an arbitrary
-    # orthonormal basis for the null space.
-    compare_orthogonal(nq[..., :k], lq[..., :k])
+  #   # Check a ~= qr
+  #   self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
 
-    # Check that q is close to unitary.
-    self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
+  #   # Compare the first 'k' vectors of Q; the remainder form an arbitrary
+  #   # orthonormal basis for the null space.
+  #   compare_orthogonal(nq[..., :k], lq[..., :k])
+
+  #   # Check that q is close to unitary.
+  #   self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
 
 
-class ScipyLinalgTest(jtu.JaxTestCase):
+# class ScipyLinalgTest(jtu.JaxTestCase):
 
-  @parameterized.named_parameters(jtu.cases_from_list(
-      {""testcase_name"":
-       ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
-           jtu.format_shape_dtype_string(lhs_shape, dtype),
-           jtu.format_shape_dtype_string(rhs_shape, dtype),
-           lower, transpose_a),
-       ""lower"": lower, ""transpose_a"": transpose_a,
-       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
-       ""rng"": rng}
-      for lower, transpose_a in itertools.product([False, True], repeat=2)
-      for lhs_shape, rhs_shape in [
-          ((4, 4), (4,)),
-          ((4, 4), (4, 3)),
-          ((2, 8, 8), (2, 8, 10)),
-      ]
-      for dtype in float_types()
-      for rng in [jtu.rand_default()]))
-  def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
-                                 rhs_shape, dtype, rng):
-    k = rng(lhs_shape, dtype)
-    l = onp.linalg.cholesky(onp.matmul(k, T(k))
-                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
-    l = l.astype(k.dtype)
-    b = rng(rhs_shape, dtype)
+  # @parameterized.named_parameters(jtu.cases_from_list(
+  #     {""testcase_name"":
+  #      ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
+  #          jtu.format_shape_dtype_string(lhs_shape, dtype),
+  #          jtu.format_shape_dtype_string(rhs_shape, dtype),
+  #          lower, transpose_a),
+  #      ""lower"": lower, ""transpose_a"": transpose_a,
+  #      ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+  #      ""rng"": rng}
+  #     for lower, transpose_a in itertools.product([False, True], repeat=2)
+  #     for lhs_shape, rhs_shape in [
+  #         ((4, 4), (4,)),
+  #         ((4, 4), (4, 3)),
+  #         ((2, 8, 8), (2, 8, 10)),
+  #     ]
+  #     for dtype in float_types()
+  #     for rng in [jtu.rand_default()]))
+  # def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
+  #                                rhs_shape, dtype, rng):
+  #   k = rng(lhs_shape, dtype)
+  #   l = onp.linalg.cholesky(onp.matmul(k, T(k))
+  #                           + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+  #   l = l.astype(k.dtype)
+  #   b = rng(rhs_shape, dtype)
 
-    a = l if lower else T(l)
-    inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
-    if len(lhs_shape) == len(rhs_shape):
-      onp_ans = onp.matmul(inv, b)
-    else:
-      onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
+  #   a = l if lower else T(l)
+  #   inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
+  #   if len(lhs_shape) == len(rhs_shape):
+  #     onp_ans = onp.matmul(inv, b)
+  #   else:
+  #     onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
 
-    # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
-    # But it seems like an inevitable extension so we support it.
-    ans = scipy.linalg.solve_triangular(
-        l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
+  #   # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
+  #   # But it seems like an inevitable extension so we support it.
+  #   ans = scipy.linalg.solve_triangular(
+  #       l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
 
-    self.assertAllClose(onp_ans, ans, check_dtypes=True)
+  #   self.assertAllClose(onp_ans, ans, check_dtypes=True)
 
 
 if __name__ == ""__main__"":",Yes
jax/numpy/linalg.py,jax/numpy/linalg.py,3aad9b68f6465485c5948aba22ec1a6f5d8b914f,32b339a582dc7a0e88725f8d3bcca9b78f806872,"Implement np.linalg.inv using a QR decomposition.

An LU decomposition would probably be preferable; we can switch the implementation when we have an LU decomposition.

Fixes #44.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 1397428db..8bb0f701f 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -30,11 +30,19 @@ dot = np.dot
 matmul = np.matmul
 trace = np.trace
 
+_T = lambda x: np.swapaxes(x, -1, -2)
 
 @_wraps(onp.linalg.cholesky)
 def cholesky(a):
   return lax_linalg.cholesky(a)
 
+@_wraps(onp.linalg.inv)
+def inv(a):
+  if np.ndim(a) < 2 or a.shape[-1] != a.shape[-2]:
+    raise ValueError(""Argument to inv must have shape [..., n, n]."")
+  q, r = qr(a, mode=""complete"")
+  return lax_linalg.triangular_solve(r, _T(q), lower=False, left_side=True)
+
 
 @_wraps(onp.linalg.qr)
 def qr(a, mode=""reduced""):","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 1397428db..8bb0f701f 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -30,11 +30,19 @@ dot = np.dot
 matmul = np.matmul
 trace = np.trace
 
+_T = lambda x: np.swapaxes(x, -1, -2)
 
 @_wraps(onp.linalg.cholesky)
 def cholesky(a):
   return lax_linalg.cholesky(a)
 
+@_wraps(onp.linalg.inv)
+def inv(a):
+  if np.ndim(a) < 2 or a.shape[-1] != a.shape[-2]:
+    raise ValueError(""Argument to inv must have shape [..., n, n]."")
+  q, r = qr(a, mode=""complete"")
+  return lax_linalg.triangular_solve(r, _T(q), lower=False, left_side=True)
+
 
 @_wraps(onp.linalg.qr)
 def qr(a, mode=""reduced""):",No
tests/linalg_test.py,tests/linalg_test.py,3aad9b68f6465485c5948aba22ec1a6f5d8b914f,32b339a582dc7a0e88725f8d3bcca9b78f806872,"Implement np.linalg.inv using a QR decomposition.

An LU decomposition would probably be preferable; we can switch the implementation when we have an LU decomposition.

Fixes #44.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index dad5d103e..837c90078 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -109,6 +109,27 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     # Check that q is close to unitary.
     self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (5, 5, 5)]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testInv(self, shape, dtype, rng):
+    def args_maker():
+      a = rng(shape, dtype)
+      try:
+        onp.linalg.inv(a)
+        invertible = True
+      except onp.linalg.LinAlgError:
+        pass
+      return [a]
+
+    self._CheckAgainstNumpy(onp.linalg.inv, np.linalg.inv, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.inv, args_maker, check_dtypes=True)
+
 
 class ScipyLinalgTest(jtu.JaxTestCase):
 ","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index dad5d103e..837c90078 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -109,6 +109,27 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     # Check that q is close to unitary.
     self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (5, 5, 5)]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testInv(self, shape, dtype, rng):
+    def args_maker():
+      a = rng(shape, dtype)
+      try:
+        onp.linalg.inv(a)
+        invertible = True
+      except onp.linalg.LinAlgError:
+        pass
+      return [a]
+
+    self._CheckAgainstNumpy(onp.linalg.inv, np.linalg.inv, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.inv, args_maker, check_dtypes=True)
+
 
 class ScipyLinalgTest(jtu.JaxTestCase):
 ",No
tests/linalg_test.py,tests/linalg_test.py,94b424e495e7812c340494b7f921ec8073db4ef3,3aad9b68f6465485c5948aba22ec1a6f5d8b914f,Fix bug in inv test harness.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 837c90078..8e32c7e32 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -118,12 +118,14 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   def testInv(self, shape, dtype, rng):
     def args_maker():
-      a = rng(shape, dtype)
-      try:
-        onp.linalg.inv(a)
-        invertible = True
-      except onp.linalg.LinAlgError:
-        pass
+      invertible = False
+      while not invertible:
+        a = rng(shape, dtype)
+        try:
+          onp.linalg.inv(a)
+          invertible = True
+        except onp.linalg.LinAlgError:
+          pass
       return [a]
 
     self._CheckAgainstNumpy(onp.linalg.inv, np.linalg.inv, args_maker,","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 837c90078..8e32c7e32 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -118,12 +118,14 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   def testInv(self, shape, dtype, rng):
     def args_maker():
-      a = rng(shape, dtype)
-      try:
-        onp.linalg.inv(a)
-        invertible = True
-      except onp.linalg.LinAlgError:
-        pass
+      invertible = False
+      while not invertible:
+        a = rng(shape, dtype)
+        try:
+          onp.linalg.inv(a)
+          invertible = True
+        except onp.linalg.LinAlgError:
+          pass
       return [a]
 
     self._CheckAgainstNumpy(onp.linalg.inv, np.linalg.inv, args_maker,",No
.travis.yml,.travis.yml,9b645364c92f5f127026f875bc645301591b0bc6,b64df0b759aa4e328bc211532686cd19fb368524,"tests depend on scipy, tweaks","diff --git a/.travis.yml b/.travis.yml
index b50031c62..0bceb1e3f 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -8,7 +8,6 @@ python:
   - ""3.6""
 env:
   - DEPS=""pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy scipy""
-  - DEPS=""pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy""
 before_install:
   - if [[ ""$TRAVIS_PYTHON_VERSION"" == ""2.7"" ]]; then
       wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;
@@ -25,4 +24,4 @@ install:
   - pip install -v .
 script:
   - cd tests
-  - PYTHONPATH=. JAX_NUM_GENERATED_CASES=10 nosetests
+  - JAX_NUM_GENERATED_CASES=10 nosetests","diff --git a/.travis.yml b/.travis.yml
index b50031c62..0bceb1e3f 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -8,7 +8,6 @@ python:
   - ""3.6""
 env:
   - DEPS=""pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy scipy""
-  - DEPS=""pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy""
 before_install:
   - if [[ ""$TRAVIS_PYTHON_VERSION"" == ""2.7"" ]]; then
       wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;
@@ -25,4 +24,4 @@ install:
   - pip install -v .
 script:
   - cd tests
-  - PYTHONPATH=. JAX_NUM_GENERATED_CASES=10 nosetests
+  - JAX_NUM_GENERATED_CASES=10 nosetests",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,9b645364c92f5f127026f875bc645301591b0bc6,b64df0b759aa4e328bc211532686cd19fb368524,"tests depend on scipy, tweaks","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 167758b59..64905c311 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -508,7 +508,7 @@ def where(condition, x=None, y=None):
   if not onp.issubdtype(_dtype(condition), onp.bool_):
     condition = lax.ne(condition, zeros_like(condition))
   condition, x, y = broadcast_arrays(condition, x, y)
-  if not x.size:
+  if not onp.size(x):
     empty, _ = _promote_dtypes(x, y)
     return empty
   else:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 167758b59..64905c311 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -508,7 +508,7 @@ def where(condition, x=None, y=None):
   if not onp.issubdtype(_dtype(condition), onp.bool_):
     condition = lax.ne(condition, zeros_like(condition))
   condition, x, y = broadcast_arrays(condition, x, y)
-  if not x.size:
+  if not onp.size(x):
     empty, _ = _promote_dtypes(x, y)
     return empty
   else:",No
tests/linalg_test.py,tests/linalg_test.py,228a5d5e3efe95030d0b43fb402afdceabfaba17,24784608ceeb6367be1fcc36353d302938c75a8d,enable linalg tests,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index c8871afdc..dad5d103e 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -42,117 +42,114 @@ def float_types():
 
 class NumpyLinalgTest(jtu.JaxTestCase):
 
-  # TODO(mattjj): put these tests back when we update jaxlib
-  pass
-
-  # @parameterized.named_parameters(jtu.cases_from_list(
-  #     {""testcase_name"":
-  #      ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
-  #      ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
-  #     for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
-  #     for dtype in float_types()
-  #     for rng in [jtu.rand_default()]))
-  # def testCholesky(self, shape, dtype, rng):
-  #   def args_maker():
-  #     a = rng(shape, dtype)
-  #     return [onp.matmul(a, T(a))]
-
-  #   self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
-  #                           check_dtypes=True, tol=1e-3)
-  #   self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
-
-  # @parameterized.named_parameters(jtu.cases_from_list(
-  #     {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
-  #         jtu.format_shape_dtype_string(shape, dtype), full_matrices),
-  #      ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
-  #      ""rng"": rng}
-  #     for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
-  #     for dtype in float_types()
-  #     for full_matrices in [False, True]
-  #     for rng in [jtu.rand_default()]))
-  # def testQr(self, shape, dtype, full_matrices, rng):
-  #   m, n = shape[-2:]
-
-  #   if full_matrices:
-  #     mode, k = ""complete"", m
-  #   else:
-  #     mode, k = ""reduced"", min(m, n)
-
-  #   a = rng(shape, dtype)
-  #   lq, lr = np.linalg.qr(a, mode=mode)
-
-  #   # onp.linalg.qr doesn't support broadcasting. But it seems like an
-  #   # inevitable extension so we support it in our version.
-  #   nq = onp.zeros(shape[:-2] + (m, k), dtype)
-  #   nr = onp.zeros(shape[:-2] + (k, n), dtype)
-  #   for index in onp.ndindex(*shape[:-2]):
-  #     nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
-
-  #   max_rank = max(m, n)
-
-  #   # Norm, adjusted for dimension and type.
-  #   def norm(x):
-  #     n = onp.linalg.norm(x, axis=(-2, -1))
-  #     return n / (max_rank * onp.finfo(dtype).eps)
-
-  #   def compare_orthogonal(q1, q2):
-  #     # Q is unique up to sign, so normalize the sign first.
-  #     sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
-  #     phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
-  #     q1 *= phases
-  #     self.assertTrue(onp.all(norm(q1 - q2) < 30))
-
-  #   # Check a ~= qr
-  #   self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
-
-  #   # Compare the first 'k' vectors of Q; the remainder form an arbitrary
-  #   # orthonormal basis for the null space.
-  #   compare_orthogonal(nq[..., :k], lq[..., :k])
-
-  #   # Check that q is close to unitary.
-  #   self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
-
-
-# class ScipyLinalgTest(jtu.JaxTestCase):
-
-  # @parameterized.named_parameters(jtu.cases_from_list(
-  #     {""testcase_name"":
-  #      ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
-  #          jtu.format_shape_dtype_string(lhs_shape, dtype),
-  #          jtu.format_shape_dtype_string(rhs_shape, dtype),
-  #          lower, transpose_a),
-  #      ""lower"": lower, ""transpose_a"": transpose_a,
-  #      ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
-  #      ""rng"": rng}
-  #     for lower, transpose_a in itertools.product([False, True], repeat=2)
-  #     for lhs_shape, rhs_shape in [
-  #         ((4, 4), (4,)),
-  #         ((4, 4), (4, 3)),
-  #         ((2, 8, 8), (2, 8, 10)),
-  #     ]
-  #     for dtype in float_types()
-  #     for rng in [jtu.rand_default()]))
-  # def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
-  #                                rhs_shape, dtype, rng):
-  #   k = rng(lhs_shape, dtype)
-  #   l = onp.linalg.cholesky(onp.matmul(k, T(k))
-  #                           + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
-  #   l = l.astype(k.dtype)
-  #   b = rng(rhs_shape, dtype)
-
-  #   a = l if lower else T(l)
-  #   inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
-  #   if len(lhs_shape) == len(rhs_shape):
-  #     onp_ans = onp.matmul(inv, b)
-  #   else:
-  #     onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
-
-  #   # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
-  #   # But it seems like an inevitable extension so we support it.
-  #   ans = scipy.linalg.solve_triangular(
-  #       l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
-
-  #   self.assertAllClose(onp_ans, ans, check_dtypes=True)
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testCholesky(self, shape, dtype, rng):
+    def args_maker():
+      a = rng(shape, dtype)
+      return [onp.matmul(a, T(a))]
+
+    self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), full_matrices),
+       ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
+       ""rng"": rng}
+      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
+      for dtype in float_types()
+      for full_matrices in [False, True]
+      for rng in [jtu.rand_default()]))
+  def testQr(self, shape, dtype, full_matrices, rng):
+    m, n = shape[-2:]
+
+    if full_matrices:
+      mode, k = ""complete"", m
+    else:
+      mode, k = ""reduced"", min(m, n)
+
+    a = rng(shape, dtype)
+    lq, lr = np.linalg.qr(a, mode=mode)
+
+    # onp.linalg.qr doesn't support broadcasting. But it seems like an
+    # inevitable extension so we support it in our version.
+    nq = onp.zeros(shape[:-2] + (m, k), dtype)
+    nr = onp.zeros(shape[:-2] + (k, n), dtype)
+    for index in onp.ndindex(*shape[:-2]):
+      nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
+
+    max_rank = max(m, n)
+
+    # Norm, adjusted for dimension and type.
+    def norm(x):
+      n = onp.linalg.norm(x, axis=(-2, -1))
+      return n / (max_rank * onp.finfo(dtype).eps)
+
+    def compare_orthogonal(q1, q2):
+      # Q is unique up to sign, so normalize the sign first.
+      sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
+      phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
+      q1 *= phases
+      self.assertTrue(onp.all(norm(q1 - q2) < 30))
+
+    # Check a ~= qr
+    self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
+
+    # Compare the first 'k' vectors of Q; the remainder form an arbitrary
+    # orthonormal basis for the null space.
+    compare_orthogonal(nq[..., :k], lq[..., :k])
+
+    # Check that q is close to unitary.
+    self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
+
+
+class ScipyLinalgTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, transpose_a),
+       ""lower"": lower, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, transpose_a in itertools.product([False, True], repeat=2)
+      for lhs_shape, rhs_shape in [
+          ((4, 4), (4,)),
+          ((4, 4), (4, 3)),
+          ((2, 8, 8), (2, 8, 10)),
+      ]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
+                                 rhs_shape, dtype, rng):
+    k = rng(lhs_shape, dtype)
+    l = onp.linalg.cholesky(onp.matmul(k, T(k))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    l = l.astype(k.dtype)
+    b = rng(rhs_shape, dtype)
+
+    a = l if lower else T(l)
+    inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
+    if len(lhs_shape) == len(rhs_shape):
+      onp_ans = onp.matmul(inv, b)
+    else:
+      onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
+
+    # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
+    # But it seems like an inevitable extension so we support it.
+    ans = scipy.linalg.solve_triangular(
+        l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
+
+    self.assertAllClose(onp_ans, ans, check_dtypes=True)
 
 
 if __name__ == ""__main__"":","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index c8871afdc..dad5d103e 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -42,117 +42,114 @@ def float_types():
 
 class NumpyLinalgTest(jtu.JaxTestCase):
 
-  # TODO(mattjj): put these tests back when we update jaxlib
-  pass
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testCholesky(self, shape, dtype, rng):
+    def args_maker():
+      a = rng(shape, dtype)
+      return [onp.matmul(a, T(a))]
 
-  # @parameterized.named_parameters(jtu.cases_from_list(
-  #     {""testcase_name"":
-  #      ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
-  #      ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
-  #     for shape in [(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)]
-  #     for dtype in float_types()
-  #     for rng in [jtu.rand_default()]))
-  # def testCholesky(self, shape, dtype, rng):
-  #   def args_maker():
-  #     a = rng(shape, dtype)
-  #     return [onp.matmul(a, T(a))]
+    self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
-  #   self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
-  #                           check_dtypes=True, tol=1e-3)
-  #   self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), full_matrices),
+       ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
+       ""rng"": rng}
+      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
+      for dtype in float_types()
+      for full_matrices in [False, True]
+      for rng in [jtu.rand_default()]))
+  def testQr(self, shape, dtype, full_matrices, rng):
+    m, n = shape[-2:]
 
-  # @parameterized.named_parameters(jtu.cases_from_list(
-  #     {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
-  #         jtu.format_shape_dtype_string(shape, dtype), full_matrices),
-  #      ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
-  #      ""rng"": rng}
-  #     for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
-  #     for dtype in float_types()
-  #     for full_matrices in [False, True]
-  #     for rng in [jtu.rand_default()]))
-  # def testQr(self, shape, dtype, full_matrices, rng):
-  #   m, n = shape[-2:]
+    if full_matrices:
+      mode, k = ""complete"", m
+    else:
+      mode, k = ""reduced"", min(m, n)
 
-  #   if full_matrices:
-  #     mode, k = ""complete"", m
-  #   else:
-  #     mode, k = ""reduced"", min(m, n)
+    a = rng(shape, dtype)
+    lq, lr = np.linalg.qr(a, mode=mode)
 
-  #   a = rng(shape, dtype)
-  #   lq, lr = np.linalg.qr(a, mode=mode)
+    # onp.linalg.qr doesn't support broadcasting. But it seems like an
+    # inevitable extension so we support it in our version.
+    nq = onp.zeros(shape[:-2] + (m, k), dtype)
+    nr = onp.zeros(shape[:-2] + (k, n), dtype)
+    for index in onp.ndindex(*shape[:-2]):
+      nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
 
-  #   # onp.linalg.qr doesn't support broadcasting. But it seems like an
-  #   # inevitable extension so we support it in our version.
-  #   nq = onp.zeros(shape[:-2] + (m, k), dtype)
-  #   nr = onp.zeros(shape[:-2] + (k, n), dtype)
-  #   for index in onp.ndindex(*shape[:-2]):
-  #     nq[index], nr[index] = onp.linalg.qr(a[index], mode=mode)
+    max_rank = max(m, n)
 
-  #   max_rank = max(m, n)
+    # Norm, adjusted for dimension and type.
+    def norm(x):
+      n = onp.linalg.norm(x, axis=(-2, -1))
+      return n / (max_rank * onp.finfo(dtype).eps)
 
-  #   # Norm, adjusted for dimension and type.
-  #   def norm(x):
-  #     n = onp.linalg.norm(x, axis=(-2, -1))
-  #     return n / (max_rank * onp.finfo(dtype).eps)
+    def compare_orthogonal(q1, q2):
+      # Q is unique up to sign, so normalize the sign first.
+      sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
+      phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
+      q1 *= phases
+      self.assertTrue(onp.all(norm(q1 - q2) < 30))
 
-  #   def compare_orthogonal(q1, q2):
-  #     # Q is unique up to sign, so normalize the sign first.
-  #     sum_of_ratios = onp.sum(onp.divide(q1, q2), axis=-2, keepdims=True)
-  #     phases = onp.divide(sum_of_ratios, onp.abs(sum_of_ratios))
-  #     q1 *= phases
-  #     self.assertTrue(onp.all(norm(q1 - q2) < 30))
+    # Check a ~= qr
+    self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
 
-  #   # Check a ~= qr
-  #   self.assertTrue(onp.all(norm(a - onp.matmul(lq, lr)) < 30))
+    # Compare the first 'k' vectors of Q; the remainder form an arbitrary
+    # orthonormal basis for the null space.
+    compare_orthogonal(nq[..., :k], lq[..., :k])
 
-  #   # Compare the first 'k' vectors of Q; the remainder form an arbitrary
-  #   # orthonormal basis for the null space.
-  #   compare_orthogonal(nq[..., :k], lq[..., :k])
-
-  #   # Check that q is close to unitary.
-  #   self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
+    # Check that q is close to unitary.
+    self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
 
 
-# class ScipyLinalgTest(jtu.JaxTestCase):
+class ScipyLinalgTest(jtu.JaxTestCase):
 
-  # @parameterized.named_parameters(jtu.cases_from_list(
-  #     {""testcase_name"":
-  #      ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
-  #          jtu.format_shape_dtype_string(lhs_shape, dtype),
-  #          jtu.format_shape_dtype_string(rhs_shape, dtype),
-  #          lower, transpose_a),
-  #      ""lower"": lower, ""transpose_a"": transpose_a,
-  #      ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
-  #      ""rng"": rng}
-  #     for lower, transpose_a in itertools.product([False, True], repeat=2)
-  #     for lhs_shape, rhs_shape in [
-  #         ((4, 4), (4,)),
-  #         ((4, 4), (4, 3)),
-  #         ((2, 8, 8), (2, 8, 10)),
-  #     ]
-  #     for dtype in float_types()
-  #     for rng in [jtu.rand_default()]))
-  # def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
-  #                                rhs_shape, dtype, rng):
-  #   k = rng(lhs_shape, dtype)
-  #   l = onp.linalg.cholesky(onp.matmul(k, T(k))
-  #                           + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
-  #   l = l.astype(k.dtype)
-  #   b = rng(rhs_shape, dtype)
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, transpose_a),
+       ""lower"": lower, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, transpose_a in itertools.product([False, True], repeat=2)
+      for lhs_shape, rhs_shape in [
+          ((4, 4), (4,)),
+          ((4, 4), (4, 3)),
+          ((2, 8, 8), (2, 8, 10)),
+      ]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
+                                 rhs_shape, dtype, rng):
+    k = rng(lhs_shape, dtype)
+    l = onp.linalg.cholesky(onp.matmul(k, T(k))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    l = l.astype(k.dtype)
+    b = rng(rhs_shape, dtype)
 
-  #   a = l if lower else T(l)
-  #   inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
-  #   if len(lhs_shape) == len(rhs_shape):
-  #     onp_ans = onp.matmul(inv, b)
-  #   else:
-  #     onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
+    a = l if lower else T(l)
+    inv = onp.linalg.inv(T(a) if transpose_a else a).astype(a.dtype)
+    if len(lhs_shape) == len(rhs_shape):
+      onp_ans = onp.matmul(inv, b)
+    else:
+      onp_ans = onp.einsum(""...ij,...j->...i"", inv, b)
 
-  #   # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
-  #   # But it seems like an inevitable extension so we support it.
-  #   ans = scipy.linalg.solve_triangular(
-  #       l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
+    # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
+    # But it seems like an inevitable extension so we support it.
+    ans = scipy.linalg.solve_triangular(
+        l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
 
-  #   self.assertAllClose(onp_ans, ans, check_dtypes=True)
+    self.assertAllClose(onp_ans, ans, check_dtypes=True)
 
 
 if __name__ == ""__main__"":",Yes
README.md,README.md,e4ef7eae27a9be593ff77ad5b8c558c1894f7084,06453b8631ffbb9cef7bf902bbe42284ae922e1b,add travis badge to readme,"diff --git a/README.md b/README.md
index a498c623b..205bbddc0 100644
--- a/README.md
+++ b/README.md
@@ -2,7 +2,7 @@
 <img src=""https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png"" alt=""logo""></img>
 </div>
 
-# JAX: Autograd and XLA
+# JAX: Autograd and XLA [![Test status](https://travis-ci.org/google/jax.svg?branch=master)](https://travis-ci.org/google/jax)
 
 JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),","diff --git a/README.md b/README.md
index a498c623b..205bbddc0 100644
--- a/README.md
+++ b/README.md
@@ -2,7 +2,7 @@
 <img src=""https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png"" alt=""logo""></img>
 </div>
 
-# JAX: Autograd and XLA
+# JAX: Autograd and XLA [![Test status](https://travis-ci.org/google/jax.svg?branch=master)](https://travis-ci.org/google/jax)
 
 JAX is [Autograd](https://github.com/hips/autograd) and
 [XLA](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md),",No
README.md,README.md,2e40e63527ab4dc1ad32b742bf89070353f5a28c,e4ef7eae27a9be593ff77ad5b8c558c1894f7084,update jaxlib wheel urls to 0.1.1,"diff --git a/README.md b/README.md
index 205bbddc0..8764ca0e1 100644
--- a/README.md
+++ b/README.md
@@ -142,7 +142,7 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
 ```","diff --git a/README.md b/README.md
index 205bbddc0..8764ca0e1 100644
--- a/README.md
+++ b/README.md
@@ -142,7 +142,7 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install jax  # install jax
 ```",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,2e40e63527ab4dc1ad32b742bf89070353f5a28c,e4ef7eae27a9be593ff77ad5b8c558c1894f7084,update jaxlib wheel urls to 0.1.1,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index db5c5e5b6..d44cca1a8 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index db5c5e5b6..d44cca1a8 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,2e40e63527ab4dc1ad32b742bf89070353f5a28c,e4ef7eae27a9be593ff77ad5b8c558c1894f7084,update jaxlib wheel urls to 0.1.1,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index d6951b895..f7f6f1b8f 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index d6951b895..f7f6f1b8f 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,2e40e63527ab4dc1ad32b742bf89070353f5a28c,e4ef7eae27a9be593ff77ad5b8c558c1894f7084,update jaxlib wheel urls to 0.1.1,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 62d805f67..80f82e22e 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 62d805f67..80f82e22e 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1-py3-none-any.whl\n"",
+        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
build/setup.py,build/setup.py,b986513e97b93698dfb3b275c601cae015b76770,2e40e63527ab4dc1ad32b742bf89070353f5a28c,bump jaxlib version to 0.1.1,"diff --git a/build/setup.py b/build/setup.py
index a92031625..5197f7d17 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1',
+    version='0.1.1',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index a92031625..5197f7d17 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1',
+    version='0.1.1',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,2048874d43094bc952715ed4ac6c61dd73affe83,b986513e97b93698dfb3b275c601cae015b76770,add '--upgrade' to pip install examples,"diff --git a/README.md b/README.md
index 8764ca0e1..87bf9939d 100644
--- a/README.md
+++ b/README.md
@@ -130,7 +130,7 @@ To install a CPU-only version, which might be useful for doing local
 development on a laptop, you can run
 
 ```bash
-pip install jax jaxlib  # CPU-only version
+pip install --upgrade jax jaxlib  # CPU-only version
 ```
 
 If you want to install JAX with both CPU and GPU support, using existing CUDA
@@ -142,9 +142,10 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
+BASE_URL='https://storage.googleapis.com/jax-wheels'
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
-pip install jax  # install jax
+pip install --upgrade jax  # install jax
 ```
 
 The library package name must correspond to the version of the existing CUDA","diff --git a/README.md b/README.md
index 8764ca0e1..87bf9939d 100644
--- a/README.md
+++ b/README.md
@@ -130,7 +130,7 @@ To install a CPU-only version, which might be useful for doing local
 development on a laptop, you can run
 
 ```bash
-pip install jax jaxlib  # CPU-only version
+pip install --upgrade jax jaxlib  # CPU-only version
 ```
 
 If you want to install JAX with both CPU and GPU support, using existing CUDA
@@ -142,9 +142,10 @@ cloud VM), you can run
 PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
-pip install https://storage.googleapis.com/jax-wheels/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
+BASE_URL='https://storage.googleapis.com/jax-wheels'
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
 
-pip install jax  # install jax
+pip install --upgrade jax  # install jax
 ```
 
 The library package name must correspond to the version of the existing CUDA",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,2048874d43094bc952715ed4ac6c61dd73affe83,b986513e97b93698dfb3b275c601cae015b76770,add '--upgrade' to pip install examples,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index d44cca1a8..14940a771 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index d44cca1a8..14940a771 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,2048874d43094bc952715ed4ac6c61dd73affe83,b986513e97b93698dfb3b275c601cae015b76770,add '--upgrade' to pip install examples,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index f7f6f1b8f..15acf1f15 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index f7f6f1b8f..15acf1f15 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,2048874d43094bc952715ed4ac6c61dd73affe83,b986513e97b93698dfb3b275c601cae015b76770,add '--upgrade' to pip install examples,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 80f82e22e..d69bcd6c3 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 80f82e22e..d69bcd6c3 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
jax/numpy/linalg.py,jax/numpy/linalg.py,23525bd9b544758a92dd2d49110a3616f5c316ab,94b424e495e7812c340494b7f921ec8073db4ef3,Add scipy.linalg.inv as well. Simplify the QR call in np.linalg.inv.,"diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 8bb0f701f..6447cdbc4 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -39,8 +39,9 @@ def cholesky(a):
 @_wraps(onp.linalg.inv)
 def inv(a):
   if np.ndim(a) < 2 or a.shape[-1] != a.shape[-2]:
-    raise ValueError(""Argument to inv must have shape [..., n, n]."")
-  q, r = qr(a, mode=""complete"")
+    raise ValueError(""Argument to inv must have shape [..., n, n], got {}.""
+      .format(np.shape(a)))
+  q, r = qr(a)
   return lax_linalg.triangular_solve(r, _T(q), lower=False, left_side=True)
 
 ","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 8bb0f701f..6447cdbc4 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -39,8 +39,9 @@ def cholesky(a):
 @_wraps(onp.linalg.inv)
 def inv(a):
   if np.ndim(a) < 2 or a.shape[-1] != a.shape[-2]:
-    raise ValueError(""Argument to inv must have shape [..., n, n]."")
-  q, r = qr(a, mode=""complete"")
+    raise ValueError(""Argument to inv must have shape [..., n, n], got {}.""
+      .format(np.shape(a)))
+  q, r = qr(a)
   return lax_linalg.triangular_solve(r, _T(q), lower=False, left_side=True)
 
 ",No
jax/scipy/linalg.py,jax/scipy/linalg.py,23525bd9b544758a92dd2d49110a3616f5c316ab,94b424e495e7812c340494b7f921ec8073db4ef3,Add scipy.linalg.inv as well. Simplify the QR call in np.linalg.inv.,"diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 73d26cada..628354d39 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -21,6 +21,7 @@ import scipy.linalg
 from .. import lax_linalg
 from ..numpy.lax_numpy import _wraps
 from ..numpy import lax_numpy as np
+from ..numpy import linalg as np_linalg
 
 
 @_wraps(scipy.linalg.cholesky)
@@ -33,6 +34,12 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   return lax_linalg.cholesky(a)
 
 
+@_wraps(scipy.linalg.inv)
+def inv(a, overwrite_a=False, check_finite=True):
+  del overwrite_a, check_finite
+  return np_linalg.inv(a)
+
+
 @_wraps(scipy.linalg.qr)
 def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
        check_finite=True):","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 73d26cada..628354d39 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -21,6 +21,7 @@ import scipy.linalg
 from .. import lax_linalg
 from ..numpy.lax_numpy import _wraps
 from ..numpy import lax_numpy as np
+from ..numpy import linalg as np_linalg
 
 
 @_wraps(scipy.linalg.cholesky)
@@ -33,6 +34,12 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   return lax_linalg.cholesky(a)
 
 
+@_wraps(scipy.linalg.inv)
+def inv(a, overwrite_a=False, check_finite=True):
+  del overwrite_a, check_finite
+  return np_linalg.inv(a)
+
+
 @_wraps(scipy.linalg.qr)
 def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
        check_finite=True):",No
examples/mnist_classifier.py,examples/mnist_classifier.py,e682af7ea50687b490be12188a090f919b41f9ae,14acd1dbfa5ab311a512b17ef98d75ac0d3002c2,increase mnist example batch size to 128,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index be063a29f..06f133cc7 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -53,7 +53,7 @@ init_random_params, predict = stax.serial(
 if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
-  batch_size = 32
+  batch_size = 128
   momentum_mass = 0.9
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index be063a29f..06f133cc7 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -53,7 +53,7 @@ init_random_params, predict = stax.serial(
 if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
-  batch_size = 32
+  batch_size = 128
   momentum_mass = 0.9
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,e682af7ea50687b490be12188a090f919b41f9ae,14acd1dbfa5ab311a512b17ef98d75ac0d3002c2,increase mnist example batch size to 128,"diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index d9b1b9a98..63e77dba8 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -59,7 +59,7 @@ if __name__ == ""__main__"":
   param_scale = 0.1
   step_size = 0.001
   num_epochs = 10
-  batch_size = 32
+  batch_size = 128
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()
   num_train = train_images.shape[0]","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index d9b1b9a98..63e77dba8 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -59,7 +59,7 @@ if __name__ == ""__main__"":
   param_scale = 0.1
   step_size = 0.001
   num_epochs = 10
-  batch_size = 32
+  batch_size = 128
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()
   num_train = train_images.shape[0]",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,e682af7ea50687b490be12188a090f919b41f9ae,14acd1dbfa5ab311a512b17ef98d75ac0d3002c2,increase mnist example batch size to 128,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 15acf1f15..a916f1996 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -130,7 +130,7 @@
         ""param_scale = 0.1\n"",
         ""step_size = 0.001\n"",
         ""num_epochs = 10\n"",
-        ""batch_size = 32\n"",
+        ""batch_size = 128\n"",
         ""n_targets = 10\n"",
         ""params = init_network_params(layer_sizes, random.PRNGKey(0))""
       ],
@@ -372,7 +372,7 @@
       ""source"": [
         ""# Define our dataset, using torch datasets\n"",
         ""mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n"",
-        ""training_generator = NumpyLoader(mnist_dataset, batch_size=32, num_workers=0)""
+        ""training_generator = NumpyLoader(mnist_dataset, batch_size=128, num_workers=0)""
       ],
       ""execution_count"": 0,
       ""outputs"": []","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 15acf1f15..a916f1996 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -130,7 +130,7 @@
         ""param_scale = 0.1\n"",
         ""step_size = 0.001\n"",
         ""num_epochs = 10\n"",
-        ""batch_size = 32\n"",
+        ""batch_size = 128\n"",
         ""n_targets = 10\n"",
         ""params = init_network_params(layer_sizes, random.PRNGKey(0))""
       ],
@@ -372,7 +372,7 @@
       ""source"": [
         ""# Define our dataset, using torch datasets\n"",
         ""mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n"",
-        ""training_generator = NumpyLoader(mnist_dataset, batch_size=32, num_workers=0)""
+        ""training_generator = NumpyLoader(mnist_dataset, batch_size=128, num_workers=0)""
       ],
       ""execution_count"": 0,
       ""outputs"": []",No
build/setup.py,build/setup.py,97d747cd083de78bc06de476138283120ccf4cc2,e682af7ea50687b490be12188a090f919b41f9ae,bump jaxlib version for pypi reasons,"diff --git a/build/setup.py b/build/setup.py
index 5197f7d17..92fc20daf 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.1',
+    version='0.1.2',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index 5197f7d17..92fc20daf 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.1',
+    version='0.1.2',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,97d747cd083de78bc06de476138283120ccf4cc2,update jaxlib references to 0.1.2,"diff --git a/README.md b/README.md
index 87bf9939d..80459d91b 100644
--- a/README.md
+++ b/README.md
@@ -143,7 +143,7 @@ PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.2-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```","diff --git a/README.md b/README.md
index 87bf9939d..80459d91b 100644
--- a/README.md
+++ b/README.md
@@ -143,7 +143,7 @@ PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.1-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.2-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,97d747cd083de78bc06de476138283120ccf4cc2,update jaxlib references to 0.1.2,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 14940a771..f5c9c3872 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 14940a771..f5c9c3872 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,97d747cd083de78bc06de476138283120ccf4cc2,update jaxlib references to 0.1.2,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index a916f1996..20b0165db 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index a916f1996..20b0165db 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,97d747cd083de78bc06de476138283120ccf4cc2,update jaxlib references to 0.1.2,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index d69bcd6c3..cc413132e 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index d69bcd6c3..cc413132e 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.1-py3-none-any.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
jax/lax.py,jax/lax.py,693365c239adac1130956b4f3d0882551b31fae6,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,"np.all and np.any should lead to monoid reducers
fixes #108","diff --git a/jax/lax.py b/jax/lax.py
index 00d3f480f..091e096ec 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -111,7 +111,12 @@ def convert_element_type(operand, new_dtype):
     return operand
 
 def bitcast_convert_type(operand, new_dtype):
-  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)
+  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
+  old_dtype = _dtype(operand)
+  if old_dtype != new_dtype:
+    return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)
+  else:
+    return operand
 
 def clamp(min, operand, max):
   return clamp_p.bind(min, operand, max)
@@ -269,9 +274,9 @@ def _get_monoid_reducer(monoid_op, x):
   if (type(aval) is ConcreteArray) and aval.shape == ():
     if monoid_op is add:
       return aval.val == 0 and _reduce_sum
-    elif monoid_op is max:
+    elif monoid_op is max or monoid_op is bitwise_or and aval.dtype == onp.bool_:
       return aval.val == _get_max_identity(aval.dtype) and _reduce_max
-    elif monoid_op is min:
+    elif monoid_op is min or monoid_op is bitwise_and and aval.dtype == onp.bool_:
       return aval.val == _get_min_identity(aval.dtype) and _reduce_min
 
 def _get_max_identity(dtype):
@@ -279,12 +284,16 @@ def _get_max_identity(dtype):
     return onp.array(-onp.inf, dtype)
   elif onp.issubdtype(dtype, onp.integer):
     return onp.array(onp.iinfo(dtype).min, dtype)
+  elif onp.issubdtype(dtype, onp.bool_):
+    return onp.array(False, onp.bool_)
 
 def _get_min_identity(dtype):
   if onp.issubdtype(dtype, onp.floating):
     return onp.array(onp.inf, dtype)
   elif onp.issubdtype(dtype, onp.integer):
     return onp.array(onp.iinfo(dtype).max, dtype)
+  elif onp.issubdtype(dtype, onp.bool_):
+    return onp.array(True, onp.bool_)
 
 def _reduce_sum(operand, axes):
   return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)
@@ -1828,7 +1837,7 @@ def _reduction_computation(c, jaxpr, consts, init_value):
 
 reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                               reduce_translation_rule)
-batching.defreducer(reduce_p)
+# batching.defreducer(reduce_p)  # TODO batching rule for general reduce
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):","diff --git a/jax/lax.py b/jax/lax.py
index 00d3f480f..091e096ec 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -111,7 +111,12 @@ def convert_element_type(operand, new_dtype):
     return operand
 
 def bitcast_convert_type(operand, new_dtype):
-  return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)
+  new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
+  old_dtype = _dtype(operand)
+  if old_dtype != new_dtype:
+    return bitcast_convert_type_p.bind(operand, new_dtype=new_dtype)
+  else:
+    return operand
 
 def clamp(min, operand, max):
   return clamp_p.bind(min, operand, max)
@@ -269,9 +274,9 @@ def _get_monoid_reducer(monoid_op, x):
   if (type(aval) is ConcreteArray) and aval.shape == ():
     if monoid_op is add:
       return aval.val == 0 and _reduce_sum
-    elif monoid_op is max:
+    elif monoid_op is max or monoid_op is bitwise_or and aval.dtype == onp.bool_:
       return aval.val == _get_max_identity(aval.dtype) and _reduce_max
-    elif monoid_op is min:
+    elif monoid_op is min or monoid_op is bitwise_and and aval.dtype == onp.bool_:
       return aval.val == _get_min_identity(aval.dtype) and _reduce_min
 
 def _get_max_identity(dtype):
@@ -279,12 +284,16 @@ def _get_max_identity(dtype):
     return onp.array(-onp.inf, dtype)
   elif onp.issubdtype(dtype, onp.integer):
     return onp.array(onp.iinfo(dtype).min, dtype)
+  elif onp.issubdtype(dtype, onp.bool_):
+    return onp.array(False, onp.bool_)
 
 def _get_min_identity(dtype):
   if onp.issubdtype(dtype, onp.floating):
     return onp.array(onp.inf, dtype)
   elif onp.issubdtype(dtype, onp.integer):
     return onp.array(onp.iinfo(dtype).max, dtype)
+  elif onp.issubdtype(dtype, onp.bool_):
+    return onp.array(True, onp.bool_)
 
 def _reduce_sum(operand, axes):
   return reduce_sum_p.bind(operand, axes=tuple(axes), input_shape=operand.shape)
@@ -1828,7 +1837,7 @@ def _reduction_computation(c, jaxpr, consts, init_value):
 
 reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                               reduce_translation_rule)
-batching.defreducer(reduce_p)
+# batching.defreducer(reduce_p)  # TODO batching rule for general reduce
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,693365c239adac1130956b4f3d0882551b31fae6,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,"np.all and np.any should lead to monoid reducers
fixes #108","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 64905c311..ef92ae235 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -25,8 +25,7 @@ from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from .. import lax
-from ..util import memoize
-from ..util import get_module_functions
+from ..util import memoize, partial, get_module_functions
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -591,15 +590,16 @@ around = round
 ### Reducers
 
 
-def _make_reduction(np_fun, op, init_val):
+def _make_reduction(np_fun, op, init_val, preproc=None):
   """"""Creates reduction function given a binary operation and monoid identity.""""""
 
-  @_wraps(op)
+  @_wraps(np_fun)
   def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
     if out is not None:
       raise ValueError(""reduction does not support `out` argument."")
 
     a = a if isinstance(a, ndarray) else asarray(a)
+    a = preproc(a) if preproc else a
     dims = _reduction_dims(a, axis)
     result_dtype = _dtype(np_fun(onp.ones((), dtype=dtype or _dtype(a))))
     if _dtype(a) != result_dtype:
@@ -614,7 +614,6 @@ def _make_reduction(np_fun, op, init_val):
 
   return reduction
 
-
 def _reduction_dims(a, axis):
   if axis is None:
     return onp.arange(ndim(a))
@@ -625,7 +624,6 @@ def _reduction_dims(a, axis):
   else:
     raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))
 
-
 def _reduction_init_val(a, init_val):
   a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
   try:
@@ -635,13 +633,14 @@ def _reduction_init_val(a, init_val):
     sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
     return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)
 
+_cast_to_bool = partial(lax.convert_element_type, new_dtype=onp.bool_)
 
 sum = _make_reduction(onp.sum, lax.add, 0)
 prod = _make_reduction(onp.prod, lax.mul, 1)
 max = _make_reduction(onp.max, lax.max, -onp.inf)
 min = _make_reduction(onp.min, lax.min, onp.inf)
-all = alltrue = _make_reduction(onp.all, logical_and, True)
-any = sometrue = _make_reduction(onp.any, logical_or, False)
+all = alltrue = _make_reduction(onp.all, lax.bitwise_and, True, _cast_to_bool)
+any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 
 
 @_wraps(onp.mean)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 64905c311..ef92ae235 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -25,8 +25,7 @@ from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from .. import lax
-from ..util import memoize
-from ..util import get_module_functions
+from ..util import memoize, partial, get_module_functions
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -591,15 +590,16 @@ around = round
 ### Reducers
 
 
-def _make_reduction(np_fun, op, init_val):
+def _make_reduction(np_fun, op, init_val, preproc=None):
   """"""Creates reduction function given a binary operation and monoid identity.""""""
 
-  @_wraps(op)
+  @_wraps(np_fun)
   def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
     if out is not None:
       raise ValueError(""reduction does not support `out` argument."")
 
     a = a if isinstance(a, ndarray) else asarray(a)
+    a = preproc(a) if preproc else a
     dims = _reduction_dims(a, axis)
     result_dtype = _dtype(np_fun(onp.ones((), dtype=dtype or _dtype(a))))
     if _dtype(a) != result_dtype:
@@ -614,7 +614,6 @@ def _make_reduction(np_fun, op, init_val):
 
   return reduction
 
-
 def _reduction_dims(a, axis):
   if axis is None:
     return onp.arange(ndim(a))
@@ -625,7 +624,6 @@ def _reduction_dims(a, axis):
   else:
     raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))
 
-
 def _reduction_init_val(a, init_val):
   a_dtype = xla_bridge.canonicalize_dtype(_dtype(a))
   try:
@@ -635,13 +633,14 @@ def _reduction_init_val(a, init_val):
     sign, iinfo = onp.sign(init_val), onp.iinfo(a_dtype)
     return onp.array(iinfo.min if sign < 0 else iinfo.max, dtype=a_dtype)
 
+_cast_to_bool = partial(lax.convert_element_type, new_dtype=onp.bool_)
 
 sum = _make_reduction(onp.sum, lax.add, 0)
 prod = _make_reduction(onp.prod, lax.mul, 1)
 max = _make_reduction(onp.max, lax.max, -onp.inf)
 min = _make_reduction(onp.min, lax.min, onp.inf)
-all = alltrue = _make_reduction(onp.all, logical_and, True)
-any = sometrue = _make_reduction(onp.any, logical_or, False)
+all = alltrue = _make_reduction(onp.all, lax.bitwise_and, True, _cast_to_bool)
+any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 
 
 @_wraps(onp.mean)",No
tests/batching_test.py,tests/batching_test.py,693365c239adac1130956b4f3d0882551b31fae6,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,"np.all and np.any should lead to monoid reducers
fixes #108","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 31f8a6cdb..4c15cc087 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -284,6 +284,13 @@ class BatchingTest(jtu.JaxTestCase):
     jacrev(func)(xs)  # don't crash
     jacfwd(func)(xs)  # don't crash
 
+  def testAny(self):
+    # test modeling the code in https://github.com/google/jax/issues/108
+
+    ans = vmap(np.any)(np.array([[True, False], [False, False]]))
+    expected = np.array([True, False])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 31f8a6cdb..4c15cc087 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -284,6 +284,13 @@ class BatchingTest(jtu.JaxTestCase):
     jacrev(func)(xs)  # don't crash
     jacfwd(func)(xs)  # don't crash
 
+  def testAny(self):
+    # test modeling the code in https://github.com/google/jax/issues/108
+
+    ans = vmap(np.any)(np.array([[True, False], [False, False]]))
+    expected = np.array([True, False])
+    self.assertAllClose(ans, expected, check_dtypes=True)
+
 
 if __name__ == '__main__':
   absltest.main()",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,693365c239adac1130956b4f3d0882551b31fae6,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,"np.all and np.any should lead to monoid reducers
fixes #108","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 04800e712..5d72ca4b3 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -142,8 +142,8 @@ JAX_REDUCER_RECORDS = [
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [
-    op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""all"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
+    op_record(""any"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
     op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
     op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 04800e712..5d72ca4b3 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -142,8 +142,8 @@ JAX_REDUCER_RECORDS = [
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [
-    op_record(""all"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""any"", 1, bool_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""all"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
+    op_record(""any"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
     op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
     op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]",No
jax/lax.py,jax/lax.py,b164d318fbc4b00f3cdebbe082cf3f07826f43f7,693365c239adac1130956b4f3d0882551b31fae6,"reduce_and / reduce_or monoid reducer primitives
The parent commit reused reduce_min / reduce_max on booleans, which is
formally equivalent but preserves less information when lowering to XLA.","diff --git a/jax/lax.py b/jax/lax.py
index 091e096ec..abc53a95a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -261,8 +261,8 @@ def reduce(operand, init_value, computation, dimensions):
     return monoid_reducer(operand, dimensions)
   else:
     jaxpr, consts = _reduction_jaxpr(computation, init_value)
-    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
-                         dimensions=tuple(dimensions))
+    return reduce_p.bind(operand, init_value, computation=computation,
+                         jaxpr=jaxpr, consts=consts, dimensions=tuple(dimensions))
 
 def _reduction_jaxpr(computation, init_value):
   pval = _abstractify(init_value)
@@ -274,10 +274,14 @@ def _get_monoid_reducer(monoid_op, x):
   if (type(aval) is ConcreteArray) and aval.shape == ():
     if monoid_op is add:
       return aval.val == 0 and _reduce_sum
-    elif monoid_op is max or monoid_op is bitwise_or and aval.dtype == onp.bool_:
+    elif monoid_op is max:
       return aval.val == _get_max_identity(aval.dtype) and _reduce_max
-    elif monoid_op is min or monoid_op is bitwise_and and aval.dtype == onp.bool_:
+    elif monoid_op is min:
       return aval.val == _get_min_identity(aval.dtype) and _reduce_min
+    elif monoid_op is bitwise_or and aval.dtype == onp.bool_:
+      return aval.val == _get_max_identity(aval.dtype) and _reduce_or
+    elif monoid_op is bitwise_and and aval.dtype == onp.bool_:
+      return aval.val == _get_min_identity(aval.dtype) and _reduce_and
 
 def _get_max_identity(dtype):
   if onp.issubdtype(dtype, onp.floating):
@@ -304,6 +308,12 @@ def _reduce_max(operand, axes):
 def _reduce_min(operand, axes):
   return reduce_min_p.bind(operand, axes=tuple(axes))
 
+def _reduce_or(operand, axes):
+  return reduce_or_p.bind(operand, axes=tuple(axes))
+
+def _reduce_and(operand, axes):
+  return reduce_and_p.bind(operand, axes=tuple(axes))
+
 def reduce_window(operand, init_value, computation, window_dimensions,
                   window_strides, padding):
   monoid_reducer = _get_monoid_window_reducer(computation, init_value)
@@ -1444,7 +1454,7 @@ def pad_batch_rule(batched_args, batch_dims, padding_config):
     padding_config.insert(operand_bdim, (0, 0, 0))
     return pad(operand, padding_value, padding_config), operand_bdim
   else:
-    raise NotImplementedError
+    raise NotImplementedError  # loop and stack
 
 pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
 ad.deflinear(pad_p, pad_transpose)
@@ -1824,20 +1834,31 @@ ad.primitive_jvps[index_untake_p] = index_untake_jvp
 ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule
 
 
-def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
+def reduce_shape_rule(operand, init_value, computation, jaxpr, consts, dimensions):
   return tuple(onp.delete(operand.shape, dimensions))
 
-def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
+def reduce_translation_rule(c, operand, init_value, computation, jaxpr, consts, dimensions):
   xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
   return c.Reduce(operand, init_value, xla_computation, dimensions)
 
+def reduce_batch_rule(batched_args, batch_dims, computation, jaxpr, consts, dimensions):
+  operand, init_value = batched_args
+  operand_bdim, init_value_bdim = batch_dims
+  if init_value_bdim is None:
+    assert operand_bdim is not None
+    new_dimensions = [d + bool(d >= operand_bdim) for d in dimensions]
+    new_operand_bdim = operand_bdim - onp.sum(onp.less(dimensions, operand_bdim))
+    return reduce(operand, init_value, computation, new_dimensions), new_operand_bdim
+  else:
+    raise NotImplementedError  # loop and stack
+
 def _reduction_computation(c, jaxpr, consts, init_value):
   shape = c.GetShape(init_value)
   return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)
 
 reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                               reduce_translation_rule)
-# batching.defreducer(reduce_p)  # TODO batching rule for general reduce
+# batching.primitive_batchers[reduce_p] = reduce_batch_rule  # TODO(mattjj): test
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):
@@ -1899,6 +1920,31 @@ ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
 batching.defreducer(reduce_min_p)
 
 
+def reduce_logical_shape_rule(operand, axes):
+  if operand.dtype != onp.bool_:
+    msg = ""logical reduction requires operand dtype bool, got {}.""
+    raise TypeError(msg.format(operand.dtype))
+  return tuple(onp.delete(operand.shape, axes))
+
+def reduce_logical_translation_rule(prim, identity, c, operand, axes):
+  scalar = xla_bridge.Shape.array_shape(onp.bool_, ())
+  return c.Reduce(operand, c.Constant(identity(onp.bool_)),
+                  xla.primitive_computation(prim, scalar, scalar), axes)
+
+reduce_or_translation_rule = partial(reduce_logical_translation_rule,
+                                     or_p, _get_max_identity)
+reduce_or_p = standard_primitive(reduce_logical_shape_rule, _fixed_dtype(onp.bool_),
+                                 'reduce_or', reduce_or_translation_rule)
+batching.defreducer(reduce_or_p)
+
+
+reduce_and_translation_rule = partial(reduce_logical_translation_rule,
+                                      and_p, _get_min_identity)
+reduce_and_p = standard_primitive(reduce_logical_shape_rule, _fixed_dtype(onp.bool_),
+                                 'reduce_and', reduce_and_translation_rule)
+batching.defreducer(reduce_and_p)
+
+
 def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                              window_dimensions, window_strides, padding):
   if operand.dtype != init_value.dtype:","diff --git a/jax/lax.py b/jax/lax.py
index 091e096ec..abc53a95a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -261,8 +261,8 @@ def reduce(operand, init_value, computation, dimensions):
     return monoid_reducer(operand, dimensions)
   else:
     jaxpr, consts = _reduction_jaxpr(computation, init_value)
-    return reduce_p.bind(operand, init_value, jaxpr=jaxpr, consts=consts,
-                         dimensions=tuple(dimensions))
+    return reduce_p.bind(operand, init_value, computation=computation,
+                         jaxpr=jaxpr, consts=consts, dimensions=tuple(dimensions))
 
 def _reduction_jaxpr(computation, init_value):
   pval = _abstractify(init_value)
@@ -274,10 +274,14 @@ def _get_monoid_reducer(monoid_op, x):
   if (type(aval) is ConcreteArray) and aval.shape == ():
     if monoid_op is add:
       return aval.val == 0 and _reduce_sum
-    elif monoid_op is max or monoid_op is bitwise_or and aval.dtype == onp.bool_:
+    elif monoid_op is max:
       return aval.val == _get_max_identity(aval.dtype) and _reduce_max
-    elif monoid_op is min or monoid_op is bitwise_and and aval.dtype == onp.bool_:
+    elif monoid_op is min:
       return aval.val == _get_min_identity(aval.dtype) and _reduce_min
+    elif monoid_op is bitwise_or and aval.dtype == onp.bool_:
+      return aval.val == _get_max_identity(aval.dtype) and _reduce_or
+    elif monoid_op is bitwise_and and aval.dtype == onp.bool_:
+      return aval.val == _get_min_identity(aval.dtype) and _reduce_and
 
 def _get_max_identity(dtype):
   if onp.issubdtype(dtype, onp.floating):
@@ -304,6 +308,12 @@ def _reduce_max(operand, axes):
 def _reduce_min(operand, axes):
   return reduce_min_p.bind(operand, axes=tuple(axes))
 
+def _reduce_or(operand, axes):
+  return reduce_or_p.bind(operand, axes=tuple(axes))
+
+def _reduce_and(operand, axes):
+  return reduce_and_p.bind(operand, axes=tuple(axes))
+
 def reduce_window(operand, init_value, computation, window_dimensions,
                   window_strides, padding):
   monoid_reducer = _get_monoid_window_reducer(computation, init_value)
@@ -1444,7 +1454,7 @@ def pad_batch_rule(batched_args, batch_dims, padding_config):
     padding_config.insert(operand_bdim, (0, 0, 0))
     return pad(operand, padding_value, padding_config), operand_bdim
   else:
-    raise NotImplementedError
+    raise NotImplementedError  # loop and stack
 
 pad_p = standard_primitive(pad_shape_rule, _input_dtype, 'pad')
 ad.deflinear(pad_p, pad_transpose)
@@ -1824,20 +1834,31 @@ ad.primitive_jvps[index_untake_p] = index_untake_jvp
 ad.primitive_transposes[index_untake_p] = index_untake_transpose_rule
 
 
-def reduce_shape_rule(operand, init_value, jaxpr, consts, dimensions):
+def reduce_shape_rule(operand, init_value, computation, jaxpr, consts, dimensions):
   return tuple(onp.delete(operand.shape, dimensions))
 
-def reduce_translation_rule(c, operand, init_value, jaxpr, consts, dimensions):
+def reduce_translation_rule(c, operand, init_value, computation, jaxpr, consts, dimensions):
   xla_computation = _reduction_computation(c, jaxpr, consts, init_value)
   return c.Reduce(operand, init_value, xla_computation, dimensions)
 
+def reduce_batch_rule(batched_args, batch_dims, computation, jaxpr, consts, dimensions):
+  operand, init_value = batched_args
+  operand_bdim, init_value_bdim = batch_dims
+  if init_value_bdim is None:
+    assert operand_bdim is not None
+    new_dimensions = [d + bool(d >= operand_bdim) for d in dimensions]
+    new_operand_bdim = operand_bdim - onp.sum(onp.less(dimensions, operand_bdim))
+    return reduce(operand, init_value, computation, new_dimensions), new_operand_bdim
+  else:
+    raise NotImplementedError  # loop and stack
+
 def _reduction_computation(c, jaxpr, consts, init_value):
   shape = c.GetShape(init_value)
   return xla.jaxpr_computation(jaxpr, consts, (), shape, shape)
 
 reduce_p = standard_primitive(reduce_shape_rule, _input_dtype, 'reduce',
                               reduce_translation_rule)
-# batching.defreducer(reduce_p)  # TODO batching rule for general reduce
+# batching.primitive_batchers[reduce_p] = reduce_batch_rule  # TODO(mattjj): test
 
 
 def reduce_sum_shape_rule(operand, axes, input_shape):
@@ -1899,6 +1920,31 @@ ad.defjvp2(reduce_min_p, reduce_chooser_jvp_rule)
 batching.defreducer(reduce_min_p)
 
 
+def reduce_logical_shape_rule(operand, axes):
+  if operand.dtype != onp.bool_:
+    msg = ""logical reduction requires operand dtype bool, got {}.""
+    raise TypeError(msg.format(operand.dtype))
+  return tuple(onp.delete(operand.shape, axes))
+
+def reduce_logical_translation_rule(prim, identity, c, operand, axes):
+  scalar = xla_bridge.Shape.array_shape(onp.bool_, ())
+  return c.Reduce(operand, c.Constant(identity(onp.bool_)),
+                  xla.primitive_computation(prim, scalar, scalar), axes)
+
+reduce_or_translation_rule = partial(reduce_logical_translation_rule,
+                                     or_p, _get_max_identity)
+reduce_or_p = standard_primitive(reduce_logical_shape_rule, _fixed_dtype(onp.bool_),
+                                 'reduce_or', reduce_or_translation_rule)
+batching.defreducer(reduce_or_p)
+
+
+reduce_and_translation_rule = partial(reduce_logical_translation_rule,
+                                      and_p, _get_min_identity)
+reduce_and_p = standard_primitive(reduce_logical_shape_rule, _fixed_dtype(onp.bool_),
+                                 'reduce_and', reduce_and_translation_rule)
+batching.defreducer(reduce_and_p)
+
+
 def reduce_window_shape_rule(operand, init_value, jaxpr, consts,
                              window_dimensions, window_strides, padding):
   if operand.dtype != init_value.dtype:",No
WORKSPACE,WORKSPACE,3d4cb9f95538fc9ca15589a0747126a0b9f87073,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,"Enable MKL-DNN contraction kernels.

Add a new build option --enable_mkl_dnn that enables MKLDNN contraction kernels in XLA. This leads to significant performance improvements for XLA's dot operator. Enable MKL-DNN by default.

Update XLA version to include MKL-DNN build fix.

Also add a new --enable_march_native build option that turns on -march=native. This is unlikely to have a significant performance impact since XLA JIT-compiles most of its code. Leaving this off by default because it also generates code unlikely to run across a wide selection of architectures and so is unsuitable for building pip wheels.","diff --git a/WORKSPACE b/WORKSPACE
index ddacdf819..50c367b1c 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,12 +16,12 @@ http_archive(
 #    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
 #    and update the sha256 with the result.
 http_archive(
-    name = ""org_tensorflow"",
-    sha256 = ""346f7d3a4248cfb4c37d5c8eed168c3cad35c54c4da6d4bf36294a144c6953c0"",
-    strip_prefix = ""tensorflow-27cffd795981f6d86e3b09b6d82c384d8b4e117a"",
-    urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/27cffd795981f6d86e3b09b6d82c384d8b4e117a.tar.gz"",
-    ],
+   name = ""org_tensorflow"",
+   sha256 = ""e4bee8b29c3fa3d22f554558dee71b64bc40e10e9bc673be575ff8502525dfa2"",
+   strip_prefix = ""tensorflow-5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d"",
+   urls = [
+       ""https://github.com/tensorflow/tensorflow/archive/5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d.tar.gz"",
+   ],
 )
 
 # For development, one can use a local TF repository instead.","diff --git a/WORKSPACE b/WORKSPACE
index ddacdf819..50c367b1c 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -16,12 +16,12 @@ http_archive(
 #    curl -L https://github.com/tensorflow/tensorflow/archive/<git hash>.tar.gz | sha256sum
 #    and update the sha256 with the result.
 http_archive(
-    name = ""org_tensorflow"",
-    sha256 = ""346f7d3a4248cfb4c37d5c8eed168c3cad35c54c4da6d4bf36294a144c6953c0"",
-    strip_prefix = ""tensorflow-27cffd795981f6d86e3b09b6d82c384d8b4e117a"",
-    urls = [
-        ""https://github.com/tensorflow/tensorflow/archive/27cffd795981f6d86e3b09b6d82c384d8b4e117a.tar.gz"",
-    ],
+   name = ""org_tensorflow"",
+   sha256 = ""e4bee8b29c3fa3d22f554558dee71b64bc40e10e9bc673be575ff8502525dfa2"",
+   strip_prefix = ""tensorflow-5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d"",
+   urls = [
+       ""https://github.com/tensorflow/tensorflow/archive/5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d.tar.gz"",
+   ],
 )
 
 # For development, one can use a local TF repository instead.",No
build/build.py,build/build.py,3d4cb9f95538fc9ca15589a0747126a0b9f87073,87ee4b7c5642ad31b90f3ef3c51b9aef83c80377,"Enable MKL-DNN contraction kernels.

Add a new build option --enable_mkl_dnn that enables MKLDNN contraction kernels in XLA. This leads to significant performance improvements for XLA's dot operator. Enable MKL-DNN by default.

Update XLA version to include MKL-DNN build fix.

Also add a new --enable_march_native build option that turns on -march=native. This is unlikely to have a significant performance impact since XLA JIT-compiles most of its code. Leaving this off by default because it also generates code unlikely to run across a wide selection of architectures and so is unsuitable for building pip wheels.","diff --git a/build/build.py b/build/build.py
index 7bb0f599c..91664a224 100755
--- a/build/build.py
+++ b/build/build.py
@@ -164,9 +164,11 @@ build --action_env TF_NEED_CUDA=""{tf_need_cuda}""
 build --action_env CUDA_TOOLKIT_PATH=""{cuda_toolkit_path}""
 build --action_env CUDNN_INSTALL_PATH=""{cudnn_install_path}""
 build --distinct_host_configuration=false
+build --copt=-Wno-sign-compare
+build -c opt
 build:opt --copt=-march=native
-build:opt --copt=-Wno-sign-compare
 build:opt --host_copt=-march=native
+build:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=1
 """"""
 
 
@@ -231,6 +233,18 @@ def main():
       ""--python_bin_path"",
       help=""Path to Python binary to use. The default is the Python ""
       ""interpreter used to run the build script."")
+  add_boolean_argument(
+      parser,
+      ""enable_march_native"",
+      default=False,
+      help_str=""Generate code targeted to the current machine? This may ""
+          ""increase performance, but may generate code that does not run on ""
+          ""older machines."")
+  add_boolean_argument(
+      parser,
+      ""enable_mkl_dnn"",
+      default=True,
+      help_str=""Should we build with MKL-DNN enabled?"")
   add_boolean_argument(
       parser,
       ""enable_cuda"",
@@ -256,6 +270,9 @@ def main():
   python_bin_path = get_python_bin_path(args.python_bin_path)
   print(""Python binary path: {}"".format(python_bin_path))
 
+  print(""MKL-DNN enabled: {}"".format(""yes"" if args.enable_mkl_dnn else ""no""))
+  print(""-march=native: {}"".format(""yes"" if args.enable_march_native else ""no""))
+
   cuda_toolkit_path = args.cuda_path
   cudnn_install_path = args.cudnn_path
   print(""CUDA enabled: {}"".format(""yes"" if args.enable_cuda else ""no""))
@@ -269,10 +286,15 @@ def main():
       cudnn_install_path=cudnn_install_path)
 
   print(""\nBuilding XLA and installing it in the jaxlib source tree..."")
-  shell([
-      bazel_path, ""run"", ""-c"", ""opt"", "":install_xla_in_source_tree"",
-      os.getcwd()
-  ])
+  config_args = []
+  if args.enable_march_native:
+    config_args += [""--config=opt""]
+  if args.enable_mkl_dnn:
+    config_args += [""--config=mkl_open_source_only""]
+  shell(
+    [bazel_path, ""run"", ""--verbose_failures=true""] +
+    config_args +
+    ["":install_xla_in_source_tree"", os.getcwd()])
 
 
 if __name__ == ""__main__"":","diff --git a/build/build.py b/build/build.py
index 7bb0f599c..91664a224 100755
--- a/build/build.py
+++ b/build/build.py
@@ -164,9 +164,11 @@ build --action_env TF_NEED_CUDA=""{tf_need_cuda}""
 build --action_env CUDA_TOOLKIT_PATH=""{cuda_toolkit_path}""
 build --action_env CUDNN_INSTALL_PATH=""{cudnn_install_path}""
 build --distinct_host_configuration=false
+build --copt=-Wno-sign-compare
+build -c opt
 build:opt --copt=-march=native
-build:opt --copt=-Wno-sign-compare
 build:opt --host_copt=-march=native
+build:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=1
 """"""
 
 
@@ -231,6 +233,18 @@ def main():
       ""--python_bin_path"",
       help=""Path to Python binary to use. The default is the Python ""
       ""interpreter used to run the build script."")
+  add_boolean_argument(
+      parser,
+      ""enable_march_native"",
+      default=False,
+      help_str=""Generate code targeted to the current machine? This may ""
+          ""increase performance, but may generate code that does not run on ""
+          ""older machines."")
+  add_boolean_argument(
+      parser,
+      ""enable_mkl_dnn"",
+      default=True,
+      help_str=""Should we build with MKL-DNN enabled?"")
   add_boolean_argument(
       parser,
       ""enable_cuda"",
@@ -256,6 +270,9 @@ def main():
   python_bin_path = get_python_bin_path(args.python_bin_path)
   print(""Python binary path: {}"".format(python_bin_path))
 
+  print(""MKL-DNN enabled: {}"".format(""yes"" if args.enable_mkl_dnn else ""no""))
+  print(""-march=native: {}"".format(""yes"" if args.enable_march_native else ""no""))
+
   cuda_toolkit_path = args.cuda_path
   cudnn_install_path = args.cudnn_path
   print(""CUDA enabled: {}"".format(""yes"" if args.enable_cuda else ""no""))
@@ -269,10 +286,15 @@ def main():
       cudnn_install_path=cudnn_install_path)
 
   print(""\nBuilding XLA and installing it in the jaxlib source tree..."")
-  shell([
-      bazel_path, ""run"", ""-c"", ""opt"", "":install_xla_in_source_tree"",
-      os.getcwd()
-  ])
+  config_args = []
+  if args.enable_march_native:
+    config_args += [""--config=opt""]
+  if args.enable_mkl_dnn:
+    config_args += [""--config=mkl_open_source_only""]
+  shell(
+    [bazel_path, ""run"", ""--verbose_failures=true""] +
+    config_args +
+    ["":install_xla_in_source_tree"", os.getcwd()])
 
 
 if __name__ == ""__main__"":",No
README.md,README.md,aa7c8c367b8cacbc6b0784de643eb015fb5b6263,7cf0e9b1a6fce6dc9552cd327989717a32bf1b2d,tweak minmax example imports in readme,"diff --git a/README.md b/README.md
index 80459d91b..2eb33052c 100644
--- a/README.md
+++ b/README.md
@@ -505,11 +505,7 @@ Heres an example:
 
 ```python
 from jax.experimental import stax
-from jax.experimental.stax import Conv
-from jax.experimental.stax import Dense
-from jax.experimental.stax import MaxPool
-from jax.experimental.stax import Relu
-from jax.experimental.stax import LogSoftmax
+from jax.experimental.stax import Conv, Dense, MaxPool, Relu, LogSoftmax
 
 # Set up network initialization and evaluation functions
 net_init, net_apply = stax.serial(","diff --git a/README.md b/README.md
index 80459d91b..2eb33052c 100644
--- a/README.md
+++ b/README.md
@@ -505,11 +505,7 @@ Heres an example:
 
 ```python
 from jax.experimental import stax
-from jax.experimental.stax import Conv
-from jax.experimental.stax import Dense
-from jax.experimental.stax import MaxPool
-from jax.experimental.stax import Relu
-from jax.experimental.stax import LogSoftmax
+from jax.experimental.stax import Conv, Dense, MaxPool, Relu, LogSoftmax
 
 # Set up network initialization and evaluation functions
 net_init, net_apply = stax.serial(",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,5d6ebba2a0a991eff4fdf80f689f2ca3e7f0b41c,aa7c8c367b8cacbc6b0784de643eb015fb5b6263,Fixed argument order in call to var from std.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef92ae235..7e7f8bb67 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -678,7 +678,7 @@ def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
 
 @_wraps(onp.std)
 def std(a, axis=None, keepdims=False, ddof=0):
-  return sqrt(var(a, axis, keepdims, ddof))
+  return sqrt(var(a, axis=axis, keepdims=keepdims, ddof=ddof))
 
 
 @_wraps(onp.allclose)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef92ae235..7e7f8bb67 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -678,7 +678,7 @@ def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
 
 @_wraps(onp.std)
 def std(a, axis=None, keepdims=False, ddof=0):
-  return sqrt(var(a, axis, keepdims, ddof))
+  return sqrt(var(a, axis=axis, keepdims=keepdims, ddof=ddof))
 
 
 @_wraps(onp.allclose)",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,c268929f2d3452dc4c0f93ef57c44459ee477a82,4919a4818cc109668ee8048ee9fc54522c4b3576,"add 'dtype' arg to np.std, add test coverage","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7e7f8bb67..649f360f8 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -677,8 +677,8 @@ def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
 
 
 @_wraps(onp.std)
-def std(a, axis=None, keepdims=False, ddof=0):
-  return sqrt(var(a, axis=axis, keepdims=keepdims, ddof=ddof))
+def std(a, axis=None, dtype=None, keepdims=False, ddof=0):
+  return sqrt(var(a, axis=axis, dtype=dtype, keepdims=keepdims, ddof=ddof))
 
 
 @_wraps(onp.allclose)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7e7f8bb67..649f360f8 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -677,8 +677,8 @@ def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
 
 
 @_wraps(onp.std)
-def std(a, axis=None, keepdims=False, ddof=0):
-  return sqrt(var(a, axis=axis, keepdims=keepdims, ddof=ddof))
+def std(a, axis=None, dtype=None, keepdims=False, ddof=0):
+  return sqrt(var(a, axis=axis, dtype=dtype, keepdims=keepdims, ddof=ddof))
 
 
 @_wraps(onp.allclose)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,c268929f2d3452dc4c0f93ef57c44459ee477a82,4919a4818cc109668ee8048ee9fc54522c4b3576,"add 'dtype' arg to np.std, add test coverage","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5d72ca4b3..8459d5041 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -139,6 +139,7 @@ JAX_REDUCER_RECORDS = [
     op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""std"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5d72ca4b3..8459d5041 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -139,6 +139,7 @@ JAX_REDUCER_RECORDS = [
     op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""std"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [",No
.travis.yml,.travis.yml,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,c268929f2d3452dc4c0f93ef57c44459ee477a82,add test-running instructions (fixes #67),"diff --git a/.travis.yml b/.travis.yml
index 0bceb1e3f..a0880b2d3 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -23,5 +23,4 @@ install:
   - pip install jaxlib
   - pip install -v .
 script:
-  - cd tests
-  - JAX_NUM_GENERATED_CASES=10 nosetests
+  - JAX_NUM_GENERATED_CASES=25 nosetests tests examples","diff --git a/.travis.yml b/.travis.yml
index 0bceb1e3f..a0880b2d3 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -23,5 +23,4 @@ install:
   - pip install jaxlib
   - pip install -v .
 script:
-  - cd tests
-  - JAX_NUM_GENERATED_CASES=10 nosetests
+  - JAX_NUM_GENERATED_CASES=25 nosetests tests examples",No
README.md,README.md,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,c268929f2d3452dc4c0f93ef57c44459ee477a82,add test-running instructions (fixes #67),"diff --git a/README.md b/README.md
index 2eb33052c..1d948e06b 100644
--- a/README.md
+++ b/README.md
@@ -66,6 +66,7 @@ open](https://github.com/google/jax) by a growing number of
 ### Contents
 * [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)
 * [Installation](#installation)
+* [Running the tests](#running-the-tests)
 * [A brief tour](#a-brief-tour)
 * [What's supported](#whats-supported)
 * [Transformations](#transformations)
@@ -158,6 +159,30 @@ nvcc --version
 grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
 ```
 
+## Running the tests
+
+To run all the JAX tests, from the repository root directory run
+
+```bash
+nosetests tests
+```
+
+JAX generates test cases combinatorially, and you can control the number of
+cases that are generated and checked for each test (default 10):
+
+```bash
+JAX_NUM_GENERATED_CASES=100 nosetests tests
+```
+
+You can run a more specific set of tests using
+[`nose`](https://nose.readthedocs.io/en/latest/usage.html)'s built-in selection
+mechanisms, or alternatively you can run a specific test file directly to see
+more detailed information about the cases being run:
+
+```bash
+python tests/lax_numpy_test.py --num_generated_cases=5
+```
+
 ## A brief tour
 
 ```python","diff --git a/README.md b/README.md
index 2eb33052c..1d948e06b 100644
--- a/README.md
+++ b/README.md
@@ -66,6 +66,7 @@ open](https://github.com/google/jax) by a growing number of
 ### Contents
 * [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)
 * [Installation](#installation)
+* [Running the tests](#running-the-tests)
 * [A brief tour](#a-brief-tour)
 * [What's supported](#whats-supported)
 * [Transformations](#transformations)
@@ -158,6 +159,30 @@ nvcc --version
 grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
 ```
 
+## Running the tests
+
+To run all the JAX tests, from the repository root directory run
+
+```bash
+nosetests tests
+```
+
+JAX generates test cases combinatorially, and you can control the number of
+cases that are generated and checked for each test (default 10):
+
+```bash
+JAX_NUM_GENERATED_CASES=100 nosetests tests
+```
+
+You can run a more specific set of tests using
+[`nose`](https://nose.readthedocs.io/en/latest/usage.html)'s built-in selection
+mechanisms, or alternatively you can run a specific test file directly to see
+more detailed information about the cases being run:
+
+```bash
+python tests/lax_numpy_test.py --num_generated_cases=5
+```
+
 ## A brief tour
 
 ```python",No
tests/examples_test.py,examples/examples_test.py,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,c268929f2d3452dc4c0f93ef57c44459ee477a82,add test-running instructions (fixes #67),"diff --git a/examples/examples_test.py b/examples/examples_test.py
new file mode 100644
index 000000000..9d5810e4d
--- /dev/null
+++ b/examples/examples_test.py
@@ -0,0 +1,101 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import sys
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import test_util as jtu
+import jax.numpy as np
+
+sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+from examples import kernel_lsq
+from examples import resnet50
+sys.path.pop()
+
+from jax.config import config
+config.parse_flags_with_absl()
+FLAGS = config.FLAGS
+
+
+def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
+  result_shape, params = init_fun(input_shape)
+  rng = onp.random.RandomState(0)
+  result = apply_fun(params, rng.randn(*input_shape).astype(dtype=""float32""))
+  test_case.assertEqual(result.shape, result_shape)
+
+
+class ExamplesTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 20, 25, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
+  def testIdentityBlockShape(self, input_shape):
+    init_fun, apply_fun = resnet50.IdentityBlock(2, (4, 3))
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 20, 25, 3)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
+  def testConvBlockShape(self, input_shape):
+    init_fun, apply_fun = resnet50.ConvBlock(3, (2, 3, 4))
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_num_classes={}_input_shape={}""
+                        .format(num_classes, input_shape),
+       ""num_classes"": num_classes, ""input_shape"": input_shape}
+      for num_classes in [5, 10]
+      for input_shape in [(224, 224, 3, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
+  def testResNet50Shape(self, num_classes, input_shape):
+    init_fun, apply_fun = resnet50.ResNet50(num_classes)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  def testKernelRegressionGram(self):
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    assert np.all(kernel_lsq.gram(kernel, xs) == np.dot(xs, xs.T))
+
+  def testKernelRegressionTrainAndPredict(self):
+    # TODO(frostig): reenable this test.
+    self.skipTest(""Test is broken"")
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    predict = kernel_lsq.train(kernel, xs, ys)
+    assert np.allclose(predict(xs), ys, atol=1e-3)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/examples/examples_test.py b/examples/examples_test.py
new file mode 100644
index 000000000..9d5810e4d
--- /dev/null
+++ b/examples/examples_test.py
@@ -0,0 +1,101 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import sys
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import test_util as jtu
+import jax.numpy as np
+
+sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+from examples import kernel_lsq
+from examples import resnet50
+sys.path.pop()
+
+from jax.config import config
+config.parse_flags_with_absl()
+FLAGS = config.FLAGS
+
+
+def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
+  result_shape, params = init_fun(input_shape)
+  rng = onp.random.RandomState(0)
+  result = apply_fun(params, rng.randn(*input_shape).astype(dtype=""float32""))
+  test_case.assertEqual(result.shape, result_shape)
+
+
+class ExamplesTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 20, 25, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
+  def testIdentityBlockShape(self, input_shape):
+    init_fun, apply_fun = resnet50.IdentityBlock(2, (4, 3))
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 20, 25, 3)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
+  def testConvBlockShape(self, input_shape):
+    init_fun, apply_fun = resnet50.ConvBlock(3, (2, 3, 4))
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_num_classes={}_input_shape={}""
+                        .format(num_classes, input_shape),
+       ""num_classes"": num_classes, ""input_shape"": input_shape}
+      for num_classes in [5, 10]
+      for input_shape in [(224, 224, 3, 2)])
+  @jtu.skip_on_flag('jax_enable_x64', True)
+  def testResNet50Shape(self, num_classes, input_shape):
+    init_fun, apply_fun = resnet50.ResNet50(num_classes)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  def testKernelRegressionGram(self):
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    assert np.all(kernel_lsq.gram(kernel, xs) == np.dot(xs, xs.T))
+
+  def testKernelRegressionTrainAndPredict(self):
+    # TODO(frostig): reenable this test.
+    self.skipTest(""Test is broken"")
+    n, d = 100, 20
+    rng = onp.random.RandomState(0)
+    truth = rng.randn(d)
+    xs = rng.randn(n, d)
+    ys = np.dot(xs, truth)
+    kernel = lambda x, y: np.dot(x, y)
+    predict = kernel_lsq.train(kernel, xs, ys)
+    assert np.allclose(predict(xs), ys, atol=1e-3)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/test_util.py,jax/test_util.py,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,c268929f2d3452dc4c0f93ef57c44459ee477a82,add test-running instructions (fixes #67),"diff --git a/jax/test_util.py b/jax/test_util.py
index 6831da56f..2a4b2bf4f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -46,7 +46,7 @@ flags.DEFINE_enum(
 
 flags.DEFINE_integer(
   'num_generated_cases',
-  os.getenv('JAX_NUM_GENERATED_CASES', 100),
+  os.getenv('JAX_NUM_GENERATED_CASES', 10),
   help='Number of generated cases to test')
 
 EPS = 1e-4","diff --git a/jax/test_util.py b/jax/test_util.py
index 6831da56f..2a4b2bf4f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -46,7 +46,7 @@ flags.DEFINE_enum(
 
 flags.DEFINE_integer(
   'num_generated_cases',
-  os.getenv('JAX_NUM_GENERATED_CASES', 100),
+  os.getenv('JAX_NUM_GENERATED_CASES', 10),
   help='Number of generated cases to test')
 
 EPS = 1e-4",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,c268929f2d3452dc4c0f93ef57c44459ee477a82,add test-running instructions (fixes #67),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 8459d5041..5730ddae7 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -139,7 +139,7 @@ JAX_REDUCER_RECORDS = [
     op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""std"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""std"", 1, float_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 8459d5041..5730ddae7 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -139,7 +139,7 @@ JAX_REDUCER_RECORDS = [
     op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""std"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""std"", 1, float_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [",No
.travis.yml,.travis.yml,13b8e21a1cfb3a76ae65bf61bb7bf855dcea1c5f,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,"squash conv grad bug introduced in 0d64aea
(loudly errored, didn't produce silently incorrect results!)","diff --git a/.travis.yml b/.travis.yml
index a0880b2d3..519d1a035 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -23,4 +23,4 @@ install:
   - pip install jaxlib
   - pip install -v .
 script:
-  - JAX_NUM_GENERATED_CASES=25 nosetests tests examples
+  - JAX_NUM_GENERATED_CASES=100 nosetests tests examples","diff --git a/.travis.yml b/.travis.yml
index a0880b2d3..519d1a035 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -23,4 +23,4 @@ install:
   - pip install jaxlib
   - pip install -v .
 script:
-  - JAX_NUM_GENERATED_CASES=25 nosetests tests examples
+  - JAX_NUM_GENERATED_CASES=100 nosetests tests examples",No
jax/lax.py,jax/lax.py,13b8e21a1cfb3a76ae65bf61bb7bf855dcea1c5f,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,"squash conv grad bug introduced in 0d64aea
(loudly errored, didn't produce silently incorrect results!)","diff --git a/jax/lax.py b/jax/lax.py
index abc53a95a..5f8b970b0 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1027,8 +1027,8 @@ def conv_general_dilated_transpose_rhs(
     dimension_numbers, lhs_shape, rhs_shape):
   assert type(dimension_numbers) is ConvDimensionNumbers
   lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
-  transposed = map(_conv_transpose, dimension_numbers)
-  trans_dimension_numbers = ConvDimensionNumbers(*transposed)
+  lhs_trans, rhs_trans, out_trans = map(_conv_transpose, dimension_numbers)
+  trans_dimension_numbers = ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
   padding = _conv_general_vjp_rhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
       window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,","diff --git a/jax/lax.py b/jax/lax.py
index abc53a95a..5f8b970b0 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1027,8 +1027,8 @@ def conv_general_dilated_transpose_rhs(
     dimension_numbers, lhs_shape, rhs_shape):
   assert type(dimension_numbers) is ConvDimensionNumbers
   lhs_sdims, rhs_sdims, out_sdims = map(_conv_sdims, dimension_numbers)
-  transposed = map(_conv_transpose, dimension_numbers)
-  trans_dimension_numbers = ConvDimensionNumbers(*transposed)
+  lhs_trans, rhs_trans, out_trans = map(_conv_transpose, dimension_numbers)
+  trans_dimension_numbers = ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
   padding = _conv_general_vjp_rhs_padding(
       onp.take(lhs_shape, lhs_sdims), onp.take(rhs_shape, rhs_sdims),
       window_strides, onp.take(g.shape, out_sdims), padding, lhs_dilation,",No
jax/test_util.py,jax/test_util.py,13b8e21a1cfb3a76ae65bf61bb7bf855dcea1c5f,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,"squash conv grad bug introduced in 0d64aea
(loudly errored, didn't produce silently incorrect results!)","diff --git a/jax/test_util.py b/jax/test_util.py
index 2a4b2bf4f..c73d58202 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -71,6 +71,7 @@ def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
   if testing_tpu or testing_x32:
     atol = max(atol, 1e-1)
     rtol = max(rtol, 1e-1)
+  assert a.shape == b.shape
   return onp.allclose(a, b, atol=atol * a.size, rtol=rtol * b.size,
                       equal_nan=equal_nan)
 ","diff --git a/jax/test_util.py b/jax/test_util.py
index 2a4b2bf4f..c73d58202 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -71,6 +71,7 @@ def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
   if testing_tpu or testing_x32:
     atol = max(atol, 1e-1)
     rtol = max(rtol, 1e-1)
+  assert a.shape == b.shape
   return onp.allclose(a, b, atol=atol * a.size, rtol=rtol * b.size,
                       equal_nan=equal_nan)
 ",No
tests/lax_test.py,tests/lax_test.py,13b8e21a1cfb3a76ae65bf61bb7bf855dcea1c5f,6de5c8a69855e403b6dfdc92b1d64177eb9864e8,"squash conv grad bug introduced in 0d64aea
(loudly errored, didn't produce silently incorrect results!)","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 17fa7f554..983a4e892 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1580,10 +1580,13 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 5, 6), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
-           [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],
-           [(1, 1), (2, 1)], [(1, 1)])
-          for b, i, j in itertools.product([2, 3], repeat=3)]
+          ((b, i, 5, 6),  # lhs_shape
+           (j, i, 1, 2),  # rhs_shape
+           [(1, 1), (1, 2), (2, 1)],  # strides
+           [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],  # pads
+           [(1, 1), (2, 1)],  # lhs_dils
+           [(1, 1)])  # rhs_dils
+          for b, i, j in itertools.product([1, 2], repeat=3)]
       for strides in all_strides
       for rhs_dil in rhs_dils
       for lhs_dil in lhs_dils
@@ -1592,7 +1595,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]
       for dim_nums, perms in [
           ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
-          # ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))
       ]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 17fa7f554..983a4e892 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1580,10 +1580,13 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 5, 6), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
-           [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],
-           [(1, 1), (2, 1)], [(1, 1)])
-          for b, i, j in itertools.product([2, 3], repeat=3)]
+          ((b, i, 5, 6),  # lhs_shape
+           (j, i, 1, 2),  # rhs_shape
+           [(1, 1), (1, 2), (2, 1)],  # strides
+           [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],  # pads
+           [(1, 1), (2, 1)],  # lhs_dils
+           [(1, 1)])  # rhs_dils
+          for b, i, j in itertools.product([1, 2], repeat=3)]
       for strides in all_strides
       for rhs_dil in rhs_dils
       for lhs_dil in lhs_dils
@@ -1592,7 +1595,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]
       for dim_nums, perms in [
           ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
-          # ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))
       ]))
   @jtu.skip_on_devices(""tpu"")
   def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,",No
tests/lax_test.py,tests/lax_test.py,f437ba371ddfdbd651469f5fb0863486ab847b06,13b8e21a1cfb3a76ae65bf61bb7bf855dcea1c5f,make conv grad test smaller (better numerics),"diff --git a/tests/lax_test.py b/tests/lax_test.py
index 983a4e892..f90168264 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1580,7 +1580,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 5, 6),  # lhs_shape
+          ((b, i, 4, 5),  # lhs_shape
            (j, i, 1, 2),  # rhs_shape
            [(1, 1), (1, 2), (2, 1)],  # strides
            [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],  # pads","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 983a4e892..f90168264 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1580,7 +1580,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 5, 6),  # lhs_shape
+          ((b, i, 4, 5),  # lhs_shape
            (j, i, 1, 2),  # rhs_shape
            [(1, 1), (1, 2), (2, 1)],  # strides
            [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],  # pads",No
tests/lax_test.py,tests/lax_test.py,6956e8a20c2c93bfb531cae0da18ec20d0abe911,f437ba371ddfdbd651469f5fb0863486ab847b06,tweak conv grad test,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index f90168264..8ef4f77d2 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1580,7 +1580,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 4, 5),  # lhs_shape
+          ((b, i, 6, 7),  # lhs_shape
            (j, i, 1, 2),  # rhs_shape
            [(1, 1), (1, 2), (2, 1)],  # strides
            [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],  # pads","diff --git a/tests/lax_test.py b/tests/lax_test.py
index f90168264..8ef4f77d2 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1580,7 +1580,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
        ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
        ""perms"": perms}
       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
-          ((b, i, 4, 5),  # lhs_shape
+          ((b, i, 6, 7),  # lhs_shape
            (j, i, 1, 2),  # rhs_shape
            [(1, 1), (1, 2), (2, 1)],  # strides
            [((0, 0), (0, 0)), ((1, 0), (0, 1)), ((0, -1), (0, 0))],  # pads",No
setup.py,setup.py,fe059f3acc00c4a422a3178cbc878b87e7b30ca8,ea5c03c8f372f654ed4be6bc149c00d244066e70,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 061bd0768..eaa44339a 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.9',
+    version='0.1.10',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 061bd0768..eaa44339a 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.9',
+    version='0.1.10',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
jax/numpy/linalg.py,jax/numpy/linalg.py,13a135d4246f4b436e0891704fb9a49df43e7c61,fe059f3acc00c4a422a3178cbc878b87e7b30ca8,"Implement lower=False case for scipy.linalg.cholesky.

Remove np.linalg.{dot,matmul,trace}, because these aren't part of the numpy API. I had previously misinterpreted the np.linalg documentation to mean that they also existed in that module.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 6447cdbc4..2af0d4008 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -26,10 +26,6 @@ from . import lax_numpy as np
 from ..util import get_module_functions
 
 
-dot = np.dot
-matmul = np.matmul
-trace = np.trace
-
 _T = lambda x: np.swapaxes(x, -1, -2)
 
 @_wraps(onp.linalg.cholesky)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 6447cdbc4..2af0d4008 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -26,10 +26,6 @@ from . import lax_numpy as np
 from ..util import get_module_functions
 
 
-dot = np.dot
-matmul = np.matmul
-trace = np.trace
-
 _T = lambda x: np.swapaxes(x, -1, -2)
 
 @_wraps(onp.linalg.cholesky)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,13a135d4246f4b436e0891704fb9a49df43e7c61,fe059f3acc00c4a422a3178cbc878b87e7b30ca8,"Implement lower=False case for scipy.linalg.cholesky.

Remove np.linalg.{dot,matmul,trace}, because these aren't part of the numpy API. I had previously misinterpreted the np.linalg documentation to mean that they also existed in that module.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 628354d39..6200e072c 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -27,11 +27,8 @@ from ..numpy import linalg as np_linalg
 @_wraps(scipy.linalg.cholesky)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
-  if not lower:
-    raise NotImplementedError(
-        ""The lower=False case of Cholesky is not implemented."")
-
-  return lax_linalg.cholesky(a)
+  l = lax_linalg.cholesky(a)
+  return l if lower else np.conj(l.T)
 
 
 @_wraps(scipy.linalg.inv)","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 628354d39..6200e072c 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -27,11 +27,8 @@ from ..numpy import linalg as np_linalg
 @_wraps(scipy.linalg.cholesky)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
-  if not lower:
-    raise NotImplementedError(
-        ""The lower=False case of Cholesky is not implemented."")
-
-  return lax_linalg.cholesky(a)
+  l = lax_linalg.cholesky(a)
+  return l if lower else np.conj(l.T)
 
 
 @_wraps(scipy.linalg.inv)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,06b7e54c020e77f479a859ff270b18146b14e6ad,13a135d4246f4b436e0891704fb9a49df43e7c61,"Fix bug in ""economic"" mode in jax.scipy.linalg.qr where it returned the full decomposition.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 6200e072c..2753d7895 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -47,7 +47,7 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
   if mode in (""full"", ""r""):
     full_matrices = True
   elif mode == ""economic"":
-    full_matrices = True
+    full_matrices = False
   else:
     raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
   q, r = lax_linalg.qr(a, full_matrices)","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 6200e072c..2753d7895 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -47,7 +47,7 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
   if mode in (""full"", ""r""):
     full_matrices = True
   elif mode == ""economic"":
-    full_matrices = True
+    full_matrices = False
   else:
     raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
   q, r = lax_linalg.qr(a, full_matrices)",No
examples/BUILD,examples/BUILD,a97f8209615799d0dd628dbc508aac2f8ed27177,06b7e54c020e77f479a859ff270b18146b14e6ad,update examples/BUILD,"diff --git a/examples/BUILD b/examples/BUILD
index 145b14e7f..fb0010601 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -14,8 +14,15 @@
 
 licenses([""notice""])  # Apache 2
 
+load(""//tests:build_defs.bzl"", ""jax_test"")
+
 package(default_visibility = [""//visibility:public""])
 
+config_setting(
+    name = ""libjax_depend"",
+    values = {""define"": ""libjax_depend=true""},
+)
+
 py_library(
     name = ""datasets"",
     srcs = [""datasets.py""],
@@ -26,7 +33,6 @@ py_binary(
     srcs = [""mnist_classifier_fromscratch.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
     ],
 )
 
@@ -35,9 +41,6 @@ py_binary(
     srcs = [""mnist_classifier.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )
 
@@ -46,9 +49,6 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )
 
@@ -57,16 +57,15 @@ py_binary(
     srcs = [""resnet50.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )
 
 py_binary(
     name = ""kernel_lsq"",
     srcs = [""kernel_lsq.py""],
-    deps = [
-        ""//jax:libjax"",
-    ],
+)
+
+jax_test(
+    name = ""examples_test"",
+    srcs = [""examples_test.py""],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 145b14e7f..fb0010601 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -14,8 +14,15 @@
 
 licenses([""notice""])  # Apache 2
 
+load(""//tests:build_defs.bzl"", ""jax_test"")
+
 package(default_visibility = [""//visibility:public""])
 
+config_setting(
+    name = ""libjax_depend"",
+    values = {""define"": ""libjax_depend=true""},
+)
+
 py_library(
     name = ""datasets"",
     srcs = [""datasets.py""],
@@ -26,7 +33,6 @@ py_binary(
     srcs = [""mnist_classifier_fromscratch.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
     ],
 )
 
@@ -35,9 +41,6 @@ py_binary(
     srcs = [""mnist_classifier.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )
 
@@ -46,9 +49,6 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )
 
@@ -57,16 +57,15 @@ py_binary(
     srcs = [""resnet50.py""],
     deps = [
         "":datasets"",
-        ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )
 
 py_binary(
     name = ""kernel_lsq"",
     srcs = [""kernel_lsq.py""],
-    deps = [
-        ""//jax:libjax"",
-    ],
+)
+
+jax_test(
+    name = ""examples_test"",
+    srcs = [""examples_test.py""],
 )",No
tests/BUILD,tests/BUILD,a97f8209615799d0dd628dbc508aac2f8ed27177,06b7e54c020e77f479a859ff270b18146b14e6ad,update examples/BUILD,"diff --git a/tests/BUILD b/tests/BUILD
index e523c1619..2f443e95a 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -109,8 +109,3 @@ jax_test(
     name = ""generated_fun_test"",
     srcs = [""generated_fun_test.py""],
 )
-
-jax_test(
-    name = ""examples_test"",
-    srcs = [""examples_test.py""],
-)","diff --git a/tests/BUILD b/tests/BUILD
index e523c1619..2f443e95a 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -109,8 +109,3 @@ jax_test(
     name = ""generated_fun_test"",
     srcs = [""generated_fun_test.py""],
 )
-
-jax_test(
-    name = ""examples_test"",
-    srcs = [""examples_test.py""],
-)",No
jax/abstract_arrays.py,jax/abstract_arrays.py,bfe653c6b0bd8873887cd607d0243ef5e1718e1a,a97f8209615799d0dd628dbc508aac2f8ed27177,"Tracer.__len__ should reflect on abstract value

This old implementation, which was meant to be revised but which we
forgot about, caused a surprising slowdown: if x were a traced array of
size 50000, evaluating len(x) would create 50000 traced temporary
objects, which led to a lot of overhead! That came up in our
implementation of jax.random.shuffle, which happened to call len()
instead of x.shape[axis] (even though it should have been using x.size
anyway, according to tjablin@'s code that it's based on).","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 4348c025f..a23e53150 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -112,6 +112,9 @@ class ShapedArray(UnshapedArray):
     except IndexError:
       raise TypeError(""len() of unsized object"")  # same as numpy error
 
+  def _len(self, ignored_tracer):
+    return len(self)
+
 
 class ConcreteArray(ShapedArray):
   __slots__ = ['val']","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 4348c025f..a23e53150 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -112,6 +112,9 @@ class ShapedArray(UnshapedArray):
     except IndexError:
       raise TypeError(""len() of unsized object"")  # same as numpy error
 
+  def _len(self, ignored_tracer):
+    return len(self)
+
 
 class ConcreteArray(ShapedArray):
   __slots__ = ['val']",No
jax/core.py,jax/core.py,bfe653c6b0bd8873887cd607d0243ef5e1718e1a,a97f8209615799d0dd628dbc508aac2f8ed27177,"Tracer.__len__ should reflect on abstract value

This old implementation, which was meant to be revised but which we
forgot about, caused a surprising slowdown: if x were a traced array of
size 50000, evaluating len(x) would create 50000 traced temporary
objects, which led to a lot of overhead! That came up in our
implementation of jax.random.shuffle, which happened to call len()
instead of x.shape[axis] (even though it should have been using x.size
anyway, according to tjablin@'s code that it's based on).","diff --git a/jax/core.py b/jax/core.py
index 2685d0deb..0f839c7be 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -196,8 +196,7 @@ class Tracer(object):
     return iter(self.aval._iter(self))
 
   def __len__(self):
-    # TODO(dougalm,mattjj): replace with something more efficient
-    return len(list(iter(self)))
+    return self.aval._len(self)
 
   @property
   def aval(self):
@@ -450,6 +449,9 @@ class AbstractTuple(AbstractValue, tuple):
   def _iter(tracer):
     return tracer.unpack()
 
+  def _len(self, ignored_tracer):
+    return len(self)  # tuples have a known length
+
   def at_least_vspace(self):
     return AbstractTuple(x.at_least_vspace() for x in self)
 ","diff --git a/jax/core.py b/jax/core.py
index 2685d0deb..0f839c7be 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -196,8 +196,7 @@ class Tracer(object):
     return iter(self.aval._iter(self))
 
   def __len__(self):
-    # TODO(dougalm,mattjj): replace with something more efficient
-    return len(list(iter(self)))
+    return self.aval._len(self)
 
   @property
   def aval(self):
@@ -450,6 +449,9 @@ class AbstractTuple(AbstractValue, tuple):
   def _iter(tracer):
     return tracer.unpack()
 
+  def _len(self, ignored_tracer):
+    return len(self)  # tuples have a known length
+
   def at_least_vspace(self):
     return AbstractTuple(x.at_least_vspace() for x in self)
 ",No
jax/random.py,jax/random.py,bfe653c6b0bd8873887cd607d0243ef5e1718e1a,a97f8209615799d0dd628dbc508aac2f8ed27177,"Tracer.__len__ should reflect on abstract value

This old implementation, which was meant to be revised but which we
forgot about, caused a surprising slowdown: if x were a traced array of
size 50000, evaluating len(x) would create 50000 traced temporary
objects, which led to a lot of overhead! That came up in our
implementation of jax.random.shuffle, which happened to call len()
instead of x.shape[axis] (even though it should have been using x.size
anyway, according to tjablin@'s code that it's based on).","diff --git a/jax/random.py b/jax/random.py
index d2fcd1837..0a9e73768 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -310,7 +310,8 @@ def shuffle(key, x, axis=0):
   # Section 2 of http://people.csail.mit.edu/costis/6896sp11/lec5s.pdf for
   # another analysis (where the keys are generated one bit at a time).
   exponent = 3  # see tjablin@'s analysis for explanation of this parameter
-  num_rounds = int(onp.ceil(exponent * onp.log(len(x)) / 32))
+  uint32max = onp.iinfo(onp.uint32).max
+  num_rounds = int(onp.ceil(exponent * onp.log(x.size) / onp.log(uint32max)))
 
   for _ in range(num_rounds):
     key, subkey = split(key)","diff --git a/jax/random.py b/jax/random.py
index d2fcd1837..0a9e73768 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -310,7 +310,8 @@ def shuffle(key, x, axis=0):
   # Section 2 of http://people.csail.mit.edu/costis/6896sp11/lec5s.pdf for
   # another analysis (where the keys are generated one bit at a time).
   exponent = 3  # see tjablin@'s analysis for explanation of this parameter
-  num_rounds = int(onp.ceil(exponent * onp.log(len(x)) / 32))
+  uint32max = onp.iinfo(onp.uint32).max
+  num_rounds = int(onp.ceil(exponent * onp.log(x.size) / onp.log(uint32max)))
 
   for _ in range(num_rounds):
     key, subkey = split(key)",No
setup.py,setup.py,617dc18bc7623095e4d681b9aef350c33df607b8,bfe653c6b0bd8873887cd607d0243ef5e1718e1a,bump version for pypi,"diff --git a/setup.py b/setup.py
index eaa44339a..e1f9a311e 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.10',
+    version='0.1.11',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index eaa44339a..e1f9a311e 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.10',
+    version='0.1.11',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,2e20a60916b01703fb040c01181d09c991b9333e,617dc18bc7623095e4d681b9aef350c33df607b8,add tensordot,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 649f360f8..ef2b38b54 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -25,7 +25,7 @@ from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from .. import lax
-from ..util import memoize, partial, get_module_functions
+from ..util import memoize, partial, get_module_functions, prod as _prod
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -980,6 +980,37 @@ def vdot(a, b):
   return dot(a.ravel(), b.ravel())
 
 
+@_wraps(onp.tensordot)
+def tensordot(a, b, axes=2):
+  _check_arraylike(""tensordot"", a, b)
+  if not (ndim(a) >= 1 and ndim(b) >= 1):
+    msg = ""tensordot requires a.ndim and b.dim to be at least 1, got {} and {}.""
+    raise TypeError(msg.format(ndim(a), ndim(b)))
+
+  if type(axes) is int:
+    a_reshape = lax.reshape(a, (_prod(a.shape[:-axes]), _prod(a.shape[-axes:])))
+    b_reshape = lax.reshape(b, (_prod(b.shape[:axes]), _prod(b.shape[axes:])))
+    out_reshape = lax.dot(a_reshape, b_reshape)
+    return lax.reshape(out_reshape, a.shape[:-axes] + b.shape[axes:])
+  elif type(axes) in (list, tuple) and len(axes) == 2:
+    ax1, ax2 = axes
+    if type(ax1) == type(ax2) == int:
+      a_transposed = moveaxis(a, ax1, -1) if ax1 != a.ndim - 1 else a
+      b_transposed = moveaxis(b, ax2, 0) if ax2 != 0 else b
+      return tensordot(a_transposed, b_transposed, 1)
+    elif type(ax1) in (list, tuple) and type(ax2) in (list, tuple):
+      if len(ax1) != len(ax2):
+        msg = ""tensordot requires axes lists to have equal length, got {} and {}.""
+        raise TypeError(msg.format(ax1, ax2))
+      num_axes = len(ax1)
+      a_transposed = moveaxis(a, ax1, tuple(range(a.ndim - num_axes, a.ndim)))
+      b_transposed = moveaxis(b, ax2, tuple(range(num_axes)))
+      return tensordot(a_transposed, b_transposed, num_axes)
+  msg = (""tensordot axes argument must be an int, a pair of ints, or a pair of ""
+         ""lists/tuples of ints."")
+  raise TypeError(msg)
+
+
 ### Misc
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 649f360f8..ef2b38b54 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -25,7 +25,7 @@ from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from .. import lax
-from ..util import memoize, partial, get_module_functions
+from ..util import memoize, partial, get_module_functions, prod as _prod
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -980,6 +980,37 @@ def vdot(a, b):
   return dot(a.ravel(), b.ravel())
 
 
+@_wraps(onp.tensordot)
+def tensordot(a, b, axes=2):
+  _check_arraylike(""tensordot"", a, b)
+  if not (ndim(a) >= 1 and ndim(b) >= 1):
+    msg = ""tensordot requires a.ndim and b.dim to be at least 1, got {} and {}.""
+    raise TypeError(msg.format(ndim(a), ndim(b)))
+
+  if type(axes) is int:
+    a_reshape = lax.reshape(a, (_prod(a.shape[:-axes]), _prod(a.shape[-axes:])))
+    b_reshape = lax.reshape(b, (_prod(b.shape[:axes]), _prod(b.shape[axes:])))
+    out_reshape = lax.dot(a_reshape, b_reshape)
+    return lax.reshape(out_reshape, a.shape[:-axes] + b.shape[axes:])
+  elif type(axes) in (list, tuple) and len(axes) == 2:
+    ax1, ax2 = axes
+    if type(ax1) == type(ax2) == int:
+      a_transposed = moveaxis(a, ax1, -1) if ax1 != a.ndim - 1 else a
+      b_transposed = moveaxis(b, ax2, 0) if ax2 != 0 else b
+      return tensordot(a_transposed, b_transposed, 1)
+    elif type(ax1) in (list, tuple) and type(ax2) in (list, tuple):
+      if len(ax1) != len(ax2):
+        msg = ""tensordot requires axes lists to have equal length, got {} and {}.""
+        raise TypeError(msg.format(ax1, ax2))
+      num_axes = len(ax1)
+      a_transposed = moveaxis(a, ax1, tuple(range(a.ndim - num_axes, a.ndim)))
+      b_transposed = moveaxis(b, ax2, tuple(range(num_axes)))
+      return tensordot(a_transposed, b_transposed, num_axes)
+  msg = (""tensordot axes argument must be an int, a pair of ints, or a pair of ""
+         ""lists/tuples of ints."")
+  raise TypeError(msg)
+
+
 ### Misc
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,2e20a60916b01703fb040c01181d09c991b9333e,617dc18bc7623095e4d681b9aef350c33df607b8,add tensordot,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5730ddae7..a9f178421 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -338,6 +338,29 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
                             check_dtypes=True)
     self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype),
+          axes),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""axes"": axes, ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for lhs_shape, rhs_shape, axes in [
+          [(2, 3, 4), (3, 4, 5, 6), 2],
+          [(2, 3, 4), (5, 4, 3, 6), [1, 2]],
+          [(2, 3, 4), (5, 4, 3, 6), [[1, 2], [2, 1]]],
+          [(1, 2, 3, 4), (4, 5, 3, 6), [[2, 3], [2, 0]]],
+      ]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+  def testTensordot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, axes, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    lnp_fun = lambda a, b: lnp.tensordot(a, b, axes)
+    onp_fun = lambda a, b: onp.tensordot(a, b, axes)
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_amin={}_amax={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 5730ddae7..a9f178421 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -338,6 +338,29 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
                             check_dtypes=True)
     self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype),
+          axes),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""axes"": axes, ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for lhs_shape, rhs_shape, axes in [
+          [(2, 3, 4), (3, 4, 5, 6), 2],
+          [(2, 3, 4), (5, 4, 3, 6), [1, 2]],
+          [(2, 3, 4), (5, 4, 3, 6), [[1, 2], [2, 1]]],
+          [(1, 2, 3, 4), (4, 5, 3, 6), [[2, 3], [2, 0]]],
+      ]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+  def testTensordot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, axes, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    lnp_fun = lambda a, b: lnp.tensordot(a, b, axes)
+    onp_fun = lambda a, b: onp.tensordot(a, b, axes)
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_amin={}_amax={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,c54e82ceff64083ded7f80cbdff21a3f392e0961,797f7afacd501e10375f019adbad27eff5b0d619,"be more explicit about last layer in example

closes #119","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 63e77dba8..419daa057 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -37,10 +37,14 @@ def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
           for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
 
 def predict(params, inputs):
-  for w, b in params:
-    outputs = np.dot(inputs, w) + b
-    inputs = np.tanh(outputs)
-  return outputs - logsumexp(outputs, axis=1, keepdims=True)
+  activations = inputs
+  for w, b in params[:-1]:
+    outputs = np.dot(activations, w) + b
+    activations = np.tanh(outputs)
+
+  final_w, final_b = params[-1]
+  logits = np.dot(activations, final_w) + final_b
+  return logits - logsumexp(logits, axis=1, keepdims=True)
 
 def loss(params, batch):
   inputs, targets = batch","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 63e77dba8..419daa057 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -37,10 +37,14 @@ def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
           for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
 
 def predict(params, inputs):
-  for w, b in params:
-    outputs = np.dot(inputs, w) + b
-    inputs = np.tanh(outputs)
-  return outputs - logsumexp(outputs, axis=1, keepdims=True)
+  activations = inputs
+  for w, b in params[:-1]:
+    outputs = np.dot(activations, w) + b
+    activations = np.tanh(outputs)
+
+  final_w, final_b = params[-1]
+  logits = np.dot(activations, final_w) + final_b
+  return logits - logsumexp(logits, axis=1, keepdims=True)
 
 def loss(params, batch):
   inputs, targets = batch",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,ea08ecd5f0e4b18f3bc7ff4176b19da40c8ded1e,b8a698893cb1e754d5ca71f6b06dc1aad9fc1609,add promote_dtypes logic to tensordot,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef2b38b54..80d06456d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -988,6 +988,7 @@ def tensordot(a, b, axes=2):
     raise TypeError(msg.format(ndim(a), ndim(b)))
 
   if type(axes) is int:
+    a, b = _promote_dtypes(a, b)
     a_reshape = lax.reshape(a, (_prod(a.shape[:-axes]), _prod(a.shape[-axes:])))
     b_reshape = lax.reshape(b, (_prod(b.shape[:axes]), _prod(b.shape[axes:])))
     out_reshape = lax.dot(a_reshape, b_reshape)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef2b38b54..80d06456d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -988,6 +988,7 @@ def tensordot(a, b, axes=2):
     raise TypeError(msg.format(ndim(a), ndim(b)))
 
   if type(axes) is int:
+    a, b = _promote_dtypes(a, b)
     a_reshape = lax.reshape(a, (_prod(a.shape[:-axes]), _prod(a.shape[-axes:])))
     b_reshape = lax.reshape(b, (_prod(b.shape[:axes]), _prod(b.shape[axes:])))
     out_reshape = lax.dot(a_reshape, b_reshape)",No
jax/api.py,jax/api.py,b318f56928e6cd201dfbf4c50a404f6268f46d6d,ea08ecd5f0e4b18f3bc7ff4176b19da40c8ded1e,"generate name, module, and doctring for functions output from `jit`.","diff --git a/jax/api.py b/jax/api.py
index 67ccd4339..4681a6dfa 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -43,6 +43,16 @@ from .interpreters import batching
 
 map = safe_map
 
+def _wraps(wrapped):
+  def decorator(wrapper):
+    wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
+    wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
+    if hasattr(wrapped, ""__doc__""):
+      wrapper.__doc__ = getattr(wrapped, ""__doc__"")
+    return wrapper
+  return decorator
+
+
 def jit(fun, static_argnums=()):
   """"""Sets up `fun` for just-in-time compilation with XLA.
 
@@ -59,6 +69,7 @@ def jit(fun, static_argnums=()):
         different values for these constants will trigger recompilation.
    Returns: A wrapped version of `fun`, set up for just-in-time compilation.
   """"""
+  @_wraps(fun)
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]","diff --git a/jax/api.py b/jax/api.py
index 67ccd4339..4681a6dfa 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -43,6 +43,16 @@ from .interpreters import batching
 
 map = safe_map
 
+def _wraps(wrapped):
+  def decorator(wrapper):
+    wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
+    wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
+    if hasattr(wrapped, ""__doc__""):
+      wrapper.__doc__ = getattr(wrapped, ""__doc__"")
+    return wrapper
+  return decorator
+
+
 def jit(fun, static_argnums=()):
   """"""Sets up `fun` for just-in-time compilation with XLA.
 
@@ -59,6 +69,7 @@ def jit(fun, static_argnums=()):
         different values for these constants will trigger recompilation.
    Returns: A wrapped version of `fun`, set up for just-in-time compilation.
   """"""
+  @_wraps(fun)
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]",No
jax/api.py,jax/api.py,3b8fdb050ab3ee3c9ec49fe343ddd79f74a34a45,b318f56928e6cd201dfbf4c50a404f6268f46d6d,"wrap ""jit"" around generated function name","diff --git a/jax/api.py b/jax/api.py
index 4681a6dfa..429aca54d 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -80,6 +80,7 @@ def jit(fun, static_argnums=()):
     out_flat = xla.xla_call(flat_fun, *args_flat)
     return build_tree(out_tree(), out_flat)
 
+  f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
   return f_jitted
 
 def grad(fun, argnums=0):","diff --git a/jax/api.py b/jax/api.py
index 4681a6dfa..429aca54d 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -80,6 +80,7 @@ def jit(fun, static_argnums=()):
     out_flat = xla.xla_call(flat_fun, *args_flat)
     return build_tree(out_tree(), out_flat)
 
+  f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
   return f_jitted
 
 def grad(fun, argnums=0):",No
jax/api.py,jax/api.py,f4a8e03ce1b0d63d80eb274dfa4ac4fd96a79b69,3b8fdb050ab3ee3c9ec49fe343ddd79f74a34a45,add a basic `make_jaxpr` transformation to the api module,"diff --git a/jax/api.py b/jax/api.py
index 429aca54d..6d310a881 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -226,6 +226,19 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
     return pe.merge_pvals(ans, pvals)
   return unflatten_fun(fun, io_tree, *py_args)
 
+def make_jaxpr(f):
+  def pv_like(x):
+    aval = ShapedArray(onp.shape(x), onp.result_type(x))
+    return pe.PartialVal((aval, core.unit))
+
+  @_wraps(f)
+  def jaxpr_maker(*args):
+    jaxpr, _, _, _ = trace_to_jaxpr(f, map(pv_like, args))
+    return jaxpr
+
+  jaxpr_maker.__name__ = ""make_jaxpr({})"".format(jaxpr_maker.__name__)
+  return jaxpr_maker
+
 
 device_put = jit(lambda x: x)
 device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x","diff --git a/jax/api.py b/jax/api.py
index 429aca54d..6d310a881 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -226,6 +226,19 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
     return pe.merge_pvals(ans, pvals)
   return unflatten_fun(fun, io_tree, *py_args)
 
+def make_jaxpr(f):
+  def pv_like(x):
+    aval = ShapedArray(onp.shape(x), onp.result_type(x))
+    return pe.PartialVal((aval, core.unit))
+
+  @_wraps(f)
+  def jaxpr_maker(*args):
+    jaxpr, _, _, _ = trace_to_jaxpr(f, map(pv_like, args))
+    return jaxpr
+
+  jaxpr_maker.__name__ = ""make_jaxpr({})"".format(jaxpr_maker.__name__)
+  return jaxpr_maker
+
 
 device_put = jit(lambda x: x)
 device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x",No
examples/kernel_lsq.py,examples/kernel_lsq.py,dda5bdb68bd58fd549ec6cac0615defc565151ce,f4a8e03ce1b0d63d80eb274dfa4ac4fd96a79b69,illustrate two-level autobatching with jaxprs of the gram matrix function in the kernel least-squares example,"diff --git a/examples/kernel_lsq.py b/examples/kernel_lsq.py
index 4d5c6e083..f6ed1f7e0 100644
--- a/examples/kernel_lsq.py
+++ b/examples/kernel_lsq.py
@@ -25,7 +25,7 @@ import numpy.random as npr
 import jax.numpy as np
 from jax.config import config
 from jax.experimental import minmax
-from jax import grad, jit, vmap
+from jax import grad, jit, make_jaxpr, vmap
 
 
 def gram(kernel, xs):
@@ -80,11 +80,23 @@ if __name__ == ""__main__"":
 
   # linear kernel
 
-  kernel = lambda x, y: np.dot(x, y)
+  linear_kernel = lambda x, y: np.dot(x, y)
   truth = npr.randn(d)
   xs = npr.randn(n, d)
   ys = np.dot(xs, truth)
 
-  predict = train(kernel, xs, ys)
+  predict = train(linear_kernel, xs, ys)
 
   print('MSE:', np.sum((predict(xs) - ys) ** 2.))
+
+  def gram_jaxpr(kernel):
+    return make_jaxpr(partial(gram, kernel))(xs)
+
+  rbf_kernel = lambda x, y: np.exp(-np.sum((x - y) ** 2))
+
+  print()
+  print('jaxpr of gram(linear_kernel):')
+  print(gram_jaxpr(linear_kernel))
+  print()
+  print('jaxpr of gram(rbf_kernel):')
+  print(gram_jaxpr(rbf_kernel))","diff --git a/examples/kernel_lsq.py b/examples/kernel_lsq.py
index 4d5c6e083..f6ed1f7e0 100644
--- a/examples/kernel_lsq.py
+++ b/examples/kernel_lsq.py
@@ -25,7 +25,7 @@ import numpy.random as npr
 import jax.numpy as np
 from jax.config import config
 from jax.experimental import minmax
-from jax import grad, jit, vmap
+from jax import grad, jit, make_jaxpr, vmap
 
 
 def gram(kernel, xs):
@@ -80,11 +80,23 @@ if __name__ == ""__main__"":
 
   # linear kernel
 
-  kernel = lambda x, y: np.dot(x, y)
+  linear_kernel = lambda x, y: np.dot(x, y)
   truth = npr.randn(d)
   xs = npr.randn(n, d)
   ys = np.dot(xs, truth)
 
-  predict = train(kernel, xs, ys)
+  predict = train(linear_kernel, xs, ys)
 
   print('MSE:', np.sum((predict(xs) - ys) ** 2.))
+
+  def gram_jaxpr(kernel):
+    return make_jaxpr(partial(gram, kernel))(xs)
+
+  rbf_kernel = lambda x, y: np.exp(-np.sum((x - y) ** 2))
+
+  print()
+  print('jaxpr of gram(linear_kernel):')
+  print(gram_jaxpr(linear_kernel))
+  print()
+  print('jaxpr of gram(rbf_kernel):')
+  print(gram_jaxpr(rbf_kernel))",No
jax/lax_linalg.py,jax/lax_linalg.py,1743a936eb26fdefa5f3b61053776d8aee9b0fcd,ea08ecd5f0e4b18f3bc7ff4176b19da40c8ded1e,Add qr decomposition jvp,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 4347ea44d..d66633dc5 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -135,7 +135,22 @@ def qr_abstract_eval(operand, full_matrices):
 def qr_dtype_rule(operand, full_matrices=True):
   return operand.dtype
 
+def qr_jvp_rule(primals, tangents, full_matrices):
+  x, = primals
+  if not full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
+      raise NotImplementedError
+  dx, = tangents
+  q, r = qr_p.bind(x, full_matrices=False)
+  dx_rinv = triangular_solve(r, dx)  # Right side solve by default
+  qt_dx_rinv = np.matmul(_T(q), dx_rinv)
+  qt_dx_rinv_lower = np.tril(qt_dx_rinv, -1)
+  domega = qt_dx_rinv_lower - _T(qt_dx_rinv_lower)  # This is skew-symmetric
+  dq = np.matmul(q, domega - qt_dx_rinv) + dx_rinv
+  dr = np.matmul(qt_dx_rinv - domega, r)
+  return core.pack((q, r)), core.pack((dq, dr))
+
 qr_p = Primitive('qr')
 qr_p.def_impl(qr_impl)
 qr_p.def_abstract_eval(qr_abstract_eval)
 xla.translations[qr_p] = qr_translation_rule
+ad.primitive_jvps[qr_p] = qr_jvp_rule","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 4347ea44d..d66633dc5 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -135,7 +135,22 @@ def qr_abstract_eval(operand, full_matrices):
 def qr_dtype_rule(operand, full_matrices=True):
   return operand.dtype
 
+def qr_jvp_rule(primals, tangents, full_matrices):
+  x, = primals
+  if not full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
+      raise NotImplementedError
+  dx, = tangents
+  q, r = qr_p.bind(x, full_matrices=False)
+  dx_rinv = triangular_solve(r, dx)  # Right side solve by default
+  qt_dx_rinv = np.matmul(_T(q), dx_rinv)
+  qt_dx_rinv_lower = np.tril(qt_dx_rinv, -1)
+  domega = qt_dx_rinv_lower - _T(qt_dx_rinv_lower)  # This is skew-symmetric
+  dq = np.matmul(q, domega - qt_dx_rinv) + dx_rinv
+  dr = np.matmul(qt_dx_rinv - domega, r)
+  return core.pack((q, r)), core.pack((dq, dr))
+
 qr_p = Primitive('qr')
 qr_p.def_impl(qr_impl)
 qr_p.def_abstract_eval(qr_abstract_eval)
 xla.translations[qr_p] = qr_translation_rule
+ad.primitive_jvps[qr_p] = qr_jvp_rule",No
jax/lax_linalg.py,jax/lax_linalg.py,f5b8d97c9513bf60ac663f30c8e64c2cbe00187e,1743a936eb26fdefa5f3b61053776d8aee9b0fcd,Add url for qr jvp notes,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index d66633dc5..db21a2c56 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -136,6 +136,7 @@ def qr_dtype_rule(operand, full_matrices=True):
   return operand.dtype
 
 def qr_jvp_rule(primals, tangents, full_matrices):
+  # See j-towns.github.io/papers/qr-derivative.pdf for a terse derivation.
   x, = primals
   if not full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
       raise NotImplementedError","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index d66633dc5..db21a2c56 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -136,6 +136,7 @@ def qr_dtype_rule(operand, full_matrices=True):
   return operand.dtype
 
 def qr_jvp_rule(primals, tangents, full_matrices):
+  # See j-towns.github.io/papers/qr-derivative.pdf for a terse derivation.
   x, = primals
   if not full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
       raise NotImplementedError",No
jax/lax_linalg.py,jax/lax_linalg.py,5cdf915fb615227ac37c51131d200342364a48f4,f5b8d97c9513bf60ac663f30c8e64c2cbe00187e,Test qr jvp,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index db21a2c56..3e5d27f4c 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -138,8 +138,8 @@ def qr_dtype_rule(operand, full_matrices=True):
 def qr_jvp_rule(primals, tangents, full_matrices):
   # See j-towns.github.io/papers/qr-derivative.pdf for a terse derivation.
   x, = primals
-  if not full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
-      raise NotImplementedError
+  if full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
+    raise NotImplementedError
   dx, = tangents
   q, r = qr_p.bind(x, full_matrices=False)
   dx_rinv = triangular_solve(r, dx)  # Right side solve by default","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index db21a2c56..3e5d27f4c 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -138,8 +138,8 @@ def qr_dtype_rule(operand, full_matrices=True):
 def qr_jvp_rule(primals, tangents, full_matrices):
   # See j-towns.github.io/papers/qr-derivative.pdf for a terse derivation.
   x, = primals
-  if not full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
-      raise NotImplementedError
+  if full_matrices or np.shape(x)[-2] < np.shape(x)[-1]:
+    raise NotImplementedError
   dx, = tangents
   q, r = qr_p.bind(x, full_matrices=False)
   dx_rinv = triangular_solve(r, dx)  # Right side solve by default",No
tests/linalg_test.py,tests/linalg_test.py,5cdf915fb615227ac37c51131d200342364a48f4,f5b8d97c9513bf60ac663f30c8e64c2cbe00187e,Test qr jvp,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 8e32c7e32..5094ed326 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -17,6 +17,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from functools import partial
 import itertools
 
 import numpy as onp
@@ -24,6 +25,7 @@ import numpy as onp
 from absl.testing import absltest
 from absl.testing import parameterized
 
+from jax import jvp
 from jax import numpy as np
 from jax import scipy
 from jax import test_util as jtu
@@ -63,7 +65,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, dtype), full_matrices),
        ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
        ""rng"": rng}
-      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
+      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 100)]
       for dtype in float_types()
       for full_matrices in [False, True]
       for rng in [jtu.rand_default()]))
@@ -109,6 +111,10 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     # Check that q is close to unitary.
     self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
 
+    if not full_matrices and m >= n:
+        jtu.check_jvp(np.linalg.qr, partial(jvp, np.linalg.qr), (a,))
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 8e32c7e32..5094ed326 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -17,6 +17,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from functools import partial
 import itertools
 
 import numpy as onp
@@ -24,6 +25,7 @@ import numpy as onp
 from absl.testing import absltest
 from absl.testing import parameterized
 
+from jax import jvp
 from jax import numpy as np
 from jax import scipy
 from jax import test_util as jtu
@@ -63,7 +65,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, dtype), full_matrices),
        ""shape"": shape, ""dtype"": dtype, ""full_matrices"": full_matrices,
        ""rng"": rng}
-      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 200)]
+      for shape in [(1, 1), (3, 4), (2, 10, 5), (2, 200, 100)]
       for dtype in float_types()
       for full_matrices in [False, True]
       for rng in [jtu.rand_default()]))
@@ -109,6 +111,10 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     # Check that q is close to unitary.
     self.assertTrue(onp.all(norm(onp.eye(k) - onp.matmul(T(lq), lq)) < 5))
 
+    if not full_matrices and m >= n:
+        jtu.check_jvp(np.linalg.qr, partial(jvp, np.linalg.qr), (a,))
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),",No
jax/api.py,jax/api.py,e0ad5bb3943d6d5a6ef55867d3d4e3743dd46a40,dda5bdb68bd58fd549ec6cac0615defc565151ce,add TODO comment for containers in `api.make_jaxpr`,"diff --git a/jax/api.py b/jax/api.py
index 6d310a881..c4e8b6236 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -227,6 +227,7 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 def make_jaxpr(f):
+  # TODO(frostig): handle container trees etc.
   def pv_like(x):
     aval = ShapedArray(onp.shape(x), onp.result_type(x))
     return pe.PartialVal((aval, core.unit))","diff --git a/jax/api.py b/jax/api.py
index 6d310a881..c4e8b6236 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -227,6 +227,7 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 def make_jaxpr(f):
+  # TODO(frostig): handle container trees etc.
   def pv_like(x):
     aval = ShapedArray(onp.shape(x), onp.result_type(x))
     return pe.PartialVal((aval, core.unit))",No
build/BUILD,build/BUILD,3c388b98f17e1c7145bee883c15e11262299a724,5a4a066ae6415cde5b96f8cd98217a58e7635cd3,Add support for calling LAPACK primitives from SciPy from JAX linalg.,"diff --git a/build/BUILD b/build/BUILD
index b7e680cbb..8f4410a71 100644
--- a/build/BUILD
+++ b/build/BUILD
@@ -14,6 +14,8 @@
 
 # JAX is Autograd and XLA
 
+load(""@org_tensorflow//tensorflow/core:platform/default/build_config.bzl"", ""pyx_library"")
+
 licenses([""notice""])  # Apache 2
 
 package(default_visibility = [""//visibility:public""])
@@ -23,6 +25,8 @@ sh_binary(
     srcs = [""install_xla_in_source_tree.sh""],
     data = [
         ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
+        ""//jaxlib:lapack.so"",
     ],
     deps = [""@bazel_tools//tools/bash/runfiles""],
 )
+","diff --git a/build/BUILD b/build/BUILD
index b7e680cbb..8f4410a71 100644
--- a/build/BUILD
+++ b/build/BUILD
@@ -14,6 +14,8 @@
 
 # JAX is Autograd and XLA
 
+load(""@org_tensorflow//tensorflow/core:platform/default/build_config.bzl"", ""pyx_library"")
+
 licenses([""notice""])  # Apache 2
 
 package(default_visibility = [""//visibility:public""])
@@ -23,6 +25,8 @@ sh_binary(
     srcs = [""install_xla_in_source_tree.sh""],
     data = [
         ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
+        ""//jaxlib:lapack.so"",
     ],
     deps = [""@bazel_tools//tools/bash/runfiles""],
 )
+",No
build/install_xla_in_source_tree.sh,build/install_xla_in_source_tree.sh,3c388b98f17e1c7145bee883c15e11262299a724,5a4a066ae6415cde5b96f8cd98217a58e7635cd3,Add support for calling LAPACK primitives from SciPy from JAX linalg.,"diff --git a/build/install_xla_in_source_tree.sh b/build/install_xla_in_source_tree.sh
index 6ace93ead..23730c06c 100755
--- a/build/install_xla_in_source_tree.sh
+++ b/build/install_xla_in_source_tree.sh
@@ -58,6 +58,7 @@ cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/pywrap_xla.py)""
   ""${TARGET}/jaxlib""
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/_pywrap_xla.so)"" \
   ""${TARGET}/jaxlib""
+cp -f ""$(rlocation __main__/jaxlib/lapack.so)"" ""${TARGET}/jaxlib""
 sed \
   -e 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' \
   -e 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' \","diff --git a/build/install_xla_in_source_tree.sh b/build/install_xla_in_source_tree.sh
index 6ace93ead..23730c06c 100755
--- a/build/install_xla_in_source_tree.sh
+++ b/build/install_xla_in_source_tree.sh
@@ -58,6 +58,7 @@ cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/pywrap_xla.py)""
   ""${TARGET}/jaxlib""
 cp -f ""$(rlocation org_tensorflow/tensorflow/compiler/xla/python/_pywrap_xla.so)"" \
   ""${TARGET}/jaxlib""
+cp -f ""$(rlocation __main__/jaxlib/lapack.so)"" ""${TARGET}/jaxlib""
 sed \
   -e 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' \
   -e 's/from tensorflow.compiler.xla import xla_data_pb2/from . import xla_data_pb2/' \",No
jax/lax_linalg.py,jax/lax_linalg.py,3c388b98f17e1c7145bee883c15e11262299a724,5a4a066ae6415cde5b96f8cd98217a58e7635cd3,Add support for calling LAPACK primitives from SciPy from JAX linalg.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 4347ea44d..e04e5a6d2 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -28,7 +28,8 @@ from jax.abstract_arrays import ShapedArray
 from jax.core import Primitive
 from jax.lax import (standard_primitive, standard_unop, binop_dtype_rule,
                      _float, _complex, _input_dtype)
-
+from jax.lib import xla_bridge
+from jaxlib import lapack
 
 # traceables
 
@@ -72,6 +73,21 @@ cholesky_p = standard_unop(_float, 'cholesky')
 ad.primitive_jvps[cholesky_p] = cholesky_jvp_rule
 
 
+def cholesky_cpu_translation_rule(c, operand):
+  shape = c.GetShape(operand)
+  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
+    return c.GetTupleElement(lapack.jax_spotrf(c, operand, lower=True), 0)
+  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
+    return c.GetTupleElement(lapack.jax_dpotrf(c, operand, lower=True), 0)
+  else:
+    # Fall back to the HLO implementation for batched Cholesky decomposition or
+    # unsupported types.
+    # TODO(phawkins): support LAPACK primitives in batched mode.
+    return c.Cholesky(operand)
+
+xla.translations[cholesky_p] = cholesky_cpu_translation_rule
+
+
 triangular_solve_dtype_rule = partial(
     binop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex),
     'triangular_solve')","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 4347ea44d..e04e5a6d2 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -28,7 +28,8 @@ from jax.abstract_arrays import ShapedArray
 from jax.core import Primitive
 from jax.lax import (standard_primitive, standard_unop, binop_dtype_rule,
                      _float, _complex, _input_dtype)
-
+from jax.lib import xla_bridge
+from jaxlib import lapack
 
 # traceables
 
@@ -72,6 +73,21 @@ cholesky_p = standard_unop(_float, 'cholesky')
 ad.primitive_jvps[cholesky_p] = cholesky_jvp_rule
 
 
+def cholesky_cpu_translation_rule(c, operand):
+  shape = c.GetShape(operand)
+  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
+    return c.GetTupleElement(lapack.jax_spotrf(c, operand, lower=True), 0)
+  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
+    return c.GetTupleElement(lapack.jax_dpotrf(c, operand, lower=True), 0)
+  else:
+    # Fall back to the HLO implementation for batched Cholesky decomposition or
+    # unsupported types.
+    # TODO(phawkins): support LAPACK primitives in batched mode.
+    return c.Cholesky(operand)
+
+xla.translations[cholesky_p] = cholesky_cpu_translation_rule
+
+
 triangular_solve_dtype_rule = partial(
     binop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex),
     'triangular_solve')",No
build/BUILD,build/BUILD,e630fda9ee296e996b3039ecc721a523609a0f88,3c388b98f17e1c7145bee883c15e11262299a724,"Add missing copyright notice.
Remove unused BUILD load() statement.","diff --git a/build/BUILD b/build/BUILD
index 8f4410a71..f70cb3975 100644
--- a/build/BUILD
+++ b/build/BUILD
@@ -14,8 +14,6 @@
 
 # JAX is Autograd and XLA
 
-load(""@org_tensorflow//tensorflow/core:platform/default/build_config.bzl"", ""pyx_library"")
-
 licenses([""notice""])  # Apache 2
 
 package(default_visibility = [""//visibility:public""])","diff --git a/build/BUILD b/build/BUILD
index 8f4410a71..f70cb3975 100644
--- a/build/BUILD
+++ b/build/BUILD
@@ -14,8 +14,6 @@
 
 # JAX is Autograd and XLA
 
-load(""@org_tensorflow//tensorflow/core:platform/default/build_config.bzl"", ""pyx_library"")
-
 licenses([""notice""])  # Apache 2
 
 package(default_visibility = [""//visibility:public""])",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,e630fda9ee296e996b3039ecc721a523609a0f88,3c388b98f17e1c7145bee883c15e11262299a724,"Add missing copyright notice.
Remove unused BUILD load() statement.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 3f96d3d56..341ed2bcc 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # distutils: language = c++
 
 # Shims that allow the XLA CPU backend to call scipy-provided LAPACK kernels","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 3f96d3d56..341ed2bcc 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # distutils: language = c++
 
 # Shims that allow the XLA CPU backend to call scipy-provided LAPACK kernels",No
jax/lax_linalg.py,jax/lax_linalg.py,0333a98bab87d285958e7700ccb55c9a95f7f7e9,e630fda9ee296e996b3039ecc721a523609a0f88,Add triangular solve BLAS implementation.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index e04e5a6d2..3f8a9e045 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -75,10 +75,9 @@ ad.primitive_jvps[cholesky_p] = cholesky_jvp_rule
 
 def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return c.GetTupleElement(lapack.jax_spotrf(c, operand, lower=True), 0)
-  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return c.GetTupleElement(lapack.jax_dpotrf(c, operand, lower=True), 0)
+  if len(shape.dimensions()) == 2 and (
+    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+    return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
     # unsupported types.
@@ -126,6 +125,24 @@ ad.defjvp(triangular_solve_p,
 ad.primitive_transposes[triangular_solve_p] = triangular_solve_transpose_rule
 
 
+def triangular_solve_cpu_translation_rule(
+    c, a, b, left_side, lower, transpose_a, conjugate_a):
+  shape = c.GetShape(a)
+  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
+    return lapack.jax_trsm(c, c.ConstantF32Scalar(1.0), a, b, left_side, lower,
+                           transpose_a, conjugate_a)
+  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
+    return lapack.jax_trsm(c, c.ConstantF64Scalar(1.0), a, b, left_side, lower,
+                           transpose_a, conjugate_a)
+  else:
+    # Fall back to the HLO implementation for batched triangular_solve or
+    # unsupported types.
+    # TODO(phawkins): support BLAS primitives in batched mode.
+    return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
+
+xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
+
+
 def qr_impl(operand, full_matrices):
   q, r = xla.apply_primitive(qr_p, operand, full_matrices=full_matrices)
   return core.pack((q, r))","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index e04e5a6d2..3f8a9e045 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -75,10 +75,9 @@ ad.primitive_jvps[cholesky_p] = cholesky_jvp_rule
 
 def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return c.GetTupleElement(lapack.jax_spotrf(c, operand, lower=True), 0)
-  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return c.GetTupleElement(lapack.jax_dpotrf(c, operand, lower=True), 0)
+  if len(shape.dimensions()) == 2 and (
+    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+    return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
     # unsupported types.
@@ -126,6 +125,24 @@ ad.defjvp(triangular_solve_p,
 ad.primitive_transposes[triangular_solve_p] = triangular_solve_transpose_rule
 
 
+def triangular_solve_cpu_translation_rule(
+    c, a, b, left_side, lower, transpose_a, conjugate_a):
+  shape = c.GetShape(a)
+  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
+    return lapack.jax_trsm(c, c.ConstantF32Scalar(1.0), a, b, left_side, lower,
+                           transpose_a, conjugate_a)
+  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
+    return lapack.jax_trsm(c, c.ConstantF64Scalar(1.0), a, b, left_side, lower,
+                           transpose_a, conjugate_a)
+  else:
+    # Fall back to the HLO implementation for batched triangular_solve or
+    # unsupported types.
+    # TODO(phawkins): support BLAS primitives in batched mode.
+    return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
+
+xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
+
+
 def qr_impl(operand, full_matrices):
   q, r = xla.apply_primitive(qr_p, operand, full_matrices=full_matrices)
   return core.pack((q, r))",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,0333a98bab87d285958e7700ccb55c9a95f7f7e9,e630fda9ee296e996b3039ecc721a523609a0f88,Add triangular solve BLAS implementation.,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 341ed2bcc..943c38709 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -23,10 +23,12 @@ from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
+from scipy.linalg.cython_blas cimport strsm, dtrsm
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
+from jaxlib.xla_client import Shape
 
 
 cdef register_cpu_custom_call_target(fn_name, void* fn):
@@ -34,7 +36,115 @@ cdef register_cpu_custom_call_target(fn_name, void* fn):
   xla_client.register_cpu_custom_call_target(
     fn_name, PyCapsule_New(fn, name, NULL))
 
-# ?potrf (Cholesky decomposition)
+# TODO(phawkins): it would be nice to avoid duplicating code for each type.
+
+# ?trsm(left_side, lower, trans_a, diag, m, n, alpha, a, b):
+# triangular solve
+
+cdef void blas_strsm(void* out, void** data) nogil:
+  cdef bint left_side = (<bint*>(data[0]))[0]
+  cdef bint lower = (<bint*>(data[1]))[0]
+  cdef int trans_a = (<int*>(data[2]))[0]
+  cdef bint diag = (<bint*>(data[3]))[0]
+  cdef int m = (<int*>(data[4]))[0]
+  cdef int n = (<int*>(data[5]))[0]
+  cdef float* alpha = <float*>(data[6])
+  cdef float* a = <float*>(data[7])
+  cdef float* b = <float*>(data[8])
+
+  cdef float* x = <float*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(float))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m
+  cdef int ldb = m if left_side else n
+  strsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_strsm"", <void*>(blas_strsm))
+
+cdef void blas_dtrsm(void* out, void** data) nogil:
+  cdef bint left_side = (<bint*>(data[0]))[0]
+  cdef bint lower = (<bint*>(data[1]))[0]
+  cdef int trans_a = (<int*>(data[2]))[0]
+  cdef bint diag = (<bint*>(data[3]))[0]
+  cdef int m = (<int*>(data[4]))[0]
+  cdef int n = (<int*>(data[5]))[0]
+  cdef double* alpha = <double*>(data[6])
+  cdef double* a = <double*>(data[7])
+  cdef double* b = <double*>(data[8])
+
+  cdef double* x = <double*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(double))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m
+  cdef int ldb = m if left_side else n
+  dtrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_dtrsm"", <void*>(blas_dtrsm))
+
+
+def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
+             conj_a=False, diag=False):
+  b_shape = c.GetShape(b)
+  dtype = b_shape.element_type()
+  #if left_side:
+  m, n = b_shape.dimensions()
+  #else:
+  #  n, m = b_shape.dimensions()
+
+  a_shape = c.GetShape(a)
+  if (m, m) != a_shape.dimensions() or a_shape.element_type() != dtype:
+    raise ValueError(""Argument mismatch for trsm, got {} and {}"".format(
+      a_shape, b_shape))
+
+  if dtype == np.float32:
+    fn = b""blas_strsm""
+  elif dtype == np.float64: 
+    fn = b""blas_dtrsm""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(
+        c.ConstantPredScalar(left_side),
+        c.ConstantPredScalar(lower),
+        c.ConstantS32Scalar(1 if trans_a else 0),
+        c.ConstantPredScalar(diag),
+        c.ConstantS32Scalar(m),
+        c.ConstantS32Scalar(n),
+        alpha, a, b),
+      shape_with_layout=Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (), ()),
+          Shape.array_shape(dtype, a_shape.dimensions(), (0, 1)),
+          Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
+      ))
+
+# ?potrf: Cholesky decomposition
 
 cdef void lapack_spotrf(void* out_tuple, void** data) nogil:
   cdef bint lower = (<bint*>(data[0]))[0]
@@ -64,24 +174,6 @@ cdef void lapack_spotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_spotrf"", <void*>(lapack_spotrf))
 
-def jax_spotrf(c, a, lower=False):
-  a_shape = c.GetShape(a)
-  m, n = a_shape.dimensions()
-  if m != n:
-    raise ValueError(""spotrf expects a square matrix, got {}"".format(a_shape))
-  return c.CustomCall(
-      b""lapack_spotrf"",
-      operands=(c.ConstantPredScalar(lower), c.ConstantS32Scalar(n), a),
-      shape_with_layout=xla_client.Shape.tuple_shape((
-          xla_client.Shape.array_shape(np.float32, (n, n), (0, 1)),
-          xla_client.Shape.array_shape(np.int32, (), ()),
-      )),
-      operand_shapes_with_layout=(
-          xla_client.Shape.array_shape(np.bool, (), ()),
-          xla_client.Shape.array_shape(np.int32, (), ()),
-          xla_client.Shape.array_shape(np.float32, (n, n), (0, 1)),
-      ))
-
 
 cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
   cdef bint lower = (<bint*>(data[0]))[0]
@@ -111,20 +203,28 @@ cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_dpotrf"", <void*>(lapack_dpotrf))
 
-def jax_dpotrf(c, a, lower=False):
+def jax_potrf(c, a, lower=False):
   a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
   m, n = a_shape.dimensions()
   if m != n:
-    raise ValueError(""dpotrf expects a square matrix, got {}"".format(a_shape))
+    raise ValueError(""potrf expects a square matrix, got {}"".format(a_shape))
+  if dtype == np.float32:
+    fn = b""lapack_spotrf""
+  elif dtype == np.float64: 
+    fn = b""lapack_dpotrf""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
   return c.CustomCall(
-      b""lapack_dpotrf"",
+      fn,
       operands=(c.ConstantPredScalar(lower), c.ConstantS32Scalar(n), a),
-      shape_with_layout=xla_client.Shape.tuple_shape((
-          xla_client.Shape.array_shape(np.float64, (n, n), (0, 1)),
-          xla_client.Shape.array_shape(np.int32, (), ()),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+          Shape.array_shape(np.int32, (), ()),
       )),
       operand_shapes_with_layout=(
-          xla_client.Shape.array_shape(np.bool, (), ()),
-          xla_client.Shape.array_shape(np.int32, (), ()),
-          xla_client.Shape.array_shape(np.float64, (n, n), (0, 1)),
-      ))
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+      ))
\ No newline at end of file","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 341ed2bcc..943c38709 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -23,10 +23,12 @@ from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
+from scipy.linalg.cython_blas cimport strsm, dtrsm
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
+from jaxlib.xla_client import Shape
 
 
 cdef register_cpu_custom_call_target(fn_name, void* fn):
@@ -34,7 +36,115 @@ cdef register_cpu_custom_call_target(fn_name, void* fn):
   xla_client.register_cpu_custom_call_target(
     fn_name, PyCapsule_New(fn, name, NULL))
 
-# ?potrf (Cholesky decomposition)
+# TODO(phawkins): it would be nice to avoid duplicating code for each type.
+
+# ?trsm(left_side, lower, trans_a, diag, m, n, alpha, a, b):
+# triangular solve
+
+cdef void blas_strsm(void* out, void** data) nogil:
+  cdef bint left_side = (<bint*>(data[0]))[0]
+  cdef bint lower = (<bint*>(data[1]))[0]
+  cdef int trans_a = (<int*>(data[2]))[0]
+  cdef bint diag = (<bint*>(data[3]))[0]
+  cdef int m = (<int*>(data[4]))[0]
+  cdef int n = (<int*>(data[5]))[0]
+  cdef float* alpha = <float*>(data[6])
+  cdef float* a = <float*>(data[7])
+  cdef float* b = <float*>(data[8])
+
+  cdef float* x = <float*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(float))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m
+  cdef int ldb = m if left_side else n
+  strsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_strsm"", <void*>(blas_strsm))
+
+cdef void blas_dtrsm(void* out, void** data) nogil:
+  cdef bint left_side = (<bint*>(data[0]))[0]
+  cdef bint lower = (<bint*>(data[1]))[0]
+  cdef int trans_a = (<int*>(data[2]))[0]
+  cdef bint diag = (<bint*>(data[3]))[0]
+  cdef int m = (<int*>(data[4]))[0]
+  cdef int n = (<int*>(data[5]))[0]
+  cdef double* alpha = <double*>(data[6])
+  cdef double* a = <double*>(data[7])
+  cdef double* b = <double*>(data[8])
+
+  cdef double* x = <double*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(double))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m
+  cdef int ldb = m if left_side else n
+  dtrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_dtrsm"", <void*>(blas_dtrsm))
+
+
+def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
+             conj_a=False, diag=False):
+  b_shape = c.GetShape(b)
+  dtype = b_shape.element_type()
+  #if left_side:
+  m, n = b_shape.dimensions()
+  #else:
+  #  n, m = b_shape.dimensions()
+
+  a_shape = c.GetShape(a)
+  if (m, m) != a_shape.dimensions() or a_shape.element_type() != dtype:
+    raise ValueError(""Argument mismatch for trsm, got {} and {}"".format(
+      a_shape, b_shape))
+
+  if dtype == np.float32:
+    fn = b""blas_strsm""
+  elif dtype == np.float64: 
+    fn = b""blas_dtrsm""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(
+        c.ConstantPredScalar(left_side),
+        c.ConstantPredScalar(lower),
+        c.ConstantS32Scalar(1 if trans_a else 0),
+        c.ConstantPredScalar(diag),
+        c.ConstantS32Scalar(m),
+        c.ConstantS32Scalar(n),
+        alpha, a, b),
+      shape_with_layout=Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (), ()),
+          Shape.array_shape(dtype, a_shape.dimensions(), (0, 1)),
+          Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
+      ))
+
+# ?potrf: Cholesky decomposition
 
 cdef void lapack_spotrf(void* out_tuple, void** data) nogil:
   cdef bint lower = (<bint*>(data[0]))[0]
@@ -64,24 +174,6 @@ cdef void lapack_spotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_spotrf"", <void*>(lapack_spotrf))
 
-def jax_spotrf(c, a, lower=False):
-  a_shape = c.GetShape(a)
-  m, n = a_shape.dimensions()
-  if m != n:
-    raise ValueError(""spotrf expects a square matrix, got {}"".format(a_shape))
-  return c.CustomCall(
-      b""lapack_spotrf"",
-      operands=(c.ConstantPredScalar(lower), c.ConstantS32Scalar(n), a),
-      shape_with_layout=xla_client.Shape.tuple_shape((
-          xla_client.Shape.array_shape(np.float32, (n, n), (0, 1)),
-          xla_client.Shape.array_shape(np.int32, (), ()),
-      )),
-      operand_shapes_with_layout=(
-          xla_client.Shape.array_shape(np.bool, (), ()),
-          xla_client.Shape.array_shape(np.int32, (), ()),
-          xla_client.Shape.array_shape(np.float32, (n, n), (0, 1)),
-      ))
-
 
 cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
   cdef bint lower = (<bint*>(data[0]))[0]
@@ -111,20 +203,28 @@ cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_dpotrf"", <void*>(lapack_dpotrf))
 
-def jax_dpotrf(c, a, lower=False):
+def jax_potrf(c, a, lower=False):
   a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
   m, n = a_shape.dimensions()
   if m != n:
-    raise ValueError(""dpotrf expects a square matrix, got {}"".format(a_shape))
+    raise ValueError(""potrf expects a square matrix, got {}"".format(a_shape))
+  if dtype == np.float32:
+    fn = b""lapack_spotrf""
+  elif dtype == np.float64: 
+    fn = b""lapack_dpotrf""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
   return c.CustomCall(
-      b""lapack_dpotrf"",
+      fn,
       operands=(c.ConstantPredScalar(lower), c.ConstantS32Scalar(n), a),
-      shape_with_layout=xla_client.Shape.tuple_shape((
-          xla_client.Shape.array_shape(np.float64, (n, n), (0, 1)),
-          xla_client.Shape.array_shape(np.int32, (), ()),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+          Shape.array_shape(np.int32, (), ()),
       )),
       operand_shapes_with_layout=(
-          xla_client.Shape.array_shape(np.bool, (), ()),
-          xla_client.Shape.array_shape(np.int32, (), ()),
-          xla_client.Shape.array_shape(np.float64, (n, n), (0, 1)),
-      ))
+          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+      ))
\ No newline at end of file",No
jax/lax_linalg.py,jax/lax_linalg.py,eac96ac2393eeef3e2e4c298f8f56f4b3a739470,0333a98bab87d285958e7700ccb55c9a95f7f7e9,Fix bugs with bool argument passing; pass PRED values as int32s instead.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 3f8a9e045..76910c915 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -84,7 +84,8 @@ def cholesky_cpu_translation_rule(c, operand):
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-xla.translations[cholesky_p] = cholesky_cpu_translation_rule
+# TODO(mattjj): add per-backend translation rule support
+# xla.translations[cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -140,7 +141,8 @@ def triangular_solve_cpu_translation_rule(
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
+# TODO(mattjj): add per-backend translation rule support
+# xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
 def qr_impl(operand, full_matrices):","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 3f8a9e045..76910c915 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -84,7 +84,8 @@ def cholesky_cpu_translation_rule(c, operand):
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-xla.translations[cholesky_p] = cholesky_cpu_translation_rule
+# TODO(mattjj): add per-backend translation rule support
+# xla.translations[cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -140,7 +141,8 @@ def triangular_solve_cpu_translation_rule(
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
+# TODO(mattjj): add per-backend translation rule support
+# xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
 def qr_impl(operand, full_matrices):",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,eac96ac2393eeef3e2e4c298f8f56f4b3a739470,0333a98bab87d285958e7700ccb55c9a95f7f7e9,Fix bugs with bool argument passing; pass PRED values as int32s instead.,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 943c38709..787793f21 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,6 +19,7 @@
 
 from __future__ import print_function
 
+from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
@@ -42,12 +43,12 @@ cdef register_cpu_custom_call_target(fn_name, void* fn):
 # triangular solve
 
 cdef void blas_strsm(void* out, void** data) nogil:
-  cdef bint left_side = (<bint*>(data[0]))[0]
-  cdef bint lower = (<bint*>(data[1]))[0]
-  cdef int trans_a = (<int*>(data[2]))[0]
-  cdef bint diag = (<bint*>(data[3]))[0]
-  cdef int m = (<int*>(data[4]))[0]
-  cdef int n = (<int*>(data[5]))[0]
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
   cdef float* alpha = <float*>(data[6])
   cdef float* a = <float*>(data[7])
   cdef float* b = <float*>(data[8])
@@ -71,12 +72,12 @@ cdef void blas_strsm(void* out, void** data) nogil:
 register_cpu_custom_call_target(b""blas_strsm"", <void*>(blas_strsm))
 
 cdef void blas_dtrsm(void* out, void** data) nogil:
-  cdef bint left_side = (<bint*>(data[0]))[0]
-  cdef bint lower = (<bint*>(data[1]))[0]
-  cdef int trans_a = (<int*>(data[2]))[0]
-  cdef bint diag = (<bint*>(data[3]))[0]
-  cdef int m = (<int*>(data[4]))[0]
-  cdef int n = (<int*>(data[5]))[0]
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
   cdef double* alpha = <double*>(data[6])
   cdef double* a = <double*>(data[7])
   cdef double* b = <double*>(data[8])
@@ -124,19 +125,19 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
   return c.CustomCall(
       fn,
       operands=(
-        c.ConstantPredScalar(left_side),
-        c.ConstantPredScalar(lower),
+        c.ConstantS32Scalar(int(left_side)),
+        c.ConstantS32Scalar(int(lower)),
         c.ConstantS32Scalar(1 if trans_a else 0),
-        c.ConstantPredScalar(diag),
+        c.ConstantS32Scalar(int(diag)),
         c.ConstantS32Scalar(m),
         c.ConstantS32Scalar(n),
         alpha, a, b),
       shape_with_layout=Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
       operand_shapes_with_layout=(
-          Shape.array_shape(np.bool, (), ()),
-          Shape.array_shape(np.bool, (), ()),
           Shape.array_shape(np.int32, (), ()),
-          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (), ()),
@@ -147,8 +148,8 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
 # ?potrf: Cholesky decomposition
 
 cdef void lapack_spotrf(void* out_tuple, void** data) nogil:
-  cdef bint lower = (<bint*>(data[0]))[0]
-  cdef int n = (<int*>(data[1]))[0]
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
   cdef const float* a_in = <float*>(data[2])
   cdef char uplo = 'L' if lower else 'U'
 
@@ -176,8 +177,8 @@ register_cpu_custom_call_target(b""lapack_spotrf"", <void*>(lapack_spotrf))
 
 
 cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
-  cdef bint lower = (<bint*>(data[0]))[0]
-  cdef int n = (<int*>(data[1]))[0]
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
   cdef const double* a_in = <double*>(data[2])
   cdef char uplo = 'L' if lower else 'U'
 
@@ -204,6 +205,8 @@ cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_dpotrf"", <void*>(lapack_dpotrf))
 
 def jax_potrf(c, a, lower=False):
+  assert sizeof(int32_t) == sizeof(int)
+
   a_shape = c.GetShape(a)
   dtype = a_shape.element_type()
   m, n = a_shape.dimensions()
@@ -218,13 +221,13 @@ def jax_potrf(c, a, lower=False):
 
   return c.CustomCall(
       fn,
-      operands=(c.ConstantPredScalar(lower), c.ConstantS32Scalar(n), a),
+      operands=(c.ConstantS32Scalar(int(lower)), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (n, n), (0, 1)),
           Shape.array_shape(np.int32, (), ()),
       )),
       operand_shapes_with_layout=(
-          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
-      ))
\ No newline at end of file
+      ))","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 943c38709..787793f21 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,6 +19,7 @@
 
 from __future__ import print_function
 
+from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
@@ -42,12 +43,12 @@ cdef register_cpu_custom_call_target(fn_name, void* fn):
 # triangular solve
 
 cdef void blas_strsm(void* out, void** data) nogil:
-  cdef bint left_side = (<bint*>(data[0]))[0]
-  cdef bint lower = (<bint*>(data[1]))[0]
-  cdef int trans_a = (<int*>(data[2]))[0]
-  cdef bint diag = (<bint*>(data[3]))[0]
-  cdef int m = (<int*>(data[4]))[0]
-  cdef int n = (<int*>(data[5]))[0]
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
   cdef float* alpha = <float*>(data[6])
   cdef float* a = <float*>(data[7])
   cdef float* b = <float*>(data[8])
@@ -71,12 +72,12 @@ cdef void blas_strsm(void* out, void** data) nogil:
 register_cpu_custom_call_target(b""blas_strsm"", <void*>(blas_strsm))
 
 cdef void blas_dtrsm(void* out, void** data) nogil:
-  cdef bint left_side = (<bint*>(data[0]))[0]
-  cdef bint lower = (<bint*>(data[1]))[0]
-  cdef int trans_a = (<int*>(data[2]))[0]
-  cdef bint diag = (<bint*>(data[3]))[0]
-  cdef int m = (<int*>(data[4]))[0]
-  cdef int n = (<int*>(data[5]))[0]
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
   cdef double* alpha = <double*>(data[6])
   cdef double* a = <double*>(data[7])
   cdef double* b = <double*>(data[8])
@@ -124,19 +125,19 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
   return c.CustomCall(
       fn,
       operands=(
-        c.ConstantPredScalar(left_side),
-        c.ConstantPredScalar(lower),
+        c.ConstantS32Scalar(int(left_side)),
+        c.ConstantS32Scalar(int(lower)),
         c.ConstantS32Scalar(1 if trans_a else 0),
-        c.ConstantPredScalar(diag),
+        c.ConstantS32Scalar(int(diag)),
         c.ConstantS32Scalar(m),
         c.ConstantS32Scalar(n),
         alpha, a, b),
       shape_with_layout=Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
       operand_shapes_with_layout=(
-          Shape.array_shape(np.bool, (), ()),
-          Shape.array_shape(np.bool, (), ()),
           Shape.array_shape(np.int32, (), ()),
-          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (), ()),
@@ -147,8 +148,8 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
 # ?potrf: Cholesky decomposition
 
 cdef void lapack_spotrf(void* out_tuple, void** data) nogil:
-  cdef bint lower = (<bint*>(data[0]))[0]
-  cdef int n = (<int*>(data[1]))[0]
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
   cdef const float* a_in = <float*>(data[2])
   cdef char uplo = 'L' if lower else 'U'
 
@@ -176,8 +177,8 @@ register_cpu_custom_call_target(b""lapack_spotrf"", <void*>(lapack_spotrf))
 
 
 cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
-  cdef bint lower = (<bint*>(data[0]))[0]
-  cdef int n = (<int*>(data[1]))[0]
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
   cdef const double* a_in = <double*>(data[2])
   cdef char uplo = 'L' if lower else 'U'
 
@@ -204,6 +205,8 @@ cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_dpotrf"", <void*>(lapack_dpotrf))
 
 def jax_potrf(c, a, lower=False):
+  assert sizeof(int32_t) == sizeof(int)
+
   a_shape = c.GetShape(a)
   dtype = a_shape.element_type()
   m, n = a_shape.dimensions()
@@ -218,13 +221,13 @@ def jax_potrf(c, a, lower=False):
 
   return c.CustomCall(
       fn,
-      operands=(c.ConstantPredScalar(lower), c.ConstantS32Scalar(n), a),
+      operands=(c.ConstantS32Scalar(int(lower)), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (n, n), (0, 1)),
           Shape.array_shape(np.int32, (), ()),
       )),
       operand_shapes_with_layout=(
-          Shape.array_shape(np.bool, (), ()),
+          Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
-      ))
\ No newline at end of file
+      ))",No
WORKSPACE,WORKSPACE,27ef207a5d36ba1873d0056a6b99306bfb77e121,eac96ac2393eeef3e2e4c298f8f56f4b3a739470,Bump XLA version to incorporate CPU custom call extensions.,"diff --git a/WORKSPACE b/WORKSPACE
index 50c367b1c..484331e78 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""e4bee8b29c3fa3d22f554558dee71b64bc40e10e9bc673be575ff8502525dfa2"",
-   strip_prefix = ""tensorflow-5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d"",
+   sha256 = ""e042c2529438c71f53d3c54bd04df2fabf1ed2491c8686d556d617021eeeff11"",
+   strip_prefix = ""tensorflow-ec3f67d9673e77a0b7078f2a988cd09141c0a741"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/ec3f67d9673e77a0b7078f2a988cd09141c0a741.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 50c367b1c..484331e78 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""e4bee8b29c3fa3d22f554558dee71b64bc40e10e9bc673be575ff8502525dfa2"",
-   strip_prefix = ""tensorflow-5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d"",
+   sha256 = ""e042c2529438c71f53d3c54bd04df2fabf1ed2491c8686d556d617021eeeff11"",
+   strip_prefix = ""tensorflow-ec3f67d9673e77a0b7078f2a988cd09141c0a741"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/5aa0b0de0ca2a931f1af924e3a0ce5b6402b3e9d.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/ec3f67d9673e77a0b7078f2a988cd09141c0a741.tar.gz"",
    ],
 )
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,7524f2c087e68436333c29650170ffc7ef20e8c7,5a4a066ae6415cde5b96f8cd98217a58e7635cd3,fix mean/var/std kwargs (closes #125),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 80d06456d..9a4b70ad3 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -644,7 +644,10 @@ any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 
 
 @_wraps(onp.mean)
-def mean(a, axis=None, dtype=None, keepdims=False):
+def mean(a, axis=None, dtype=None, out=None, keepdims=False):
+  if out is not None:
+    raise ValueError(""mean does not support `out` argument."")
+
   if axis is None:
     normalizer = size(a)
   else:
@@ -663,7 +666,10 @@ def mean(a, axis=None, dtype=None, keepdims=False):
 
 
 @_wraps(onp.var)
-def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
+def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
+  if out is not None:
+    raise ValueError(""mean does not support `out` argument."")
+
   if ddof != 0:
     raise NotImplementedError(""Only implemented for ddof=0."")
   if dtype is None:
@@ -677,8 +683,9 @@ def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
 
 
 @_wraps(onp.std)
-def std(a, axis=None, dtype=None, keepdims=False, ddof=0):
-  return sqrt(var(a, axis=axis, dtype=dtype, keepdims=keepdims, ddof=ddof))
+def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
+  return sqrt(var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
+                  keepdims=keepdims))
 
 
 @_wraps(onp.allclose)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 80d06456d..9a4b70ad3 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -644,7 +644,10 @@ any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 
 
 @_wraps(onp.mean)
-def mean(a, axis=None, dtype=None, keepdims=False):
+def mean(a, axis=None, dtype=None, out=None, keepdims=False):
+  if out is not None:
+    raise ValueError(""mean does not support `out` argument."")
+
   if axis is None:
     normalizer = size(a)
   else:
@@ -663,7 +666,10 @@ def mean(a, axis=None, dtype=None, keepdims=False):
 
 
 @_wraps(onp.var)
-def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
+def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
+  if out is not None:
+    raise ValueError(""mean does not support `out` argument."")
+
   if ddof != 0:
     raise NotImplementedError(""Only implemented for ddof=0."")
   if dtype is None:
@@ -677,8 +683,9 @@ def var(a, axis=None, dtype=None, keepdims=False, ddof=0):
 
 
 @_wraps(onp.std)
-def std(a, axis=None, dtype=None, keepdims=False, ddof=0):
-  return sqrt(var(a, axis=axis, dtype=dtype, keepdims=keepdims, ddof=ddof))
+def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
+  return sqrt(var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
+                  keepdims=keepdims))
 
 
 @_wraps(onp.allclose)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,7524f2c087e68436333c29650170ffc7ef20e8c7,5a4a066ae6415cde5b96f8cd98217a58e7635cd3,fix mean/var/std kwargs (closes #125),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index a9f178421..b6bc1706e 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -810,6 +810,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   # TODO(mattjj): test other ndarray-like method overrides
 
+  def testOnpMean(self):
+    # from https://github.com/google/jax/issues/125
+    x = lnp.eye(3) + 0.
+    ans = onp.mean(x)
+    self.assertAllClose(ans, onp.array([1./3, 1./3, 1./3]), check_dtypes=False)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index a9f178421..b6bc1706e 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -810,6 +810,12 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   # TODO(mattjj): test other ndarray-like method overrides
 
+  def testOnpMean(self):
+    # from https://github.com/google/jax/issues/125
+    x = lnp.eye(3) + 0.
+    ans = onp.mean(x)
+    self.assertAllClose(ans, onp.array([1./3, 1./3, 1./3]), check_dtypes=False)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/interpreters/xla.py,jax/interpreters/xla.py,1f2925ea8af5ba838776269b4943eb5e0ba51cd3,27ef207a5d36ba1873d0056a6b99306bfb77e121,add backend-specific translation table in xla.py,"diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 6bdf01d4e..a10deab14 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -16,7 +16,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from collections import namedtuple
+from collections import namedtuple, defaultdict
 import itertools as it
 import numpy as onp
 import operator as op
@@ -153,14 +153,16 @@ def unit_constant(c, val):
 xb.register_constant_handler(JaxTuple, unit_constant)
 
 def translation_rule(p):
+  backend_specific_rule = backend_specific_translations[xb._platform_name].get(p)
   try:
-    return translations[p]
+    return backend_specific_rule or translations[p]
   except KeyError:
     raise NotImplementedError(
         ""XLA translation rule for '{}' not implemented"".format(p))
 
 
 translations = {}
+backend_specific_translations = defaultdict(dict)
 
 translations[core.pack_p] = lambda c, *xs: c.Tuple(*xs)
 translations[core.call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 6bdf01d4e..a10deab14 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -16,7 +16,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from collections import namedtuple
+from collections import namedtuple, defaultdict
 import itertools as it
 import numpy as onp
 import operator as op
@@ -153,14 +153,16 @@ def unit_constant(c, val):
 xb.register_constant_handler(JaxTuple, unit_constant)
 
 def translation_rule(p):
+  backend_specific_rule = backend_specific_translations[xb._platform_name].get(p)
   try:
-    return translations[p]
+    return backend_specific_rule or translations[p]
   except KeyError:
     raise NotImplementedError(
         ""XLA translation rule for '{}' not implemented"".format(p))
 
 
 translations = {}
+backend_specific_translations = defaultdict(dict)
 
 translations[core.pack_p] = lambda c, *xs: c.Tuple(*xs)
 translations[core.call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],",No
jax/lax_linalg.py,jax/lax_linalg.py,1f2925ea8af5ba838776269b4943eb5e0ba51cd3,27ef207a5d36ba1873d0056a6b99306bfb77e121,add backend-specific translation table in xla.py,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 76910c915..95e02f5be 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -84,8 +84,7 @@ def cholesky_cpu_translation_rule(c, operand):
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-# TODO(mattjj): add per-backend translation rule support
-# xla.translations[cholesky_p] = cholesky_cpu_translation_rule
+xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -141,8 +140,7 @@ def triangular_solve_cpu_translation_rule(
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-# TODO(mattjj): add per-backend translation rule support
-# xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
+xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
 def qr_impl(operand, full_matrices):","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 76910c915..95e02f5be 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -84,8 +84,7 @@ def cholesky_cpu_translation_rule(c, operand):
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-# TODO(mattjj): add per-backend translation rule support
-# xla.translations[cholesky_p] = cholesky_cpu_translation_rule
+xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -141,8 +140,7 @@ def triangular_solve_cpu_translation_rule(
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-# TODO(mattjj): add per-backend translation rule support
-# xla.translations[triangular_solve_p] = triangular_solve_cpu_translation_rule
+xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
 def qr_impl(operand, full_matrices):",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,1f2925ea8af5ba838776269b4943eb5e0ba51cd3,27ef207a5d36ba1873d0056a6b99306bfb77e121,add backend-specific translation table in xla.py,"diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 7f13190ec..b9963ceb1 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -61,6 +61,8 @@ flags.DEFINE_string(
 # visible output files.
 _SPONGE_PREFIX = '/SPONGE/'
 
+_platform_name = None  # set to the active platform name
+
 
 def _hlo_path(path, name):
   path = path.replace(_SPONGE_PREFIX,
@@ -127,16 +129,20 @@ def _get_xla_client(backend_name, platform_name, replica_count):
   Returns:
     A client library module, or an object that behaves identically to one.
   """"""
+  global _platform_name
   xla_client.initialize_replica_count(replica_count)
   if backend_name == 'xla':
     if platform_name:
       xla_client.initialize_platform_name(platform_name)
+      _platform_name = platform_name
     else:
       try:
         xla_client.initialize_platform_name('CUDA')
+        _platform_name = 'CUDA'
       except RuntimeError:
         warnings.warn('No GPU found, falling back to CPU.')
         xla_client.initialize_platform_name('Host')
+        _platform_name = 'Host'
   return xla_client
 
 ","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 7f13190ec..b9963ceb1 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -61,6 +61,8 @@ flags.DEFINE_string(
 # visible output files.
 _SPONGE_PREFIX = '/SPONGE/'
 
+_platform_name = None  # set to the active platform name
+
 
 def _hlo_path(path, name):
   path = path.replace(_SPONGE_PREFIX,
@@ -127,16 +129,20 @@ def _get_xla_client(backend_name, platform_name, replica_count):
   Returns:
     A client library module, or an object that behaves identically to one.
   """"""
+  global _platform_name
   xla_client.initialize_replica_count(replica_count)
   if backend_name == 'xla':
     if platform_name:
       xla_client.initialize_platform_name(platform_name)
+      _platform_name = platform_name
     else:
       try:
         xla_client.initialize_platform_name('CUDA')
+        _platform_name = 'CUDA'
       except RuntimeError:
         warnings.warn('No GPU found, falling back to CPU.')
         xla_client.initialize_platform_name('Host')
+        _platform_name = 'Host'
   return xla_client
 
 ",No
jax/numpy/linalg.py,jax/numpy/linalg.py,ab1ebc6bad17948eede6ae687ad1ab331731fb6d,1f2925ea8af5ba838776269b4943eb5e0ba51cd3,Add experimental warning to numpy.linalg and scipy.linalg.,"diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 2af0d4008..4dcb3d8e6 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 
 import numpy as onp
+import warnings
 
 from .. import lax_linalg
 from .lax_numpy import _not_implemented
@@ -25,15 +26,18 @@ from .lax_numpy import _wraps
 from . import lax_numpy as np
 from ..util import get_module_functions
 
+_EXPERIMENTAL_WARNING = ""numpy.linalg support is experimental and may cause silent failures or wrong outputs""
 
 _T = lambda x: np.swapaxes(x, -1, -2)
 
 @_wraps(onp.linalg.cholesky)
 def cholesky(a):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   return lax_linalg.cholesky(a)
 
 @_wraps(onp.linalg.inv)
 def inv(a):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   if np.ndim(a) < 2 or a.shape[-1] != a.shape[-2]:
     raise ValueError(""Argument to inv must have shape [..., n, n], got {}.""
       .format(np.shape(a)))
@@ -43,6 +47,7 @@ def inv(a):
 
 @_wraps(onp.linalg.qr)
 def qr(a, mode=""reduced""):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   if mode in (""reduced"", ""r"", ""full""):
     full_matrices = False
   elif mode == ""complete"":","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 2af0d4008..4dcb3d8e6 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 
 import numpy as onp
+import warnings
 
 from .. import lax_linalg
 from .lax_numpy import _not_implemented
@@ -25,15 +26,18 @@ from .lax_numpy import _wraps
 from . import lax_numpy as np
 from ..util import get_module_functions
 
+_EXPERIMENTAL_WARNING = ""numpy.linalg support is experimental and may cause silent failures or wrong outputs""
 
 _T = lambda x: np.swapaxes(x, -1, -2)
 
 @_wraps(onp.linalg.cholesky)
 def cholesky(a):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   return lax_linalg.cholesky(a)
 
 @_wraps(onp.linalg.inv)
 def inv(a):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   if np.ndim(a) < 2 or a.shape[-1] != a.shape[-2]:
     raise ValueError(""Argument to inv must have shape [..., n, n], got {}.""
       .format(np.shape(a)))
@@ -43,6 +47,7 @@ def inv(a):
 
 @_wraps(onp.linalg.qr)
 def qr(a, mode=""reduced""):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   if mode in (""reduced"", ""r"", ""full""):
     full_matrices = False
   elif mode == ""complete"":",No
jax/scipy/linalg.py,jax/scipy/linalg.py,ab1ebc6bad17948eede6ae687ad1ab331731fb6d,1f2925ea8af5ba838776269b4943eb5e0ba51cd3,Add experimental warning to numpy.linalg and scipy.linalg.,"diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 2753d7895..e736526ff 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -16,6 +16,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import warnings
+
 import scipy.linalg
 
 from .. import lax_linalg
@@ -24,8 +26,11 @@ from ..numpy import lax_numpy as np
 from ..numpy import linalg as np_linalg
 
 
+_EXPERIMENTAL_WARNING = ""scipy.linalg support is experimental and may cause silent failures or wrong outputs""
+
 @_wraps(scipy.linalg.cholesky)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
   l = lax_linalg.cholesky(a)
   return l if lower else np.conj(l.T)
@@ -33,6 +38,7 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
 
 @_wraps(scipy.linalg.inv)
 def inv(a, overwrite_a=False, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
   return np_linalg.inv(a)
 
@@ -40,6 +46,7 @@ def inv(a, overwrite_a=False, check_finite=True):
 @_wraps(scipy.linalg.qr)
 def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
        check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, lwork, check_finite
   if pivoting:
     raise NotImplementedError(
@@ -59,6 +66,7 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
 @_wraps(scipy.linalg.solve_triangular)
 def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,
                      overwrite_b=False, debug=None, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_b, debug, check_finite
 
   if unit_diagonal:","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 2753d7895..e736526ff 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -16,6 +16,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import warnings
+
 import scipy.linalg
 
 from .. import lax_linalg
@@ -24,8 +26,11 @@ from ..numpy import lax_numpy as np
 from ..numpy import linalg as np_linalg
 
 
+_EXPERIMENTAL_WARNING = ""scipy.linalg support is experimental and may cause silent failures or wrong outputs""
+
 @_wraps(scipy.linalg.cholesky)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
   l = lax_linalg.cholesky(a)
   return l if lower else np.conj(l.T)
@@ -33,6 +38,7 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
 
 @_wraps(scipy.linalg.inv)
 def inv(a, overwrite_a=False, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
   return np_linalg.inv(a)
 
@@ -40,6 +46,7 @@ def inv(a, overwrite_a=False, check_finite=True):
 @_wraps(scipy.linalg.qr)
 def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
        check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, lwork, check_finite
   if pivoting:
     raise NotImplementedError(
@@ -59,6 +66,7 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
 @_wraps(scipy.linalg.solve_triangular)
 def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,
                      overwrite_b=False, debug=None, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_b, debug, check_finite
 
   if unit_diagonal:",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,895c122b90a70a1c6d6fd7ee1e31872c56638af6,7524f2c087e68436333c29650170ffc7ef20e8c7,tweak test to better reflect #125,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index b6bc1706e..77a8d9696 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -27,6 +27,7 @@ from absl.testing import parameterized
 import numpy as onp
 
 from jax import api
+from jax import lax
 from jax import numpy as lnp
 from jax import test_util as jtu
 
@@ -812,7 +813,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   def testOnpMean(self):
     # from https://github.com/google/jax/issues/125
-    x = lnp.eye(3) + 0.
+    x = lax.add(lnp.eye(3), 0.)
     ans = onp.mean(x)
     self.assertAllClose(ans, onp.array([1./3, 1./3, 1./3]), check_dtypes=False)
 ","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index b6bc1706e..77a8d9696 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -27,6 +27,7 @@ from absl.testing import parameterized
 import numpy as onp
 
 from jax import api
+from jax import lax
 from jax import numpy as lnp
 from jax import test_util as jtu
 
@@ -812,7 +813,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   def testOnpMean(self):
     # from https://github.com/google/jax/issues/125
-    x = lnp.eye(3) + 0.
+    x = lax.add(lnp.eye(3), 0.)
     ans = onp.mean(x)
     self.assertAllClose(ans, onp.array([1./3, 1./3, 1./3]), check_dtypes=False)
 ",No
jax/lax_linalg.py,jax/lax_linalg.py,e0f421746b8c72afae25ac6a6ad357d5223b7524,ab1ebc6bad17948eede6ae687ad1ab331731fb6d,Import CPU Lapack implementation conditionally to ease jaxlib upgrade.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 95e02f5be..132e38196 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -28,8 +28,7 @@ from jax.abstract_arrays import ShapedArray
 from jax.core import Primitive
 from jax.lax import (standard_primitive, standard_unop, binop_dtype_rule,
                      _float, _complex, _input_dtype)
-from jax.lib import xla_bridge
-from jaxlib import lapack
+import jaxlib
 
 # traceables
 
@@ -77,14 +76,15 @@ def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
   if len(shape.dimensions()) == 2 and (
     shape.element_type() == np.float32 or shape.element_type() == np.float64):
-    return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
+    return c.GetTupleElement(jaxlib.lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
     # unsupported types.
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
+if hasattr(jaxlib, ""lapack""):
+  xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -129,18 +129,21 @@ def triangular_solve_cpu_translation_rule(
     c, a, b, left_side, lower, transpose_a, conjugate_a):
   shape = c.GetShape(a)
   if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return lapack.jax_trsm(c, c.ConstantF32Scalar(1.0), a, b, left_side, lower,
-                           transpose_a, conjugate_a)
+    return jaxlib.lapack.jax_trsm(
+      c, c.ConstantF32Scalar(1.0), a, b, left_side, lower, transpose_a,
+      conjugate_a)
   elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return lapack.jax_trsm(c, c.ConstantF64Scalar(1.0), a, b, left_side, lower,
-                           transpose_a, conjugate_a)
+    return jaxlib.lapack.jax_trsm(
+      c, c.ConstantF64Scalar(1.0), a, b, left_side, lower, transpose_a,
+      conjugate_a)
   else:
     # Fall back to the HLO implementation for batched triangular_solve or
     # unsupported types.
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
+if hasattr(jaxlib, ""lapack""):
+  xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
 def qr_impl(operand, full_matrices):","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 95e02f5be..132e38196 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -28,8 +28,7 @@ from jax.abstract_arrays import ShapedArray
 from jax.core import Primitive
 from jax.lax import (standard_primitive, standard_unop, binop_dtype_rule,
                      _float, _complex, _input_dtype)
-from jax.lib import xla_bridge
-from jaxlib import lapack
+import jaxlib
 
 # traceables
 
@@ -77,14 +76,15 @@ def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
   if len(shape.dimensions()) == 2 and (
     shape.element_type() == np.float32 or shape.element_type() == np.float64):
-    return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
+    return c.GetTupleElement(jaxlib.lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
     # unsupported types.
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
+if hasattr(jaxlib, ""lapack""):
+  xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -129,18 +129,21 @@ def triangular_solve_cpu_translation_rule(
     c, a, b, left_side, lower, transpose_a, conjugate_a):
   shape = c.GetShape(a)
   if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return lapack.jax_trsm(c, c.ConstantF32Scalar(1.0), a, b, left_side, lower,
-                           transpose_a, conjugate_a)
+    return jaxlib.lapack.jax_trsm(
+      c, c.ConstantF32Scalar(1.0), a, b, left_side, lower, transpose_a,
+      conjugate_a)
   elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return lapack.jax_trsm(c, c.ConstantF64Scalar(1.0), a, b, left_side, lower,
-                           transpose_a, conjugate_a)
+    return jaxlib.lapack.jax_trsm(
+      c, c.ConstantF64Scalar(1.0), a, b, left_side, lower, transpose_a,
+      conjugate_a)
   else:
     # Fall back to the HLO implementation for batched triangular_solve or
     # unsupported types.
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
+if hasattr(jaxlib, ""lapack""):
+  xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
 def qr_impl(operand, full_matrices):",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,5589a3cc58dc6382cb657131ef54f1c9ce3903a6,895c122b90a70a1c6d6fd7ee1e31872c56638af6,fix up error messages,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 9a4b70ad3..a9d2c66b6 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -596,7 +596,7 @@ def _make_reduction(np_fun, op, init_val, preproc=None):
   @_wraps(np_fun)
   def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
     if out is not None:
-      raise ValueError(""reduction does not support `out` argument."")
+      raise ValueError(""reduction does not support the `out` argument."")
 
     a = a if isinstance(a, ndarray) else asarray(a)
     a = preproc(a) if preproc else a
@@ -646,7 +646,7 @@ any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 @_wraps(onp.mean)
 def mean(a, axis=None, dtype=None, out=None, keepdims=False):
   if out is not None:
-    raise ValueError(""mean does not support `out` argument."")
+    raise ValueError(""mean does not support the `out` argument."")
 
   if axis is None:
     normalizer = size(a)
@@ -668,7 +668,7 @@ def mean(a, axis=None, dtype=None, out=None, keepdims=False):
 @_wraps(onp.var)
 def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
   if out is not None:
-    raise ValueError(""mean does not support `out` argument."")
+    raise ValueError(""var does not support the `out` argument."")
 
   if ddof != 0:
     raise NotImplementedError(""Only implemented for ddof=0."")
@@ -684,8 +684,9 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
 
 @_wraps(onp.std)
 def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
-  return sqrt(var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
-                  keepdims=keepdims))
+  if out is not None:
+    raise ValueError(""std does not support the `out` argument."")
+  return sqrt(var(a, axis=axis, dtype=dtype, ddof=ddof, keepdims=keepdims))
 
 
 @_wraps(onp.allclose)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 9a4b70ad3..a9d2c66b6 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -596,7 +596,7 @@ def _make_reduction(np_fun, op, init_val, preproc=None):
   @_wraps(np_fun)
   def reduction(a, axis=None, dtype=None, out=None, keepdims=False):
     if out is not None:
-      raise ValueError(""reduction does not support `out` argument."")
+      raise ValueError(""reduction does not support the `out` argument."")
 
     a = a if isinstance(a, ndarray) else asarray(a)
     a = preproc(a) if preproc else a
@@ -646,7 +646,7 @@ any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 @_wraps(onp.mean)
 def mean(a, axis=None, dtype=None, out=None, keepdims=False):
   if out is not None:
-    raise ValueError(""mean does not support `out` argument."")
+    raise ValueError(""mean does not support the `out` argument."")
 
   if axis is None:
     normalizer = size(a)
@@ -668,7 +668,7 @@ def mean(a, axis=None, dtype=None, out=None, keepdims=False):
 @_wraps(onp.var)
 def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
   if out is not None:
-    raise ValueError(""mean does not support `out` argument."")
+    raise ValueError(""var does not support the `out` argument."")
 
   if ddof != 0:
     raise NotImplementedError(""Only implemented for ddof=0."")
@@ -684,8 +684,9 @@ def var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
 
 @_wraps(onp.std)
 def std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False):
-  return sqrt(var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
-                  keepdims=keepdims))
+  if out is not None:
+    raise ValueError(""std does not support the `out` argument."")
+  return sqrt(var(a, axis=axis, dtype=dtype, ddof=ddof, keepdims=keepdims))
 
 
 @_wraps(onp.allclose)",No
jax/core.py,jax/core.py,f9714152183a63e2e365aa2485f587b606801aa7,25cf9358d1e4b3165a0f9e6c094c1a711e0d113b,add tie_in and full primitives (constant creation),"diff --git a/jax/core.py b/jax/core.py
index 0f839c7be..eb26cebc0 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -249,6 +249,8 @@ class Tracer(object):
   def __hex__(self): return self.aval._hex(self)
   def __oct__(self): return self.aval._oct(self)
 
+  def __setitem__(self, idx, val):
+    raise TypeError(""JAX 'Tracer' objects do not support item assignment"")
 
   def __getattr__(self, name):
     # if the aval property raises an AttributeError, gets caught here","diff --git a/jax/core.py b/jax/core.py
index 0f839c7be..eb26cebc0 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -249,6 +249,8 @@ class Tracer(object):
   def __hex__(self): return self.aval._hex(self)
   def __oct__(self): return self.aval._oct(self)
 
+  def __setitem__(self, idx, val):
+    raise TypeError(""JAX 'Tracer' objects do not support item assignment"")
 
   def __getattr__(self, name):
     # if the aval property raises an AttributeError, gets caught here",No
jax/interpreters/xla.py,jax/interpreters/xla.py,f9714152183a63e2e365aa2485f587b606801aa7,25cf9358d1e4b3165a0f9e6c094c1a711e0d113b,add tie_in and full primitives (constant creation),"diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index a10deab14..81e7fde6c 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -74,6 +74,8 @@ def execute_compiled_primitive(compiled, result_handler, *args):
 def device_put(x):
   if type(x) is DeviceArray:
     return x.device_buffer
+  elif isinstance(x, DeviceConstant):
+    return instantiate_device_constant(x)
   else:
     return xb.device_put(x)  # can round-trip elements of tuples here
 
@@ -237,6 +239,7 @@ class DeviceArray(DeviceValue):
     self.size = size
     self._npy_value = None
 
+  # TODO make device_buffer a property, make the _npy_value writeable, invalidate
   @property
   def _value(self):
     if self._npy_value is None:
@@ -319,6 +322,31 @@ xb.register_constant_handler(DeviceArray,
                              lambda c, val: c.Constant(onp.asarray(val)))
 
 
+class DeviceConstant(DeviceArray):
+  @staticmethod
+  def constant_handler(c, constant_instance):
+    assert False
+
+# TODO(mattjj): tune cutoff
+def instantiate_device_constant(const, cutoff=1000000):
+  # dispatch an XLA Computation to build the constant on the device if it's
+  # large, or alternatively build it on the host and transfer it if it's small
+  assert isinstance(const, DeviceConstant)
+  if const.size > cutoff:
+    c = xb.make_computation_builder(""constant_instantiating_computation"")
+    xla_const = const.constant_handler(c, const)
+    compiled = c.Build(xla_const).Compile((), xb.get_compile_options())
+    return compiled.Execute(())
+  else:
+    return xb.device_put(onp.asarray(const))
+
+def register_device_constant(cls):
+  pytype_aval_mappings[cls] = pytype_aval_mappings[DeviceArray]
+  canonicalize_dtype_handlers[cls] = identity
+  core.pytype_aval_mappings[cls] = ConcreteArray
+  xb.register_constant_handler(cls, cls.constant_handler)
+
+
 def xla_shape(x):
   try:
     return xb.Shape.array_shape(x.dtype, x.shape)","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index a10deab14..81e7fde6c 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -74,6 +74,8 @@ def execute_compiled_primitive(compiled, result_handler, *args):
 def device_put(x):
   if type(x) is DeviceArray:
     return x.device_buffer
+  elif isinstance(x, DeviceConstant):
+    return instantiate_device_constant(x)
   else:
     return xb.device_put(x)  # can round-trip elements of tuples here
 
@@ -237,6 +239,7 @@ class DeviceArray(DeviceValue):
     self.size = size
     self._npy_value = None
 
+  # TODO make device_buffer a property, make the _npy_value writeable, invalidate
   @property
   def _value(self):
     if self._npy_value is None:
@@ -319,6 +322,31 @@ xb.register_constant_handler(DeviceArray,
                              lambda c, val: c.Constant(onp.asarray(val)))
 
 
+class DeviceConstant(DeviceArray):
+  @staticmethod
+  def constant_handler(c, constant_instance):
+    assert False
+
+# TODO(mattjj): tune cutoff
+def instantiate_device_constant(const, cutoff=1000000):
+  # dispatch an XLA Computation to build the constant on the device if it's
+  # large, or alternatively build it on the host and transfer it if it's small
+  assert isinstance(const, DeviceConstant)
+  if const.size > cutoff:
+    c = xb.make_computation_builder(""constant_instantiating_computation"")
+    xla_const = const.constant_handler(c, const)
+    compiled = c.Build(xla_const).Compile((), xb.get_compile_options())
+    return compiled.Execute(())
+  else:
+    return xb.device_put(onp.asarray(const))
+
+def register_device_constant(cls):
+  pytype_aval_mappings[cls] = pytype_aval_mappings[DeviceArray]
+  canonicalize_dtype_handlers[cls] = identity
+  core.pytype_aval_mappings[cls] = ConcreteArray
+  xb.register_constant_handler(cls, cls.constant_handler)
+
+
 def xla_shape(x):
   try:
     return xb.Shape.array_shape(x.dtype, x.shape)",No
jax/lax.py,jax/lax.py,f9714152183a63e2e365aa2485f587b606801aa7,25cf9358d1e4b3165a0f9e6c094c1a711e0d113b,add tie_in and full primitives (constant creation),"diff --git a/jax/lax.py b/jax/lax.py
index 5f8b970b0..3b60cd5cf 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -17,7 +17,7 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-from .util import partial
+from .util import partial, prod
 import itertools
 import operator
 import six
@@ -43,6 +43,9 @@ from .lib import xla_bridge
 
 _max = builtins.max
 _min = builtins.max
+_reduce = six.moves.reduce
+
+def identity(x): return x
 
 ### traceables
 
@@ -411,6 +414,25 @@ class OpaqueParam(object):
 opaque_param_ids = itertools.count()
 
 
+def tie_in(x, y):
+  return tie_in_p.bind(x, y)
+
+def full(shape, fill_value, dtype=None):
+  if onp.shape(fill_value):
+    msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
+    raise TypeError(msg.format(onp.shape(fill_value)))
+
+  dtype = dtype and xla_bridge.canonicalize_dtype(dtype)
+  if dtype is not None and _dtype(fill_value) != dtype:
+    # for Python scalars and raw ndarrays, we keep fill_value as a cpu ndarray
+    if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
+      fill_value = onp.array(fill_value, dtype)
+    else:
+      fill_value = convert_element_type(fill_value, dtype)
+
+  return full_p.bind(fill_value, shape=shape)
+
+
 ### convenience wrappers around traceables
 
 
@@ -439,7 +461,8 @@ def full_like(x, fill_value, dtype=None, shape=None):
     `fill_value`, similar to the output of np.full.
   """"""
   shape = onp.shape(x) if shape is None else shape
-  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)
+  out = full(shape, fill_value, dtype or _dtype(x))
+  return tie_in(x, out)
 
 
 def collapse(operand, start_dimension, stop_dimension):
@@ -631,8 +654,7 @@ ShapedArray._iter = staticmethod(_iter)
 # Add some ad handlers that use (or could use) lax primitives
 
 def zeros_like_array(x):
-  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
-  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))
+  return full_like(x, 0)
 
 for t in itertools.chain(array_types, [xla.DeviceArray]):
   ad_util.jaxval_adders[t] = add
@@ -648,8 +670,6 @@ _input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
 _fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
 _complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype
 
-def identity(x): return x
-
 
 def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
   prim = Primitive(name)
@@ -1561,9 +1581,11 @@ def select_dtype_rule(pred, on_true, on_false):
   return on_true.dtype
 
 def select_transpose_rule(t, pred, on_true, on_false):
+  assert pred is not None
+  zeros = full_like(t, 0)
   return [None,
-          select(pred, t, _zeros(on_false)) if on_true is None else None,
-          select(pred, _zeros(on_true), t) if on_false is None else None]
+          select(pred, t, zeros) if on_true is None else None,
+          select(pred, zeros, t) if on_false is None else None]
 
 def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
   oprand, on_true, on_false, = batched_args
@@ -2218,6 +2240,71 @@ while_p.def_abstract_eval(while_loop_abstract_eval)
 xla.translations[while_p] = while_loop_translation_rule
 
 
+### primitives for handling constants
+
+
+def tie_in_transpose_rule(t):
+  return [ad_util.zero, t]
+
+def tie_in_batch_rule(batched_args, batch_dims):
+  y = tie_in(*batched_args)
+  _, bdim_y = batch_dims
+  return y, bdim_y
+
+tie_in_p = Primitive('tie_in')
+tie_in_p.def_impl(lambda x, y: y)
+tie_in_p.def_abstract_eval(lambda x, y: y)
+xla.translations[tie_in_p] = lambda c, x, y: y
+ad.deflinear(tie_in_p, tie_in_transpose_rule)
+batching.primitive_batchers[tie_in_p] = tie_in_batch_rule
+
+
+class FilledConstant(xla.DeviceConstant):
+  __slots__ = [""fill_value""]
+
+  def __init__(self, fill_value, shape):
+    assert type(fill_value) is onp.ndarray
+    self.shape = shape
+    self.dtype = _dtype(fill_value)
+    self.ndim = len(shape)
+    self.size = prod(shape)
+    self._npy_value = None
+
+    self.fill_value = fill_value
+
+  @property
+  def _value(self):
+    return onp.full(self.shape, self.fill_value)
+
+  @staticmethod
+  def constant_handler(c, filled_const):
+    return c.Broadcast(c.NumpyArrayConstant(filled_const.fill_value),
+                       filled_const.shape)
+xla.register_device_constant(FilledConstant)
+
+# TODO(mattjj): if we used isinstance rather than handlers here, these would all
+# be covered as subclasses of DeviceArray. alternatively, just set up these in a
+# loop after we've defined all the constant DeviceConstant subclasses.
+batching.pytype_aval_mappings[FilledConstant] = make_shaped_array
+ad_util.jaxval_adders[FilledConstant] = add
+ad_util.jaxval_zeros_likers[FilledConstant] = zeros_like_array
+
+def full_batch_rule(batched_args, batch_dims, shape):
+  fill_value, = batched_args
+  bdim, = batch_dims
+  assert bdim == 0
+  return broadcast_in_dim(fill_value, fill_value.shape + shape, [bdim])
+
+full_p = Primitive('full_p')
+full_p.def_impl(FilledConstant)
+full_p.def_abstract_eval(
+    lambda fill_value, shape: ShapedArray(shape, _dtype(fill_value)))
+xla.translations[full_p] = \
+    lambda c, fill_value, shape: c.Broadcast(fill_value, shape)
+ad.deflinear(full_p, lambda t, shape: [_reduce_sum(t, tuple(range(len(shape))))])
+batching.primitive_batchers[full_p] = full_batch_rule
+
+
 ### util
 
 def _ndim(x):
@@ -2350,13 +2437,19 @@ def _dynamic_slice_indices(operand, start_indices):
   return rem(start_indices, onp.array(operand.shape, start_indices.dtype))
 
 
-_const = lambda example, val: onp.array(val, _dtype(example))
-_zeros = partial(full_like, fill_value=0)
-_zero = partial(full_like, shape=(), fill_value=0)
-_ones = partial(full_like, fill_value=1)
-_one = partial(full_like, shape=(), fill_value=1)
-_twos = partial(full_like, fill_value=2)
-_two = partial(full_like, shape=(), fill_value=2)
+def _ndarray_full_like(x, fill_value, dtype=None, shape=None):
+  return onp.broadcast_to(onp.array(fill_value, dtype or _dtype(x)),
+                          onp.shape(x) if shape is None else shape)
+
+def _const(example, val):
+  return onp.array(val, _dtype(example))
+
+_zeros = partial(_ndarray_full_like, fill_value=0)
+_zero = partial(_ndarray_full_like, shape=(), fill_value=0)
+_ones = partial(_ndarray_full_like, fill_value=1)
+_one = partial(_ndarray_full_like, shape=(), fill_value=1)
+_twos = partial(_ndarray_full_like, fill_value=2)
+_two = partial(_ndarray_full_like, shape=(), fill_value=2)
 
 _dtype = onp.result_type
 _iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)","diff --git a/jax/lax.py b/jax/lax.py
index 5f8b970b0..3b60cd5cf 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -17,7 +17,7 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-from .util import partial
+from .util import partial, prod
 import itertools
 import operator
 import six
@@ -43,6 +43,9 @@ from .lib import xla_bridge
 
 _max = builtins.max
 _min = builtins.max
+_reduce = six.moves.reduce
+
+def identity(x): return x
 
 ### traceables
 
@@ -411,6 +414,25 @@ class OpaqueParam(object):
 opaque_param_ids = itertools.count()
 
 
+def tie_in(x, y):
+  return tie_in_p.bind(x, y)
+
+def full(shape, fill_value, dtype=None):
+  if onp.shape(fill_value):
+    msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
+    raise TypeError(msg.format(onp.shape(fill_value)))
+
+  dtype = dtype and xla_bridge.canonicalize_dtype(dtype)
+  if dtype is not None and _dtype(fill_value) != dtype:
+    # for Python scalars and raw ndarrays, we keep fill_value as a cpu ndarray
+    if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
+      fill_value = onp.array(fill_value, dtype)
+    else:
+      fill_value = convert_element_type(fill_value, dtype)
+
+  return full_p.bind(fill_value, shape=shape)
+
+
 ### convenience wrappers around traceables
 
 
@@ -439,7 +461,8 @@ def full_like(x, fill_value, dtype=None, shape=None):
     `fill_value`, similar to the output of np.full.
   """"""
   shape = onp.shape(x) if shape is None else shape
-  return broadcast(onp.array(fill_value, dtype or _dtype(x)), shape)
+  out = full(shape, fill_value, dtype or _dtype(x))
+  return tie_in(x, out)
 
 
 def collapse(operand, start_dimension, stop_dimension):
@@ -631,8 +654,7 @@ ShapedArray._iter = staticmethod(_iter)
 # Add some ad handlers that use (or could use) lax primitives
 
 def zeros_like_array(x):
-  dtype = xla_bridge.canonicalize_dtype(_dtype(x))
-  return onp.broadcast_to(onp.zeros((), dtype), onp.shape(x))
+  return full_like(x, 0)
 
 for t in itertools.chain(array_types, [xla.DeviceArray]):
   ad_util.jaxval_adders[t] = add
@@ -648,8 +670,6 @@ _input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
 _fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
 _complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype
 
-def identity(x): return x
-
 
 def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
   prim = Primitive(name)
@@ -1561,9 +1581,11 @@ def select_dtype_rule(pred, on_true, on_false):
   return on_true.dtype
 
 def select_transpose_rule(t, pred, on_true, on_false):
+  assert pred is not None
+  zeros = full_like(t, 0)
   return [None,
-          select(pred, t, _zeros(on_false)) if on_true is None else None,
-          select(pred, _zeros(on_true), t) if on_false is None else None]
+          select(pred, t, zeros) if on_true is None else None,
+          select(pred, zeros, t) if on_false is None else None]
 
 def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
   oprand, on_true, on_false, = batched_args
@@ -2218,6 +2240,71 @@ while_p.def_abstract_eval(while_loop_abstract_eval)
 xla.translations[while_p] = while_loop_translation_rule
 
 
+### primitives for handling constants
+
+
+def tie_in_transpose_rule(t):
+  return [ad_util.zero, t]
+
+def tie_in_batch_rule(batched_args, batch_dims):
+  y = tie_in(*batched_args)
+  _, bdim_y = batch_dims
+  return y, bdim_y
+
+tie_in_p = Primitive('tie_in')
+tie_in_p.def_impl(lambda x, y: y)
+tie_in_p.def_abstract_eval(lambda x, y: y)
+xla.translations[tie_in_p] = lambda c, x, y: y
+ad.deflinear(tie_in_p, tie_in_transpose_rule)
+batching.primitive_batchers[tie_in_p] = tie_in_batch_rule
+
+
+class FilledConstant(xla.DeviceConstant):
+  __slots__ = [""fill_value""]
+
+  def __init__(self, fill_value, shape):
+    assert type(fill_value) is onp.ndarray
+    self.shape = shape
+    self.dtype = _dtype(fill_value)
+    self.ndim = len(shape)
+    self.size = prod(shape)
+    self._npy_value = None
+
+    self.fill_value = fill_value
+
+  @property
+  def _value(self):
+    return onp.full(self.shape, self.fill_value)
+
+  @staticmethod
+  def constant_handler(c, filled_const):
+    return c.Broadcast(c.NumpyArrayConstant(filled_const.fill_value),
+                       filled_const.shape)
+xla.register_device_constant(FilledConstant)
+
+# TODO(mattjj): if we used isinstance rather than handlers here, these would all
+# be covered as subclasses of DeviceArray. alternatively, just set up these in a
+# loop after we've defined all the constant DeviceConstant subclasses.
+batching.pytype_aval_mappings[FilledConstant] = make_shaped_array
+ad_util.jaxval_adders[FilledConstant] = add
+ad_util.jaxval_zeros_likers[FilledConstant] = zeros_like_array
+
+def full_batch_rule(batched_args, batch_dims, shape):
+  fill_value, = batched_args
+  bdim, = batch_dims
+  assert bdim == 0
+  return broadcast_in_dim(fill_value, fill_value.shape + shape, [bdim])
+
+full_p = Primitive('full_p')
+full_p.def_impl(FilledConstant)
+full_p.def_abstract_eval(
+    lambda fill_value, shape: ShapedArray(shape, _dtype(fill_value)))
+xla.translations[full_p] = \
+    lambda c, fill_value, shape: c.Broadcast(fill_value, shape)
+ad.deflinear(full_p, lambda t, shape: [_reduce_sum(t, tuple(range(len(shape))))])
+batching.primitive_batchers[full_p] = full_batch_rule
+
+
 ### util
 
 def _ndim(x):
@@ -2350,13 +2437,19 @@ def _dynamic_slice_indices(operand, start_indices):
   return rem(start_indices, onp.array(operand.shape, start_indices.dtype))
 
 
-_const = lambda example, val: onp.array(val, _dtype(example))
-_zeros = partial(full_like, fill_value=0)
-_zero = partial(full_like, shape=(), fill_value=0)
-_ones = partial(full_like, fill_value=1)
-_one = partial(full_like, shape=(), fill_value=1)
-_twos = partial(full_like, fill_value=2)
-_two = partial(full_like, shape=(), fill_value=2)
+def _ndarray_full_like(x, fill_value, dtype=None, shape=None):
+  return onp.broadcast_to(onp.array(fill_value, dtype or _dtype(x)),
+                          onp.shape(x) if shape is None else shape)
+
+def _const(example, val):
+  return onp.array(val, _dtype(example))
+
+_zeros = partial(_ndarray_full_like, fill_value=0)
+_zero = partial(_ndarray_full_like, shape=(), fill_value=0)
+_ones = partial(_ndarray_full_like, fill_value=1)
+_one = partial(_ndarray_full_like, shape=(), fill_value=1)
+_twos = partial(_ndarray_full_like, fill_value=2)
+_two = partial(_ndarray_full_like, shape=(), fill_value=2)
 
 _dtype = onp.result_type
 _iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,f9714152183a63e2e365aa2485f587b606801aa7,25cf9358d1e4b3165a0f9e6c094c1a711e0d113b,add tie_in and full primitives (constant creation),"diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index b9963ceb1..6d5cbb44d 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -355,6 +355,7 @@ def _ndarray_constant_handler(c, val):
     An XLA ComputationDataHandle / XlaOp representing the constant ndarray
     staged into the XLA Computation.
   """"""
+  # TODO(mattjj): revise this to use c.BroadcastInDim rather than Transpose
   if onp.any(onp.equal(0, val.strides)) and val.size > 0:
     zero_stride_axes, = onp.where(onp.equal(0, val.strides))
     other_axes, = onp.where(onp.not_equal(0, val.strides))","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index b9963ceb1..6d5cbb44d 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -355,6 +355,7 @@ def _ndarray_constant_handler(c, val):
     An XLA ComputationDataHandle / XlaOp representing the constant ndarray
     staged into the XLA Computation.
   """"""
+  # TODO(mattjj): revise this to use c.BroadcastInDim rather than Transpose
   if onp.any(onp.equal(0, val.strides)) and val.size > 0:
     zero_stride_axes, = onp.where(onp.equal(0, val.strides))
     other_axes, = onp.where(onp.not_equal(0, val.strides))",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,f9714152183a63e2e365aa2485f587b606801aa7,25cf9358d1e4b3165a0f9e6c094c1a711e0d113b,add tie_in and full primitives (constant creation),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a9d2c66b6..01809f6f3 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -794,33 +794,27 @@ asarray = array
 
 @_wraps(onp.zeros_like)
 def zeros_like(x, dtype=None):
-  return zeros(_shape(x), dtype or _dtype(x))
+  return lax.full_like(x, 0, dtype)
 
 
 @_wraps(onp.ones_like)
 def ones_like(x, dtype=None):
-  return ones(_shape(x), dtype or _dtype(x))
+  return lax.full_like(x, 1, dtype)
 
 
-@_wraps(onp.full)
-def full(shape, fill_value, dtype=None):
-  if dtype:
-    fill_value = lax.convert_element_type(fill_value, dtype)
-  return lax.broadcast(fill_value, tuple(shape))
+full = _wraps(onp.full)(lax.full)
 
 
 @_wraps(onp.zeros)
 def zeros(shape, dtype=onp.dtype(""float64"")):
   shape = (shape,) if onp.isscalar(shape) else shape
-  dtype = xla_bridge.canonicalize_dtype(dtype)
-  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))
+  return lax.full(shape, 0, dtype)
 
 
 @_wraps(onp.ones)
 def ones(shape, dtype=onp.dtype(""float64"")):
   shape = (shape,) if onp.isscalar(shape) else shape
-  dtype = xla_bridge.canonicalize_dtype(dtype)
-  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))
+  return lax.full(shape, 1, dtype)
 
 
 @_wraps(onp.repeat)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a9d2c66b6..01809f6f3 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -794,33 +794,27 @@ asarray = array
 
 @_wraps(onp.zeros_like)
 def zeros_like(x, dtype=None):
-  return zeros(_shape(x), dtype or _dtype(x))
+  return lax.full_like(x, 0, dtype)
 
 
 @_wraps(onp.ones_like)
 def ones_like(x, dtype=None):
-  return ones(_shape(x), dtype or _dtype(x))
+  return lax.full_like(x, 1, dtype)
 
 
-@_wraps(onp.full)
-def full(shape, fill_value, dtype=None):
-  if dtype:
-    fill_value = lax.convert_element_type(fill_value, dtype)
-  return lax.broadcast(fill_value, tuple(shape))
+full = _wraps(onp.full)(lax.full)
 
 
 @_wraps(onp.zeros)
 def zeros(shape, dtype=onp.dtype(""float64"")):
   shape = (shape,) if onp.isscalar(shape) else shape
-  dtype = xla_bridge.canonicalize_dtype(dtype)
-  return onp.broadcast_to(onp.zeros((), dtype), tuple(shape))
+  return lax.full(shape, 0, dtype)
 
 
 @_wraps(onp.ones)
 def ones(shape, dtype=onp.dtype(""float64"")):
   shape = (shape,) if onp.isscalar(shape) else shape
-  dtype = xla_bridge.canonicalize_dtype(dtype)
-  return onp.broadcast_to(onp.ones((), dtype), tuple(shape))
+  return lax.full(shape, 1, dtype)
 
 
 @_wraps(onp.repeat)",No
jax/lax.py,jax/lax.py,dfc25a06d996501c0d18457ad1fe83453aac6e38,f9714152183a63e2e365aa2485f587b606801aa7,add IotaConstant (untested),"diff --git a/jax/lax.py b/jax/lax.py
index 3b60cd5cf..c11b47e65 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -432,6 +432,12 @@ def full(shape, fill_value, dtype=None):
 
   return full_p.bind(fill_value, shape=shape)
 
+def iota(dtype, shape, dimension):
+  dtype = xla_bridge.canonicalize_dtype(dtype)
+  shape = tuple(map(int(shape)))
+  dimension = int(dimension)
+  return IotaConstant(dtype, shape, dimension)
+
 
 ### convenience wrappers around traceables
 
@@ -2282,13 +2288,6 @@ class FilledConstant(xla.DeviceConstant):
                        filled_const.shape)
 xla.register_device_constant(FilledConstant)
 
-# TODO(mattjj): if we used isinstance rather than handlers here, these would all
-# be covered as subclasses of DeviceArray. alternatively, just set up these in a
-# loop after we've defined all the constant DeviceConstant subclasses.
-batching.pytype_aval_mappings[FilledConstant] = make_shaped_array
-ad_util.jaxval_adders[FilledConstant] = add
-ad_util.jaxval_zeros_likers[FilledConstant] = zeros_like_array
-
 def full_batch_rule(batched_args, batch_dims, shape):
   fill_value, = batched_args
   bdim, = batch_dims
@@ -2305,6 +2304,40 @@ ad.deflinear(full_p, lambda t, shape: [_reduce_sum(t, tuple(range(len(shape))))]
 batching.primitive_batchers[full_p] = full_batch_rule
 
 
+class IotaConstant(xla.DeviceConstant):
+  __slots__ = [""axis""]
+
+  def __init__(self, dtype, shape, axis):
+    self.shape = shape
+    self.dtype = onp.dtype(dtype)
+    self.ndim = len(shape)
+    self.size = prod(shape)
+    self._npy_value = None
+
+    self.axis = axis
+
+  @property
+  def _value(self):
+    if self._npy_value is None:
+      iota = onp.arange(self.shape[self.axis], dtype=self.dtype)
+      iota = iota.reshape([self.shape[self.axis] if i == self.axis else 1
+                           for i in range(self.ndim)])
+      self._npy_value = onp.broadcast_to(iota, self.shape)
+    return self._npy_value
+
+  @staticmethod
+  def constant_handler(c, iota_constant):
+    return c.BroadcastedIota(iota_constant.dtype, iota_constant.shape,
+                             iota_constant.axis)
+xla.register_device_constant(IotaConstant)
+
+
+for t in [FilledConstant, IotaConstant]:
+  batching.pytype_aval_mappings[t] = make_shaped_array
+  ad_util.jaxval_adders[t] = add
+  ad_util.jaxval_zeros_likers[t] = zeros_like_array
+
+
 ### util
 
 def _ndim(x):
@@ -2437,19 +2470,15 @@ def _dynamic_slice_indices(operand, start_indices):
   return rem(start_indices, onp.array(operand.shape, start_indices.dtype))
 
 
-def _ndarray_full_like(x, fill_value, dtype=None, shape=None):
-  return onp.broadcast_to(onp.array(fill_value, dtype or _dtype(x)),
-                          onp.shape(x) if shape is None else shape)
-
 def _const(example, val):
   return onp.array(val, _dtype(example))
 
-_zeros = partial(_ndarray_full_like, fill_value=0)
-_zero = partial(_ndarray_full_like, shape=(), fill_value=0)
-_ones = partial(_ndarray_full_like, fill_value=1)
-_one = partial(_ndarray_full_like, shape=(), fill_value=1)
-_twos = partial(_ndarray_full_like, fill_value=2)
-_two = partial(_ndarray_full_like, shape=(), fill_value=2)
+_zeros = partial(full_like, fill_value=0)
+_zero = partial(full_like, shape=(), fill_value=0)
+_ones = partial(full_like, fill_value=1)
+_one = partial(full_like, shape=(), fill_value=1)
+_twos = partial(full_like, fill_value=2)
+_two = partial(full_like, shape=(), fill_value=2)
 
 _dtype = onp.result_type
 _iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)","diff --git a/jax/lax.py b/jax/lax.py
index 3b60cd5cf..c11b47e65 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -432,6 +432,12 @@ def full(shape, fill_value, dtype=None):
 
   return full_p.bind(fill_value, shape=shape)
 
+def iota(dtype, shape, dimension):
+  dtype = xla_bridge.canonicalize_dtype(dtype)
+  shape = tuple(map(int(shape)))
+  dimension = int(dimension)
+  return IotaConstant(dtype, shape, dimension)
+
 
 ### convenience wrappers around traceables
 
@@ -2282,13 +2288,6 @@ class FilledConstant(xla.DeviceConstant):
                        filled_const.shape)
 xla.register_device_constant(FilledConstant)
 
-# TODO(mattjj): if we used isinstance rather than handlers here, these would all
-# be covered as subclasses of DeviceArray. alternatively, just set up these in a
-# loop after we've defined all the constant DeviceConstant subclasses.
-batching.pytype_aval_mappings[FilledConstant] = make_shaped_array
-ad_util.jaxval_adders[FilledConstant] = add
-ad_util.jaxval_zeros_likers[FilledConstant] = zeros_like_array
-
 def full_batch_rule(batched_args, batch_dims, shape):
   fill_value, = batched_args
   bdim, = batch_dims
@@ -2305,6 +2304,40 @@ ad.deflinear(full_p, lambda t, shape: [_reduce_sum(t, tuple(range(len(shape))))]
 batching.primitive_batchers[full_p] = full_batch_rule
 
 
+class IotaConstant(xla.DeviceConstant):
+  __slots__ = [""axis""]
+
+  def __init__(self, dtype, shape, axis):
+    self.shape = shape
+    self.dtype = onp.dtype(dtype)
+    self.ndim = len(shape)
+    self.size = prod(shape)
+    self._npy_value = None
+
+    self.axis = axis
+
+  @property
+  def _value(self):
+    if self._npy_value is None:
+      iota = onp.arange(self.shape[self.axis], dtype=self.dtype)
+      iota = iota.reshape([self.shape[self.axis] if i == self.axis else 1
+                           for i in range(self.ndim)])
+      self._npy_value = onp.broadcast_to(iota, self.shape)
+    return self._npy_value
+
+  @staticmethod
+  def constant_handler(c, iota_constant):
+    return c.BroadcastedIota(iota_constant.dtype, iota_constant.shape,
+                             iota_constant.axis)
+xla.register_device_constant(IotaConstant)
+
+
+for t in [FilledConstant, IotaConstant]:
+  batching.pytype_aval_mappings[t] = make_shaped_array
+  ad_util.jaxval_adders[t] = add
+  ad_util.jaxval_zeros_likers[t] = zeros_like_array
+
+
 ### util
 
 def _ndim(x):
@@ -2437,19 +2470,15 @@ def _dynamic_slice_indices(operand, start_indices):
   return rem(start_indices, onp.array(operand.shape, start_indices.dtype))
 
 
-def _ndarray_full_like(x, fill_value, dtype=None, shape=None):
-  return onp.broadcast_to(onp.array(fill_value, dtype or _dtype(x)),
-                          onp.shape(x) if shape is None else shape)
-
 def _const(example, val):
   return onp.array(val, _dtype(example))
 
-_zeros = partial(_ndarray_full_like, fill_value=0)
-_zero = partial(_ndarray_full_like, shape=(), fill_value=0)
-_ones = partial(_ndarray_full_like, fill_value=1)
-_one = partial(_ndarray_full_like, shape=(), fill_value=1)
-_twos = partial(_ndarray_full_like, fill_value=2)
-_two = partial(_ndarray_full_like, shape=(), fill_value=2)
+_zeros = partial(full_like, fill_value=0)
+_zero = partial(full_like, shape=(), fill_value=0)
+_ones = partial(full_like, fill_value=1)
+_one = partial(full_like, shape=(), fill_value=1)
+_twos = partial(full_like, fill_value=2)
+_two = partial(full_like, shape=(), fill_value=2)
 
 _dtype = onp.result_type
 _iscomplex = lambda x: onp.issubdtype(_dtype(x), onp.complexfloating)",No
jax/abstract_arrays.py,jax/abstract_arrays.py,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,dfc25a06d996501c0d18457ad1fe83453aac6e38,"add EyeConstant, new np.eye and np.array code","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index a23e53150..caeb5d0a7 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -27,9 +27,11 @@ from .lib import xla_bridge
 
 def concretization_err_msg(fun):
   fname = getattr(fun, ""__name__"", fun)
-  return (""Abstract value passed to function {} that requires a concrete value. ""
-          ""Possibly tracing Python control flow using abstract values. ""
-          ""If so, try using lax.cond or lax.while instead."").format(fname)
+  msg = (""Abstract value passed to `{}`, which requires a concrete value. ""
+         ""The function to be transformed can't be traced at the required level ""
+         ""of abstraction. If using `jit`, try using `static_argnums` or ""
+         ""applying `jit` to smaller subfunctions instead."")
+  return msg.format(fname)
 
 def concretization_function_error(fun):
   def error(self, *args):","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index a23e53150..caeb5d0a7 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -27,9 +27,11 @@ from .lib import xla_bridge
 
 def concretization_err_msg(fun):
   fname = getattr(fun, ""__name__"", fun)
-  return (""Abstract value passed to function {} that requires a concrete value. ""
-          ""Possibly tracing Python control flow using abstract values. ""
-          ""If so, try using lax.cond or lax.while instead."").format(fname)
+  msg = (""Abstract value passed to `{}`, which requires a concrete value. ""
+         ""The function to be transformed can't be traced at the required level ""
+         ""of abstraction. If using `jit`, try using `static_argnums` or ""
+         ""applying `jit` to smaller subfunctions instead."")
+  return msg.format(fname)
 
 def concretization_function_error(fun):
   def error(self, *args):",No
jax/interpreters/xla.py,jax/interpreters/xla.py,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,dfc25a06d996501c0d18457ad1fe83453aac6e38,"add EyeConstant, new np.eye and np.array code","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 81e7fde6c..5fc06f9e2 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -263,7 +263,7 @@ class DeviceArray(DeviceValue):
 
   def __repr__(self):
     shape_str = "","".join(map(str, self.shape))
-    return ""DeviceArray{{{}[{}]}}"".format(self.dtype.name, shape_str)
+    return ""DeviceArray{{{}[{}]}}"".format(onp.dtype(self.dtype).name, shape_str)
 
   def __len__(self):
     try:","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 81e7fde6c..5fc06f9e2 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -263,7 +263,7 @@ class DeviceArray(DeviceValue):
 
   def __repr__(self):
     shape_str = "","".join(map(str, self.shape))
-    return ""DeviceArray{{{}[{}]}}"".format(self.dtype.name, shape_str)
+    return ""DeviceArray{{{}[{}]}}"".format(onp.dtype(self.dtype).name, shape_str)
 
   def __len__(self):
     try:",No
jax/lax.py,jax/lax.py,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,dfc25a06d996501c0d18457ad1fe83453aac6e38,"add EyeConstant, new np.eye and np.array code","diff --git a/jax/lax.py b/jax/lax.py
index c11b47e65..f65feb086 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -432,12 +432,26 @@ def full(shape, fill_value, dtype=None):
 
   return full_p.bind(fill_value, shape=shape)
 
-def iota(dtype, shape, dimension):
+def iota(dtype, size):
+  return broadcasted_iota(dtype, (size,), 0)
+
+def broadcasted_iota(dtype, shape, dimension):
   dtype = xla_bridge.canonicalize_dtype(dtype)
-  shape = tuple(map(int(shape)))
+  shape = tuple(map(int, shape))
   dimension = int(dimension)
   return IotaConstant(dtype, shape, dimension)
 
+def eye(dtype, size):
+  return broadcasted_eye(dtype, (size, size), (0, 1))
+
+def broadcasted_eye(dtype, shape, axes):
+  if not isinstance(axes, (list, tuple)) or not len(axes) >= 2:
+    raise TypeError(""make_diagonal `axes` must be a tuple with len at least 2."")
+  dtype = xla_bridge.canonicalize_dtype(dtype)
+  shape = tuple(map(int, shape))
+  axes = tuple(map(int, axes))
+  return EyeConstant(shape, axes, dtype)
+
 
 ### convenience wrappers around traceables
 
@@ -2309,7 +2323,7 @@ class IotaConstant(xla.DeviceConstant):
 
   def __init__(self, dtype, shape, axis):
     self.shape = shape
-    self.dtype = onp.dtype(dtype)
+    self.dtype = dtype
     self.ndim = len(shape)
     self.size = prod(shape)
     self._npy_value = None
@@ -2332,7 +2346,38 @@ class IotaConstant(xla.DeviceConstant):
 xla.register_device_constant(IotaConstant)
 
 
-for t in [FilledConstant, IotaConstant]:
+class EyeConstant(xla.DeviceConstant):
+  __slots__ = [""axes""]
+
+  def __init__(self, shape, axes, dtype):
+    self.shape = shape
+    self.dtype = dtype
+    self.ndim = len(shape)
+    self.size = prod(shape)
+    self._npy_value = None
+
+    self.axes = axes
+
+  @property
+  def _value(self):
+    if self._npy_value is None:
+      ones = [1] * self.ndim
+      iotas = [onp.arange(self.shape[axis]).reshape(subvals(ones, [(axis, -1)]))
+               for axis in self.axes]
+      result = onp.asarray(_reduce(operator.eq, iotas), self.dtype)
+      self._npy_value = onp.broadcast_to(result, self.shape)
+    return self._npy_value
+
+  @staticmethod
+  def constant_handler(c, diag_const):
+    etype = xla_bridge.dtype_to_etype(diag_const.dtype)
+    iotas = [c.BroadcastedIota(diag_const.dtype, diag_const.shape, axis)
+             for axis in diag_const.axes]
+    return c.ConvertElementType(_reduce(c.Eq, iotas), etype)
+xla.register_device_constant(EyeConstant)
+
+
+for t in [FilledConstant, IotaConstant, EyeConstant]:
   batching.pytype_aval_mappings[t] = make_shaped_array
   ad_util.jaxval_adders[t] = add
   ad_util.jaxval_zeros_likers[t] = zeros_like_array","diff --git a/jax/lax.py b/jax/lax.py
index c11b47e65..f65feb086 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -432,12 +432,26 @@ def full(shape, fill_value, dtype=None):
 
   return full_p.bind(fill_value, shape=shape)
 
-def iota(dtype, shape, dimension):
+def iota(dtype, size):
+  return broadcasted_iota(dtype, (size,), 0)
+
+def broadcasted_iota(dtype, shape, dimension):
   dtype = xla_bridge.canonicalize_dtype(dtype)
-  shape = tuple(map(int(shape)))
+  shape = tuple(map(int, shape))
   dimension = int(dimension)
   return IotaConstant(dtype, shape, dimension)
 
+def eye(dtype, size):
+  return broadcasted_eye(dtype, (size, size), (0, 1))
+
+def broadcasted_eye(dtype, shape, axes):
+  if not isinstance(axes, (list, tuple)) or not len(axes) >= 2:
+    raise TypeError(""make_diagonal `axes` must be a tuple with len at least 2."")
+  dtype = xla_bridge.canonicalize_dtype(dtype)
+  shape = tuple(map(int, shape))
+  axes = tuple(map(int, axes))
+  return EyeConstant(shape, axes, dtype)
+
 
 ### convenience wrappers around traceables
 
@@ -2309,7 +2323,7 @@ class IotaConstant(xla.DeviceConstant):
 
   def __init__(self, dtype, shape, axis):
     self.shape = shape
-    self.dtype = onp.dtype(dtype)
+    self.dtype = dtype
     self.ndim = len(shape)
     self.size = prod(shape)
     self._npy_value = None
@@ -2332,7 +2346,38 @@ class IotaConstant(xla.DeviceConstant):
 xla.register_device_constant(IotaConstant)
 
 
-for t in [FilledConstant, IotaConstant]:
+class EyeConstant(xla.DeviceConstant):
+  __slots__ = [""axes""]
+
+  def __init__(self, shape, axes, dtype):
+    self.shape = shape
+    self.dtype = dtype
+    self.ndim = len(shape)
+    self.size = prod(shape)
+    self._npy_value = None
+
+    self.axes = axes
+
+  @property
+  def _value(self):
+    if self._npy_value is None:
+      ones = [1] * self.ndim
+      iotas = [onp.arange(self.shape[axis]).reshape(subvals(ones, [(axis, -1)]))
+               for axis in self.axes]
+      result = onp.asarray(_reduce(operator.eq, iotas), self.dtype)
+      self._npy_value = onp.broadcast_to(result, self.shape)
+    return self._npy_value
+
+  @staticmethod
+  def constant_handler(c, diag_const):
+    etype = xla_bridge.dtype_to_etype(diag_const.dtype)
+    iotas = [c.BroadcastedIota(diag_const.dtype, diag_const.shape, axis)
+             for axis in diag_const.axes]
+    return c.ConvertElementType(_reduce(c.Eq, iotas), etype)
+xla.register_device_constant(EyeConstant)
+
+
+for t in [FilledConstant, IotaConstant, EyeConstant]:
   batching.pytype_aval_mappings[t] = make_shaped_array
   ad_util.jaxval_adders[t] = add
   ad_util.jaxval_zeros_likers[t] = zeros_like_array",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,dfc25a06d996501c0d18457ad1fe83453aac6e38,"add EyeConstant, new np.eye and np.array code","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 01809f6f3..939bb8eeb 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -697,10 +697,6 @@ def allclose(a, b, rtol=1e-05, atol=1e-08):
 ### Array-creation functions
 
 
-arange = onp.arange
-eye = onp.eye
-
-
 @_wraps(onp.stack)
 def stack(arrays):
   if not arrays:
@@ -817,6 +813,48 @@ def ones(shape, dtype=onp.dtype(""float64"")):
   return lax.full(shape, 1, dtype)
 
 
+@_wraps(onp.eye)
+def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
+  M = N if M is None else M
+  if k is None:
+    return lax.broadcasted_eye(dtype, (N, M), (0, 1))
+  else:
+    k_dtype = _dtype(k)
+    if not onp.issubdtype(k_dtype, onp.integer):
+      msg = ""eye argument `k` must be of integer dtype, got {}""
+      raise TypeError(msg.format(k_dtype))
+    rows = k + lax.broadcasted_iota(k_dtype, (N, M), 0)
+    cols = lax.broadcasted_iota(k_dtype, (N, M), 1)
+    return lax.convert_element_type(lax.eq(rows, cols), dtype)
+
+
+@_wraps(onp.arange)
+def arange(*args, **kwargs):
+  nargs = len(args)
+  start, step = 0, 1
+  dtype = kwargs.pop(""dtype"", None)
+  if kwargs:
+    raise TypeError(""arange only accepts 'dtype' kwarg, got {}"".format(kwargs))
+  if nargs == 0:
+    raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
+  elif nargs == 1:
+    stop, = args
+    dtype = dtype or _dtype(stop)
+    return lax.iota(dtype, stop)  # avoids materializing
+  elif nargs == 2:
+    start, stop = args
+    dtype = dtype or onp.result_type(start, stop)
+  elif nargs == 3:
+    start, stop, step = args
+    dtype = dtype or onp.result_type(start, stop, step)
+  elif nargs == 4:
+    start, stop, step, dtype = args
+    dtype = dtype or onp.result_type(start, stop, step)
+
+  size = (stop - start - 1) // step + 1
+  return start + step * lax.iota(dtype, size)
+
+
 @_wraps(onp.repeat)
 def repeat(a, repeats, axis=None):
   if not isscalar(repeats):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 01809f6f3..939bb8eeb 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -697,10 +697,6 @@ def allclose(a, b, rtol=1e-05, atol=1e-08):
 ### Array-creation functions
 
 
-arange = onp.arange
-eye = onp.eye
-
-
 @_wraps(onp.stack)
 def stack(arrays):
   if not arrays:
@@ -817,6 +813,48 @@ def ones(shape, dtype=onp.dtype(""float64"")):
   return lax.full(shape, 1, dtype)
 
 
+@_wraps(onp.eye)
+def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
+  M = N if M is None else M
+  if k is None:
+    return lax.broadcasted_eye(dtype, (N, M), (0, 1))
+  else:
+    k_dtype = _dtype(k)
+    if not onp.issubdtype(k_dtype, onp.integer):
+      msg = ""eye argument `k` must be of integer dtype, got {}""
+      raise TypeError(msg.format(k_dtype))
+    rows = k + lax.broadcasted_iota(k_dtype, (N, M), 0)
+    cols = lax.broadcasted_iota(k_dtype, (N, M), 1)
+    return lax.convert_element_type(lax.eq(rows, cols), dtype)
+
+
+@_wraps(onp.arange)
+def arange(*args, **kwargs):
+  nargs = len(args)
+  start, step = 0, 1
+  dtype = kwargs.pop(""dtype"", None)
+  if kwargs:
+    raise TypeError(""arange only accepts 'dtype' kwarg, got {}"".format(kwargs))
+  if nargs == 0:
+    raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
+  elif nargs == 1:
+    stop, = args
+    dtype = dtype or _dtype(stop)
+    return lax.iota(dtype, stop)  # avoids materializing
+  elif nargs == 2:
+    start, stop = args
+    dtype = dtype or onp.result_type(start, stop)
+  elif nargs == 3:
+    start, stop, step = args
+    dtype = dtype or onp.result_type(start, stop, step)
+  elif nargs == 4:
+    start, stop, step, dtype = args
+    dtype = dtype or onp.result_type(start, stop, step)
+
+  size = (stop - start - 1) // step + 1
+  return start + step * lax.iota(dtype, size)
+
+
 @_wraps(onp.repeat)
 def repeat(a, repeats, axis=None):
   if not isscalar(repeats):",No
jax/random.py,jax/random.py,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,dfc25a06d996501c0d18457ad1fe83453aac6e38,"add EyeConstant, new np.eye and np.array code","diff --git a/jax/random.py b/jax/random.py
index 0a9e73768..c823a03de 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -168,7 +168,7 @@ def split(key, num=2):
   Returns:
     A tuple of length `num` of new PRNGKey instances.
   """"""
-  counts = onp.arange(num * 2, dtype=onp.uint32)
+  counts = lax.iota(onp.uint32, num * 2)
   bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
   keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
   return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)
@@ -183,7 +183,7 @@ def _random_bits(key, bit_width, shape):
     # TODO(mattjj): just split the key here
     raise TypeError(""requesting more random bits than a single call provides."")
 
-  bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
+  bits = threefry_2x32(key.keypair, lax.iota(onp.uint32, max_count))
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
     bits = (bits[0] << onp.uint64(32)) | bits[1]","diff --git a/jax/random.py b/jax/random.py
index 0a9e73768..c823a03de 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -168,7 +168,7 @@ def split(key, num=2):
   Returns:
     A tuple of length `num` of new PRNGKey instances.
   """"""
-  counts = onp.arange(num * 2, dtype=onp.uint32)
+  counts = lax.iota(onp.uint32, num * 2)
   bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
   keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
   return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)
@@ -183,7 +183,7 @@ def _random_bits(key, bit_width, shape):
     # TODO(mattjj): just split the key here
     raise TypeError(""requesting more random bits than a single call provides."")
 
-  bits = threefry_2x32(key.keypair, onp.arange(max_count, dtype=onp.uint32))
+  bits = threefry_2x32(key.keypair, lax.iota(onp.uint32, max_count))
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
     bits = (bits[0] << onp.uint64(32)) | bits[1]",No
jax/interpreters/xla.py,jax/interpreters/xla.py,bdc9e92f9491307ba1bfa0be74ae9be1cd742f2f,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,remove full_p to improve power:weight,"diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 5fc06f9e2..0449c1bf1 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -340,12 +340,6 @@ def instantiate_device_constant(const, cutoff=1000000):
   else:
     return xb.device_put(onp.asarray(const))
 
-def register_device_constant(cls):
-  pytype_aval_mappings[cls] = pytype_aval_mappings[DeviceArray]
-  canonicalize_dtype_handlers[cls] = identity
-  core.pytype_aval_mappings[cls] = ConcreteArray
-  xb.register_constant_handler(cls, cls.constant_handler)
-
 
 def xla_shape(x):
   try:","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 5fc06f9e2..0449c1bf1 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -340,12 +340,6 @@ def instantiate_device_constant(const, cutoff=1000000):
   else:
     return xb.device_put(onp.asarray(const))
 
-def register_device_constant(cls):
-  pytype_aval_mappings[cls] = pytype_aval_mappings[DeviceArray]
-  canonicalize_dtype_handlers[cls] = identity
-  core.pytype_aval_mappings[cls] = ConcreteArray
-  xb.register_constant_handler(cls, cls.constant_handler)
-
 
 def xla_shape(x):
   try:",No
jax/lax.py,jax/lax.py,bdc9e92f9491307ba1bfa0be74ae9be1cd742f2f,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,remove full_p to improve power:weight,"diff --git a/jax/lax.py b/jax/lax.py
index f65feb086..175c12928 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -422,15 +422,15 @@ def full(shape, fill_value, dtype=None):
     msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
     raise TypeError(msg.format(onp.shape(fill_value)))
 
+  # For constants (defined as Python scalars, raw ndarrays, or DeviceValues),
+  # create a FilledConstant value, otherwise just call broadcast.
   dtype = dtype and xla_bridge.canonicalize_dtype(dtype)
-  if dtype is not None and _dtype(fill_value) != dtype:
-    # for Python scalars and raw ndarrays, we keep fill_value as a cpu ndarray
-    if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
-      fill_value = onp.array(fill_value, dtype)
-    else:
-      fill_value = convert_element_type(fill_value, dtype)
-
-  return full_p.bind(fill_value, shape=shape)
+  if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
+    return FilledConstant(onp.asarray(fill_value, dtype), shape)
+  elif isinstance(fill_value, xla.DeviceValue):
+    return FilledConstant(convert_element_type(fill_value, dtype), shape)
+  else:
+    return broadcast(convert_element_type(fill_value, dtype), shape)
 
 def iota(dtype, size):
   return broadcasted_iota(dtype, (size,), 0)
@@ -2260,7 +2260,7 @@ while_p.def_abstract_eval(while_loop_abstract_eval)
 xla.translations[while_p] = while_loop_translation_rule
 
 
-### primitives for handling constants
+### constants
 
 
 def tie_in_transpose_rule(t):
@@ -2300,22 +2300,6 @@ class FilledConstant(xla.DeviceConstant):
   def constant_handler(c, filled_const):
     return c.Broadcast(c.NumpyArrayConstant(filled_const.fill_value),
                        filled_const.shape)
-xla.register_device_constant(FilledConstant)
-
-def full_batch_rule(batched_args, batch_dims, shape):
-  fill_value, = batched_args
-  bdim, = batch_dims
-  assert bdim == 0
-  return broadcast_in_dim(fill_value, fill_value.shape + shape, [bdim])
-
-full_p = Primitive('full_p')
-full_p.def_impl(FilledConstant)
-full_p.def_abstract_eval(
-    lambda fill_value, shape: ShapedArray(shape, _dtype(fill_value)))
-xla.translations[full_p] = \
-    lambda c, fill_value, shape: c.Broadcast(fill_value, shape)
-ad.deflinear(full_p, lambda t, shape: [_reduce_sum(t, tuple(range(len(shape))))])
-batching.primitive_batchers[full_p] = full_batch_rule
 
 
 class IotaConstant(xla.DeviceConstant):
@@ -2343,7 +2327,6 @@ class IotaConstant(xla.DeviceConstant):
   def constant_handler(c, iota_constant):
     return c.BroadcastedIota(iota_constant.dtype, iota_constant.shape,
                              iota_constant.axis)
-xla.register_device_constant(IotaConstant)
 
 
 class EyeConstant(xla.DeviceConstant):
@@ -2374,10 +2357,13 @@ class EyeConstant(xla.DeviceConstant):
     iotas = [c.BroadcastedIota(diag_const.dtype, diag_const.shape, axis)
              for axis in diag_const.axes]
     return c.ConvertElementType(_reduce(c.Eq, iotas), etype)
-xla.register_device_constant(EyeConstant)
 
 
 for t in [FilledConstant, IotaConstant, EyeConstant]:
+  xla_bridge.register_constant_handler(t, t.constant_handler)
+  core.pytype_aval_mappings[t] = ConcreteArray
+  xla.pytype_aval_mappings[t] = xla.pytype_aval_mappings[xla.DeviceArray]
+  xla.canonicalize_dtype_handlers[t] = identity
   batching.pytype_aval_mappings[t] = make_shaped_array
   ad_util.jaxval_adders[t] = add
   ad_util.jaxval_zeros_likers[t] = zeros_like_array","diff --git a/jax/lax.py b/jax/lax.py
index f65feb086..175c12928 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -422,15 +422,15 @@ def full(shape, fill_value, dtype=None):
     msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
     raise TypeError(msg.format(onp.shape(fill_value)))
 
+  # For constants (defined as Python scalars, raw ndarrays, or DeviceValues),
+  # create a FilledConstant value, otherwise just call broadcast.
   dtype = dtype and xla_bridge.canonicalize_dtype(dtype)
-  if dtype is not None and _dtype(fill_value) != dtype:
-    # for Python scalars and raw ndarrays, we keep fill_value as a cpu ndarray
-    if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
-      fill_value = onp.array(fill_value, dtype)
-    else:
-      fill_value = convert_element_type(fill_value, dtype)
-
-  return full_p.bind(fill_value, shape=shape)
+  if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
+    return FilledConstant(onp.asarray(fill_value, dtype), shape)
+  elif isinstance(fill_value, xla.DeviceValue):
+    return FilledConstant(convert_element_type(fill_value, dtype), shape)
+  else:
+    return broadcast(convert_element_type(fill_value, dtype), shape)
 
 def iota(dtype, size):
   return broadcasted_iota(dtype, (size,), 0)
@@ -2260,7 +2260,7 @@ while_p.def_abstract_eval(while_loop_abstract_eval)
 xla.translations[while_p] = while_loop_translation_rule
 
 
-### primitives for handling constants
+### constants
 
 
 def tie_in_transpose_rule(t):
@@ -2300,22 +2300,6 @@ class FilledConstant(xla.DeviceConstant):
   def constant_handler(c, filled_const):
     return c.Broadcast(c.NumpyArrayConstant(filled_const.fill_value),
                        filled_const.shape)
-xla.register_device_constant(FilledConstant)
-
-def full_batch_rule(batched_args, batch_dims, shape):
-  fill_value, = batched_args
-  bdim, = batch_dims
-  assert bdim == 0
-  return broadcast_in_dim(fill_value, fill_value.shape + shape, [bdim])
-
-full_p = Primitive('full_p')
-full_p.def_impl(FilledConstant)
-full_p.def_abstract_eval(
-    lambda fill_value, shape: ShapedArray(shape, _dtype(fill_value)))
-xla.translations[full_p] = \
-    lambda c, fill_value, shape: c.Broadcast(fill_value, shape)
-ad.deflinear(full_p, lambda t, shape: [_reduce_sum(t, tuple(range(len(shape))))])
-batching.primitive_batchers[full_p] = full_batch_rule
 
 
 class IotaConstant(xla.DeviceConstant):
@@ -2343,7 +2327,6 @@ class IotaConstant(xla.DeviceConstant):
   def constant_handler(c, iota_constant):
     return c.BroadcastedIota(iota_constant.dtype, iota_constant.shape,
                              iota_constant.axis)
-xla.register_device_constant(IotaConstant)
 
 
 class EyeConstant(xla.DeviceConstant):
@@ -2374,10 +2357,13 @@ class EyeConstant(xla.DeviceConstant):
     iotas = [c.BroadcastedIota(diag_const.dtype, diag_const.shape, axis)
              for axis in diag_const.axes]
     return c.ConvertElementType(_reduce(c.Eq, iotas), etype)
-xla.register_device_constant(EyeConstant)
 
 
 for t in [FilledConstant, IotaConstant, EyeConstant]:
+  xla_bridge.register_constant_handler(t, t.constant_handler)
+  core.pytype_aval_mappings[t] = ConcreteArray
+  xla.pytype_aval_mappings[t] = xla.pytype_aval_mappings[xla.DeviceArray]
+  xla.canonicalize_dtype_handlers[t] = identity
   batching.pytype_aval_mappings[t] = make_shaped_array
   ad_util.jaxval_adders[t] = add
   ad_util.jaxval_zeros_likers[t] = zeros_like_array",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,bdc9e92f9491307ba1bfa0be74ae9be1cd742f2f,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,remove full_p to improve power:weight,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 939bb8eeb..a180bbf92 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -798,7 +798,9 @@ def ones_like(x, dtype=None):
   return lax.full_like(x, 1, dtype)
 
 
-full = _wraps(onp.full)(lax.full)
+@_wraps(onp.full)
+def full(shape, fill_value, dtype=None):
+  return lax.full(shape, fill_value, dtype)
 
 
 @_wraps(onp.zeros)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 939bb8eeb..a180bbf92 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -798,7 +798,9 @@ def ones_like(x, dtype=None):
   return lax.full_like(x, 1, dtype)
 
 
-full = _wraps(onp.full)(lax.full)
+@_wraps(onp.full)
+def full(shape, fill_value, dtype=None):
+  return lax.full(shape, fill_value, dtype)
 
 
 @_wraps(onp.zeros)",No
tests/api_test.py,tests/api_test.py,bdc9e92f9491307ba1bfa0be74ae9be1cd742f2f,1ae1ae17a2ca4769051fb01a64b6e09391728fb3,remove full_p to improve power:weight,"diff --git a/tests/api_test.py b/tests/api_test.py
index 17c5e9a04..6566386f0 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -180,17 +180,13 @@ class APITest(jtu.JaxTestCase):
 
     assert jit(f, static_argnums=(1,))(0, 5) == 10
     jtu.check_raises_regexp(
-        lambda: jit(f)(0, 5), TypeError,
-        ""('JaxprTracer' object cannot be interpreted as an integer""
-        ""|Abstract value passed to function.*)"")
+        lambda: jit(f)(0, 5), TypeError, ""Abstract value passed to .*"")
 
   def test_casts(self):
     for castfun in [float, complex, hex, oct] + list(six.integer_types):
       f = lambda x: castfun(x)
       jtu.check_raises_regexp(
-          lambda: jit(f)(0), TypeError,
-          ""('JaxprTracer' object cannot be interpreted as an integer""
-          ""|Abstract value passed to function.*)"")
+          lambda: jit(f)(0), TypeError, ""Abstract value passed to .*"")
 
   def test_unimplemented_interpreter_rules(self):
     foo_p = Primitive('foo')","diff --git a/tests/api_test.py b/tests/api_test.py
index 17c5e9a04..6566386f0 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -180,17 +180,13 @@ class APITest(jtu.JaxTestCase):
 
     assert jit(f, static_argnums=(1,))(0, 5) == 10
     jtu.check_raises_regexp(
-        lambda: jit(f)(0, 5), TypeError,
-        ""('JaxprTracer' object cannot be interpreted as an integer""
-        ""|Abstract value passed to function.*)"")
+        lambda: jit(f)(0, 5), TypeError, ""Abstract value passed to .*"")
 
   def test_casts(self):
     for castfun in [float, complex, hex, oct] + list(six.integer_types):
       f = lambda x: castfun(x)
       jtu.check_raises_regexp(
-          lambda: jit(f)(0), TypeError,
-          ""('JaxprTracer' object cannot be interpreted as an integer""
-          ""|Abstract value passed to function.*)"")
+          lambda: jit(f)(0), TypeError, ""Abstract value passed to .*"")
 
   def test_unimplemented_interpreter_rules(self):
     foo_p = Primitive('foo')",No
jax/interpreters/xla.py,jax/interpreters/xla.py,52c6eac3de854a025866e498dc1f8cd1c2ca0afe,bdc9e92f9491307ba1bfa0be74ae9be1cd742f2f,use lax.tie_in in jax.random for better consts,"diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 0449c1bf1..56d88414e 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -328,7 +328,7 @@ class DeviceConstant(DeviceArray):
     assert False
 
 # TODO(mattjj): tune cutoff
-def instantiate_device_constant(const, cutoff=1000000):
+def instantiate_device_constant(const, cutoff=0):
   # dispatch an XLA Computation to build the constant on the device if it's
   # large, or alternatively build it on the host and transfer it if it's small
   assert isinstance(const, DeviceConstant)","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 0449c1bf1..56d88414e 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -328,7 +328,7 @@ class DeviceConstant(DeviceArray):
     assert False
 
 # TODO(mattjj): tune cutoff
-def instantiate_device_constant(const, cutoff=1000000):
+def instantiate_device_constant(const, cutoff=0):
   # dispatch an XLA Computation to build the constant on the device if it's
   # large, or alternatively build it on the host and transfer it if it's small
   assert isinstance(const, DeviceConstant)",No
jax/random.py,jax/random.py,52c6eac3de854a025866e498dc1f8cd1c2ca0afe,bdc9e92f9491307ba1bfa0be74ae9be1cd742f2f,use lax.tie_in in jax.random for better consts,"diff --git a/jax/random.py b/jax/random.py
index c823a03de..60b6ba0fc 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -27,6 +27,7 @@ from . import numpy as np
 from . import tree_util
 from .api import jit
 from jax.lib import xla_bridge
+from jax import core
 
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
@@ -51,13 +52,13 @@ class PRNGKey(object):
     else:
       k1 = convert(lax.shift_right_logical(seed, 32))
     k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
-    self.keypair = (k1, k2)
+    self.keypair = core.pack((k1, k2))
 
   @classmethod
   def from_keypair(cls, keypair):
     """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
     new = cls.__new__(cls)
-    new.keypair = tuple(keypair)
+    new.keypair = core.pack(keypair)
     return new
 
 
@@ -100,7 +101,7 @@ def threefry_2x32(keypair, count):
     An array of dtype uint32 with the same shape as `count`.
   """"""
   # Based on ThreeFry2x32 by phawkins@ in //.../xla/client/lib/prng.cc
-  key1, key2 = keypair[0], keypair[1]
+  key1, key2 = keypair
   if not lax._dtype(key1) == lax._dtype(key2) == lax._dtype(count) == onp.uint32:
     msg = ""threefry_2x32 requires uint32 arguments, got {}""
     raise TypeError(msg.format([lax._dtype(x) for x in [key1, key2, count]]))
@@ -168,7 +169,7 @@ def split(key, num=2):
   Returns:
     A tuple of length `num` of new PRNGKey instances.
   """"""
-  counts = lax.iota(onp.uint32, num * 2)
+  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, num * 2))
   bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
   keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
   return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)
@@ -183,7 +184,8 @@ def _random_bits(key, bit_width, shape):
     # TODO(mattjj): just split the key here
     raise TypeError(""requesting more random bits than a single call provides."")
 
-  bits = threefry_2x32(key.keypair, lax.iota(onp.uint32, max_count))
+  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, max_count))
+  bits = threefry_2x32(key.keypair, counts)
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
     bits = (bits[0] << onp.uint64(32)) | bits[1]","diff --git a/jax/random.py b/jax/random.py
index c823a03de..60b6ba0fc 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -27,6 +27,7 @@ from . import numpy as np
 from . import tree_util
 from .api import jit
 from jax.lib import xla_bridge
+from jax import core
 
 class PRNGKey(object):
   """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
@@ -51,13 +52,13 @@ class PRNGKey(object):
     else:
       k1 = convert(lax.shift_right_logical(seed, 32))
     k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
-    self.keypair = (k1, k2)
+    self.keypair = core.pack((k1, k2))
 
   @classmethod
   def from_keypair(cls, keypair):
     """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
     new = cls.__new__(cls)
-    new.keypair = tuple(keypair)
+    new.keypair = core.pack(keypair)
     return new
 
 
@@ -100,7 +101,7 @@ def threefry_2x32(keypair, count):
     An array of dtype uint32 with the same shape as `count`.
   """"""
   # Based on ThreeFry2x32 by phawkins@ in //.../xla/client/lib/prng.cc
-  key1, key2 = keypair[0], keypair[1]
+  key1, key2 = keypair
   if not lax._dtype(key1) == lax._dtype(key2) == lax._dtype(count) == onp.uint32:
     msg = ""threefry_2x32 requires uint32 arguments, got {}""
     raise TypeError(msg.format([lax._dtype(x) for x in [key1, key2, count]]))
@@ -168,7 +169,7 @@ def split(key, num=2):
   Returns:
     A tuple of length `num` of new PRNGKey instances.
   """"""
-  counts = lax.iota(onp.uint32, num * 2)
+  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, num * 2))
   bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
   keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
   return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)
@@ -183,7 +184,8 @@ def _random_bits(key, bit_width, shape):
     # TODO(mattjj): just split the key here
     raise TypeError(""requesting more random bits than a single call provides."")
 
-  bits = threefry_2x32(key.keypair, lax.iota(onp.uint32, max_count))
+  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, max_count))
+  bits = threefry_2x32(key.keypair, counts)
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
     bits = (bits[0] << onp.uint64(32)) | bits[1]",No
jax/lax.py,jax/lax.py,78a4240581a55f9b89b7f8765304bfb3d801f7e5,52c6eac3de854a025866e498dc1f8cd1c2ca0afe,add fix from einsum branch,"diff --git a/jax/lax.py b/jax/lax.py
index 175c12928..3674bf1fa 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2347,16 +2347,18 @@ class EyeConstant(xla.DeviceConstant):
       ones = [1] * self.ndim
       iotas = [onp.arange(self.shape[axis]).reshape(subvals(ones, [(axis, -1)]))
                for axis in self.axes]
-      result = onp.asarray(_reduce(operator.eq, iotas), self.dtype)
+      eyes = [i1 == i2 for i1, i2 in zip(iotas[:-1], iotas[1:])]
+      result = onp.asarray(_reduce(operator.and_, eyes), self.dtype)
       self._npy_value = onp.broadcast_to(result, self.shape)
     return self._npy_value
 
   @staticmethod
   def constant_handler(c, diag_const):
     etype = xla_bridge.dtype_to_etype(diag_const.dtype)
-    iotas = [c.BroadcastedIota(diag_const.dtype, diag_const.shape, axis)
+    iotas = [c.BroadcastedIota(onp.bool_, diag_const.shape, axis)
              for axis in diag_const.axes]
-    return c.ConvertElementType(_reduce(c.Eq, iotas), etype)
+    eyes = [c.Eq(i1, i2) for i1, i2 in zip(iotas[:-1], iotas[1:])]
+    return c.ConvertElementType(_reduce(c.And, eyes), etype)
 
 
 for t in [FilledConstant, IotaConstant, EyeConstant]:","diff --git a/jax/lax.py b/jax/lax.py
index 175c12928..3674bf1fa 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2347,16 +2347,18 @@ class EyeConstant(xla.DeviceConstant):
       ones = [1] * self.ndim
       iotas = [onp.arange(self.shape[axis]).reshape(subvals(ones, [(axis, -1)]))
                for axis in self.axes]
-      result = onp.asarray(_reduce(operator.eq, iotas), self.dtype)
+      eyes = [i1 == i2 for i1, i2 in zip(iotas[:-1], iotas[1:])]
+      result = onp.asarray(_reduce(operator.and_, eyes), self.dtype)
       self._npy_value = onp.broadcast_to(result, self.shape)
     return self._npy_value
 
   @staticmethod
   def constant_handler(c, diag_const):
     etype = xla_bridge.dtype_to_etype(diag_const.dtype)
-    iotas = [c.BroadcastedIota(diag_const.dtype, diag_const.shape, axis)
+    iotas = [c.BroadcastedIota(onp.bool_, diag_const.shape, axis)
              for axis in diag_const.axes]
-    return c.ConvertElementType(_reduce(c.Eq, iotas), etype)
+    eyes = [c.Eq(i1, i2) for i1, i2 in zip(iotas[:-1], iotas[1:])]
+    return c.ConvertElementType(_reduce(c.And, eyes), etype)
 
 
 for t in [FilledConstant, IotaConstant, EyeConstant]:",No
tests/api_test.py,tests/api_test.py,88bb264e07e430f5b0851c960509a6b941d3d320,78a4240581a55f9b89b7f8765304bfb3d801f7e5,fix exception check for python3,"diff --git a/tests/api_test.py b/tests/api_test.py
index 6566386f0..b8f1d72d6 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -180,13 +180,17 @@ class APITest(jtu.JaxTestCase):
 
     assert jit(f, static_argnums=(1,))(0, 5) == 10
     jtu.check_raises_regexp(
-        lambda: jit(f)(0, 5), TypeError, ""Abstract value passed to .*"")
+        lambda: jit(f)(0, 5), TypeError,
+        ""('JaxprTracer' object cannot be interpreted as an integer""
+        ""|Abstract value passed to .*)"")
 
   def test_casts(self):
     for castfun in [float, complex, hex, oct] + list(six.integer_types):
       f = lambda x: castfun(x)
       jtu.check_raises_regexp(
-          lambda: jit(f)(0), TypeError, ""Abstract value passed to .*"")
+          lambda: jit(f)(0), TypeError,
+          ""('JaxprTracer' object cannot be interpreted as an integer""
+          ""|Abstract value passed to .*)"")
 
   def test_unimplemented_interpreter_rules(self):
     foo_p = Primitive('foo')","diff --git a/tests/api_test.py b/tests/api_test.py
index 6566386f0..b8f1d72d6 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -180,13 +180,17 @@ class APITest(jtu.JaxTestCase):
 
     assert jit(f, static_argnums=(1,))(0, 5) == 10
     jtu.check_raises_regexp(
-        lambda: jit(f)(0, 5), TypeError, ""Abstract value passed to .*"")
+        lambda: jit(f)(0, 5), TypeError,
+        ""('JaxprTracer' object cannot be interpreted as an integer""
+        ""|Abstract value passed to .*)"")
 
   def test_casts(self):
     for castfun in [float, complex, hex, oct] + list(six.integer_types):
       f = lambda x: castfun(x)
       jtu.check_raises_regexp(
-          lambda: jit(f)(0), TypeError, ""Abstract value passed to .*"")
+          lambda: jit(f)(0), TypeError,
+          ""('JaxprTracer' object cannot be interpreted as an integer""
+          ""|Abstract value passed to .*)"")
 
   def test_unimplemented_interpreter_rules(self):
     foo_p = Primitive('foo')",No
WORKSPACE,WORKSPACE,d065f8630ccd40a6369e6ff81325fb9db0195f14,25cf9358d1e4b3165a0f9e6c094c1a711e0d113b,"Update XLA release to incorporate TriangularSolve fix.

Fixes #128 (requires a jaxlib rebuild)","diff --git a/WORKSPACE b/WORKSPACE
index 484331e78..9b21f2d70 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""e042c2529438c71f53d3c54bd04df2fabf1ed2491c8686d556d617021eeeff11"",
-   strip_prefix = ""tensorflow-ec3f67d9673e77a0b7078f2a988cd09141c0a741"",
+   sha256 = ""f10fbc64cc4cd5ada4ebddcdd26f8f76cd97e6aff8e25956172ee18f98a705ea"",
+   strip_prefix = ""tensorflow-787c2a684e39d9c21525c07da3e56bc68621e9b7"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/ec3f67d9673e77a0b7078f2a988cd09141c0a741.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/787c2a684e39d9c21525c07da3e56bc68621e9b7.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 484331e78..9b21f2d70 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""e042c2529438c71f53d3c54bd04df2fabf1ed2491c8686d556d617021eeeff11"",
-   strip_prefix = ""tensorflow-ec3f67d9673e77a0b7078f2a988cd09141c0a741"",
+   sha256 = ""f10fbc64cc4cd5ada4ebddcdd26f8f76cd97e6aff8e25956172ee18f98a705ea"",
+   strip_prefix = ""tensorflow-787c2a684e39d9c21525c07da3e56bc68621e9b7"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/ec3f67d9673e77a0b7078f2a988cd09141c0a741.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/787c2a684e39d9c21525c07da3e56bc68621e9b7.tar.gz"",
    ],
 )
 ",No
README.md,README.md,6865783cf9ba8c659c7e0176e3e1a71eec8fec9a,d065f8630ccd40a6369e6ff81325fb9db0195f14,fix some longstanding readme typos,"diff --git a/README.md b/README.md
index 1d948e06b..a84a1a94b 100644
--- a/README.md
+++ b/README.md
@@ -529,23 +529,25 @@ pairs can be composed in series using `stax.serial` or in parallel using
 Heres an example:
 
 ```python
+import jax.numpy as np
 from jax.experimental import stax
-from jax.experimental.stax import Conv, Dense, MaxPool, Relu, LogSoftmax
+from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax
 
-# Set up network initialization and evaluation functions
+# Use stax to set up network initialization and evaluation functions
 net_init, net_apply = stax.serial(
     Conv(32, (3, 3), padding='SAME'), Relu,
     Conv(64, (3, 3), padding='SAME'), Relu,
     MaxPool((2, 2)), Flatten,
     Dense(128), Relu,
-    Dense(10), SoftMax,
+    Dense(10), LogSoftmax,
 )
 
 # Initialize parameters, not committing to a batch shape
-in_shape = (-1, 28 * 28)
+in_shape = (-1, 28, 28, 1)
 out_shape, net_params = net_init(in_shape)
 
-# Apply network
+# Apply network to dummy inputs
+inputs = np.zeros((128, 28, 28, 1))
 predictions = net_apply(net_params, inputs)
 ```
 
@@ -563,9 +565,15 @@ Heres an example, using `jit` to compile the whole update end-to-end:
 
 ```python
 from jax.experimental import minmax
-from jax import jit
+from jax import jit, grad
 
-# Set up an optimizer
+# Define a simple squared-error loss
+def loss(params, batch):
+  inputs, targets = batch
+  predictions = net_apply(params, inputs)
+  return np.sum((predictions - targets)**2)
+
+# Use minmax to set optimizer initialization and update functions
 opt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)
 
 # Define a compiled update step
@@ -575,9 +583,13 @@ def step(i, opt_state, batch):
   g = grad(loss)(params, batch)
   return opt_update(i, g, opt_state)
 
+# Dummy input data stream
+data_generator = ((np.zeros((128, 28, 28, 1)), np.zeros((128, 10)))
+                  for _ in range(10))
+
 # Optimize parameters in a loop
 opt_state = opt_init(net_params)
-for i in range(num_steps):
+for i in range(10):
   opt_state = step(i, opt_state, next(data_generator))
 net_params = minmax.get_params(opt_state)
 ```","diff --git a/README.md b/README.md
index 1d948e06b..a84a1a94b 100644
--- a/README.md
+++ b/README.md
@@ -529,23 +529,25 @@ pairs can be composed in series using `stax.serial` or in parallel using
 Heres an example:
 
 ```python
+import jax.numpy as np
 from jax.experimental import stax
-from jax.experimental.stax import Conv, Dense, MaxPool, Relu, LogSoftmax
+from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax
 
-# Set up network initialization and evaluation functions
+# Use stax to set up network initialization and evaluation functions
 net_init, net_apply = stax.serial(
     Conv(32, (3, 3), padding='SAME'), Relu,
     Conv(64, (3, 3), padding='SAME'), Relu,
     MaxPool((2, 2)), Flatten,
     Dense(128), Relu,
-    Dense(10), SoftMax,
+    Dense(10), LogSoftmax,
 )
 
 # Initialize parameters, not committing to a batch shape
-in_shape = (-1, 28 * 28)
+in_shape = (-1, 28, 28, 1)
 out_shape, net_params = net_init(in_shape)
 
-# Apply network
+# Apply network to dummy inputs
+inputs = np.zeros((128, 28, 28, 1))
 predictions = net_apply(net_params, inputs)
 ```
 
@@ -563,9 +565,15 @@ Heres an example, using `jit` to compile the whole update end-to-end:
 
 ```python
 from jax.experimental import minmax
-from jax import jit
+from jax import jit, grad
 
-# Set up an optimizer
+# Define a simple squared-error loss
+def loss(params, batch):
+  inputs, targets = batch
+  predictions = net_apply(params, inputs)
+  return np.sum((predictions - targets)**2)
+
+# Use minmax to set optimizer initialization and update functions
 opt_init, opt_update = minmax.momentum(step_size=1e-3, mass=0.9)
 
 # Define a compiled update step
@@ -575,9 +583,13 @@ def step(i, opt_state, batch):
   g = grad(loss)(params, batch)
   return opt_update(i, g, opt_state)
 
+# Dummy input data stream
+data_generator = ((np.zeros((128, 28, 28, 1)), np.zeros((128, 10)))
+                  for _ in range(10))
+
 # Optimize parameters in a loop
 opt_state = opt_init(net_params)
-for i in range(num_steps):
+for i in range(10):
   opt_state = step(i, opt_state, next(data_generator))
 net_params = minmax.get_params(opt_state)
 ```",No
examples/mnist_vae.py,examples/mnist_vae.py,6865783cf9ba8c659c7e0176e3e1a71eec8fec9a,d065f8630ccd40a6369e6ff81325fb9db0195f14,fix some longstanding readme typos,"diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 9f0449bcc..55f12f0b8 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -111,13 +111,14 @@ if __name__ == ""__main__"":
 
   @jit
   def run_epoch(rng, opt_state):
-    def body_fun(i, rng__opt_state__images):
-      (rng, opt_state, images) = rng__opt_state__images
+    def body_fun(i, loop_carry):
+      (rng, opt_state, images) = loop_carry
       rng, elbo_rng, data_rng = random.split(rng, 3)
       batch = binarize_batch(data_rng, i, images)
       loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
       g = grad(loss)(minmax.get_params(opt_state))
-      return rng, opt_update(i, g, opt_state), images
+      loop_carry = rng, opt_update(i, g, opt_state), images
+      return loop_carry
     init_val = rng, opt_state, train_images
     _, opt_state, _ =  lax.fori_loop(0, num_batches, body_fun, init_val)
     return opt_state","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 9f0449bcc..55f12f0b8 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -111,13 +111,14 @@ if __name__ == ""__main__"":
 
   @jit
   def run_epoch(rng, opt_state):
-    def body_fun(i, rng__opt_state__images):
-      (rng, opt_state, images) = rng__opt_state__images
+    def body_fun(i, loop_carry):
+      (rng, opt_state, images) = loop_carry
       rng, elbo_rng, data_rng = random.split(rng, 3)
       batch = binarize_batch(data_rng, i, images)
       loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
       g = grad(loss)(minmax.get_params(opt_state))
-      return rng, opt_update(i, g, opt_state), images
+      loop_carry = rng, opt_update(i, g, opt_state), images
+      return loop_carry
     init_val = rng, opt_state, train_images
     _, opt_state, _ =  lax.fori_loop(0, num_batches, body_fun, init_val)
     return opt_state",No
jax/interpreters/xla.py,jax/interpreters/xla.py,20ca0bd7334343695580eeff10eaeeb0bb2f6e4f,88bb264e07e430f5b0851c960509a6b941d3d320,"add cutoff for materialize-and-xfer vs build-on-device

see https://github.com/google/jax/pull/140#issuecomment-448433620","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 56d88414e..23e32fa1d 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -327,8 +327,7 @@ class DeviceConstant(DeviceArray):
   def constant_handler(c, constant_instance):
     assert False
 
-# TODO(mattjj): tune cutoff
-def instantiate_device_constant(const, cutoff=0):
+def instantiate_device_constant(const, cutoff=1e6):
   # dispatch an XLA Computation to build the constant on the device if it's
   # large, or alternatively build it on the host and transfer it if it's small
   assert isinstance(const, DeviceConstant)","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 56d88414e..23e32fa1d 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -327,8 +327,7 @@ class DeviceConstant(DeviceArray):
   def constant_handler(c, constant_instance):
     assert False
 
-# TODO(mattjj): tune cutoff
-def instantiate_device_constant(const, cutoff=0):
+def instantiate_device_constant(const, cutoff=1e6):
   # dispatch an XLA Computation to build the constant on the device if it's
   # large, or alternatively build it on the host and transfer it if it's small
   assert isinstance(const, DeviceConstant)",No
build/setup.py,build/setup.py,6757b758a2f0bf58ea42e58ed7fb8469f34b5c15,6865783cf9ba8c659c7e0176e3e1a71eec8fec9a,bump jaxlib version for wheel building,"diff --git a/build/setup.py b/build/setup.py
index 92fc20daf..b0f12773e 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.2',
+    version='0.1.3',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index 92fc20daf..b0f12773e 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.2',
+    version='0.1.3',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
build/Dockerfile,build/Dockerfile,f277609708eb39839fc9a539ef860c768f92c091,6757b758a2f0bf58ea42e58ed7fb8469f34b5c15,add scipy and cython to wheel dockerfile,"diff --git a/build/Dockerfile b/build/Dockerfile
index e20c320b2..6b219be3f 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -6,7 +6,7 @@ RUN apt-get update && apt-get install -y --no-install-recommends \
             dh-autoreconf git curl \
             python python-pip python-dev \
             python3 python3-pip python3-dev
-RUN pip install numpy setuptools wheel && pip3 install numpy setuptools wheel
+RUN pip install numpy scipy cython setuptools wheel && pip3 install numpy scipy cython setuptools wheel
 
 RUN git clone https://github.com/nixos/patchelf /tmp/patchelf
 WORKDIR /tmp/patchelf","diff --git a/build/Dockerfile b/build/Dockerfile
index e20c320b2..6b219be3f 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -6,7 +6,7 @@ RUN apt-get update && apt-get install -y --no-install-recommends \
             dh-autoreconf git curl \
             python python-pip python-dev \
             python3 python3-pip python3-dev
-RUN pip install numpy setuptools wheel && pip3 install numpy setuptools wheel
+RUN pip install numpy scipy cython setuptools wheel && pip3 install numpy scipy cython setuptools wheel
 
 RUN git clone https://github.com/nixos/patchelf /tmp/patchelf
 WORKDIR /tmp/patchelf",No
jax/lax.py,jax/lax.py,a18e3f27ace89b8b1bfde2d0e6b563030989c40e,20ca0bd7334343695580eeff10eaeeb0bb2f6e4f,add tests for device constants,"diff --git a/jax/lax.py b/jax/lax.py
index 3674bf1fa..7409a5fca 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -417,14 +417,14 @@ opaque_param_ids = itertools.count()
 def tie_in(x, y):
   return tie_in_p.bind(x, y)
 
-def full(shape, fill_value, dtype=None):
+def full(shape, fill_value, dtype):
   if onp.shape(fill_value):
     msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
     raise TypeError(msg.format(onp.shape(fill_value)))
+  dtype = xla_bridge.canonicalize_dtype(dtype)
 
   # For constants (defined as Python scalars, raw ndarrays, or DeviceValues),
   # create a FilledConstant value, otherwise just call broadcast.
-  dtype = dtype and xla_bridge.canonicalize_dtype(dtype)
   if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
     return FilledConstant(onp.asarray(fill_value, dtype), shape)
   elif isinstance(fill_value, xla.DeviceValue):","diff --git a/jax/lax.py b/jax/lax.py
index 3674bf1fa..7409a5fca 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -417,14 +417,14 @@ opaque_param_ids = itertools.count()
 def tie_in(x, y):
   return tie_in_p.bind(x, y)
 
-def full(shape, fill_value, dtype=None):
+def full(shape, fill_value, dtype):
   if onp.shape(fill_value):
     msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
     raise TypeError(msg.format(onp.shape(fill_value)))
+  dtype = xla_bridge.canonicalize_dtype(dtype)
 
   # For constants (defined as Python scalars, raw ndarrays, or DeviceValues),
   # create a FilledConstant value, otherwise just call broadcast.
-  dtype = dtype and xla_bridge.canonicalize_dtype(dtype)
   if onp.isscalar(fill_value) or type(fill_value) is onp.ndarray:
     return FilledConstant(onp.asarray(fill_value, dtype), shape)
   elif isinstance(fill_value, xla.DeviceValue):",No
tests/lax_test.py,tests/lax_test.py,a18e3f27ace89b8b1bfde2d0e6b563030989c40e,20ca0bd7334343695580eeff10eaeeb0bb2f6e4f,add tests for device constants,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index 8ef4f77d2..866fd5794 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1346,6 +1346,76 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
 
+class DeviceConstantTest(jtu.JaxTestCase):
+  def _CheckDeviceConstant(self, make_const, expected):
+    # check casting to ndarray works
+    asarray_result = onp.asarray(make_const())
+
+    # check passing as an argument works (should hit constant handler)
+    zero = onp.array(0, expected.dtype)
+    argument_result = lax.add(zero, make_const())
+
+    # check looping into a compiled computation works
+    jit_result = api.jit(lambda x: lax.add(x, make_const()))(zero)
+
+    # ensure they're all the same
+    self.assertAllClose(asarray_result, expected, check_dtypes=True)
+    self.assertAllClose(argument_result, expected, check_dtypes=True)
+    self.assertAllClose(jit_result, expected, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_fill={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), fill_value),
+       ""shape"": shape, ""dtype"": dtype, ""fill_value"": fill_value}
+      # for dtype in itertools.chain(all_dtypes, [None])
+      for dtype in [None]
+      for shape in [(), (3,), (2, 3), (2, 3, 4)]
+      for fill_value in [0, 1, onp.pi]))
+  def testFilledConstant(self, shape, fill_value, dtype):
+    make_const = lambda: lax.full(shape, fill_value, dtype)
+    expected = onp.full(shape, fill_value, xla_bridge.canonicalize_dtype(dtype))
+    self._CheckDeviceConstant(make_const, expected)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_dim={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), dimension),
+       ""shape"": shape, ""dtype"": dtype, ""dimension"": dimension}
+      for dtype in default_dtypes
+      for shape in [(), (3,), (2, 3), (2, 3, 4)]
+      for dimension in range(len(shape))))
+  def testIotaConstant(self, dtype, shape, dimension):
+    make_const = lambda: lax.broadcasted_iota(dtype, shape, dimension)
+
+    arr = onp.arange(shape[dimension], dtype=xla_bridge.canonicalize_dtype(dtype))
+    singleton_shape = [1] * len(shape)
+    singleton_shape[dimension] = shape[dimension]
+    expected = onp.broadcast_to(arr.reshape(singleton_shape), shape)
+
+    self._CheckDeviceConstant(make_const, expected)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axes),
+       ""shape"": shape, ""dtype"": dtype, ""axes"": axes}
+      for dtype in default_dtypes
+      for shape, axes in [
+          [(2, 3), (0, 1)],
+          [(2, 3, 4), (0, 1)],
+          [(2, 3, 4), (0, 2)],
+          [(2, 3, 4), (1, 2)],
+          [(2, 3, 4), (0, 1, 2)],
+          [(2, 3, 4, 2), (0, 1, 2)],
+          [(2, 3, 4, 2), (0, 2, 3)],
+      ]))
+  def testEyeConstant(self, dtype, shape, axes):
+    make_const = lambda: lax.broadcasted_eye(dtype, shape, axes)
+
+    # don't check the asarray case, just assume it's right
+    expected = onp.asarray(make_const())
+
+    self._CheckDeviceConstant(make_const, expected)
+
+
 GradTestSpec = collections.namedtuple(
     ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])
 ","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 8ef4f77d2..866fd5794 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1346,6 +1346,76 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
 
+class DeviceConstantTest(jtu.JaxTestCase):
+  def _CheckDeviceConstant(self, make_const, expected):
+    # check casting to ndarray works
+    asarray_result = onp.asarray(make_const())
+
+    # check passing as an argument works (should hit constant handler)
+    zero = onp.array(0, expected.dtype)
+    argument_result = lax.add(zero, make_const())
+
+    # check looping into a compiled computation works
+    jit_result = api.jit(lambda x: lax.add(x, make_const()))(zero)
+
+    # ensure they're all the same
+    self.assertAllClose(asarray_result, expected, check_dtypes=True)
+    self.assertAllClose(argument_result, expected, check_dtypes=True)
+    self.assertAllClose(jit_result, expected, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_fill={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), fill_value),
+       ""shape"": shape, ""dtype"": dtype, ""fill_value"": fill_value}
+      # for dtype in itertools.chain(all_dtypes, [None])
+      for dtype in [None]
+      for shape in [(), (3,), (2, 3), (2, 3, 4)]
+      for fill_value in [0, 1, onp.pi]))
+  def testFilledConstant(self, shape, fill_value, dtype):
+    make_const = lambda: lax.full(shape, fill_value, dtype)
+    expected = onp.full(shape, fill_value, xla_bridge.canonicalize_dtype(dtype))
+    self._CheckDeviceConstant(make_const, expected)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_dim={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), dimension),
+       ""shape"": shape, ""dtype"": dtype, ""dimension"": dimension}
+      for dtype in default_dtypes
+      for shape in [(), (3,), (2, 3), (2, 3, 4)]
+      for dimension in range(len(shape))))
+  def testIotaConstant(self, dtype, shape, dimension):
+    make_const = lambda: lax.broadcasted_iota(dtype, shape, dimension)
+
+    arr = onp.arange(shape[dimension], dtype=xla_bridge.canonicalize_dtype(dtype))
+    singleton_shape = [1] * len(shape)
+    singleton_shape[dimension] = shape[dimension]
+    expected = onp.broadcast_to(arr.reshape(singleton_shape), shape)
+
+    self._CheckDeviceConstant(make_const, expected)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axes),
+       ""shape"": shape, ""dtype"": dtype, ""axes"": axes}
+      for dtype in default_dtypes
+      for shape, axes in [
+          [(2, 3), (0, 1)],
+          [(2, 3, 4), (0, 1)],
+          [(2, 3, 4), (0, 2)],
+          [(2, 3, 4), (1, 2)],
+          [(2, 3, 4), (0, 1, 2)],
+          [(2, 3, 4, 2), (0, 1, 2)],
+          [(2, 3, 4, 2), (0, 2, 3)],
+      ]))
+  def testEyeConstant(self, dtype, shape, axes):
+    make_const = lambda: lax.broadcasted_eye(dtype, shape, axes)
+
+    # don't check the asarray case, just assume it's right
+    expected = onp.asarray(make_const())
+
+    self._CheckDeviceConstant(make_const, expected)
+
+
 GradTestSpec = collections.namedtuple(
     ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])
 ",No
tests/scipy_stats_test.py,tests/scipy_stats_test.py,910672809f47da636a3f902f1bc51efff8dbfab3,a18e3f27ace89b8b1bfde2d0e6b563030989c40e,tweak scipy stats test prngs,"diff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py
index af393be71..7d7338f62 100644
--- a/tests/scipy_stats_test.py
+++ b/tests/scipy_stats_test.py
@@ -17,7 +17,6 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-import functools
 import itertools
 
 from absl.testing import absltest, parameterized
@@ -31,26 +30,18 @@ from lax_scipy_test import CombosWithReplacement, float_dtypes
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
-def genNamedParametersNArgs(n):
+def genNamedParametersNArgs(n, rng):
     return parameterized.named_parameters(
-            jtu.cases_from_list(
-              {""testcase_name"": jtu.format_test_name_suffix("""", shapes, dtypes),
-               ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
-              for shapes in CombosWithReplacement(all_shapes, n)
-              for dtypes in CombosWithReplacement(float_dtypes, n)
-              for rng in [jtu.rand_default()]
-            ))
+        jtu.cases_from_list(
+          {""testcase_name"": jtu.format_test_name_suffix("""", shapes, dtypes),
+            ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+          for shapes in CombosWithReplacement(all_shapes, n)
+          for dtypes in CombosWithReplacement(float_dtypes, n)))
 
 class LaxBackedScipyStatsTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed scipy.stats implementations""""""
 
-  beta_decorator = genNamedParametersNArgs(5)
-  expon_decorator = genNamedParametersNArgs(3)
-  laplace_decorator = genNamedParametersNArgs(3)
-  norm_decorator = genNamedParametersNArgs(3)
-  uniform_decorator = genNamedParametersNArgs(3)
-
-  @beta_decorator
+  @genNamedParametersNArgs(5, jtu.rand_positive())
   def testBetaLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.beta.logpdf
     lax_fun = lsp_stats.beta.logpdf
@@ -62,20 +53,21 @@ class LaxBackedScipyStatsTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @norm_decorator
+  @genNamedParametersNArgs(3, jtu.rand_default())
   def testNormLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.norm.logpdf
     lax_fun = lsp_stats.norm.logpdf
 
     def args_maker():
       x, loc, scale = map(rng, shapes, dtypes)
-      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      # clipping to ensure that scale is not too low
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)
       return [x, loc, scale]
 
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @expon_decorator
+  @genNamedParametersNArgs(3, jtu.rand_positive())
   def testExponLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.expon.logpdf
     lax_fun = lsp_stats.expon.logpdf
@@ -87,20 +79,21 @@ class LaxBackedScipyStatsTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @laplace_decorator
+  @genNamedParametersNArgs(3, jtu.rand_positive())
   def testLaplaceLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.laplace.logpdf
     lax_fun = lsp_stats.laplace.logpdf
 
     def args_maker():
       x, loc, scale = map(rng, shapes, dtypes)
-      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      # clipping to ensure that scale is not too low
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)
       return [x, loc, scale]
 
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @uniform_decorator
+  @genNamedParametersNArgs(3, jtu.rand_default())
   def testUniformLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.uniform.logpdf
     lax_fun = lsp_stats.uniform.logpdf","diff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py
index af393be71..7d7338f62 100644
--- a/tests/scipy_stats_test.py
+++ b/tests/scipy_stats_test.py
@@ -17,7 +17,6 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-import functools
 import itertools
 
 from absl.testing import absltest, parameterized
@@ -31,26 +30,18 @@ from lax_scipy_test import CombosWithReplacement, float_dtypes
 
 all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
-def genNamedParametersNArgs(n):
+def genNamedParametersNArgs(n, rng):
     return parameterized.named_parameters(
-            jtu.cases_from_list(
-              {""testcase_name"": jtu.format_test_name_suffix("""", shapes, dtypes),
-               ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
-              for shapes in CombosWithReplacement(all_shapes, n)
-              for dtypes in CombosWithReplacement(float_dtypes, n)
-              for rng in [jtu.rand_default()]
-            ))
+        jtu.cases_from_list(
+          {""testcase_name"": jtu.format_test_name_suffix("""", shapes, dtypes),
+            ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+          for shapes in CombosWithReplacement(all_shapes, n)
+          for dtypes in CombosWithReplacement(float_dtypes, n)))
 
 class LaxBackedScipyStatsTests(jtu.JaxTestCase):
   """"""Tests for LAX-backed scipy.stats implementations""""""
 
-  beta_decorator = genNamedParametersNArgs(5)
-  expon_decorator = genNamedParametersNArgs(3)
-  laplace_decorator = genNamedParametersNArgs(3)
-  norm_decorator = genNamedParametersNArgs(3)
-  uniform_decorator = genNamedParametersNArgs(3)
-
-  @beta_decorator
+  @genNamedParametersNArgs(5, jtu.rand_positive())
   def testBetaLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.beta.logpdf
     lax_fun = lsp_stats.beta.logpdf
@@ -62,20 +53,21 @@ class LaxBackedScipyStatsTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @norm_decorator
+  @genNamedParametersNArgs(3, jtu.rand_default())
   def testNormLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.norm.logpdf
     lax_fun = lsp_stats.norm.logpdf
 
     def args_maker():
       x, loc, scale = map(rng, shapes, dtypes)
-      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      # clipping to ensure that scale is not too low
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)
       return [x, loc, scale]
 
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @expon_decorator
+  @genNamedParametersNArgs(3, jtu.rand_positive())
   def testExponLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.expon.logpdf
     lax_fun = lsp_stats.expon.logpdf
@@ -87,20 +79,21 @@ class LaxBackedScipyStatsTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @laplace_decorator
+  @genNamedParametersNArgs(3, jtu.rand_positive())
   def testLaplaceLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.laplace.logpdf
     lax_fun = lsp_stats.laplace.logpdf
 
     def args_maker():
       x, loc, scale = map(rng, shapes, dtypes)
-      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)  # clipping to ensure that scale is not too low
+      # clipping to ensure that scale is not too low
+      scale = onp.clip(onp.abs(scale), a_min=0.1, a_max=None)
       return [x, loc, scale]
 
     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
-  @uniform_decorator
+  @genNamedParametersNArgs(3, jtu.rand_default())
   def testUniformLogPdf(self, rng, shapes, dtypes):
     scipy_fun = osp_stats.uniform.logpdf
     lax_fun = lsp_stats.uniform.logpdf",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6a71e9d6ec80093b75f11c820ed1ebcbc21b4d23,bd5c1fd130f59b307e486f0e2d19bfad028f8863,start drafting an einsum implementation,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a180bbf92..da55a0341 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -20,12 +20,15 @@ from six.moves import builtins
 
 import six
 import numpy as onp
+import opt_einsum
+import collections
+import itertools
 
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from .. import lax
-from ..util import memoize, partial, get_module_functions, prod as _prod
+from ..util import memoize, partial, get_module_functions, unzip2, prod as _prod
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -1054,6 +1057,78 @@ def tensordot(a, b, axes=2):
   raise TypeError(msg)
 
 
+def einsum(*operands):
+  operands, contractions = opt_einsum.contract_path(
+      *operands, einsum_call=True, use_blas=True)
+  for operand_indices, contracted_names, einstr, _, _ in contractions:
+    input_str, result_names = einstr.split('->')
+    input_names = input_str.split(',')
+
+    # switch on the number of operands to be processed in this loop iteration.
+    # every case here sets 'result' and 'names'.
+    if len(operand_indices) == 1:
+      operand = operands.pop(operand_indices[0])
+      names, = input_names
+      counts = collections.Counter(names)
+
+      # sum out non-repeated indices with a single reduction
+      uniques = tuple(name for name in contracted_names if counts[name] == 1)
+      if uniques:
+        axes = tuple(names.index(name) for name in uniques)
+        operand = lax.reduce(operand, onp.array(0, _dtype(operand)), lax.add, axes)
+        names = names.translate(None, ''.join(uniques))
+        map(counts.pop, uniques)
+
+      # for every repeated index, do a contraction against an identity matrix
+      for name, count in counts.items():
+        if count > 1:
+          raise NotImplementedError
+
+      result = operand
+
+    elif len(operand_indices) == 2:
+      lhs, rhs = map(operands.pop, operand_indices)
+      lhs_counts, rhs_counts = map(collections.Counter, input_names)
+      lhs_names, rhs_names = input_names
+
+      all_counts = itertools.chain(lhs_counts.values(), rhs_counts.values())
+      if _any(count > 1 for count in all_counts):
+        # TODO handle repeated indices (trace out first)
+        # TODO handle unique axes (sum out first)
+        raise NotImplementedError
+
+      batch_names = set(lhs_names) & set(rhs_names) - contracted_names
+      lhs_cont, rhs_cont = unzip2((lhs_names.index(name), rhs_names.index(name))
+                                  for name in contracted_names)
+      lhs_batch, rhs_batch = unzip2((lhs_names.find(name), rhs_names.find(name))
+                                    for name in batch_names)
+      batch_dims = tuple(range(len(batch_names)))
+      if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
+        lhs = moveaxis(lhs, lhs_batch, batch_dims)
+        rhs = moveaxis(rhs, rhs_batch, batch_dims)
+        batch_names = ''.join(batch_names)
+      else:
+        batch_dims = tuple(lhs_batch)
+        batch_names = ''.join(lhs_names[i] for i in batch_dims)
+
+      dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+      result = lax.dot_general(lhs, rhs, dimension_numbers)
+      deleted_names = batch_names + ''.join(contracted_names)
+      names = (batch_names
+               + lhs_names.translate(None, deleted_names)
+               + rhs_names.translate(None, deleted_names))
+
+    else:
+      raise NotImplementedError
+
+    if names != result_names:
+      perm = tuple([names.index(name) for name in result_names])
+      result = lax.transpose(result, perm)
+    operands.append(result)  # used in next iteration
+
+  return operands[0]
+
+
 ### Misc
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a180bbf92..da55a0341 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -20,12 +20,15 @@ from six.moves import builtins
 
 import six
 import numpy as onp
+import opt_einsum
+import collections
+import itertools
 
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
 from .. import lax
-from ..util import memoize, partial, get_module_functions, prod as _prod
+from ..util import memoize, partial, get_module_functions, unzip2, prod as _prod
 from ..lib import xla_bridge
 
 # To provide the same module-level names as Numpy, we need to redefine builtins
@@ -1054,6 +1057,78 @@ def tensordot(a, b, axes=2):
   raise TypeError(msg)
 
 
+def einsum(*operands):
+  operands, contractions = opt_einsum.contract_path(
+      *operands, einsum_call=True, use_blas=True)
+  for operand_indices, contracted_names, einstr, _, _ in contractions:
+    input_str, result_names = einstr.split('->')
+    input_names = input_str.split(',')
+
+    # switch on the number of operands to be processed in this loop iteration.
+    # every case here sets 'result' and 'names'.
+    if len(operand_indices) == 1:
+      operand = operands.pop(operand_indices[0])
+      names, = input_names
+      counts = collections.Counter(names)
+
+      # sum out non-repeated indices with a single reduction
+      uniques = tuple(name for name in contracted_names if counts[name] == 1)
+      if uniques:
+        axes = tuple(names.index(name) for name in uniques)
+        operand = lax.reduce(operand, onp.array(0, _dtype(operand)), lax.add, axes)
+        names = names.translate(None, ''.join(uniques))
+        map(counts.pop, uniques)
+
+      # for every repeated index, do a contraction against an identity matrix
+      for name, count in counts.items():
+        if count > 1:
+          raise NotImplementedError
+
+      result = operand
+
+    elif len(operand_indices) == 2:
+      lhs, rhs = map(operands.pop, operand_indices)
+      lhs_counts, rhs_counts = map(collections.Counter, input_names)
+      lhs_names, rhs_names = input_names
+
+      all_counts = itertools.chain(lhs_counts.values(), rhs_counts.values())
+      if _any(count > 1 for count in all_counts):
+        # TODO handle repeated indices (trace out first)
+        # TODO handle unique axes (sum out first)
+        raise NotImplementedError
+
+      batch_names = set(lhs_names) & set(rhs_names) - contracted_names
+      lhs_cont, rhs_cont = unzip2((lhs_names.index(name), rhs_names.index(name))
+                                  for name in contracted_names)
+      lhs_batch, rhs_batch = unzip2((lhs_names.find(name), rhs_names.find(name))
+                                    for name in batch_names)
+      batch_dims = tuple(range(len(batch_names)))
+      if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
+        lhs = moveaxis(lhs, lhs_batch, batch_dims)
+        rhs = moveaxis(rhs, rhs_batch, batch_dims)
+        batch_names = ''.join(batch_names)
+      else:
+        batch_dims = tuple(lhs_batch)
+        batch_names = ''.join(lhs_names[i] for i in batch_dims)
+
+      dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+      result = lax.dot_general(lhs, rhs, dimension_numbers)
+      deleted_names = batch_names + ''.join(contracted_names)
+      names = (batch_names
+               + lhs_names.translate(None, deleted_names)
+               + rhs_names.translate(None, deleted_names))
+
+    else:
+      raise NotImplementedError
+
+    if names != result_names:
+      perm = tuple([names.index(name) for name in result_names])
+      result = lax.transpose(result, perm)
+    operands.append(result)  # used in next iteration
+
+  return operands[0]
+
+
 ### Misc
 
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,13a0e1168e092c773aebf784c115eeef70c81db8,6a71e9d6ec80093b75f11c820ed1ebcbc21b4d23,"fix broadcasted eye bug, enable more einsum","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index da55a0341..bf49f8b64 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1060,6 +1060,8 @@ def tensordot(a, b, axes=2):
 def einsum(*operands):
   operands, contractions = opt_einsum.contract_path(
       *operands, einsum_call=True, use_blas=True)
+  sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
+
   for operand_indices, contracted_names, einstr, _, _ in contractions:
     input_str, result_names = einstr.split('->')
     input_names = input_str.split(',')
@@ -1075,14 +1077,21 @@ def einsum(*operands):
       uniques = tuple(name for name in contracted_names if counts[name] == 1)
       if uniques:
         axes = tuple(names.index(name) for name in uniques)
-        operand = lax.reduce(operand, onp.array(0, _dtype(operand)), lax.add, axes)
+        operand = sum(operand, axes)
         names = names.translate(None, ''.join(uniques))
         map(counts.pop, uniques)
 
       # for every repeated index, do a contraction against an identity matrix
       for name, count in counts.items():
         if count > 1:
-          raise NotImplementedError
+          axes = [i for i, n in enumerate(names) if n == name]
+          eye = lax.broadcasted_eye(operand.dtype, operand.shape, axes)
+          if name not in result_names:
+            operand = sum(operand * eye, axes)
+            names = names.replace(name, '')
+          else:
+            operand = sum(operand * eye, axes[:-1])
+            names = names.replace(name, '', count - 1)
 
       result = operand
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index da55a0341..bf49f8b64 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1060,6 +1060,8 @@ def tensordot(a, b, axes=2):
 def einsum(*operands):
   operands, contractions = opt_einsum.contract_path(
       *operands, einsum_call=True, use_blas=True)
+  sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
+
   for operand_indices, contracted_names, einstr, _, _ in contractions:
     input_str, result_names = einstr.split('->')
     input_names = input_str.split(',')
@@ -1075,14 +1077,21 @@ def einsum(*operands):
       uniques = tuple(name for name in contracted_names if counts[name] == 1)
       if uniques:
         axes = tuple(names.index(name) for name in uniques)
-        operand = lax.reduce(operand, onp.array(0, _dtype(operand)), lax.add, axes)
+        operand = sum(operand, axes)
         names = names.translate(None, ''.join(uniques))
         map(counts.pop, uniques)
 
       # for every repeated index, do a contraction against an identity matrix
       for name, count in counts.items():
         if count > 1:
-          raise NotImplementedError
+          axes = [i for i, n in enumerate(names) if n == name]
+          eye = lax.broadcasted_eye(operand.dtype, operand.shape, axes)
+          if name not in result_names:
+            operand = sum(operand * eye, axes)
+            names = names.replace(name, '')
+          else:
+            operand = sum(operand * eye, axes[:-1])
+            names = names.replace(name, '', count - 1)
 
       result = operand
 ",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,13a0e1168e092c773aebf784c115eeef70c81db8,6a71e9d6ec80093b75f11c820ed1ebcbc21b4d23,"fix broadcasted eye bug, enable more einsum","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 84a243dc0..ea3b65c51 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -73,25 +73,46 @@ class EinsumTest(jtu.JaxTestCase):
     s = '...ijk->ki'
     check(s, x)
 
-  # def test_one_operand_7(self):
-  #   x = rng().randn(3, 3, 3)
-  #   s = 'iii->'
-  #   check(s, x)
-
-  # def test_one_operand_8(self):
-  #   x = rng().randn(3, 3)
-  #   s = 'ii->i'
-  #   check(s, x)
-
-  # def test_one_operand_9(self):
-  #   x = rng().randn(3, 3, 4)
-  #   s = 'iij->i'
-  #   check(s, x)
-
-  # def test_one_operand_10(self):
-  #   x = rng().randn(3, 3, 3)
-  #   s = 'iii->i'
-  #   check(s, x)
+  def test_one_operand_7(self):
+    x = rng().randn(3, 3)
+    s = 'ii->'
+    check(s, x)
+
+  def test_one_operand_8(self):
+    x = rng().randn(3, 3, 3)
+    s = 'iii->'
+    check(s, x)
+
+  def test_one_operand_9(self):
+    x = rng().randn(3, 3)
+    s = 'ii->i'
+    check(s, x)
+
+  def test_one_operand_10(self):
+    x = rng().randn(3, 3, 4)
+    s = 'iij->i'
+    check(s, x)
+
+  def test_one_operand_11(self):
+    x = rng().randn(3, 3, 3)
+    s = 'iii->i'
+    check(s, x)
+
+  def test_one_operand_12(self):
+    x = rng().randn(3, 3, 5, 4, 4)
+    s = 'iijkk->i'
+    check(s, x)
+
+  def test_one_operand_13(self):
+    x = rng().randn(3, 3, 5, 4, 4)
+    s = 'iijkk->ik'
+    check(s, x)
+
+  def test_one_operand_14(self):
+    x = rng().randn(3, 3, 5, 4, 4)
+    s = 'iijkl->il'
+    check(s, x)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 84a243dc0..ea3b65c51 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -73,25 +73,46 @@ class EinsumTest(jtu.JaxTestCase):
     s = '...ijk->ki'
     check(s, x)
 
-  # def test_one_operand_7(self):
-  #   x = rng().randn(3, 3, 3)
-  #   s = 'iii->'
-  #   check(s, x)
+  def test_one_operand_7(self):
+    x = rng().randn(3, 3)
+    s = 'ii->'
+    check(s, x)
 
-  # def test_one_operand_8(self):
-  #   x = rng().randn(3, 3)
-  #   s = 'ii->i'
-  #   check(s, x)
+  def test_one_operand_8(self):
+    x = rng().randn(3, 3, 3)
+    s = 'iii->'
+    check(s, x)
 
-  # def test_one_operand_9(self):
-  #   x = rng().randn(3, 3, 4)
-  #   s = 'iij->i'
-  #   check(s, x)
+  def test_one_operand_9(self):
+    x = rng().randn(3, 3)
+    s = 'ii->i'
+    check(s, x)
+
+  def test_one_operand_10(self):
+    x = rng().randn(3, 3, 4)
+    s = 'iij->i'
+    check(s, x)
+
+  def test_one_operand_11(self):
+    x = rng().randn(3, 3, 3)
+    s = 'iii->i'
+    check(s, x)
+
+  def test_one_operand_12(self):
+    x = rng().randn(3, 3, 5, 4, 4)
+    s = 'iijkk->i'
+    check(s, x)
+
+  def test_one_operand_13(self):
+    x = rng().randn(3, 3, 5, 4, 4)
+    s = 'iijkk->ik'
+    check(s, x)
+
+  def test_one_operand_14(self):
+    x = rng().randn(3, 3, 5, 4, 4)
+    s = 'iijkl->il'
+    check(s, x)
 
-  # def test_one_operand_10(self):
-  #   x = rng().randn(3, 3, 3)
-  #   s = 'iii->i'
-  #   check(s, x)
 
 if __name__ == '__main__':
   absltest.main()",Yes
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,fdde6841e6f7f4e62817a709a49f08aee6c880cb,13a0e1168e092c773aebf784c115eeef70c81db8,add support for two-operrand cases,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index bf49f8b64..72cdc4ab5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1062,6 +1062,26 @@ def einsum(*operands):
       *operands, einsum_call=True, use_blas=True)
   sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
 
+  def sum_uniques(operand, names, uniques):
+    if uniques:
+      axes = [names.index(name) for name in uniques]
+      operand = sum(operand, axes)
+      names = names.translate(None, ''.join(uniques))
+    return operand, names
+
+  def sum_repeats(operand, names, counts, keep_names):
+    for name, count in counts.items():
+      if count > 1:
+        axes = [i for i, n in enumerate(names) if n == name]
+        eye = lax.broadcasted_eye(operand.dtype, operand.shape, axes)
+        if name not in keep_names:
+          operand = sum(operand * eye, axes)
+          names = names.replace(name, '')
+        else:
+          operand = sum(operand * eye, axes[:-1])
+          names = names.replace(name, '', count - 1)
+    return operand, names
+
   for operand_indices, contracted_names, einstr, _, _ in contractions:
     input_str, result_names = einstr.split('->')
     input_names = input_str.split(',')
@@ -1073,39 +1093,35 @@ def einsum(*operands):
       names, = input_names
       counts = collections.Counter(names)
 
-      # sum out non-repeated indices with a single reduction
-      uniques = tuple(name for name in contracted_names if counts[name] == 1)
-      if uniques:
-        axes = tuple(names.index(name) for name in uniques)
-        operand = sum(operand, axes)
-        names = names.translate(None, ''.join(uniques))
-        map(counts.pop, uniques)
+      # sum out unique contracted indices with a single reduce-sum
+      uniques = [name for name in contracted_names if counts[name] == 1]
+      operand, names = sum_uniques(operand, names, uniques)
 
       # for every repeated index, do a contraction against an identity matrix
-      for name, count in counts.items():
-        if count > 1:
-          axes = [i for i, n in enumerate(names) if n == name]
-          eye = lax.broadcasted_eye(operand.dtype, operand.shape, axes)
-          if name not in result_names:
-            operand = sum(operand * eye, axes)
-            names = names.replace(name, '')
-          else:
-            operand = sum(operand * eye, axes[:-1])
-            names = names.replace(name, '', count - 1)
-
-      result = operand
+      operand, names = sum_repeats(operand, names, counts, result_names)
 
     elif len(operand_indices) == 2:
       lhs, rhs = map(operands.pop, operand_indices)
       lhs_counts, rhs_counts = map(collections.Counter, input_names)
       lhs_names, rhs_names = input_names
 
-      all_counts = itertools.chain(lhs_counts.values(), rhs_counts.values())
-      if _any(count > 1 for count in all_counts):
-        # TODO handle repeated indices (trace out first)
-        # TODO handle unique axes (sum out first)
-        raise NotImplementedError
+      # sum out unique contracted indices in lhs and rhs
+      lhs_uniques = [name for name in contracted_names
+                     if lhs_counts[name] == 1 and rhs_counts[name] == 0]
+      lhs, lhs_names = sum_uniques(lhs, lhs_names, lhs_uniques)
+
+      rhs_uniques = [name for name in contracted_names
+                     if rhs_counts[name] == 1 and lhs_counts[name] == 0]
+      rhs, rhs_names = sum_uniques(rhs, rhs_names, rhs_uniques)
+
+      # for every repeated index, contract against an identity matrix
+      lhs, lhs_names = sum_repeats(lhs, lhs_names, lhs_counts,
+                                   result_names + rhs_names)
+      rhs, rhs_names = sum_repeats(rhs, rhs_names, rhs_counts,
+                                   result_names + lhs_names)
 
+      # contract lhs against rhs
+      contracted_names = contracted_names & (set(lhs_names) | set(rhs_names))
       batch_names = set(lhs_names) & set(rhs_names) - contracted_names
       lhs_cont, rhs_cont = unzip2((lhs_names.index(name), rhs_names.index(name))
                                   for name in contracted_names)
@@ -1120,8 +1136,9 @@ def einsum(*operands):
         batch_dims = tuple(lhs_batch)
         batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
+      # TODO need to flatten lhs_cont / rhs_cont if more than 1d
       dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
-      result = lax.dot_general(lhs, rhs, dimension_numbers)
+      operand = lax.dot_general(lhs, rhs, dimension_numbers)
       deleted_names = batch_names + ''.join(contracted_names)
       names = (batch_names
                + lhs_names.translate(None, deleted_names)
@@ -1130,10 +1147,14 @@ def einsum(*operands):
     else:
       raise NotImplementedError
 
+    # the resulting 'operand' with axis labels 'names' should be a permutation
+    # of the desired result
+    assert len(names) == len(result_names) == len(set(names))
+    assert set(names) == set(result_names)
     if names != result_names:
       perm = tuple([names.index(name) for name in result_names])
-      result = lax.transpose(result, perm)
-    operands.append(result)  # used in next iteration
+      operand = lax.transpose(operand, perm)
+    operands.append(operand)  # used in next iteration
 
   return operands[0]
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index bf49f8b64..72cdc4ab5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1062,6 +1062,26 @@ def einsum(*operands):
       *operands, einsum_call=True, use_blas=True)
   sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
 
+  def sum_uniques(operand, names, uniques):
+    if uniques:
+      axes = [names.index(name) for name in uniques]
+      operand = sum(operand, axes)
+      names = names.translate(None, ''.join(uniques))
+    return operand, names
+
+  def sum_repeats(operand, names, counts, keep_names):
+    for name, count in counts.items():
+      if count > 1:
+        axes = [i for i, n in enumerate(names) if n == name]
+        eye = lax.broadcasted_eye(operand.dtype, operand.shape, axes)
+        if name not in keep_names:
+          operand = sum(operand * eye, axes)
+          names = names.replace(name, '')
+        else:
+          operand = sum(operand * eye, axes[:-1])
+          names = names.replace(name, '', count - 1)
+    return operand, names
+
   for operand_indices, contracted_names, einstr, _, _ in contractions:
     input_str, result_names = einstr.split('->')
     input_names = input_str.split(',')
@@ -1073,39 +1093,35 @@ def einsum(*operands):
       names, = input_names
       counts = collections.Counter(names)
 
-      # sum out non-repeated indices with a single reduction
-      uniques = tuple(name for name in contracted_names if counts[name] == 1)
-      if uniques:
-        axes = tuple(names.index(name) for name in uniques)
-        operand = sum(operand, axes)
-        names = names.translate(None, ''.join(uniques))
-        map(counts.pop, uniques)
+      # sum out unique contracted indices with a single reduce-sum
+      uniques = [name for name in contracted_names if counts[name] == 1]
+      operand, names = sum_uniques(operand, names, uniques)
 
       # for every repeated index, do a contraction against an identity matrix
-      for name, count in counts.items():
-        if count > 1:
-          axes = [i for i, n in enumerate(names) if n == name]
-          eye = lax.broadcasted_eye(operand.dtype, operand.shape, axes)
-          if name not in result_names:
-            operand = sum(operand * eye, axes)
-            names = names.replace(name, '')
-          else:
-            operand = sum(operand * eye, axes[:-1])
-            names = names.replace(name, '', count - 1)
-
-      result = operand
+      operand, names = sum_repeats(operand, names, counts, result_names)
 
     elif len(operand_indices) == 2:
       lhs, rhs = map(operands.pop, operand_indices)
       lhs_counts, rhs_counts = map(collections.Counter, input_names)
       lhs_names, rhs_names = input_names
 
-      all_counts = itertools.chain(lhs_counts.values(), rhs_counts.values())
-      if _any(count > 1 for count in all_counts):
-        # TODO handle repeated indices (trace out first)
-        # TODO handle unique axes (sum out first)
-        raise NotImplementedError
+      # sum out unique contracted indices in lhs and rhs
+      lhs_uniques = [name for name in contracted_names
+                     if lhs_counts[name] == 1 and rhs_counts[name] == 0]
+      lhs, lhs_names = sum_uniques(lhs, lhs_names, lhs_uniques)
 
+      rhs_uniques = [name for name in contracted_names
+                     if rhs_counts[name] == 1 and lhs_counts[name] == 0]
+      rhs, rhs_names = sum_uniques(rhs, rhs_names, rhs_uniques)
+
+      # for every repeated index, contract against an identity matrix
+      lhs, lhs_names = sum_repeats(lhs, lhs_names, lhs_counts,
+                                   result_names + rhs_names)
+      rhs, rhs_names = sum_repeats(rhs, rhs_names, rhs_counts,
+                                   result_names + lhs_names)
+
+      # contract lhs against rhs
+      contracted_names = contracted_names & (set(lhs_names) | set(rhs_names))
       batch_names = set(lhs_names) & set(rhs_names) - contracted_names
       lhs_cont, rhs_cont = unzip2((lhs_names.index(name), rhs_names.index(name))
                                   for name in contracted_names)
@@ -1120,8 +1136,9 @@ def einsum(*operands):
         batch_dims = tuple(lhs_batch)
         batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
+      # TODO need to flatten lhs_cont / rhs_cont if more than 1d
       dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
-      result = lax.dot_general(lhs, rhs, dimension_numbers)
+      operand = lax.dot_general(lhs, rhs, dimension_numbers)
       deleted_names = batch_names + ''.join(contracted_names)
       names = (batch_names
                + lhs_names.translate(None, deleted_names)
@@ -1130,10 +1147,14 @@ def einsum(*operands):
     else:
       raise NotImplementedError
 
+    # the resulting 'operand' with axis labels 'names' should be a permutation
+    # of the desired result
+    assert len(names) == len(result_names) == len(set(names))
+    assert set(names) == set(result_names)
     if names != result_names:
       perm = tuple([names.index(name) for name in result_names])
-      result = lax.transpose(result, perm)
-    operands.append(result)  # used in next iteration
+      operand = lax.transpose(operand, perm)
+    operands.append(operand)  # used in next iteration
 
   return operands[0]
 ",Yes
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,fdde6841e6f7f4e62817a709a49f08aee6c880cb,13a0e1168e092c773aebf784c115eeef70c81db8,add support for two-operrand cases,"diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index ea3b65c51..6a9ffb82b 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -43,6 +43,18 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij,j->i'
     check(s, x, y)
 
+  def test_two_operands_2(self):
+    x = rng().randn(3, 4, 5)
+    y = rng().randn(4)
+    s = 'ijk,j->i'
+    check(s, x, y)
+
+  def test_two_operands_3(self):
+    x = rng().randn(3, 4, 3)
+    y = rng().randn(3)
+    s = 'iji,i->j'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'
@@ -79,40 +91,50 @@ class EinsumTest(jtu.JaxTestCase):
     check(s, x)
 
   def test_one_operand_8(self):
+    x = rng().randn(3, 3)
+    s = 'ij->'
+    check(s, x)
+
+  def test_one_operand_9(self):
     x = rng().randn(3, 3, 3)
     s = 'iii->'
     check(s, x)
 
-  def test_one_operand_9(self):
+  def test_one_operand_10(self):
     x = rng().randn(3, 3)
     s = 'ii->i'
     check(s, x)
 
-  def test_one_operand_10(self):
+  def test_one_operand_11(self):
     x = rng().randn(3, 3, 4)
     s = 'iij->i'
     check(s, x)
 
-  def test_one_operand_11(self):
+  def test_one_operand_12(self):
     x = rng().randn(3, 3, 3)
     s = 'iii->i'
     check(s, x)
 
-  def test_one_operand_12(self):
+  def test_one_operand_13(self):
     x = rng().randn(3, 3, 5, 4, 4)
     s = 'iijkk->i'
     check(s, x)
 
-  def test_one_operand_13(self):
+  def test_one_operand_14(self):
     x = rng().randn(3, 3, 5, 4, 4)
     s = 'iijkk->ik'
     check(s, x)
 
-  def test_one_operand_14(self):
+  def test_one_operand_15(self):
     x = rng().randn(3, 3, 5, 4, 4)
     s = 'iijkl->il'
     check(s, x)
 
+  def test_one_operand_16(self):
+    x = rng().randn(3, 3)
+    s = 'ij->ij'
+    check(s, x)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index ea3b65c51..6a9ffb82b 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -43,6 +43,18 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij,j->i'
     check(s, x, y)
 
+  def test_two_operands_2(self):
+    x = rng().randn(3, 4, 5)
+    y = rng().randn(4)
+    s = 'ijk,j->i'
+    check(s, x, y)
+
+  def test_two_operands_3(self):
+    x = rng().randn(3, 4, 3)
+    y = rng().randn(3)
+    s = 'iji,i->j'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'
@@ -79,40 +91,50 @@ class EinsumTest(jtu.JaxTestCase):
     check(s, x)
 
   def test_one_operand_8(self):
+    x = rng().randn(3, 3)
+    s = 'ij->'
+    check(s, x)
+
+  def test_one_operand_9(self):
     x = rng().randn(3, 3, 3)
     s = 'iii->'
     check(s, x)
 
-  def test_one_operand_9(self):
+  def test_one_operand_10(self):
     x = rng().randn(3, 3)
     s = 'ii->i'
     check(s, x)
 
-  def test_one_operand_10(self):
+  def test_one_operand_11(self):
     x = rng().randn(3, 3, 4)
     s = 'iij->i'
     check(s, x)
 
-  def test_one_operand_11(self):
+  def test_one_operand_12(self):
     x = rng().randn(3, 3, 3)
     s = 'iii->i'
     check(s, x)
 
-  def test_one_operand_12(self):
+  def test_one_operand_13(self):
     x = rng().randn(3, 3, 5, 4, 4)
     s = 'iijkk->i'
     check(s, x)
 
-  def test_one_operand_13(self):
+  def test_one_operand_14(self):
     x = rng().randn(3, 3, 5, 4, 4)
     s = 'iijkk->ik'
     check(s, x)
 
-  def test_one_operand_14(self):
+  def test_one_operand_15(self):
     x = rng().randn(3, 3, 5, 4, 4)
     s = 'iijkl->il'
     check(s, x)
 
+  def test_one_operand_16(self):
+    x = rng().randn(3, 3)
+    s = 'ij->ij'
+    check(s, x)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,d8388e2d80379051063aef61b9ff06752d9f31ac,fdde6841e6f7f4e62817a709a49f08aee6c880cb,complete support for two-operand einsum,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 72cdc4ab5..21f518a33 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1120,29 +1120,53 @@ def einsum(*operands):
       rhs, rhs_names = sum_repeats(rhs, rhs_names, rhs_counts,
                                    result_names + lhs_names)
 
-      # contract lhs against rhs
       contracted_names = contracted_names & (set(lhs_names) | set(rhs_names))
       batch_names = set(lhs_names) & set(rhs_names) - contracted_names
-      lhs_cont, rhs_cont = unzip2((lhs_names.index(name), rhs_names.index(name))
-                                  for name in contracted_names)
-      lhs_batch, rhs_batch = unzip2((lhs_names.find(name), rhs_names.find(name))
-                                    for name in batch_names)
-      batch_dims = tuple(range(len(batch_names)))
-      if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
-        lhs = moveaxis(lhs, lhs_batch, batch_dims)
-        rhs = moveaxis(rhs, rhs_batch, batch_dims)
-        batch_names = ''.join(batch_names)
+      lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
+                                    for n in batch_names)
+      if contracted_names:
+        # contract usint lax.dot_general
+        lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
+                                    for n in contracted_names)
+
+        # lax.dot_general batch dims have to precede non-batch dims
+        batch_dims = tuple(range(len(batch_names)))
+        if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
+          lhs = moveaxis(lhs, lhs_batch, batch_dims)
+          rhs = moveaxis(rhs, rhs_batch, batch_dims)
+          batch_names = ''.join(batch_names)
+        else:
+          batch_dims = tuple(lhs_batch)
+          batch_names = ''.join(lhs_names[i] for i in batch_dims)
+
+        # lax.dot_general only allows one contracting dimension, move it to last
+        ncont = len(contracted_names)
+        if ncont > 1:
+          cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
+          lhs = moveaxis(lhs, lhs_cont, cdims).reshape(lhs.shape[:-ncont] + (-1,))
+          rhs = moveaxis(rhs, rhs_cont, cdims).reshape(rhs.shape[:-ncont] + (-1,))
+          lhs_cont = rhs_cont = [lhs.ndim - 1]
+
+        dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+        operand = lax.dot_general(lhs, rhs, dimension_numbers)
+        deleted_names = batch_names + ''.join(contracted_names)
+        names = (batch_names
+                + lhs_names.translate(None, deleted_names)
+                + rhs_names.translate(None, deleted_names))
       else:
-        batch_dims = tuple(lhs_batch)
-        batch_names = ''.join(lhs_names[i] for i in batch_dims)
-
-      # TODO need to flatten lhs_cont / rhs_cont if more than 1d
-      dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
-      operand = lax.dot_general(lhs, rhs, dimension_numbers)
-      deleted_names = batch_names + ''.join(contracted_names)
-      names = (batch_names
-               + lhs_names.translate(None, deleted_names)
-               + rhs_names.translate(None, deleted_names))
+        # no contraction, just a tensor product
+        if lhs_batch != rhs_batch:
+          rhs = moveaxis(rhs, rhs_batch, lhs_batch)
+        batch_names = ''.join(lhs_names[i] for i in lhs_batch)
+
+        names = batch_names + lhs_names + rhs_names
+        lhs_shape = iter(lhs.shape)
+        lhs_shape = [next(lhs_shape) if n in batch_names + lhs_names else 1
+                     for n in names]
+        rhs_shape = iter(rhs.shape)
+        rhs_shape = [next(rhs_shape) if n in batch_names + rhs_names else 1
+                     for n in names]
+        operand = lax.reshape(lhs, lhs_shape) * lax.reshape(rhs, rhs_shape)
 
     else:
       raise NotImplementedError","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 72cdc4ab5..21f518a33 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1120,29 +1120,53 @@ def einsum(*operands):
       rhs, rhs_names = sum_repeats(rhs, rhs_names, rhs_counts,
                                    result_names + lhs_names)
 
-      # contract lhs against rhs
       contracted_names = contracted_names & (set(lhs_names) | set(rhs_names))
       batch_names = set(lhs_names) & set(rhs_names) - contracted_names
-      lhs_cont, rhs_cont = unzip2((lhs_names.index(name), rhs_names.index(name))
-                                  for name in contracted_names)
-      lhs_batch, rhs_batch = unzip2((lhs_names.find(name), rhs_names.find(name))
-                                    for name in batch_names)
-      batch_dims = tuple(range(len(batch_names)))
-      if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
-        lhs = moveaxis(lhs, lhs_batch, batch_dims)
-        rhs = moveaxis(rhs, rhs_batch, batch_dims)
-        batch_names = ''.join(batch_names)
-      else:
-        batch_dims = tuple(lhs_batch)
-        batch_names = ''.join(lhs_names[i] for i in batch_dims)
+      lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
+                                    for n in batch_names)
+      if contracted_names:
+        # contract usint lax.dot_general
+        lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
+                                    for n in contracted_names)
 
-      # TODO need to flatten lhs_cont / rhs_cont if more than 1d
-      dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
-      operand = lax.dot_general(lhs, rhs, dimension_numbers)
-      deleted_names = batch_names + ''.join(contracted_names)
-      names = (batch_names
-               + lhs_names.translate(None, deleted_names)
-               + rhs_names.translate(None, deleted_names))
+        # lax.dot_general batch dims have to precede non-batch dims
+        batch_dims = tuple(range(len(batch_names)))
+        if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
+          lhs = moveaxis(lhs, lhs_batch, batch_dims)
+          rhs = moveaxis(rhs, rhs_batch, batch_dims)
+          batch_names = ''.join(batch_names)
+        else:
+          batch_dims = tuple(lhs_batch)
+          batch_names = ''.join(lhs_names[i] for i in batch_dims)
+
+        # lax.dot_general only allows one contracting dimension, move it to last
+        ncont = len(contracted_names)
+        if ncont > 1:
+          cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
+          lhs = moveaxis(lhs, lhs_cont, cdims).reshape(lhs.shape[:-ncont] + (-1,))
+          rhs = moveaxis(rhs, rhs_cont, cdims).reshape(rhs.shape[:-ncont] + (-1,))
+          lhs_cont = rhs_cont = [lhs.ndim - 1]
+
+        dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+        operand = lax.dot_general(lhs, rhs, dimension_numbers)
+        deleted_names = batch_names + ''.join(contracted_names)
+        names = (batch_names
+                + lhs_names.translate(None, deleted_names)
+                + rhs_names.translate(None, deleted_names))
+      else:
+        # no contraction, just a tensor product
+        if lhs_batch != rhs_batch:
+          rhs = moveaxis(rhs, rhs_batch, lhs_batch)
+        batch_names = ''.join(lhs_names[i] for i in lhs_batch)
+
+        names = batch_names + lhs_names + rhs_names
+        lhs_shape = iter(lhs.shape)
+        lhs_shape = [next(lhs_shape) if n in batch_names + lhs_names else 1
+                     for n in names]
+        rhs_shape = iter(rhs.shape)
+        rhs_shape = [next(rhs_shape) if n in batch_names + rhs_names else 1
+                     for n in names]
+        operand = lax.reshape(lhs, lhs_shape) * lax.reshape(rhs, rhs_shape)
 
     else:
       raise NotImplementedError",Yes
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,d8388e2d80379051063aef61b9ff06752d9f31ac,fdde6841e6f7f4e62817a709a49f08aee6c880cb,complete support for two-operand einsum,"diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 6a9ffb82b..55b30e120 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -37,6 +37,20 @@ def check(s, *ops):
 
 class EinsumTest(jtu.JaxTestCase):
 
+  def test_three_operands_1(self):
+    x = rng().randn(3)
+    y = rng().randn(4)
+    z = rng().randn(5)
+    s = 'i,j,k->ijk'
+    check(s, x, y, z)
+
+  def test_three_operands_2(self):
+    x = rng().randn(3)
+    y = rng().randn(4)
+    z = rng().randn(5)
+    s = 'i,j,k->ijk'
+    check(s, x, y, z)
+
   def test_two_operands_1(self):
     x = rng().randn(3, 4)
     y = rng().randn(4)
@@ -55,6 +69,12 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'iji,i->j'
     check(s, x, y)
 
+  def test_two_operands_4(self):
+    x = rng().randn(3, 4)
+    y = rng().randn(3, 4)
+    s = 'ij,ij->'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 6a9ffb82b..55b30e120 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -37,6 +37,20 @@ def check(s, *ops):
 
 class EinsumTest(jtu.JaxTestCase):
 
+  def test_three_operands_1(self):
+    x = rng().randn(3)
+    y = rng().randn(4)
+    z = rng().randn(5)
+    s = 'i,j,k->ijk'
+    check(s, x, y, z)
+
+  def test_three_operands_2(self):
+    x = rng().randn(3)
+    y = rng().randn(4)
+    z = rng().randn(5)
+    s = 'i,j,k->ijk'
+    check(s, x, y, z)
+
   def test_two_operands_1(self):
     x = rng().randn(3, 4)
     y = rng().randn(4)
@@ -55,6 +69,12 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'iji,i->j'
     check(s, x, y)
 
+  def test_two_operands_4(self):
+    x = rng().randn(3, 4)
+    y = rng().randn(3, 4)
+    s = 'ij,ij->'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6261ef729a5cd4b9ad3a9bcfca94a7140cb16d60,d8388e2d80379051063aef61b9ff06752d9f31ac,more einsum improvements (complete?),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 21f518a33..c344b809c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1060,6 +1060,7 @@ def tensordot(a, b, axes=2):
 def einsum(*operands):
   operands, contractions = opt_einsum.contract_path(
       *operands, einsum_call=True, use_blas=True)
+  operands = list(_promote_dtypes(*operands))
   sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
 
   def sum_uniques(operand, names, uniques):
@@ -1139,16 +1140,7 @@ def einsum(*operands):
           batch_dims = tuple(lhs_batch)
           batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
-        # lax.dot_general only allows one contracting dimension, move it to last
-        ncont = len(contracted_names)
-        if ncont > 1:
-          cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
-          lhs = moveaxis(lhs, lhs_cont, cdims).reshape(lhs.shape[:-ncont] + (-1,))
-          rhs = moveaxis(rhs, rhs_cont, cdims).reshape(rhs.shape[:-ncont] + (-1,))
-          lhs_cont = rhs_cont = [lhs.ndim - 1]
-
-        dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
-        operand = lax.dot_general(lhs, rhs, dimension_numbers)
+        operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
         deleted_names = batch_names + ''.join(contracted_names)
         names = (batch_names
                 + lhs_names.translate(None, deleted_names)
@@ -1183,6 +1175,47 @@ def einsum(*operands):
   return operands[0]
 
 
+def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
+  # lax.dot_general has some tight constraints on dimension_numbers that this
+  # wrapper loosens via transposes and reshapes
+  assert len(lhs_cont) == len(rhs_cont) > 0
+  ncont = len(lhs_cont)
+  lhs_ntensor = lhs.ndim - nbatch - ncont
+  rhs_ntensor = rhs.ndim - nbatch - ncont
+  batch_dims = tuple(range(nbatch))
+
+  if ncont == 1 and 0 <= lhs_ntensor <= 1 and 0 <= rhs_ntensor <= 1:
+    dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+    return lax.dot_general(lhs, rhs, dimension_numbers)
+  else:
+    # move contracting dimensions to the end. lax.dot_general only allows one
+    # contracting dimension, so if there's more than one we collapse them.
+    if ncont > 1:
+      lhs_cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
+      lhs = moveaxis(lhs, lhs_cont, lhs_cdims).reshape(lhs.shape[:-ncont] + (-1,))
+
+      rhs_cdims = tuple(range(rhs.ndim - ncont, rhs.ndim))
+      rhs = moveaxis(rhs, rhs_cont, rhs_cdims).reshape(rhs.shape[:-ncont] + (-1,))
+    else:
+      lhs = moveaxis(lhs, lhs_cont[0], -1)
+      rhs = moveaxis(rhs, rhs_cont[0], -1)
+
+    # lax.dot_general only allows zero or one tensor product dims per operand,
+    # so if there's more than one we collapse them.
+    result_shape = lhs.shape[:nbatch] + lhs.shape[nbatch:-1] + rhs.shape[nbatch:-1]
+
+    if lhs_ntensor > 1:
+      lhs = lhs.reshape(lhs.shape[:nbatch] + (-1,) + lhs.shape[-1:])
+
+    if rhs_ntensor > 1:
+      rhs = rhs.reshape(rhs.shape[:nbatch] + (-1,) + rhs.shape[-1:])
+
+    lhs_cont, rhs_cont = [lhs.ndim - 1], [rhs.ndim - 1]
+    dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+    result = lax.dot_general(lhs, rhs, dimension_numbers)
+    return lax.reshape(result, result_shape)
+
+
 ### Misc
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 21f518a33..c344b809c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1060,6 +1060,7 @@ def tensordot(a, b, axes=2):
 def einsum(*operands):
   operands, contractions = opt_einsum.contract_path(
       *operands, einsum_call=True, use_blas=True)
+  operands = list(_promote_dtypes(*operands))
   sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
 
   def sum_uniques(operand, names, uniques):
@@ -1139,16 +1140,7 @@ def einsum(*operands):
           batch_dims = tuple(lhs_batch)
           batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
-        # lax.dot_general only allows one contracting dimension, move it to last
-        ncont = len(contracted_names)
-        if ncont > 1:
-          cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
-          lhs = moveaxis(lhs, lhs_cont, cdims).reshape(lhs.shape[:-ncont] + (-1,))
-          rhs = moveaxis(rhs, rhs_cont, cdims).reshape(rhs.shape[:-ncont] + (-1,))
-          lhs_cont = rhs_cont = [lhs.ndim - 1]
-
-        dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
-        operand = lax.dot_general(lhs, rhs, dimension_numbers)
+        operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
         deleted_names = batch_names + ''.join(contracted_names)
         names = (batch_names
                 + lhs_names.translate(None, deleted_names)
@@ -1183,6 +1175,47 @@ def einsum(*operands):
   return operands[0]
 
 
+def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
+  # lax.dot_general has some tight constraints on dimension_numbers that this
+  # wrapper loosens via transposes and reshapes
+  assert len(lhs_cont) == len(rhs_cont) > 0
+  ncont = len(lhs_cont)
+  lhs_ntensor = lhs.ndim - nbatch - ncont
+  rhs_ntensor = rhs.ndim - nbatch - ncont
+  batch_dims = tuple(range(nbatch))
+
+  if ncont == 1 and 0 <= lhs_ntensor <= 1 and 0 <= rhs_ntensor <= 1:
+    dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+    return lax.dot_general(lhs, rhs, dimension_numbers)
+  else:
+    # move contracting dimensions to the end. lax.dot_general only allows one
+    # contracting dimension, so if there's more than one we collapse them.
+    if ncont > 1:
+      lhs_cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
+      lhs = moveaxis(lhs, lhs_cont, lhs_cdims).reshape(lhs.shape[:-ncont] + (-1,))
+
+      rhs_cdims = tuple(range(rhs.ndim - ncont, rhs.ndim))
+      rhs = moveaxis(rhs, rhs_cont, rhs_cdims).reshape(rhs.shape[:-ncont] + (-1,))
+    else:
+      lhs = moveaxis(lhs, lhs_cont[0], -1)
+      rhs = moveaxis(rhs, rhs_cont[0], -1)
+
+    # lax.dot_general only allows zero or one tensor product dims per operand,
+    # so if there's more than one we collapse them.
+    result_shape = lhs.shape[:nbatch] + lhs.shape[nbatch:-1] + rhs.shape[nbatch:-1]
+
+    if lhs_ntensor > 1:
+      lhs = lhs.reshape(lhs.shape[:nbatch] + (-1,) + lhs.shape[-1:])
+
+    if rhs_ntensor > 1:
+      rhs = rhs.reshape(rhs.shape[:nbatch] + (-1,) + rhs.shape[-1:])
+
+    lhs_cont, rhs_cont = [lhs.ndim - 1], [rhs.ndim - 1]
+    dimension_numbers = [(lhs_cont, rhs_cont), (batch_dims, batch_dims)]
+    result = lax.dot_general(lhs, rhs, dimension_numbers)
+    return lax.reshape(result, result_shape)
+
+
 ### Misc
 
 ",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,6261ef729a5cd4b9ad3a9bcfca94a7140cb16d60,d8388e2d80379051063aef61b9ff06752d9f31ac,more einsum improvements (complete?),"diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 55b30e120..ec0fc4c76 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -75,6 +75,12 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij,ij->'
     check(s, x, y)
 
+  def test_two_operands_5(self):
+    x = rng().randn(10, 2, 3)
+    y = rng().randn(3, 4)
+    s = 'nij,jk->nik'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 55b30e120..ec0fc4c76 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -75,6 +75,12 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij,ij->'
     check(s, x, y)
 
+  def test_two_operands_5(self):
+    x = rng().randn(10, 2, 3)
+    y = rng().randn(3, 4)
+    s = 'nij,jk->nik'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,061d033c2bd82b53c448f4e0d81ccfd4d98e268b,6261ef729a5cd4b9ad3a9bcfca94a7140cb16d60,add jit around einsum,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index c344b809c..40937a632 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -24,6 +24,7 @@ import opt_einsum
 import collections
 import itertools
 
+from jax import jit
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
@@ -1058,8 +1059,15 @@ def tensordot(a, b, axes=2):
 
 
 def einsum(*operands):
+  # using einsum_call=True here is an internal api for opt_einsum
   operands, contractions = opt_einsum.contract_path(
       *operands, einsum_call=True, use_blas=True)
+  contractions = tuple(data[:3] for data in contractions)
+  return _einsum(operands, contractions)
+
+
+@partial(jit, static_argnums=(1,))
+def _einsum(operands, contractions):
   operands = list(_promote_dtypes(*operands))
   sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
 
@@ -1083,7 +1091,7 @@ def einsum(*operands):
           names = names.replace(name, '', count - 1)
     return operand, names
 
-  for operand_indices, contracted_names, einstr, _, _ in contractions:
+  for operand_indices, contracted_names, einstr in contractions:
     input_str, result_names = einstr.split('->')
     input_names = input_str.split(',')
 
@@ -1176,6 +1184,7 @@ def einsum(*operands):
 
 
 def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
+  """"""Helper for einsum contractions.""""""
   # lax.dot_general has some tight constraints on dimension_numbers that this
   # wrapper loosens via transposes and reshapes
   assert len(lhs_cont) == len(rhs_cont) > 0","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index c344b809c..40937a632 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -24,6 +24,7 @@ import opt_einsum
 import collections
 import itertools
 
+from jax import jit
 from .. import core
 from ..abstract_arrays import UnshapedArray, ShapedArray, ConcreteArray
 from ..interpreters.xla import DeviceArray
@@ -1058,8 +1059,15 @@ def tensordot(a, b, axes=2):
 
 
 def einsum(*operands):
+  # using einsum_call=True here is an internal api for opt_einsum
   operands, contractions = opt_einsum.contract_path(
       *operands, einsum_call=True, use_blas=True)
+  contractions = tuple(data[:3] for data in contractions)
+  return _einsum(operands, contractions)
+
+
+@partial(jit, static_argnums=(1,))
+def _einsum(operands, contractions):
   operands = list(_promote_dtypes(*operands))
   sum = lambda x, axes: lax.reduce(x, onp.array(0, x.dtype), lax.add, axes)
 
@@ -1083,7 +1091,7 @@ def einsum(*operands):
           names = names.replace(name, '', count - 1)
     return operand, names
 
-  for operand_indices, contracted_names, einstr, _, _ in contractions:
+  for operand_indices, contracted_names, einstr in contractions:
     input_str, result_names = einstr.split('->')
     input_names = input_str.split(',')
 
@@ -1176,6 +1184,7 @@ def einsum(*operands):
 
 
 def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
+  """"""Helper for einsum contractions.""""""
   # lax.dot_general has some tight constraints on dimension_numbers that this
   # wrapper loosens via transposes and reshapes
   assert len(lhs_cont) == len(rhs_cont) > 0",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,d7745dd9af8039f558f63b1eeef8044b88ed2055,061d033c2bd82b53c448f4e0d81ccfd4d98e268b,actually fix py3 str translate,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 40937a632..d9ccf6149 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -16,13 +16,14 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from six.moves import builtins
+import collections
+import itertools
+import string
 
-import six
 import numpy as onp
 import opt_einsum
-import collections
-import itertools
+import six
+from six.moves import builtins
 
 from jax import jit
 from .. import core
@@ -32,15 +33,12 @@ from .. import lax
 from ..util import memoize, partial, get_module_functions, unzip2, prod as _prod
 from ..lib import xla_bridge
 
-# To provide the same module-level names as Numpy, we need to redefine builtins
-# and also use some common names (like 'shape' and 'dtype') at the top-level.
-# pylint: disable=redefined-builtin,redefined-outer-name
-
-# There might be a pylint bug with tuple unpacking.
-# pylint: disable=unbalanced-tuple-unpacking
-
-# We get docstrings from the underlying numpy functions.
-# pylint: disable=missing-docstring
+if six.PY3:
+  def removechars(s, chars):
+    return s.translate(str.maketrans(dict.fromkeys(chars)))
+else:
+  def removechars(s, chars):
+    return s.translate(None, ''.join(chars))
 
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
@@ -1075,7 +1073,7 @@ def _einsum(operands, contractions):
     if uniques:
       axes = [names.index(name) for name in uniques]
       operand = sum(operand, axes)
-      names = names.translate(None, ''.join(uniques))
+      names = removechars(names, uniques)
     return operand, names
 
   def sum_repeats(operand, names, counts, keep_names):
@@ -1150,9 +1148,8 @@ def _einsum(operands, contractions):
 
         operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
         deleted_names = batch_names + ''.join(contracted_names)
-        names = (batch_names
-                + lhs_names.translate(None, deleted_names)
-                + rhs_names.translate(None, deleted_names))
+        names = (batch_names + removechars(lhs_names, deleted_names)
+                 + removechars(rhs_names, deleted_names))
       else:
         # no contraction, just a tensor product
         if lhs_batch != rhs_batch:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 40937a632..d9ccf6149 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -16,13 +16,14 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from six.moves import builtins
-
-import six
-import numpy as onp
-import opt_einsum
 import collections
 import itertools
+import string
+
+import numpy as onp
+import opt_einsum
+import six
+from six.moves import builtins
 
 from jax import jit
 from .. import core
@@ -32,15 +33,12 @@ from .. import lax
 from ..util import memoize, partial, get_module_functions, unzip2, prod as _prod
 from ..lib import xla_bridge
 
-# To provide the same module-level names as Numpy, we need to redefine builtins
-# and also use some common names (like 'shape' and 'dtype') at the top-level.
-# pylint: disable=redefined-builtin,redefined-outer-name
-
-# There might be a pylint bug with tuple unpacking.
-# pylint: disable=unbalanced-tuple-unpacking
-
-# We get docstrings from the underlying numpy functions.
-# pylint: disable=missing-docstring
+if six.PY3:
+  def removechars(s, chars):
+    return s.translate(str.maketrans(dict.fromkeys(chars)))
+else:
+  def removechars(s, chars):
+    return s.translate(None, ''.join(chars))
 
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
@@ -1075,7 +1073,7 @@ def _einsum(operands, contractions):
     if uniques:
       axes = [names.index(name) for name in uniques]
       operand = sum(operand, axes)
-      names = names.translate(None, ''.join(uniques))
+      names = removechars(names, uniques)
     return operand, names
 
   def sum_repeats(operand, names, counts, keep_names):
@@ -1150,9 +1148,8 @@ def _einsum(operands, contractions):
 
         operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
         deleted_names = batch_names + ''.join(contracted_names)
-        names = (batch_names
-                + lhs_names.translate(None, deleted_names)
-                + rhs_names.translate(None, deleted_names))
+        names = (batch_names + removechars(lhs_names, deleted_names)
+                 + removechars(rhs_names, deleted_names))
       else:
         # no contraction, just a tensor product
         if lhs_batch != rhs_batch:",Yes
setup.py,setup.py,308843f2d4d0c1e08d3168642c94532ebd9d6232,b2cff9d35a6518d808872028006a014e919e7b1c,bump version number for pypi,"diff --git a/setup.py b/setup.py
index e1f9a311e..582aed3f5 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.11',
+    version='0.1.12',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index e1f9a311e..582aed3f5 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.11',
+    version='0.1.12',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
jax/interpreters/ad.py,jax/interpreters/ad.py,0a9ee106b382326ccf8373707f8577ba8a2afec9,308843f2d4d0c1e08d3168642c94532ebd9d6232,implement triangular solve lhs jvp (w/ @froystig),"diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 92777b885..072b3a5d2 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -328,7 +328,7 @@ defbilinear = partial(defbilinear_broadcasting, lambda g, x: g)
 
 
 def bilinear_transpose(lhs_rule, rhs_rule, cotangent, x, y, **kwargs):
-  assert x is None or y is None
+  assert (x is None) ^ (y is None)
   if x is None:
     out = zero if cotangent is zero else lhs_rule(cotangent, y, **kwargs)
     return out, None","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 92777b885..072b3a5d2 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -328,7 +328,7 @@ defbilinear = partial(defbilinear_broadcasting, lambda g, x: g)
 
 
 def bilinear_transpose(lhs_rule, rhs_rule, cotangent, x, y, **kwargs):
-  assert x is None or y is None
+  assert (x is None) ^ (y is None)
   if x is None:
     out = zero if cotangent is zero else lhs_rule(cotangent, y, **kwargs)
     return out, None",No
jax/lax_linalg.py,jax/lax_linalg.py,0a9ee106b382326ccf8373707f8577ba8a2afec9,308843f2d4d0c1e08d3168642c94532ebd9d6232,implement triangular solve lhs jvp (w/ @froystig),"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 40a17d5fe..20eb50432 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -109,6 +109,16 @@ def triangular_solve_shape_rule(a, b, left_side=False, **unused_kwargs):
     raise TypeError(msg.format(a.shape, b.shape))
   return b.shape
 
+def triangular_solve_jvp_rule_a(
+    g_a, ans, a, b, left_side, lower, transpose_a, conjugate_a):
+  g_a = lax.neg(g_a)
+  tmp = triangular_solve(a, g_a, left_side, lower, transpose_a, conjugate_a)
+  dot = lax.dot if g_a.ndim == 2 else lax.batch_matmul
+  if left_side:
+    return dot(tmp, ans)
+  else:
+    return dot(ans, tmp)
+
 def triangular_solve_transpose_rule(
     cotangent, a, b, left_side, lower, transpose_a, conjugate_a):
   assert a is not None and b is None
@@ -119,9 +129,9 @@ def triangular_solve_transpose_rule(
 triangular_solve_p = standard_primitive(
     triangular_solve_shape_rule, triangular_solve_dtype_rule,
     'triangular_solve')
-ad.defjvp(triangular_solve_p,
-          None,
-          lambda g_b, a, b, **kwargs: triangular_solve(a, g_b, **kwargs))
+ad.defjvp2(triangular_solve_p,
+           triangular_solve_jvp_rule_a,
+           lambda g_b, _, a, b, **kws: triangular_solve(a, g_b, **kws))
 ad.primitive_transposes[triangular_solve_p] = triangular_solve_transpose_rule
 
 ","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 40a17d5fe..20eb50432 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -109,6 +109,16 @@ def triangular_solve_shape_rule(a, b, left_side=False, **unused_kwargs):
     raise TypeError(msg.format(a.shape, b.shape))
   return b.shape
 
+def triangular_solve_jvp_rule_a(
+    g_a, ans, a, b, left_side, lower, transpose_a, conjugate_a):
+  g_a = lax.neg(g_a)
+  tmp = triangular_solve(a, g_a, left_side, lower, transpose_a, conjugate_a)
+  dot = lax.dot if g_a.ndim == 2 else lax.batch_matmul
+  if left_side:
+    return dot(tmp, ans)
+  else:
+    return dot(ans, tmp)
+
 def triangular_solve_transpose_rule(
     cotangent, a, b, left_side, lower, transpose_a, conjugate_a):
   assert a is not None and b is None
@@ -119,9 +129,9 @@ def triangular_solve_transpose_rule(
 triangular_solve_p = standard_primitive(
     triangular_solve_shape_rule, triangular_solve_dtype_rule,
     'triangular_solve')
-ad.defjvp(triangular_solve_p,
-          None,
-          lambda g_b, a, b, **kwargs: triangular_solve(a, g_b, **kwargs))
+ad.defjvp2(triangular_solve_p,
+           triangular_solve_jvp_rule_a,
+           lambda g_b, _, a, b, **kws: triangular_solve(a, g_b, **kws))
 ad.primitive_transposes[triangular_solve_p] = triangular_solve_transpose_rule
 
 ",No
jax/scipy/linalg.py,jax/scipy/linalg.py,0a9ee106b382326ccf8373707f8577ba8a2afec9,308843f2d4d0c1e08d3168642c94532ebd9d6232,implement triangular solve lhs jvp (w/ @froystig),"diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index e736526ff..060238eec 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -81,6 +81,8 @@ def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,
   else:
     raise ValueError(""Invalid 'trans' value {}"".format(trans))
 
+  a = np.tril(a) if lower else np.triu(a)
+
   # lax_linalg.triangular_solve only supports matrix 'b's at the moment.
   b_is_vector = np.ndim(a) == np.ndim(b) + 1
   if b_is_vector:","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index e736526ff..060238eec 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -81,6 +81,8 @@ def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,
   else:
     raise ValueError(""Invalid 'trans' value {}"".format(trans))
 
+  a = np.tril(a) if lower else np.triu(a)
+
   # lax_linalg.triangular_solve only supports matrix 'b's at the moment.
   b_is_vector = np.ndim(a) == np.ndim(b) + 1
   if b_is_vector:",No
jax/test_util.py,jax/test_util.py,0a9ee106b382326ccf8373707f8577ba8a2afec9,308843f2d4d0c1e08d3168642c94532ebd9d6232,implement triangular solve lhs jvp (w/ @froystig),"diff --git a/jax/test_util.py b/jax/test_util.py
index c73d58202..65acfc2b1 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -141,6 +141,22 @@ def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
   check_close(ip, ip_expected, atol=atol, rtol=rtol)
 
 
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  if order > 1:
+    def f_vjp(*args):
+      out_primal_py, vjp_py = api.vjp(f, *args)
+      return vjp_py(out_primal_py)
+
+    check_grads(f_vjp, args, order - 1, atol=atol, rtol=rtol, eps=eps)
+  else:
+    default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+    atol = atol or default_tol
+    rtol = rtol or default_tol
+    eps = eps or default_tol
+    check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+    check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
 def skip_on_devices(*disabled_devices):
   """"""A decorator for test methods to skip the test on certain devices.""""""
   def skip(test_method):","diff --git a/jax/test_util.py b/jax/test_util.py
index c73d58202..65acfc2b1 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -141,6 +141,22 @@ def check_vjp(f, f_vjp, args, atol=ATOL, rtol=RTOL, eps=EPS):
   check_close(ip, ip_expected, atol=atol, rtol=rtol)
 
 
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  if order > 1:
+    def f_vjp(*args):
+      out_primal_py, vjp_py = api.vjp(f, *args)
+      return vjp_py(out_primal_py)
+
+    check_grads(f_vjp, args, order - 1, atol=atol, rtol=rtol, eps=eps)
+  else:
+    default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+    atol = atol or default_tol
+    rtol = rtol or default_tol
+    eps = eps or default_tol
+    check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+    check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
 def skip_on_devices(*disabled_devices):
   """"""A decorator for test methods to skip the test on certain devices.""""""
   def skip(test_method):",No
tests/lax_test.py,tests/lax_test.py,0a9ee106b382326ccf8373707f8577ba8a2afec9,308843f2d4d0c1e08d3168642c94532ebd9d6232,implement triangular solve lhs jvp (w/ @froystig),"diff --git a/tests/lax_test.py b/tests/lax_test.py
index 866fd5794..867577100 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -33,6 +33,7 @@ from jax import core
 from jax import lax
 from jax import test_util as jtu
 from jax import lax_reference
+from jax.test_util import check_grads
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
@@ -1484,22 +1485,6 @@ LAX_GRAD_OPS = [
 ]
 
 
-def check_grads(f, args, order, atol=None, rtol=None, eps=None):
-  if order > 1:
-    def f_vjp(*args):
-      out_primal_py, vjp_py = api.vjp(f, *args)
-      return vjp_py(out_primal_py)
-
-    check_grads(f_vjp, args, order - 1, atol=atol, rtol=rtol, eps=eps)
-  else:
-    default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
-    atol = atol or default_tol
-    rtol = rtol or default_tol
-    eps = eps or default_tol
-    jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
-    jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
-
-
 def check_grads_bilinear(f, args, order, atol=None, rtol=None):
   # Can use large eps to make up for numerical inaccuracies since the op is
   # bilinear (relying on the fact that we only check one arg at a time)","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 866fd5794..867577100 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -33,6 +33,7 @@ from jax import core
 from jax import lax
 from jax import test_util as jtu
 from jax import lax_reference
+from jax.test_util import check_grads
 from jax.interpreters import xla
 from jax.lib import xla_bridge
 
@@ -1484,22 +1485,6 @@ LAX_GRAD_OPS = [
 ]
 
 
-def check_grads(f, args, order, atol=None, rtol=None, eps=None):
-  if order > 1:
-    def f_vjp(*args):
-      out_primal_py, vjp_py = api.vjp(f, *args)
-      return vjp_py(out_primal_py)
-
-    check_grads(f_vjp, args, order - 1, atol=atol, rtol=rtol, eps=eps)
-  else:
-    default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
-    atol = atol or default_tol
-    rtol = rtol or default_tol
-    eps = eps or default_tol
-    jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
-    jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
-
-
 def check_grads_bilinear(f, args, order, atol=None, rtol=None):
   # Can use large eps to make up for numerical inaccuracies since the op is
   # bilinear (relying on the fact that we only check one arg at a time)",No
tests/linalg_test.py,tests/linalg_test.py,0a9ee106b382326ccf8373707f8577ba8a2afec9,308843f2d4d0c1e08d3168642c94532ebd9d6232,implement triangular solve lhs jvp (w/ @froystig),"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 5094ed326..2f3a70b71 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -180,6 +180,31 @@ class ScipyLinalgTest(jtu.JaxTestCase):
 
     self.assertAllClose(onp_ans, ans, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, transpose_a),
+       ""lower"": lower, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, transpose_a in itertools.product([False, True], repeat=2)
+      for lhs_shape, rhs_shape in [
+          ((4, 4), (4,)),
+          ((4, 4), (4, 3)),
+          ((2, 8, 8), (2, 8, 10)),
+      ]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testSolveTriangularBlockedGrad(self, lower, transpose_a, lhs_shape,
+                                     rhs_shape, dtype, rng):
+    A = np.tril(rng(lhs_shape, dtype) + 10 * onp.eye(lhs_shape[-1], dtype=dtype))
+    A = A if lower else T(A)
+    B = rng(rhs_shape, dtype)
+    f = partial(scipy.linalg.solve_triangular, lower=lower,
+                trans=1 if transpose_a else 0)
+    jtu.check_grads(f, (A, B), 2)
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 5094ed326..2f3a70b71 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -180,6 +180,31 @@ class ScipyLinalgTest(jtu.JaxTestCase):
 
     self.assertAllClose(onp_ans, ans, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, transpose_a),
+       ""lower"": lower, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, transpose_a in itertools.product([False, True], repeat=2)
+      for lhs_shape, rhs_shape in [
+          ((4, 4), (4,)),
+          ((4, 4), (4, 3)),
+          ((2, 8, 8), (2, 8, 10)),
+      ]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testSolveTriangularBlockedGrad(self, lower, transpose_a, lhs_shape,
+                                     rhs_shape, dtype, rng):
+    A = np.tril(rng(lhs_shape, dtype) + 10 * onp.eye(lhs_shape[-1], dtype=dtype))
+    A = A if lower else T(A)
+    B = rng(rhs_shape, dtype)
+    f = partial(scipy.linalg.solve_triangular, lower=lower,
+                trans=1 if transpose_a else 0)
+    jtu.check_grads(f, (A, B), 2)
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,dbab47fdb0fb58a1b19097f5acc320ac1d097163,308843f2d4d0c1e08d3168642c94532ebd9d6232,Implement np.inner and np.outer.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d9ccf6149..ebc9c3c5f 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1221,6 +1221,19 @@ def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
     result = lax.dot_general(lhs, rhs, dimension_numbers)
     return lax.reshape(result, result_shape)
 
+@_wraps(onp.inner)
+def inner(a, b):
+  if ndim(a) == 0 or ndim(b) == 0:
+    return a * b
+  return tensordot(a, b, (-1, -1))
+
+
+@_wraps(onp.outer)
+def outer(a, b, out=None):
+  if out:
+    raise NotImplementedError(""The 'out' argument to outer is not supported."")
+  return ravel(a)[:, None] * ravel(b)
+
 
 ### Misc
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d9ccf6149..ebc9c3c5f 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1221,6 +1221,19 @@ def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
     result = lax.dot_general(lhs, rhs, dimension_numbers)
     return lax.reshape(result, result_shape)
 
+@_wraps(onp.inner)
+def inner(a, b):
+  if ndim(a) == 0 or ndim(b) == 0:
+    return a * b
+  return tensordot(a, b, (-1, -1))
+
+
+@_wraps(onp.outer)
+def outer(a, b, out=None):
+  if out:
+    raise NotImplementedError(""The 'out' argument to outer is not supported."")
+  return ravel(a)[:, None] * ravel(b)
+
 
 ### Misc
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,dbab47fdb0fb58a1b19097f5acc320ac1d097163,308843f2d4d0c1e08d3168642c94532ebd9d6232,Implement np.inner and np.outer.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 77a8d9696..f3d93bb77 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -108,6 +108,7 @@ JAX_COMPOUND_OP_RECORDS = [
               test_name=""expm1_large""),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""outer"", 2, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""log10"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
@@ -362,6 +363,26 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_{}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": jtu.rand_default()}
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(numeric_dtypes, 2)
+      for lhs_shape, rhs_shape in [
+        (l, r) for l, r in CombosWithReplacement(all_shapes, 2)
+        if len(jtu._dims_of_shape(l)) == 0
+        or len(jtu._dims_of_shape(r)) == 0
+        or l[-1] == r[-1]]))
+  def testInner(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    onp_fun = lambda lhs, rhs: onp.inner(lhs, rhs)
+    lnp_fun = lambda lhs, rhs: lnp.inner(lhs, rhs)
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_amin={}_amax={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 77a8d9696..f3d93bb77 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -108,6 +108,7 @@ JAX_COMPOUND_OP_RECORDS = [
               test_name=""expm1_large""),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""outer"", 2, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""log10"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
@@ -362,6 +363,26 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_{}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": jtu.rand_default()}
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(numeric_dtypes, 2)
+      for lhs_shape, rhs_shape in [
+        (l, r) for l, r in CombosWithReplacement(all_shapes, 2)
+        if len(jtu._dims_of_shape(l)) == 0
+        or len(jtu._dims_of_shape(r)) == 0
+        or l[-1] == r[-1]]))
+  def testInner(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    onp_fun = lambda lhs, rhs: onp.inner(lhs, rhs)
+    lnp_fun = lambda lhs, rhs: lnp.inner(lhs, rhs)
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_amin={}_amax={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,473207b9ebafc89d61f8dda6541c75e0ae48d301,dbab47fdb0fb58a1b19097f5acc320ac1d097163,Don't test integer types for np.inner because lax.dot doesn't support them.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index f3d93bb77..4160e4b8d 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -370,7 +370,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
        ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
        ""rng"": jtu.rand_default()}
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(numeric_dtypes, 2)
+      # TODO(phawkins): support integer dtypes too.
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)
       for lhs_shape, rhs_shape in [
         (l, r) for l, r in CombosWithReplacement(all_shapes, 2)
         if len(jtu._dims_of_shape(l)) == 0","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index f3d93bb77..4160e4b8d 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -370,7 +370,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
        ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
        ""rng"": jtu.rand_default()}
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(numeric_dtypes, 2)
+      # TODO(phawkins): support integer dtypes too.
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)
       for lhs_shape, rhs_shape in [
         (l, r) for l, r in CombosWithReplacement(all_shapes, 2)
         if len(jtu._dims_of_shape(l)) == 0",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,d3bb93f82a8f89d2d3aa51077b5b148547c77ba6,308843f2d4d0c1e08d3168642c94532ebd9d6232,"Fix implementation of ndarray.astype method, add a test.

Previously we were creating an operator named __astype__, which isn't a thing in numpy.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d9ccf6149..136379d8b 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1447,7 +1447,6 @@ def _swap_args(f):
   return lambda x, y: f(y, x)
 
 _operators = {
-    ""astype"": lax.convert_element_type,
     ""getitem"": _rewriting_take,
     ""neg"": negative,
     ""eq"": equal,
@@ -1507,6 +1506,7 @@ for method_name in _nondiff_methods + _diff_methods:
   setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
 setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
 setattr(ShapedArray, ""T"", core.aval_property(transpose))
+setattr(ShapedArray, ""astype"", core.aval_method(lax.convert_element_type))
 
 
 # Forward operators, methods, and properies on DeviceArray to lax_numpy
@@ -1517,6 +1517,7 @@ for method_name in _nondiff_methods + _diff_methods:
   setattr(DeviceArray, method_name, globals()[method_name])
 setattr(DeviceArray, ""flatten"", ravel)
 setattr(DeviceArray, ""T"", property(transpose))
+setattr(DeviceArray, ""astype"", lax.convert_element_type)
 
 
 # Extra methods that are handy","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d9ccf6149..136379d8b 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1447,7 +1447,6 @@ def _swap_args(f):
   return lambda x, y: f(y, x)
 
 _operators = {
-    ""astype"": lax.convert_element_type,
     ""getitem"": _rewriting_take,
     ""neg"": negative,
     ""eq"": equal,
@@ -1507,6 +1506,7 @@ for method_name in _nondiff_methods + _diff_methods:
   setattr(ShapedArray, method_name, core.aval_method(globals()[method_name]))
 setattr(ShapedArray, ""flatten"", core.aval_method(ravel))
 setattr(ShapedArray, ""T"", core.aval_property(transpose))
+setattr(ShapedArray, ""astype"", core.aval_method(lax.convert_element_type))
 
 
 # Forward operators, methods, and properies on DeviceArray to lax_numpy
@@ -1517,6 +1517,7 @@ for method_name in _nondiff_methods + _diff_methods:
   setattr(DeviceArray, method_name, globals()[method_name])
 setattr(DeviceArray, ""flatten"", ravel)
 setattr(DeviceArray, ""T"", property(transpose))
+setattr(DeviceArray, ""astype"", lax.convert_element_type)
 
 
 # Extra methods that are handy",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,d3bb93f82a8f89d2d3aa51077b5b148547c77ba6,308843f2d4d0c1e08d3168642c94532ebd9d6232,"Fix implementation of ndarray.astype method, add a test.

Previously we were creating an operator named __astype__, which isn't a thing in numpy.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 77a8d9696..3d8161624 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -804,11 +804,17 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   def testRavel(self):
     # TODO(mattjj): support this method-based syntax?
-    self.skipTest(""test disabled"")
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
     self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
 
+  def testAstype(self):
+    rng = onp.random.RandomState(0)
+    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
+    op = lambda x: x.astype(lnp.int32)
+    self._CheckAgainstNumpy(op, op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
   # TODO(mattjj): test other ndarray-like method overrides
 
   def testOnpMean(self):","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 77a8d9696..3d8161624 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -804,11 +804,17 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
 
   def testRavel(self):
     # TODO(mattjj): support this method-based syntax?
-    self.skipTest(""test disabled"")
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
     self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
 
+  def testAstype(self):
+    rng = onp.random.RandomState(0)
+    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
+    op = lambda x: x.astype(lnp.int32)
+    self._CheckAgainstNumpy(op, op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
   # TODO(mattjj): test other ndarray-like method overrides
 
   def testOnpMean(self):",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,997c9c5a50febab1dfd7203cf711a3371993c319,308843f2d4d0c1e08d3168642c94532ebd9d6232,"fix einsum tensor product logic (fixes #37)

The error was that `lhs_names` and `rhs_names` included `batch_names` as
prefixes, but the reshaping logic was written as if they did not include
batch_names (and so batch_names had to be prepended).","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d9ccf6149..1a897c448 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1155,14 +1155,16 @@ def _einsum(operands, contractions):
         if lhs_batch != rhs_batch:
           rhs = moveaxis(rhs, rhs_batch, lhs_batch)
         batch_names = ''.join(lhs_names[i] for i in lhs_batch)
+        nbatch = len(batch_names)
 
-        names = batch_names + lhs_names + rhs_names
+        assert len(lhs_names) == lhs.ndim and len(rhs_names) == rhs.ndim
+        assert lhs_names.startswith(batch_names) and rhs_names.startswith(batch_names)
+
+        names = batch_names + lhs_names[nbatch:] + rhs_names[nbatch:]
         lhs_shape = iter(lhs.shape)
-        lhs_shape = [next(lhs_shape) if n in batch_names + lhs_names else 1
-                     for n in names]
+        lhs_shape = [next(lhs_shape) if n in lhs_names else 1 for n in names]
         rhs_shape = iter(rhs.shape)
-        rhs_shape = [next(rhs_shape) if n in batch_names + rhs_names else 1
-                     for n in names]
+        rhs_shape = [next(rhs_shape) if n in rhs_names else 1 for n in names]
         operand = lax.reshape(lhs, lhs_shape) * lax.reshape(rhs, rhs_shape)
 
     else:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index d9ccf6149..1a897c448 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1155,14 +1155,16 @@ def _einsum(operands, contractions):
         if lhs_batch != rhs_batch:
           rhs = moveaxis(rhs, rhs_batch, lhs_batch)
         batch_names = ''.join(lhs_names[i] for i in lhs_batch)
+        nbatch = len(batch_names)
 
-        names = batch_names + lhs_names + rhs_names
+        assert len(lhs_names) == lhs.ndim and len(rhs_names) == rhs.ndim
+        assert lhs_names.startswith(batch_names) and rhs_names.startswith(batch_names)
+
+        names = batch_names + lhs_names[nbatch:] + rhs_names[nbatch:]
         lhs_shape = iter(lhs.shape)
-        lhs_shape = [next(lhs_shape) if n in batch_names + lhs_names else 1
-                     for n in names]
+        lhs_shape = [next(lhs_shape) if n in lhs_names else 1 for n in names]
         rhs_shape = iter(rhs.shape)
-        rhs_shape = [next(rhs_shape) if n in batch_names + rhs_names else 1
-                     for n in names]
+        rhs_shape = [next(rhs_shape) if n in rhs_names else 1 for n in names]
         operand = lax.reshape(lhs, lhs_shape) * lax.reshape(rhs, rhs_shape)
 
     else:",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,997c9c5a50febab1dfd7203cf711a3371993c319,308843f2d4d0c1e08d3168642c94532ebd9d6232,"fix einsum tensor product logic (fixes #37)

The error was that `lhs_names` and `rhs_names` included `batch_names` as
prefixes, but the reshaping logic was written as if they did not include
batch_names (and so batch_names had to be prepended).","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index ec0fc4c76..39cca5e63 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -81,6 +81,13 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'nij,jk->nik'
     check(s, x, y)
 
+  def test_two_operands_6(self):
+    # based on https://github.com/google/jax/issues/37#issuecomment-448572187
+    x = rng().randn(2, 1)
+    y = rng().randn(2, 3, 4)
+    s = 'sa,shb->shab'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index ec0fc4c76..39cca5e63 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -81,6 +81,13 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'nij,jk->nik'
     check(s, x, y)
 
+  def test_two_operands_6(self):
+    # based on https://github.com/google/jax/issues/37#issuecomment-448572187
+    x = rng().randn(2, 1)
+    y = rng().randn(2, 3, 4)
+    s = 'sa,shb->shab'
+    check(s, x, y)
+
   def test_one_operand_1(self):
     x = rng().randn(3, 4, 5)
     s = 'ijk->j'",No
jax/api.py,jax/api.py,87922fdf13b76990b4929be1b6e50e7d74250884,a154b9c691a36389588c5cc9be6c35dadede7bbc,Generalized make_jaxpr to handle python containers,"diff --git a/jax/api.py b/jax/api.py
index c4e8b6236..1d5451467 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -227,14 +227,17 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 def make_jaxpr(f):
-  # TODO(frostig): handle container trees etc.
   def pv_like(x):
-    aval = ShapedArray(onp.shape(x), onp.result_type(x))
+    aval = xla.abstractify(x)
     return pe.PartialVal((aval, core.unit))
 
+  fun = lu.wrap_init(f)
   @_wraps(f)
-  def jaxpr_maker(*args):
-    jaxpr, _, _, _ = trace_to_jaxpr(f, map(pv_like, args))
+  def jaxpr_maker(*args, **kwargs):
+    jax_args, in_trees = unzip2(map(tree_to_jaxtuples, args))
+    flat_fun, out_tree = flatten_fun(fun, in_trees)
+    pvals = map(pv_like, jax_args)
+    jaxpr, _, _ = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
     return jaxpr
 
   jaxpr_maker.__name__ = ""make_jaxpr({})"".format(jaxpr_maker.__name__)","diff --git a/jax/api.py b/jax/api.py
index c4e8b6236..1d5451467 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -227,14 +227,17 @@ def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   return unflatten_fun(fun, io_tree, *py_args)
 
 def make_jaxpr(f):
-  # TODO(frostig): handle container trees etc.
   def pv_like(x):
-    aval = ShapedArray(onp.shape(x), onp.result_type(x))
+    aval = xla.abstractify(x)
     return pe.PartialVal((aval, core.unit))
 
+  fun = lu.wrap_init(f)
   @_wraps(f)
-  def jaxpr_maker(*args):
-    jaxpr, _, _, _ = trace_to_jaxpr(f, map(pv_like, args))
+  def jaxpr_maker(*args, **kwargs):
+    jax_args, in_trees = unzip2(map(tree_to_jaxtuples, args))
+    flat_fun, out_tree = flatten_fun(fun, in_trees)
+    pvals = map(pv_like, jax_args)
+    jaxpr, _, _ = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
     return jaxpr
 
   jaxpr_maker.__name__ = ""make_jaxpr({})"".format(jaxpr_maker.__name__)",No
README.md,README.md,b781e4509ab4e617f5673ae45f47ff5697017f88,c56f43f2a5f62c467b295caad1e32b1a548a2c18,update jaxlib references to 0.1.3,"diff --git a/README.md b/README.md
index a84a1a94b..daa85fa35 100644
--- a/README.md
+++ b/README.md
@@ -144,7 +144,7 @@ PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.2-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.3-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```","diff --git a/README.md b/README.md
index a84a1a94b..daa85fa35 100644
--- a/README.md
+++ b/README.md
@@ -144,7 +144,7 @@ PYTHON_VERSION=py2  # alternatives: py2, py3
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.2-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.3-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,b781e4509ab4e617f5673ae45f47ff5697017f88,c56f43f2a5f62c467b295caad1e32b1a548a2c18,update jaxlib references to 0.1.3,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index f5c9c3872..3dd1d14fd 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index f5c9c3872..3dd1d14fd 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,b781e4509ab4e617f5673ae45f47ff5697017f88,c56f43f2a5f62c467b295caad1e32b1a548a2c18,update jaxlib references to 0.1.3,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 20b0165db..ec81e583d 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 20b0165db..ec81e583d 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,b781e4509ab4e617f5673ae45f47ff5697017f88,c56f43f2a5f62c467b295caad1e32b1a548a2c18,update jaxlib references to 0.1.3,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index cc413132e..1f3099209 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index cc413132e..1f3099209 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.2-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6a9952a939dec83c977508c596093ae4db17308a,b781e4509ab4e617f5673ae45f47ff5697017f88,"jax.numpy.arange should fall back to onp.arange

fixes #145","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 6329f9b31..f9bfa8318 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -834,29 +834,16 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
 
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
-  nargs = len(args)
-  start, step = 0, 1
+  # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy
   dtype = kwargs.pop(""dtype"", None)
-  if kwargs:
-    raise TypeError(""arange only accepts 'dtype' kwarg, got {}"".format(kwargs))
-  if nargs == 0:
+  if not args:
     raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
-  elif nargs == 1:
+  elif len(args) == 1 and not kwargs:
     stop, = args
     dtype = dtype or _dtype(stop)
-    return lax.iota(dtype, stop)  # avoids materializing
-  elif nargs == 2:
-    start, stop = args
-    dtype = dtype or onp.result_type(start, stop)
-  elif nargs == 3:
-    start, stop, step = args
-    dtype = dtype or onp.result_type(start, stop, step)
-  elif nargs == 4:
-    start, stop, step, dtype = args
-    dtype = dtype or onp.result_type(start, stop, step)
-
-  size = (stop - start - 1) // step + 1
-  return start + step * lax.iota(dtype, size)
+    if onp.issubdtype(dtype, onp.integer):
+      return lax.iota(dtype, stop)  # avoids materializing
+  return onp.arange(*args, **kwargs)
 
 
 @_wraps(onp.repeat)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 6329f9b31..f9bfa8318 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -834,29 +834,16 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
 
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
-  nargs = len(args)
-  start, step = 0, 1
+  # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy
   dtype = kwargs.pop(""dtype"", None)
-  if kwargs:
-    raise TypeError(""arange only accepts 'dtype' kwarg, got {}"".format(kwargs))
-  if nargs == 0:
+  if not args:
     raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
-  elif nargs == 1:
+  elif len(args) == 1 and not kwargs:
     stop, = args
     dtype = dtype or _dtype(stop)
-    return lax.iota(dtype, stop)  # avoids materializing
-  elif nargs == 2:
-    start, stop = args
-    dtype = dtype or onp.result_type(start, stop)
-  elif nargs == 3:
-    start, stop, step = args
-    dtype = dtype or onp.result_type(start, stop, step)
-  elif nargs == 4:
-    start, stop, step, dtype = args
-    dtype = dtype or onp.result_type(start, stop, step)
-
-  size = (stop - start - 1) // step + 1
-  return start + step * lax.iota(dtype, size)
+    if onp.issubdtype(dtype, onp.integer):
+      return lax.iota(dtype, stop)  # avoids materializing
+  return onp.arange(*args, **kwargs)
 
 
 @_wraps(onp.repeat)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,6a9952a939dec83c977508c596093ae4db17308a,b781e4509ab4e617f5673ae45f47ff5697017f88,"jax.numpy.arange should fall back to onp.arange

fixes #145","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 52aa98996..e7b80ef0d 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -845,6 +845,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     ans = onp.mean(x)
     self.assertAllClose(ans, onp.array([1./3, 1./3, 1./3]), check_dtypes=False)
 
+  # TODO(mattjj): more exhaustive arange tests
+  def testArangeOnFloats(self):
+    # from https://github.com/google/jax/issues/145
+    expected = onp.arange(0.0, 1.0, 0.1)
+    ans = lnp.arange(0.0, 1.0, 0.1)
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 52aa98996..e7b80ef0d 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -845,6 +845,13 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     ans = onp.mean(x)
     self.assertAllClose(ans, onp.array([1./3, 1./3, 1./3]), check_dtypes=False)
 
+  # TODO(mattjj): more exhaustive arange tests
+  def testArangeOnFloats(self):
+    # from https://github.com/google/jax/issues/145
+    expected = onp.arange(0.0, 1.0, 0.1)
+    ans = lnp.arange(0.0, 1.0, 0.1)
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
setup.py,setup.py,ae4bb8d34665e097a803d902913c189d5c09ab78,6a9952a939dec83c977508c596093ae4db17308a,bump version number for pypi,"diff --git a/setup.py b/setup.py
index 582aed3f5..e3d98c954 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.12',
+    version='0.1.13',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index 582aed3f5..e3d98c954 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.12',
+    version='0.1.13',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,dc1d0c260a4df15bd857faf00e72f4cfbab26b61,ae4bb8d34665e097a803d902913c189d5c09ab78,always fall back to onp.arange for now,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f9bfa8318..2cb34e79c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -835,14 +835,15 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
   # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy
-  dtype = kwargs.pop(""dtype"", None)
-  if not args:
-    raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
-  elif len(args) == 1 and not kwargs:
-    stop, = args
-    dtype = dtype or _dtype(stop)
-    if onp.issubdtype(dtype, onp.integer):
-      return lax.iota(dtype, stop)  # avoids materializing
+  # TODO(mattjj): add tests for this function, then re-enable
+  # dtype = kwargs.pop(""dtype"", None)
+  # if not args:
+  #   raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
+  # elif len(args) == 1 and not kwargs:
+  #   stop, = args
+  #   dtype = dtype or _dtype(stop)
+  #   if onp.issubdtype(dtype, onp.integer):
+  #     return lax.iota(dtype, stop)  # avoids materializing
   return onp.arange(*args, **kwargs)
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f9bfa8318..2cb34e79c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -835,14 +835,15 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
   # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy
-  dtype = kwargs.pop(""dtype"", None)
-  if not args:
-    raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
-  elif len(args) == 1 and not kwargs:
-    stop, = args
-    dtype = dtype or _dtype(stop)
-    if onp.issubdtype(dtype, onp.integer):
-      return lax.iota(dtype, stop)  # avoids materializing
+  # TODO(mattjj): add tests for this function, then re-enable
+  # dtype = kwargs.pop(""dtype"", None)
+  # if not args:
+  #   raise TypeError(""Required argument 'start' (pos 1) not found"")  # same as numpy error
+  # elif len(args) == 1 and not kwargs:
+  #   stop, = args
+  #   dtype = dtype or _dtype(stop)
+  #   if onp.issubdtype(dtype, onp.integer):
+  #     return lax.iota(dtype, stop)  # avoids materializing
   return onp.arange(*args, **kwargs)
 
 ",No
jax/lax.py,jax/lax.py,43e77acca5e7621fba68c4a5c5dba2494e58b9ff,70866f4d77a718d7f0b034e55aa88aca64d4b4ea,fix select transpose rule,"diff --git a/jax/lax.py b/jax/lax.py
index 7409a5fca..d22f3f9a8 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1602,10 +1602,15 @@ def select_dtype_rule(pred, on_true, on_false):
 
 def select_transpose_rule(t, pred, on_true, on_false):
   assert pred is not None
-  zeros = full_like(t, 0)
-  return [None,
-          select(pred, t, zeros) if on_true is None else None,
-          select(pred, zeros, t) if on_false is None else None]
+  if t is ad_util.zero:
+    return [None,
+            ad_util.zero if on_true is None else None,
+            ad_util.zero if on_false is None else None]
+  else:
+    zeros = full_like(t, 0)
+    return [None,
+            select(pred, t, zeros) if on_true is None else None,
+            select(pred, zeros, t) if on_false is None else None]
 
 def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
   oprand, on_true, on_false, = batched_args","diff --git a/jax/lax.py b/jax/lax.py
index 7409a5fca..d22f3f9a8 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1602,10 +1602,15 @@ def select_dtype_rule(pred, on_true, on_false):
 
 def select_transpose_rule(t, pred, on_true, on_false):
   assert pred is not None
-  zeros = full_like(t, 0)
-  return [None,
-          select(pred, t, zeros) if on_true is None else None,
-          select(pred, zeros, t) if on_false is None else None]
+  if t is ad_util.zero:
+    return [None,
+            ad_util.zero if on_true is None else None,
+            ad_util.zero if on_false is None else None]
+  else:
+    zeros = full_like(t, 0)
+    return [None,
+            select(pred, t, zeros) if on_true is None else None,
+            select(pred, zeros, t) if on_false is None else None]
 
 def select_batch_rule(batched_args, batch_dims, **unused_kwargs):
   oprand, on_true, on_false, = batched_args",No
setup.py,setup.py,9c6f4915e33cbcd32f6ce3d5b4a785c999358b9f,43e77acca5e7621fba68c4a5c5dba2494e58b9ff,bump version number for pypi,"diff --git a/setup.py b/setup.py
index e3d98c954..f7cbef252 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.13',
+    version='0.1.14',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index e3d98c954..f7cbef252 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.13',
+    version='0.1.14',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,166f45bf2b3e4b88e8cd1babbb7c242680304f89,9c6f4915e33cbcd32f6ce3d5b4a785c999358b9f,"add tests for cases tf.einsum doesn't handle

from https://www.tensorflow.org/api_docs/python/tf/einsum
one currently fails","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 39cca5e63..b89fd56f1 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -38,136 +38,186 @@ def check(s, *ops):
 class EinsumTest(jtu.JaxTestCase):
 
   def test_three_operands_1(self):
-    x = rng().randn(3)
-    y = rng().randn(4)
-    z = rng().randn(5)
+    r = rng()
+    x = r.randn(3)
+    y = r.randn(4)
+    z = r.randn(5)
     s = 'i,j,k->ijk'
     check(s, x, y, z)
 
   def test_three_operands_2(self):
-    x = rng().randn(3)
-    y = rng().randn(4)
-    z = rng().randn(5)
+    r = rng()
+    x = r.randn(3)
+    y = r.randn(4)
+    z = r.randn(5)
     s = 'i,j,k->ijk'
     check(s, x, y, z)
 
   def test_two_operands_1(self):
-    x = rng().randn(3, 4)
-    y = rng().randn(4)
+    r = rng()
+    x = r.randn(3, 4)
+    y = r.randn(4)
     s = 'ij,j->i'
     check(s, x, y)
 
   def test_two_operands_2(self):
-    x = rng().randn(3, 4, 5)
-    y = rng().randn(4)
+    r = rng()
+    x = r.randn(3, 4, 5)
+    y = r.randn(4)
     s = 'ijk,j->i'
     check(s, x, y)
 
   def test_two_operands_3(self):
-    x = rng().randn(3, 4, 3)
-    y = rng().randn(3)
+    r = rng()
+    x = r.randn(3, 4, 3)
+    y = r.randn(3)
     s = 'iji,i->j'
     check(s, x, y)
 
   def test_two_operands_4(self):
-    x = rng().randn(3, 4)
-    y = rng().randn(3, 4)
+    r = rng()
+    x = r.randn(3, 4)
+    y = r.randn(3, 4)
     s = 'ij,ij->'
     check(s, x, y)
 
   def test_two_operands_5(self):
-    x = rng().randn(10, 2, 3)
-    y = rng().randn(3, 4)
+    r = rng()
+    x = r.randn(10, 2, 3)
+    y = r.randn(3, 4)
     s = 'nij,jk->nik'
     check(s, x, y)
 
   def test_two_operands_6(self):
     # based on https://github.com/google/jax/issues/37#issuecomment-448572187
-    x = rng().randn(2, 1)
-    y = rng().randn(2, 3, 4)
+    r = rng()
+    x = r.randn(2, 1)
+    y = r.randn(2, 3, 4)
     s = 'sa,shb->shab'
     check(s, x, y)
 
   def test_one_operand_1(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->j'
     check(s, x)
 
   def test_one_operand_2(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->kij'
     check(s, x)
 
   def test_one_operand_3(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->ki'
     check(s, x)
 
   def test_one_operand_4(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->ki'
     check(s, x)
 
   def test_one_operand_5(self):
-    x = rng().randn(2, 3, 4, 5)
+    r = rng()
+    x = r.randn(2, 3, 4, 5)
     s = '...ijk->...ki'
     check(s, x)
 
   def test_one_operand_6(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = '...ijk->ki'
     check(s, x)
 
   def test_one_operand_7(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ii->'
     check(s, x)
 
   def test_one_operand_8(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ij->'
     check(s, x)
 
   def test_one_operand_9(self):
-    x = rng().randn(3, 3, 3)
+    r = rng()
+    x = r.randn(3, 3, 3)
     s = 'iii->'
     check(s, x)
 
   def test_one_operand_10(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ii->i'
     check(s, x)
 
   def test_one_operand_11(self):
-    x = rng().randn(3, 3, 4)
+    r = rng()
+    x = r.randn(3, 3, 4)
     s = 'iij->i'
     check(s, x)
 
   def test_one_operand_12(self):
-    x = rng().randn(3, 3, 3)
+    r = rng()
+    x = r.randn(3, 3, 3)
     s = 'iii->i'
     check(s, x)
 
   def test_one_operand_13(self):
-    x = rng().randn(3, 3, 5, 4, 4)
+    r = rng()
+    x = r.randn(3, 3, 5, 4, 4)
     s = 'iijkk->i'
     check(s, x)
 
   def test_one_operand_14(self):
-    x = rng().randn(3, 3, 5, 4, 4)
+    r = rng()
+    x = r.randn(3, 3, 5, 4, 4)
     s = 'iijkk->ik'
     check(s, x)
 
   def test_one_operand_15(self):
-    x = rng().randn(3, 3, 5, 4, 4)
+    r = rng()
+    x = r.randn(3, 3, 5, 4, 4)
     s = 'iijkl->il'
     check(s, x)
 
   def test_one_operand_16(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ij->ij'
     check(s, x)
 
+  # TODO(mattjj): patch this up!
+  # def test_tf_unsupported_1(self):
+  #   # from https://www.tensorflow.org/api_docs/python/tf/einsum
+  #   r = rng()
+  #   x = r.randn(2, 3, 5, 1)
+  #   y = r.randn(3, 4, 5, 1)
+  #   s = 'ij...,jk...->ik...'
+  #   check(s, x, y)
+
+  def test_tf_unsupported_2(self):
+    # from https://www.tensorflow.org/api_docs/python/tf/einsum
+    r = rng()
+    x = r.randn(2, 3, 3)
+    y = r.randn(4)
+    s = 'ijj,k->ik'
+    check(s, x, y)
+
+  def test_tf_unsupported_3(self):
+    # from https://www.tensorflow.org/api_docs/python/tf/einsum
+    r = rng()
+    x = r.randn(2, 3)
+    y = r.randn(2, 3)
+    z = r.randn(3, 4)
+    s = 'ij,ij,jk->ik'
+    check(s, x, y, z)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 39cca5e63..b89fd56f1 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -38,136 +38,186 @@ def check(s, *ops):
 class EinsumTest(jtu.JaxTestCase):
 
   def test_three_operands_1(self):
-    x = rng().randn(3)
-    y = rng().randn(4)
-    z = rng().randn(5)
+    r = rng()
+    x = r.randn(3)
+    y = r.randn(4)
+    z = r.randn(5)
     s = 'i,j,k->ijk'
     check(s, x, y, z)
 
   def test_three_operands_2(self):
-    x = rng().randn(3)
-    y = rng().randn(4)
-    z = rng().randn(5)
+    r = rng()
+    x = r.randn(3)
+    y = r.randn(4)
+    z = r.randn(5)
     s = 'i,j,k->ijk'
     check(s, x, y, z)
 
   def test_two_operands_1(self):
-    x = rng().randn(3, 4)
-    y = rng().randn(4)
+    r = rng()
+    x = r.randn(3, 4)
+    y = r.randn(4)
     s = 'ij,j->i'
     check(s, x, y)
 
   def test_two_operands_2(self):
-    x = rng().randn(3, 4, 5)
-    y = rng().randn(4)
+    r = rng()
+    x = r.randn(3, 4, 5)
+    y = r.randn(4)
     s = 'ijk,j->i'
     check(s, x, y)
 
   def test_two_operands_3(self):
-    x = rng().randn(3, 4, 3)
-    y = rng().randn(3)
+    r = rng()
+    x = r.randn(3, 4, 3)
+    y = r.randn(3)
     s = 'iji,i->j'
     check(s, x, y)
 
   def test_two_operands_4(self):
-    x = rng().randn(3, 4)
-    y = rng().randn(3, 4)
+    r = rng()
+    x = r.randn(3, 4)
+    y = r.randn(3, 4)
     s = 'ij,ij->'
     check(s, x, y)
 
   def test_two_operands_5(self):
-    x = rng().randn(10, 2, 3)
-    y = rng().randn(3, 4)
+    r = rng()
+    x = r.randn(10, 2, 3)
+    y = r.randn(3, 4)
     s = 'nij,jk->nik'
     check(s, x, y)
 
   def test_two_operands_6(self):
     # based on https://github.com/google/jax/issues/37#issuecomment-448572187
-    x = rng().randn(2, 1)
-    y = rng().randn(2, 3, 4)
+    r = rng()
+    x = r.randn(2, 1)
+    y = r.randn(2, 3, 4)
     s = 'sa,shb->shab'
     check(s, x, y)
 
   def test_one_operand_1(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->j'
     check(s, x)
 
   def test_one_operand_2(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->kij'
     check(s, x)
 
   def test_one_operand_3(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->ki'
     check(s, x)
 
   def test_one_operand_4(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = 'ijk->ki'
     check(s, x)
 
   def test_one_operand_5(self):
-    x = rng().randn(2, 3, 4, 5)
+    r = rng()
+    x = r.randn(2, 3, 4, 5)
     s = '...ijk->...ki'
     check(s, x)
 
   def test_one_operand_6(self):
-    x = rng().randn(3, 4, 5)
+    r = rng()
+    x = r.randn(3, 4, 5)
     s = '...ijk->ki'
     check(s, x)
 
   def test_one_operand_7(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ii->'
     check(s, x)
 
   def test_one_operand_8(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ij->'
     check(s, x)
 
   def test_one_operand_9(self):
-    x = rng().randn(3, 3, 3)
+    r = rng()
+    x = r.randn(3, 3, 3)
     s = 'iii->'
     check(s, x)
 
   def test_one_operand_10(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ii->i'
     check(s, x)
 
   def test_one_operand_11(self):
-    x = rng().randn(3, 3, 4)
+    r = rng()
+    x = r.randn(3, 3, 4)
     s = 'iij->i'
     check(s, x)
 
   def test_one_operand_12(self):
-    x = rng().randn(3, 3, 3)
+    r = rng()
+    x = r.randn(3, 3, 3)
     s = 'iii->i'
     check(s, x)
 
   def test_one_operand_13(self):
-    x = rng().randn(3, 3, 5, 4, 4)
+    r = rng()
+    x = r.randn(3, 3, 5, 4, 4)
     s = 'iijkk->i'
     check(s, x)
 
   def test_one_operand_14(self):
-    x = rng().randn(3, 3, 5, 4, 4)
+    r = rng()
+    x = r.randn(3, 3, 5, 4, 4)
     s = 'iijkk->ik'
     check(s, x)
 
   def test_one_operand_15(self):
-    x = rng().randn(3, 3, 5, 4, 4)
+    r = rng()
+    x = r.randn(3, 3, 5, 4, 4)
     s = 'iijkl->il'
     check(s, x)
 
   def test_one_operand_16(self):
-    x = rng().randn(3, 3)
+    r = rng()
+    x = r.randn(3, 3)
     s = 'ij->ij'
     check(s, x)
 
+  # TODO(mattjj): patch this up!
+  # def test_tf_unsupported_1(self):
+  #   # from https://www.tensorflow.org/api_docs/python/tf/einsum
+  #   r = rng()
+  #   x = r.randn(2, 3, 5, 1)
+  #   y = r.randn(3, 4, 5, 1)
+  #   s = 'ij...,jk...->ik...'
+  #   check(s, x, y)
+
+  def test_tf_unsupported_2(self):
+    # from https://www.tensorflow.org/api_docs/python/tf/einsum
+    r = rng()
+    x = r.randn(2, 3, 3)
+    y = r.randn(4)
+    s = 'ijj,k->ik'
+    check(s, x, y)
+
+  def test_tf_unsupported_3(self):
+    # from https://www.tensorflow.org/api_docs/python/tf/einsum
+    r = rng()
+    x = r.randn(2, 3)
+    y = r.randn(2, 3)
+    z = r.randn(3, 4)
+    s = 'ij,ij,jk->ik'
+    check(s, x, y, z)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,9a68bce567b13ebf0ccea4c9bdd44294c66850e2,166f45bf2b3e4b88e8cd1babbb7c242680304f89,add comment marking a bug,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 2cb34e79c..ace8f6b1d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1082,7 +1082,7 @@ def _einsum(operands, contractions):
     input_names = input_str.split(',')
 
     # switch on the number of operands to be processed in this loop iteration.
-    # every case here sets 'result' and 'names'.
+    # every case here sets 'operand' and 'names'.
     if len(operand_indices) == 1:
       operand = operands.pop(operand_indices[0])
       names, = input_names
@@ -1120,7 +1120,7 @@ def _einsum(operands, contractions):
       lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
                                     for n in batch_names)
       if contracted_names:
-        # contract usint lax.dot_general
+        # contract using lax.dot_general
         lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
                                     for n in contracted_names)
 
@@ -1130,6 +1130,7 @@ def _einsum(operands, contractions):
           lhs = moveaxis(lhs, lhs_batch, batch_dims)
           rhs = moveaxis(rhs, rhs_batch, batch_dims)
           batch_names = ''.join(batch_names)
+          # TODO(mattjj): may need to update lhs_cont and rhs_cont here
         else:
           batch_dims = tuple(lhs_batch)
           batch_names = ''.join(lhs_names[i] for i in batch_dims)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 2cb34e79c..ace8f6b1d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1082,7 +1082,7 @@ def _einsum(operands, contractions):
     input_names = input_str.split(',')
 
     # switch on the number of operands to be processed in this loop iteration.
-    # every case here sets 'result' and 'names'.
+    # every case here sets 'operand' and 'names'.
     if len(operand_indices) == 1:
       operand = operands.pop(operand_indices[0])
       names, = input_names
@@ -1120,7 +1120,7 @@ def _einsum(operands, contractions):
       lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
                                     for n in batch_names)
       if contracted_names:
-        # contract usint lax.dot_general
+        # contract using lax.dot_general
         lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
                                     for n in contracted_names)
 
@@ -1130,6 +1130,7 @@ def _einsum(operands, contractions):
           lhs = moveaxis(lhs, lhs_batch, batch_dims)
           rhs = moveaxis(rhs, rhs_batch, batch_dims)
           batch_names = ''.join(batch_names)
+          # TODO(mattjj): may need to update lhs_cont and rhs_cont here
         else:
           batch_dims = tuple(lhs_batch)
           batch_names = ''.join(lhs_names[i] for i in batch_dims)",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,9a68bce567b13ebf0ccea4c9bdd44294c66850e2,166f45bf2b3e4b88e8cd1babbb7c242680304f89,add comment marking a bug,"diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index b89fd56f1..f4f7a221f 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -192,14 +192,13 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij->ij'
     check(s, x)
 
-  # TODO(mattjj): patch this up!
-  # def test_tf_unsupported_1(self):
-  #   # from https://www.tensorflow.org/api_docs/python/tf/einsum
-  #   r = rng()
-  #   x = r.randn(2, 3, 5, 1)
-  #   y = r.randn(3, 4, 5, 1)
-  #   s = 'ij...,jk...->ik...'
-  #   check(s, x, y)
+  def test_tf_unsupported_1(self):
+    # from https://www.tensorflow.org/api_docs/python/tf/einsum
+    r = rng()
+    x = r.randn(2, 3, 5, 1)
+    y = r.randn(3, 4, 5, 1)
+    s = 'ij...,jk...->ik...'
+    check(s, x, y)
 
   def test_tf_unsupported_2(self):
     # from https://www.tensorflow.org/api_docs/python/tf/einsum","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index b89fd56f1..f4f7a221f 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -192,14 +192,13 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij->ij'
     check(s, x)
 
-  # TODO(mattjj): patch this up!
-  # def test_tf_unsupported_1(self):
-  #   # from https://www.tensorflow.org/api_docs/python/tf/einsum
-  #   r = rng()
-  #   x = r.randn(2, 3, 5, 1)
-  #   y = r.randn(3, 4, 5, 1)
-  #   s = 'ij...,jk...->ik...'
-  #   check(s, x, y)
+  def test_tf_unsupported_1(self):
+    # from https://www.tensorflow.org/api_docs/python/tf/einsum
+    r = rng()
+    x = r.randn(2, 3, 5, 1)
+    y = r.randn(3, 4, 5, 1)
+    s = 'ij...,jk...->ik...'
+    check(s, x, y)
 
   def test_tf_unsupported_2(self):
     # from https://www.tensorflow.org/api_docs/python/tf/einsum",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,9c722db37304f293180119501c355d1d35431e16,9a68bce567b13ebf0ccea4c9bdd44294c66850e2,einsum: update id strings after moving batch dims,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ace8f6b1d..3b6549e66 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1121,20 +1121,21 @@ def _einsum(operands, contractions):
                                     for n in batch_names)
       if contracted_names:
         # contract using lax.dot_general
-        lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
-                                    for n in contracted_names)
 
         # lax.dot_general batch dims have to precede non-batch dims
         batch_dims = tuple(range(len(batch_names)))
         if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
           lhs = moveaxis(lhs, lhs_batch, batch_dims)
+          lhs_names = _movechars(lhs_names, lhs_batch, batch_dims)
           rhs = moveaxis(rhs, rhs_batch, batch_dims)
+          rhs_names = _movechars(rhs_names, rhs_batch, batch_dims)
           batch_names = ''.join(batch_names)
-          # TODO(mattjj): may need to update lhs_cont and rhs_cont here
         else:
           batch_dims = tuple(lhs_batch)
           batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
+        lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
+                                    for n in contracted_names)
         operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
         deleted_names = batch_names + ''.join(contracted_names)
         names = (batch_names + removechars(lhs_names, deleted_names)
@@ -1212,6 +1213,15 @@ def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
     result = lax.dot_general(lhs, rhs, dimension_numbers)
     return lax.reshape(result, result_shape)
 
+
+def _movechars(s, src, dst):
+  """"""Helper for einsum string munging, like moveaxis on identifier strings.""""""
+  chars = [c for i, c in enumerate(s) if i not in src]
+  for i, j in sorted(zip(dst, src)):
+    chars.insert(i, s[j])
+  return ''.join(chars)
+
+
 @_wraps(onp.inner)
 def inner(a, b):
   if ndim(a) == 0 or ndim(b) == 0:","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ace8f6b1d..3b6549e66 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1121,20 +1121,21 @@ def _einsum(operands, contractions):
                                     for n in batch_names)
       if contracted_names:
         # contract using lax.dot_general
-        lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
-                                    for n in contracted_names)
 
         # lax.dot_general batch dims have to precede non-batch dims
         batch_dims = tuple(range(len(batch_names)))
         if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
           lhs = moveaxis(lhs, lhs_batch, batch_dims)
+          lhs_names = _movechars(lhs_names, lhs_batch, batch_dims)
           rhs = moveaxis(rhs, rhs_batch, batch_dims)
+          rhs_names = _movechars(rhs_names, rhs_batch, batch_dims)
           batch_names = ''.join(batch_names)
-          # TODO(mattjj): may need to update lhs_cont and rhs_cont here
         else:
           batch_dims = tuple(lhs_batch)
           batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
+        lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
+                                    for n in contracted_names)
         operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
         deleted_names = batch_names + ''.join(contracted_names)
         names = (batch_names + removechars(lhs_names, deleted_names)
@@ -1212,6 +1213,15 @@ def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
     result = lax.dot_general(lhs, rhs, dimension_numbers)
     return lax.reshape(result, result_shape)
 
+
+def _movechars(s, src, dst):
+  """"""Helper for einsum string munging, like moveaxis on identifier strings.""""""
+  chars = [c for i, c in enumerate(s) if i not in src]
+  for i, j in sorted(zip(dst, src)):
+    chars.insert(i, s[j])
+  return ''.join(chars)
+
+
 @_wraps(onp.inner)
 def inner(a, b):
   if ndim(a) == 0 or ndim(b) == 0:",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,8ba29c2dfc828df091cfafcb9ded19991f2b0a47,9c722db37304f293180119501c355d1d35431e16,"einsum test cases from dask (thanks @sjperkins)

The new cases are based on these test cases from dask:
https://github.com/dask/dask/pull/3412","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index f4f7a221f..9483ca0d8 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -16,8 +16,12 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from collections import defaultdict
+import itertools
+
 import numpy as onp
 from absl.testing import absltest
+from absl.testing import parameterized
 
 import jax.numpy as np
 import jax.test_util as jtu
@@ -217,6 +221,54 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij,ij,jk->ik'
     check(s, x, y, z)
 
+  # these tests are based on https://github.com/dask/dask/pull/3412/files
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(einstr), ""einstr"": einstr}
+      for einstr in [
+          'abc,bad->abcd',
+          'abcdef,bcdfg->abcdeg',
+          'ea,fb,abcd,gc,hd->efgh',
+          'ab,b',
+          'aa',
+          'a,a->',
+          'a,a->a',
+          'a,a',
+          'a,b',
+          'a,b,c',
+          'a',
+          'ba,b',
+          'ba,b->',
+          'defab,fedbc->defac',
+          'ab...,bc...->ac...',
+          'a...a',
+          'abc...->cba...',
+          '...ab->...a',
+          'a...a->a...',
+          # Following 2 from # https://stackoverflow.com/a/19203475/1611416
+          # '...abc,...abcd->...d',  # TODO hard crash
+          'ab...,b->ab...',
+          # https://github.com/dask/dask/pull/3412#discussion_r182413444
+          'aa->a',
+          'ab,ab,c->c',
+          'aab,bc->ac',
+          'aab,bcc->ac',
+          # 'fdf,cdd,ccd,afe->ae',  # TODO hard crash
+          'fff,fae,bef,def->abd',
+      ])
+  def test_from_dask(self, einstr):
+    r = rng()
+    if '->' in einstr:
+      input_str, result_names = einstr.split('->')
+    else:
+      input_str = einstr
+    input_names = input_str.split(',')
+
+    shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
+    input_shapes = [tuple(shapes[c] for c in names) for names in input_names]
+    operands = [r.randn(*shape) for shape in input_shapes]
+
+    check(einstr, *operands)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index f4f7a221f..9483ca0d8 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -16,8 +16,12 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from collections import defaultdict
+import itertools
+
 import numpy as onp
 from absl.testing import absltest
+from absl.testing import parameterized
 
 import jax.numpy as np
 import jax.test_util as jtu
@@ -217,6 +221,54 @@ class EinsumTest(jtu.JaxTestCase):
     s = 'ij,ij,jk->ik'
     check(s, x, y, z)
 
+  # these tests are based on https://github.com/dask/dask/pull/3412/files
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(einstr), ""einstr"": einstr}
+      for einstr in [
+          'abc,bad->abcd',
+          'abcdef,bcdfg->abcdeg',
+          'ea,fb,abcd,gc,hd->efgh',
+          'ab,b',
+          'aa',
+          'a,a->',
+          'a,a->a',
+          'a,a',
+          'a,b',
+          'a,b,c',
+          'a',
+          'ba,b',
+          'ba,b->',
+          'defab,fedbc->defac',
+          'ab...,bc...->ac...',
+          'a...a',
+          'abc...->cba...',
+          '...ab->...a',
+          'a...a->a...',
+          # Following 2 from # https://stackoverflow.com/a/19203475/1611416
+          # '...abc,...abcd->...d',  # TODO hard crash
+          'ab...,b->ab...',
+          # https://github.com/dask/dask/pull/3412#discussion_r182413444
+          'aa->a',
+          'ab,ab,c->c',
+          'aab,bc->ac',
+          'aab,bcc->ac',
+          # 'fdf,cdd,ccd,afe->ae',  # TODO hard crash
+          'fff,fae,bef,def->abd',
+      ])
+  def test_from_dask(self, einstr):
+    r = rng()
+    if '->' in einstr:
+      input_str, result_names = einstr.split('->')
+    else:
+      input_str = einstr
+    input_names = input_str.split(',')
+
+    shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
+    input_shapes = [tuple(shapes[c] for c in names) for names in input_names]
+    operands = [r.randn(*shape) for shape in input_shapes]
+
+    check(einstr, *operands)
+
 
 if __name__ == '__main__':
   absltest.main()",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,2b934dae22d3536d5ffbc717e0e085a5bd269262,8ba29c2dfc828df091cfafcb9ded19991f2b0a47,"einsum test cases from dask (thanks @sjperkins)

The new cases are based on these test cases from dask:
https://github.com/dask/dask/pull/3412","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 9483ca0d8..5e26eb78b 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -263,6 +263,7 @@ class EinsumTest(jtu.JaxTestCase):
       input_str = einstr
     input_names = input_str.split(',')
 
+    # TODO(mattjj): handle '...' better here
     shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
     input_shapes = [tuple(shapes[c] for c in names) for names in input_names]
     operands = [r.randn(*shape) for shape in input_shapes]","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 9483ca0d8..5e26eb78b 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -263,6 +263,7 @@ class EinsumTest(jtu.JaxTestCase):
       input_str = einstr
     input_names = input_str.split(',')
 
+    # TODO(mattjj): handle '...' better here
     shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
     input_shapes = [tuple(shapes[c] for c in names) for names in input_names]
     operands = [r.randn(*shape) for shape in input_shapes]",No
jaxlib/BUILD,jaxlib/BUILD,20935448b9ffb71e2f0ad1bf460dd388bf846078,9c6f4915e33cbcd32f6ce3d5b4a785c999358b9f,Whitespace fix to jaxlib/BUILD.,"diff --git a/jaxlib/BUILD b/jaxlib/BUILD
index 5b57a822c..cf16b5406 100644
--- a/jaxlib/BUILD
+++ b/jaxlib/BUILD
@@ -23,7 +23,5 @@ package(default_visibility = [""//visibility:public""])
 pyx_library(
     name = ""lapack"",
     srcs = [""lapack.pyx""],
-    py_deps = [
-        ""@org_tensorflow//third_party/py/numpy"",
-    ],
+    py_deps = [""@org_tensorflow//third_party/py/numpy""],
 )","diff --git a/jaxlib/BUILD b/jaxlib/BUILD
index 5b57a822c..cf16b5406 100644
--- a/jaxlib/BUILD
+++ b/jaxlib/BUILD
@@ -23,7 +23,5 @@ package(default_visibility = [""//visibility:public""])
 pyx_library(
     name = ""lapack"",
     srcs = [""lapack.pyx""],
-    py_deps = [
-        ""@org_tensorflow//third_party/py/numpy"",
-    ],
+    py_deps = [""@org_tensorflow//third_party/py/numpy""],
 )",No
jax/lax_linalg.py,jax/lax_linalg.py,e6b23dd2b7e78dd123b18392b774d7fc1e85dcb0,0a9ee106b382326ccf8373707f8577ba8a2afec9,Fixed triangular_solve_jvp_rule for transpose_a=True case,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 20eb50432..bf823e4cb 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -112,6 +112,7 @@ def triangular_solve_shape_rule(a, b, left_side=False, **unused_kwargs):
 def triangular_solve_jvp_rule_a(
     g_a, ans, a, b, left_side, lower, transpose_a, conjugate_a):
   g_a = lax.neg(g_a)
+  g_a = np.swapaxes(g_a, -1, -2) if transpose_a else g_a
   tmp = triangular_solve(a, g_a, left_side, lower, transpose_a, conjugate_a)
   dot = lax.dot if g_a.ndim == 2 else lax.batch_matmul
   if left_side:","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 20eb50432..bf823e4cb 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -112,6 +112,7 @@ def triangular_solve_shape_rule(a, b, left_side=False, **unused_kwargs):
 def triangular_solve_jvp_rule_a(
     g_a, ans, a, b, left_side, lower, transpose_a, conjugate_a):
   g_a = lax.neg(g_a)
+  g_a = np.swapaxes(g_a, -1, -2) if transpose_a else g_a
   tmp = triangular_solve(a, g_a, left_side, lower, transpose_a, conjugate_a)
   dot = lax.dot if g_a.ndim == 2 else lax.batch_matmul
   if left_side:",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6a138202ef1c32f935cb6ec66c33340d67266466,2b934dae22d3536d5ffbc717e0e085a5bd269262,fix several einsum bugs,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 3b6549e66..79c2b57c0 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1119,21 +1119,21 @@ def _einsum(operands, contractions):
       batch_names = set(lhs_names) & set(rhs_names) - contracted_names
       lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
                                     for n in batch_names)
-      if contracted_names:
-        # contract using lax.dot_general
 
-        # lax.dot_general batch dims have to precede non-batch dims
-        batch_dims = tuple(range(len(batch_names)))
-        if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
-          lhs = moveaxis(lhs, lhs_batch, batch_dims)
-          lhs_names = _movechars(lhs_names, lhs_batch, batch_dims)
-          rhs = moveaxis(rhs, rhs_batch, batch_dims)
-          rhs_names = _movechars(rhs_names, rhs_batch, batch_dims)
-          batch_names = ''.join(batch_names)
-        else:
-          batch_dims = tuple(lhs_batch)
-          batch_names = ''.join(lhs_names[i] for i in batch_dims)
+      # move batch dims to the front (required by lax.dot_general, and easier)
+      batch_dims = tuple(range(len(batch_names)))
+      if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
+        lhs = moveaxis(lhs, lhs_batch, batch_dims)
+        lhs_names = _movechars(lhs_names, lhs_batch, batch_dims)
+        rhs = moveaxis(rhs, rhs_batch, batch_dims)
+        rhs_names = _movechars(rhs_names, rhs_batch, batch_dims)
+        batch_names = ''.join(batch_names)
+      else:
+        batch_dims = tuple(lhs_batch)
+        batch_names = ''.join(lhs_names[i] for i in batch_dims)
 
+      if contracted_names:
+        # contract using lax.dot_general
         lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
                                     for n in contracted_names)
         operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
@@ -1142,23 +1142,15 @@ def _einsum(operands, contractions):
                  + removechars(rhs_names, deleted_names))
       else:
         # no contraction, just a tensor product
-        if lhs_batch != rhs_batch:
-          rhs = moveaxis(rhs, rhs_batch, lhs_batch)
-        batch_names = ''.join(lhs_names[i] for i in lhs_batch)
         nbatch = len(batch_names)
-
-        assert len(lhs_names) == lhs.ndim and len(rhs_names) == rhs.ndim
-        assert lhs_names.startswith(batch_names) and rhs_names.startswith(batch_names)
-
+        assert lhs.shape[:nbatch] == rhs.shape[:nbatch]
         names = batch_names + lhs_names[nbatch:] + rhs_names[nbatch:]
-        lhs_shape = iter(lhs.shape)
-        lhs_shape = [next(lhs_shape) if n in lhs_names else 1 for n in names]
-        rhs_shape = iter(rhs.shape)
-        rhs_shape = [next(rhs_shape) if n in rhs_names else 1 for n in names]
+        lhs_shape = lhs.shape + (1,) * (rhs.ndim - nbatch)
+        rhs_shape = rhs.shape[:nbatch] + (1,) * (lhs.ndim - nbatch) + rhs.shape[nbatch:]
         operand = lax.reshape(lhs, lhs_shape) * lax.reshape(rhs, rhs_shape)
 
     else:
-      raise NotImplementedError
+      raise NotImplementedError  # if this is actually reachable, open an issue!
 
     # the resulting 'operand' with axis labels 'names' should be a permutation
     # of the desired result
@@ -1190,10 +1182,12 @@ def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
     # contracting dimension, so if there's more than one we collapse them.
     if ncont > 1:
       lhs_cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
-      lhs = moveaxis(lhs, lhs_cont, lhs_cdims).reshape(lhs.shape[:-ncont] + (-1,))
+      lhs = moveaxis(lhs, lhs_cont, lhs_cdims)
+      lhs = lhs.reshape(lhs.shape[:-ncont] + (-1,))
 
       rhs_cdims = tuple(range(rhs.ndim - ncont, rhs.ndim))
-      rhs = moveaxis(rhs, rhs_cont, rhs_cdims).reshape(rhs.shape[:-ncont] + (-1,))
+      rhs = moveaxis(rhs, rhs_cont, rhs_cdims)
+      rhs = rhs.reshape(rhs.shape[:-ncont] + (-1,))
     else:
       lhs = moveaxis(lhs, lhs_cont[0], -1)
       rhs = moveaxis(rhs, rhs_cont[0], -1)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 3b6549e66..79c2b57c0 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1119,21 +1119,21 @@ def _einsum(operands, contractions):
       batch_names = set(lhs_names) & set(rhs_names) - contracted_names
       lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
                                     for n in batch_names)
+
+      # move batch dims to the front (required by lax.dot_general, and easier)
+      batch_dims = tuple(range(len(batch_names)))
+      if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
+        lhs = moveaxis(lhs, lhs_batch, batch_dims)
+        lhs_names = _movechars(lhs_names, lhs_batch, batch_dims)
+        rhs = moveaxis(rhs, rhs_batch, batch_dims)
+        rhs_names = _movechars(rhs_names, rhs_batch, batch_dims)
+        batch_names = ''.join(batch_names)
+      else:
+        batch_dims = tuple(lhs_batch)
+        batch_names = ''.join(lhs_names[i] for i in batch_dims)
+
       if contracted_names:
         # contract using lax.dot_general
-
-        # lax.dot_general batch dims have to precede non-batch dims
-        batch_dims = tuple(range(len(batch_names)))
-        if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):
-          lhs = moveaxis(lhs, lhs_batch, batch_dims)
-          lhs_names = _movechars(lhs_names, lhs_batch, batch_dims)
-          rhs = moveaxis(rhs, rhs_batch, batch_dims)
-          rhs_names = _movechars(rhs_names, rhs_batch, batch_dims)
-          batch_names = ''.join(batch_names)
-        else:
-          batch_dims = tuple(lhs_batch)
-          batch_names = ''.join(lhs_names[i] for i in batch_dims)
-
         lhs_cont, rhs_cont = unzip2((lhs_names.index(n), rhs_names.index(n))
                                     for n in contracted_names)
         operand = _dot_general(lhs, rhs, lhs_cont, rhs_cont, len(batch_dims))
@@ -1142,23 +1142,15 @@ def _einsum(operands, contractions):
                  + removechars(rhs_names, deleted_names))
       else:
         # no contraction, just a tensor product
-        if lhs_batch != rhs_batch:
-          rhs = moveaxis(rhs, rhs_batch, lhs_batch)
-        batch_names = ''.join(lhs_names[i] for i in lhs_batch)
         nbatch = len(batch_names)
-
-        assert len(lhs_names) == lhs.ndim and len(rhs_names) == rhs.ndim
-        assert lhs_names.startswith(batch_names) and rhs_names.startswith(batch_names)
-
+        assert lhs.shape[:nbatch] == rhs.shape[:nbatch]
         names = batch_names + lhs_names[nbatch:] + rhs_names[nbatch:]
-        lhs_shape = iter(lhs.shape)
-        lhs_shape = [next(lhs_shape) if n in lhs_names else 1 for n in names]
-        rhs_shape = iter(rhs.shape)
-        rhs_shape = [next(rhs_shape) if n in rhs_names else 1 for n in names]
+        lhs_shape = lhs.shape + (1,) * (rhs.ndim - nbatch)
+        rhs_shape = rhs.shape[:nbatch] + (1,) * (lhs.ndim - nbatch) + rhs.shape[nbatch:]
         operand = lax.reshape(lhs, lhs_shape) * lax.reshape(rhs, rhs_shape)
 
     else:
-      raise NotImplementedError
+      raise NotImplementedError  # if this is actually reachable, open an issue!
 
     # the resulting 'operand' with axis labels 'names' should be a permutation
     # of the desired result
@@ -1190,10 +1182,12 @@ def _dot_general(lhs, rhs, lhs_cont, rhs_cont, nbatch):
     # contracting dimension, so if there's more than one we collapse them.
     if ncont > 1:
       lhs_cdims = tuple(range(lhs.ndim - ncont, lhs.ndim))
-      lhs = moveaxis(lhs, lhs_cont, lhs_cdims).reshape(lhs.shape[:-ncont] + (-1,))
+      lhs = moveaxis(lhs, lhs_cont, lhs_cdims)
+      lhs = lhs.reshape(lhs.shape[:-ncont] + (-1,))
 
       rhs_cdims = tuple(range(rhs.ndim - ncont, rhs.ndim))
-      rhs = moveaxis(rhs, rhs_cont, rhs_cdims).reshape(rhs.shape[:-ncont] + (-1,))
+      rhs = moveaxis(rhs, rhs_cont, rhs_cdims)
+      rhs = rhs.reshape(rhs.shape[:-ncont] + (-1,))
     else:
       lhs = moveaxis(lhs, lhs_cont[0], -1)
       rhs = moveaxis(rhs, rhs_cont[0], -1)",Yes
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,6a138202ef1c32f935cb6ec66c33340d67266466,2b934dae22d3536d5ffbc717e0e085a5bd269262,fix several einsum bugs,"diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 5e26eb78b..c386993ae 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -263,9 +263,9 @@ class EinsumTest(jtu.JaxTestCase):
       input_str = einstr
     input_names = input_str.split(',')
 
-    # TODO(mattjj): handle '...' better here
     shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
-    input_shapes = [tuple(shapes[c] for c in names) for names in input_names]
+    input_shapes = [tuple(shapes[c] for c in names.replace('...', '01'))
+                    for names in input_names]
     operands = [r.randn(*shape) for shape in input_shapes]
 
     check(einstr, *operands)","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index 5e26eb78b..c386993ae 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -263,9 +263,9 @@ class EinsumTest(jtu.JaxTestCase):
       input_str = einstr
     input_names = input_str.split(',')
 
-    # TODO(mattjj): handle '...' better here
     shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
-    input_shapes = [tuple(shapes[c] for c in names) for names in input_names]
+    input_shapes = [tuple(shapes[c] for c in names.replace('...', '01'))
+                    for names in input_names]
     operands = [r.randn(*shape) for shape in input_shapes]
 
     check(einstr, *operands)",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6bb9609fb8fd36490557dda3de4da486f3c4430a,6a138202ef1c32f935cb6ec66c33340d67266466,"disable test, py3 opt_einsum nondeterministic bug?","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 79c2b57c0..c43eb34bd 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1116,10 +1116,16 @@ def _einsum(operands, contractions):
                                    result_names + lhs_names)
 
       contracted_names = contracted_names & (set(lhs_names) | set(rhs_names))
-      batch_names = set(lhs_names) & set(rhs_names) - contracted_names
+      batch_names = (set(lhs_names) & set(rhs_names)) - contracted_names
       lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
                                     for n in batch_names)
 
+      # NOTE(mattjj): this can fail non-deterministically in python3, maybe
+      # due to opt_einsum
+      assert _all(name in lhs_names and name in rhs_names and
+                  lhs.shape[lhs_names.index(name)] == rhs.shape[rhs_names.index(name)]
+                  for name in contracted_names)
+
       # move batch dims to the front (required by lax.dot_general, and easier)
       batch_dims = tuple(range(len(batch_names)))
       if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 79c2b57c0..c43eb34bd 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1116,10 +1116,16 @@ def _einsum(operands, contractions):
                                    result_names + lhs_names)
 
       contracted_names = contracted_names & (set(lhs_names) | set(rhs_names))
-      batch_names = set(lhs_names) & set(rhs_names) - contracted_names
+      batch_names = (set(lhs_names) & set(rhs_names)) - contracted_names
       lhs_batch, rhs_batch = unzip2((lhs_names.find(n), rhs_names.find(n))
                                     for n in batch_names)
 
+      # NOTE(mattjj): this can fail non-deterministically in python3, maybe
+      # due to opt_einsum
+      assert _all(name in lhs_names and name in rhs_names and
+                  lhs.shape[lhs_names.index(name)] == rhs.shape[rhs_names.index(name)]
+                  for name in contracted_names)
+
       # move batch dims to the front (required by lax.dot_general, and easier)
       batch_dims = tuple(range(len(batch_names)))
       if lhs_batch != rhs_batch or set(lhs_batch) != set(batch_dims):",No
tests/lax_numpy_einsum_test.py,tests/lax_numpy_einsum_test.py,6bb9609fb8fd36490557dda3de4da486f3c4430a,6a138202ef1c32f935cb6ec66c33340d67266466,"disable test, py3 opt_einsum nondeterministic bug?","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index c386993ae..0aec7a1f2 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -22,6 +22,7 @@ import itertools
 import numpy as onp
 from absl.testing import absltest
 from absl.testing import parameterized
+import six
 
 import jax.numpy as np
 import jax.test_util as jtu
@@ -213,6 +214,10 @@ class EinsumTest(jtu.JaxTestCase):
     check(s, x, y)
 
   def test_tf_unsupported_3(self):
+    # TODO(mattjj): heisenbug! fails sometimes in python3. opt_einsum bug?
+    if six.PY3:
+      return absltest.unittest.skip(""py3 failures"")
+
     # from https://www.tensorflow.org/api_docs/python/tf/einsum
     r = rng()
     x = r.randn(2, 3)
@@ -263,7 +268,8 @@ class EinsumTest(jtu.JaxTestCase):
       input_str = einstr
     input_names = input_str.split(',')
 
-    shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
+    dims = itertools.cycle([2, 3, 4])
+    shapes = defaultdict(lambda: next(dims))
     input_shapes = [tuple(shapes[c] for c in names.replace('...', '01'))
                     for names in input_names]
     operands = [r.randn(*shape) for shape in input_shapes]","diff --git a/tests/lax_numpy_einsum_test.py b/tests/lax_numpy_einsum_test.py
index c386993ae..0aec7a1f2 100644
--- a/tests/lax_numpy_einsum_test.py
+++ b/tests/lax_numpy_einsum_test.py
@@ -22,6 +22,7 @@ import itertools
 import numpy as onp
 from absl.testing import absltest
 from absl.testing import parameterized
+import six
 
 import jax.numpy as np
 import jax.test_util as jtu
@@ -213,6 +214,10 @@ class EinsumTest(jtu.JaxTestCase):
     check(s, x, y)
 
   def test_tf_unsupported_3(self):
+    # TODO(mattjj): heisenbug! fails sometimes in python3. opt_einsum bug?
+    if six.PY3:
+      return absltest.unittest.skip(""py3 failures"")
+
     # from https://www.tensorflow.org/api_docs/python/tf/einsum
     r = rng()
     x = r.randn(2, 3)
@@ -263,7 +268,8 @@ class EinsumTest(jtu.JaxTestCase):
       input_str = einstr
     input_names = input_str.split(',')
 
-    shapes = defaultdict(itertools.cycle([2, 3, 4]).next)
+    dims = itertools.cycle([2, 3, 4])
+    shapes = defaultdict(lambda: next(dims))
     input_shapes = [tuple(shapes[c] for c in names.replace('...', '01'))
                     for names in input_names]
     operands = [r.randn(*shape) for shape in input_shapes]",No
tests/linalg_test.py,tests/linalg_test.py,72bc882925352f4f5ec02feec8b735eab3b1dc6b,449da4cdb59872ec7464f20bfb20863a217cd98d,lower tolerance on triangular-solve test for x64 mode,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2f3a70b71..1cc6a23cd 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -199,12 +199,13 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   def testSolveTriangularBlockedGrad(self, lower, transpose_a, lhs_shape,
                                      rhs_shape, dtype, rng):
-    A = np.tril(rng(lhs_shape, dtype) + 10 * onp.eye(lhs_shape[-1], dtype=dtype))
+    # TODO(frostig): change ensemble to support a bigger rtol
+    A = np.tril(rng(lhs_shape, dtype) + 5 * onp.eye(lhs_shape[-1], dtype=dtype))
     A = A if lower else T(A)
     B = rng(rhs_shape, dtype)
     f = partial(scipy.linalg.solve_triangular, lower=lower,
                 trans=1 if transpose_a else 0)
-    jtu.check_grads(f, (A, B), 2)
+    jtu.check_grads(f, (A, B), 2, rtol=1e-3)
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2f3a70b71..1cc6a23cd 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -199,12 +199,13 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   def testSolveTriangularBlockedGrad(self, lower, transpose_a, lhs_shape,
                                      rhs_shape, dtype, rng):
-    A = np.tril(rng(lhs_shape, dtype) + 10 * onp.eye(lhs_shape[-1], dtype=dtype))
+    # TODO(frostig): change ensemble to support a bigger rtol
+    A = np.tril(rng(lhs_shape, dtype) + 5 * onp.eye(lhs_shape[-1], dtype=dtype))
     A = A if lower else T(A)
     B = rng(rhs_shape, dtype)
     f = partial(scipy.linalg.solve_triangular, lower=lower,
                 trans=1 if transpose_a else 0)
-    jtu.check_grads(f, (A, B), 2)
+    jtu.check_grads(f, (A, B), 2, rtol=1e-3)
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,95135377d0ed3d3954a44a650ad2580ded61037f,72bc882925352f4f5ec02feec8b735eab3b1dc6b,"Add implementations of np.{meshgrid,linspace,logspace,geomspace,diag_indices} that forward to the usual numpy implementation.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index c43eb34bd..30fc61ba2 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -88,13 +88,20 @@ ndim = _ndim = onp.ndim
 size = onp.size
 _dtype = lax._dtype
 
+bool_ = onp.bool_
+uint8 = onp.uint8
+uint16 = onp.uint16
 uint32 = onp.uint32
-int32 = onp.int32
 uint64 = onp.uint64
+int8 = onp.int8
+int16 = onp.int16
+int32 = onp.int32
 int64 = onp.int64
+float16 = onp.float16
 float32 = onp.float32
 float64 = onp.float64
 complex64 = onp.complex64
+complex128 = onp.complex128
 
 integer = onp.integer
 
@@ -831,7 +838,6 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
     cols = lax.broadcasted_iota(k_dtype, (N, M), 1)
     return lax.convert_element_type(lax.eq(rows, cols), dtype)
 
-
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
   # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy
@@ -846,6 +852,10 @@ def arange(*args, **kwargs):
   #     return lax.iota(dtype, stop)  # avoids materializing
   return onp.arange(*args, **kwargs)
 
+linspace = onp.linspace
+logspace = onp.logspace
+geomspace = onp.geomspace
+meshgrid = onp.meshgrid
 
 @_wraps(onp.repeat)
 def repeat(a, repeats, axis=None):
@@ -925,6 +935,9 @@ def trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):
   return sum(a, axis=(-2, -1), dtype=dtype)
 
 
+diag_indices = onp.diag_indices
+
+
 @_wraps(onp.diagonal)
 def diagonal(a, offset=0, axis1=0, axis2=1):
   a_shape = shape(a)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index c43eb34bd..30fc61ba2 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -88,13 +88,20 @@ ndim = _ndim = onp.ndim
 size = onp.size
 _dtype = lax._dtype
 
+bool_ = onp.bool_
+uint8 = onp.uint8
+uint16 = onp.uint16
 uint32 = onp.uint32
-int32 = onp.int32
 uint64 = onp.uint64
+int8 = onp.int8
+int16 = onp.int16
+int32 = onp.int32
 int64 = onp.int64
+float16 = onp.float16
 float32 = onp.float32
 float64 = onp.float64
 complex64 = onp.complex64
+complex128 = onp.complex128
 
 integer = onp.integer
 
@@ -831,7 +838,6 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
     cols = lax.broadcasted_iota(k_dtype, (N, M), 1)
     return lax.convert_element_type(lax.eq(rows, cols), dtype)
 
-
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
   # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy
@@ -846,6 +852,10 @@ def arange(*args, **kwargs):
   #     return lax.iota(dtype, stop)  # avoids materializing
   return onp.arange(*args, **kwargs)
 
+linspace = onp.linspace
+logspace = onp.logspace
+geomspace = onp.geomspace
+meshgrid = onp.meshgrid
 
 @_wraps(onp.repeat)
 def repeat(a, repeats, axis=None):
@@ -925,6 +935,9 @@ def trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):
   return sum(a, axis=(-2, -1), dtype=dtype)
 
 
+diag_indices = onp.diag_indices
+
+
 @_wraps(onp.diagonal)
 def diagonal(a, offset=0, axis1=0, axis2=1):
   a_shape = shape(a)",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,7f6119c7cd04d230878472ad5d1efc028f02e476,95135377d0ed3d3954a44a650ad2580ded61037f,"Implement np.{full_like,isinf,isnan}. Fix np.isfinite.

Note that inf/nan behavior may not be correct in fastmath mode.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 30fc61ba2..8e37d790d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -53,6 +53,7 @@ _sum = builtins.sum
 pi = onp.pi
 e = onp.e
 inf = onp.inf
+NINF = onp.NINF
 nan = onp.nan
 
 # And some numpy utility functions
@@ -103,6 +104,8 @@ float64 = onp.float64
 complex64 = onp.complex64
 complex128 = onp.complex128
 
+complexfloating = onp.complexfloating
+floating = onp.floating
 integer = onp.integer
 
 iinfo = onp.iinfo
@@ -256,7 +259,6 @@ left_shift = _one_to_one_binop(onp.left_shift, lax.shift_left)
 equal = _one_to_one_binop(onp.equal, lax.eq)
 greater_equal = _one_to_one_binop(onp.greater_equal, lax.ge)
 greater = _one_to_one_binop(onp.greater, lax.gt)
-isfinite = _one_to_one_binop(onp.isfinite, lax.is_finite)
 less_equal = _one_to_one_binop(onp.less_equal, lax.le)
 less = _one_to_one_binop(onp.less, lax.lt)
 maximum = _one_to_one_binop(onp.maximum, lax.max)
@@ -596,6 +598,34 @@ def round(a, decimals=0):
 around = round
 
 
+# Caution: If fast math mode is enabled, the semantics of inf and nan are not
+# preserved by XLA/LLVM, and the behavior of inf/nan values is unpredictable.
+# To disable fast math mode on CPU, set the environment variable
+# XLA_FLAGS=--xla_cpu_enable_fast_math=false.
+
+def isfinite(x):
+  dtype = _dtype(x)
+  if issubdtype(dtype, floating):
+    return lax.is_finite(x)
+  elif issubdtype(dtype, complexfloating):
+    return lax.bitwise_and(lax.is_finite(real(x)), lax.is_finite(imag(x)))
+  else:
+    return full_like(x, True, dtype=bool_)
+
+def isinf(x):
+  dtype = _dtype(x)
+  if issubdtype(dtype, floating):
+    return lax.eq(lax.abs(x), inf)
+  elif issubdtype(dtype, complexfloating):
+    return lax.bitwise_or(lax.eq(lax.abs(real(x)), inf),
+                          lax.eq(lax.abs(imag(x)), inf))
+  else:
+    return full_like(x, False, dtype=bool_)
+
+def isnan(x):
+  return bitwise_and(bitwise_not(isfinite(x)), bitwise_not(isinf(x)))
+
+
 ### Reducers
 
 
@@ -812,6 +842,11 @@ def full(shape, fill_value, dtype=None):
   return lax.full(shape, fill_value, dtype)
 
 
+@_wraps(onp.full_like)
+def full_like(a, fill_value, dtype=None):
+  return lax.full_like(a, fill_value, dtype)
+
+
 @_wraps(onp.zeros)
 def zeros(shape, dtype=onp.dtype(""float64"")):
   shape = (shape,) if onp.isscalar(shape) else shape","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 30fc61ba2..8e37d790d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -53,6 +53,7 @@ _sum = builtins.sum
 pi = onp.pi
 e = onp.e
 inf = onp.inf
+NINF = onp.NINF
 nan = onp.nan
 
 # And some numpy utility functions
@@ -103,6 +104,8 @@ float64 = onp.float64
 complex64 = onp.complex64
 complex128 = onp.complex128
 
+complexfloating = onp.complexfloating
+floating = onp.floating
 integer = onp.integer
 
 iinfo = onp.iinfo
@@ -256,7 +259,6 @@ left_shift = _one_to_one_binop(onp.left_shift, lax.shift_left)
 equal = _one_to_one_binop(onp.equal, lax.eq)
 greater_equal = _one_to_one_binop(onp.greater_equal, lax.ge)
 greater = _one_to_one_binop(onp.greater, lax.gt)
-isfinite = _one_to_one_binop(onp.isfinite, lax.is_finite)
 less_equal = _one_to_one_binop(onp.less_equal, lax.le)
 less = _one_to_one_binop(onp.less, lax.lt)
 maximum = _one_to_one_binop(onp.maximum, lax.max)
@@ -596,6 +598,34 @@ def round(a, decimals=0):
 around = round
 
 
+# Caution: If fast math mode is enabled, the semantics of inf and nan are not
+# preserved by XLA/LLVM, and the behavior of inf/nan values is unpredictable.
+# To disable fast math mode on CPU, set the environment variable
+# XLA_FLAGS=--xla_cpu_enable_fast_math=false.
+
+def isfinite(x):
+  dtype = _dtype(x)
+  if issubdtype(dtype, floating):
+    return lax.is_finite(x)
+  elif issubdtype(dtype, complexfloating):
+    return lax.bitwise_and(lax.is_finite(real(x)), lax.is_finite(imag(x)))
+  else:
+    return full_like(x, True, dtype=bool_)
+
+def isinf(x):
+  dtype = _dtype(x)
+  if issubdtype(dtype, floating):
+    return lax.eq(lax.abs(x), inf)
+  elif issubdtype(dtype, complexfloating):
+    return lax.bitwise_or(lax.eq(lax.abs(real(x)), inf),
+                          lax.eq(lax.abs(imag(x)), inf))
+  else:
+    return full_like(x, False, dtype=bool_)
+
+def isnan(x):
+  return bitwise_and(bitwise_not(isfinite(x)), bitwise_not(isinf(x)))
+
+
 ### Reducers
 
 
@@ -812,6 +842,11 @@ def full(shape, fill_value, dtype=None):
   return lax.full(shape, fill_value, dtype)
 
 
+@_wraps(onp.full_like)
+def full_like(a, fill_value, dtype=None):
+  return lax.full_like(a, fill_value, dtype)
+
+
 @_wraps(onp.zeros)
 def zeros(shape, dtype=onp.dtype(""float64"")):
   shape = (shape,) if onp.isscalar(shape) else shape",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,7f6119c7cd04d230878472ad5d1efc028f02e476,95135377d0ed3d3954a44a650ad2580ded61037f,"Implement np.{full_like,isinf,isnan}. Fix np.isfinite.

Note that inf/nan behavior may not be correct in fastmath mode.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index e7b80ef0d..fb2fb79ab 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -72,6 +72,7 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""isfinite"", 1, numeric_dtypes, all_shapes, jtu.rand_some_inf(), []),
     op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
@@ -565,6 +566,25 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_inshape={}_filldtype={}_outdtype={}"".format(
+          jtu.format_shape_dtype_string(shape, in_dtype),
+          onp.dtype(fill_value_dtype).name,
+          onp.dtype(out_dtype).name),
+       ""shape"": shape, ""in_dtype"": in_dtype,
+       ""fill_value_dtype"": fill_value_dtype, ""out_dtype"": out_dtype,
+       ""rng"": jtu.rand_default()}
+      for shape in array_shapes
+      for in_dtype in default_dtypes
+      for fill_value_dtype in default_dtypes
+      for out_dtype in default_dtypes))
+  def testFullLike(self, shape, in_dtype, fill_value_dtype, out_dtype, rng):
+    onp_fun = lambda x, fill_value: onp.full_like(x, fill_value, dtype=out_dtype)
+    lnp_fun = lambda x, fill_value: lnp.full_like(x, fill_value, dtype=out_dtype)
+    args_maker = lambda: [rng(shape, in_dtype), rng((), fill_value_dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_axis={}_{}sections"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index e7b80ef0d..fb2fb79ab 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -72,6 +72,7 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""isfinite"", 1, numeric_dtypes, all_shapes, jtu.rand_some_inf(), []),
     op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
     op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
@@ -565,6 +566,25 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_inshape={}_filldtype={}_outdtype={}"".format(
+          jtu.format_shape_dtype_string(shape, in_dtype),
+          onp.dtype(fill_value_dtype).name,
+          onp.dtype(out_dtype).name),
+       ""shape"": shape, ""in_dtype"": in_dtype,
+       ""fill_value_dtype"": fill_value_dtype, ""out_dtype"": out_dtype,
+       ""rng"": jtu.rand_default()}
+      for shape in array_shapes
+      for in_dtype in default_dtypes
+      for fill_value_dtype in default_dtypes
+      for out_dtype in default_dtypes))
+  def testFullLike(self, shape, in_dtype, fill_value_dtype, out_dtype, rng):
+    onp_fun = lambda x, fill_value: onp.full_like(x, fill_value, dtype=out_dtype)
+    lnp_fun = lambda x, fill_value: lnp.full_like(x, fill_value, dtype=out_dtype)
+    args_maker = lambda: [rng(shape, in_dtype), rng((), fill_value_dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_{}_axis={}_{}sections"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,bf73c1e282dcf3e845cfb6289ae0749ec80485b3,7f6119c7cd04d230878472ad5d1efc028f02e476,Use lax.bitwise_ in isnan().,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 8e37d790d..788c862da 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -623,7 +623,8 @@ def isinf(x):
     return full_like(x, False, dtype=bool_)
 
 def isnan(x):
-  return bitwise_and(bitwise_not(isfinite(x)), bitwise_not(isinf(x)))
+  return lax.bitwise_and(lax.bitwise_not(isfinite(x)),
+                         lax.bitwise_not(isinf(x)))
 
 
 ### Reducers","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 8e37d790d..788c862da 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -623,7 +623,8 @@ def isinf(x):
     return full_like(x, False, dtype=bool_)
 
 def isnan(x):
-  return bitwise_and(bitwise_not(isfinite(x)), bitwise_not(isinf(x)))
+  return lax.bitwise_and(lax.bitwise_not(isfinite(x)),
+                         lax.bitwise_not(isinf(x)))
 
 
 ### Reducers",No
jax/api.py,jax/api.py,8f1bc997ca0146f4f537ee6418d9b7fc3cee9983,449da4cdb59872ec7464f20bfb20863a217cd98d,add value-and-grad fun (closes #149),"diff --git a/jax/api.py b/jax/api.py
index 1d5451467..79eebb01c 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -58,16 +58,17 @@ def jit(fun, static_argnums=()):
 
   Args:
     fun: Function to be jitted. Should be a pure function, as side-effects may
-        only be executed once. Its positional arguments and return value should
-        be arrays, scalars, or standard Python containers (tuple/list/dict)
-        thereof. Keyword arguments and positional arguments specified by
-        `static_argnums` can be anything at all. These are treated as static
-        (see below).
+      only be executed once. Its positional arguments and return value should be
+      arrays, scalars, or standard Python containers (tuple/list/dict) thereof.
+      Keyword arguments and positional arguments specified by `static_argnums`
+      can be anything at all. These are treated as static (see below).
     static_argnums: A tuple of ints. Specifies which arguments to treat as
-        static (compile-time constant). Operations that only depend on static
-        arguments will be constant-folded. Calling the jitted function with
-        different values for these constants will trigger recompilation.
-   Returns: A wrapped version of `fun`, set up for just-in-time compilation.
+      static (compile-time constant). Operations that only depend on static
+      arguments will be constant-folded. Calling the jitted function with
+      different values for these constants will trigger recompilation.
+
+  Returns:
+    A wrapped version of `fun`, set up for just-in-time compilation.
   """"""
   @_wraps(fun)
   def f_jitted(*args, **kwargs):
@@ -83,31 +84,62 @@ def jit(fun, static_argnums=()):
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
   return f_jitted
 
+
 def grad(fun, argnums=0):
   """"""Creates a function which evaluates the gradient of `fun`.
 
   Args:
     fun: Function to be differentiated. Its arguments at positions specified by
-        `argnums` should be arrays, scalars, or standard Python containers. It
-        should return a scalar (which includes arrays with shape `()` but not
-        arrays with shape `(1,)` etc.)
+      `argnums` should be arrays, scalars, or standard Python containers. It
+      should return a scalar (which includes arrays with shape `()` but not
+      arrays with shape `(1,)` etc.)
     argnums: Integer or tuple of integers. Specifies which positional
-        argument(s) to differentiate with respect to.
-  Returns: A function with the same arguments as `fun`, that evaluates the
-      gradient of `fun`. If `argnums` is an integer then the gradient has the
-      same shape and type as the positional argument indicated by that integer.
-      If argnums is a tuple of integers, the gradient is a tuple of values with
-      the same shapes and types as the corresponding arguments.
+    argument(s) to differentiate with respect to.
+
+  Returns:
+    A function with the same arguments as `fun`, that evaluates the gradient of
+    `fun`. If `argnums` is an integer then the gradient has the same shape and
+    type as the positional argument indicated by that integer. If argnums is a
+    tuple of integers, the gradient is a tuple of values with the same shapes
+    and types as the corresponding arguments.
   """"""
+  value_and_grad_f = value_and_grad(fun, argnums)
+
   def grad_f(*args, **kwargs):
+    ans, g = value_and_grad_f(*args, **kwargs)
+    return g
+
+  return grad_f
+
+def value_and_grad(fun, argnums=0):
+  """"""Creates a function which evaluates both `fun` and the gradient of `fun`.
+
+  Args:
+    fun: Function to be differentiated. Its arguments at positions specified by
+      `argnums` should be arrays, scalars, or standard Python containers. It
+      should return a scalar (which includes arrays with shape `()` but not
+      arrays with shape `(1,)` etc.)
+    argnums: Integer or tuple of integers. Specifies which positional
+      argument(s) to differentiate with respect to.
+
+  Returns:
+    A function with the same arguments as `fun` that evaluates both `fun` and
+    the gradient of `fun` and returns them as a pair (a two-element tuple). If
+    `argnums` is an integer then the gradient has the same shape and type as the
+    positional argument indicated by that integer. If argnums is a tuple of
+    integers, the gradient is a tuple of values with the same shapes and types
+    as the corresponding arguments.
+  """"""
+  def value_and_grad_f(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     f_partial, dyn_args = argnums_partial(f, argnums, args)
     ans, vjp_py = vjp(f_partial, *dyn_args)
     check_scalar(ans)
     g = vjp_py(onp.ones((), onp.result_type(ans)))
-    return g[0] if isinstance(argnums, int) else g
+    g = g[0] if isinstance(argnums, int) else g
+    return (ans, g)
 
-  return grad_f
+  return value_and_grad_f
 
 @curry
 def jacfwd(fun, x):
@@ -136,11 +168,13 @@ def vmap(fun, in_axes=0, out_axes=0):
   Args:
     fun: Function to be mapped over additional axes.
     in_axes, out_axes: Specifies which axes to map over. These may be integers,
-        None, or (possibly nested) tuples of integers or None.
-  Returns: Batched/vectorized version of `fun` with arguments that correspond to
-      those of `fun`, but with extra array axes at positions indicated by
-      `in_axes`, and a return value that corresponds to that of `fun`, but with
-      extra array axes at positions indicated by `out_axes`.
+      None, or (possibly nested) tuples of integers or None.
+
+  Returns:
+    Batched/vectorized version of `fun` with arguments that correspond to those
+    of `fun`, but with extra array axes at positions indicated by `in_axes`, and
+    a return value that corresponds to that of `fun`, but with extra array axes
+    at positions indicated by `out_axes`.
 
   For example, we can implement a matrix-matrix product using a vector dot
   product:
@@ -150,7 +184,6 @@ def vmap(fun, in_axes=0, out_axes=0):
     mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
 
   (`[a,b]` indicates an array with shape (a,b))
-
   """"""
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):","diff --git a/jax/api.py b/jax/api.py
index 1d5451467..79eebb01c 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -58,16 +58,17 @@ def jit(fun, static_argnums=()):
 
   Args:
     fun: Function to be jitted. Should be a pure function, as side-effects may
-        only be executed once. Its positional arguments and return value should
-        be arrays, scalars, or standard Python containers (tuple/list/dict)
-        thereof. Keyword arguments and positional arguments specified by
-        `static_argnums` can be anything at all. These are treated as static
-        (see below).
+      only be executed once. Its positional arguments and return value should be
+      arrays, scalars, or standard Python containers (tuple/list/dict) thereof.
+      Keyword arguments and positional arguments specified by `static_argnums`
+      can be anything at all. These are treated as static (see below).
     static_argnums: A tuple of ints. Specifies which arguments to treat as
-        static (compile-time constant). Operations that only depend on static
-        arguments will be constant-folded. Calling the jitted function with
-        different values for these constants will trigger recompilation.
-   Returns: A wrapped version of `fun`, set up for just-in-time compilation.
+      static (compile-time constant). Operations that only depend on static
+      arguments will be constant-folded. Calling the jitted function with
+      different values for these constants will trigger recompilation.
+
+  Returns:
+    A wrapped version of `fun`, set up for just-in-time compilation.
   """"""
   @_wraps(fun)
   def f_jitted(*args, **kwargs):
@@ -83,31 +84,62 @@ def jit(fun, static_argnums=()):
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
   return f_jitted
 
+
 def grad(fun, argnums=0):
   """"""Creates a function which evaluates the gradient of `fun`.
 
   Args:
     fun: Function to be differentiated. Its arguments at positions specified by
-        `argnums` should be arrays, scalars, or standard Python containers. It
-        should return a scalar (which includes arrays with shape `()` but not
-        arrays with shape `(1,)` etc.)
+      `argnums` should be arrays, scalars, or standard Python containers. It
+      should return a scalar (which includes arrays with shape `()` but not
+      arrays with shape `(1,)` etc.)
     argnums: Integer or tuple of integers. Specifies which positional
-        argument(s) to differentiate with respect to.
-  Returns: A function with the same arguments as `fun`, that evaluates the
-      gradient of `fun`. If `argnums` is an integer then the gradient has the
-      same shape and type as the positional argument indicated by that integer.
-      If argnums is a tuple of integers, the gradient is a tuple of values with
-      the same shapes and types as the corresponding arguments.
+    argument(s) to differentiate with respect to.
+
+  Returns:
+    A function with the same arguments as `fun`, that evaluates the gradient of
+    `fun`. If `argnums` is an integer then the gradient has the same shape and
+    type as the positional argument indicated by that integer. If argnums is a
+    tuple of integers, the gradient is a tuple of values with the same shapes
+    and types as the corresponding arguments.
   """"""
+  value_and_grad_f = value_and_grad(fun, argnums)
+
   def grad_f(*args, **kwargs):
+    ans, g = value_and_grad_f(*args, **kwargs)
+    return g
+
+  return grad_f
+
+def value_and_grad(fun, argnums=0):
+  """"""Creates a function which evaluates both `fun` and the gradient of `fun`.
+
+  Args:
+    fun: Function to be differentiated. Its arguments at positions specified by
+      `argnums` should be arrays, scalars, or standard Python containers. It
+      should return a scalar (which includes arrays with shape `()` but not
+      arrays with shape `(1,)` etc.)
+    argnums: Integer or tuple of integers. Specifies which positional
+      argument(s) to differentiate with respect to.
+
+  Returns:
+    A function with the same arguments as `fun` that evaluates both `fun` and
+    the gradient of `fun` and returns them as a pair (a two-element tuple). If
+    `argnums` is an integer then the gradient has the same shape and type as the
+    positional argument indicated by that integer. If argnums is a tuple of
+    integers, the gradient is a tuple of values with the same shapes and types
+    as the corresponding arguments.
+  """"""
+  def value_and_grad_f(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     f_partial, dyn_args = argnums_partial(f, argnums, args)
     ans, vjp_py = vjp(f_partial, *dyn_args)
     check_scalar(ans)
     g = vjp_py(onp.ones((), onp.result_type(ans)))
-    return g[0] if isinstance(argnums, int) else g
+    g = g[0] if isinstance(argnums, int) else g
+    return (ans, g)
 
-  return grad_f
+  return value_and_grad_f
 
 @curry
 def jacfwd(fun, x):
@@ -136,11 +168,13 @@ def vmap(fun, in_axes=0, out_axes=0):
   Args:
     fun: Function to be mapped over additional axes.
     in_axes, out_axes: Specifies which axes to map over. These may be integers,
-        None, or (possibly nested) tuples of integers or None.
-  Returns: Batched/vectorized version of `fun` with arguments that correspond to
-      those of `fun`, but with extra array axes at positions indicated by
-      `in_axes`, and a return value that corresponds to that of `fun`, but with
-      extra array axes at positions indicated by `out_axes`.
+      None, or (possibly nested) tuples of integers or None.
+
+  Returns:
+    Batched/vectorized version of `fun` with arguments that correspond to those
+    of `fun`, but with extra array axes at positions indicated by `in_axes`, and
+    a return value that corresponds to that of `fun`, but with extra array axes
+    at positions indicated by `out_axes`.
 
   For example, we can implement a matrix-matrix product using a vector dot
   product:
@@ -150,7 +184,6 @@ def vmap(fun, in_axes=0, out_axes=0):
     mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
 
   (`[a,b]` indicates an array with shape (a,b))
-
   """"""
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):",No
tests/api_test.py,tests/api_test.py,565c95582dd0fe77c85f16bf6d537bb9393ef077,8f1bc997ca0146f4f537ee6418d9b7fc3cee9983,add simple value_and_grad test,"diff --git a/tests/api_test.py b/tests/api_test.py
index b8f1d72d6..786391573 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -44,6 +44,15 @@ class APITest(jtu.JaxTestCase):
     assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
     assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
 
+  def value_and_grad_argnums(self):
+    def f(x, y, z, flag=False):
+      assert flag
+      return 1.0 * x + 2.0 * y + 3.0 * z
+
+    y = f(1.0, 1.0, 1.0, flag=True)
+    assert value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
+    assert value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
+    assert value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
 
   def test_jit_static_args(self):
     side = []","diff --git a/tests/api_test.py b/tests/api_test.py
index b8f1d72d6..786391573 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -44,6 +44,15 @@ class APITest(jtu.JaxTestCase):
     assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
     assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
 
+  def value_and_grad_argnums(self):
+    def f(x, y, z, flag=False):
+      assert flag
+      return 1.0 * x + 2.0 * y + 3.0 * z
+
+    y = f(1.0, 1.0, 1.0, flag=True)
+    assert value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
+    assert value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
+    assert value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
 
   def test_jit_static_args(self):
     side = []",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,4fc73205b2d1888496e582d5adaf0fc3825f6a7f,bf73c1e282dcf3e845cfb6289ae0749ec80485b3,"Implement np.{diagonal,count_nonzero}.
Fix shape error if a scalar is passed to reducers. Add test for scalar reductions.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 788c862da..f670b4026 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -734,6 +734,12 @@ def allclose(a, b, rtol=1e-05, atol=1e-08):
   return all(isclose(a, b, rtol, atol))
 
 
+@_wraps(onp.count_nonzero)
+def count_nonzero(a, axis=None):
+  return sum(lax.ne(a, _constant_like(a, 0)), axis=axis,
+             dtype=xla_bridge.canonicalize_dtype(onp.int_))
+
+
 ### Array-creation functions
 
 
@@ -863,6 +869,9 @@ def ones(shape, dtype=onp.dtype(""float64"")):
 @_wraps(onp.eye)
 def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
   M = N if M is None else M
+  if N < 0 or M < 0:
+    msg = ""negative dimensions are not allowed, got {} and {}""
+    raise ValueError(msg.format(N, M))
   if k is None:
     return lax.broadcasted_eye(dtype, (N, M), (0, 1))
   else:
@@ -874,6 +883,12 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
     cols = lax.broadcasted_iota(k_dtype, (N, M), 1)
     return lax.convert_element_type(lax.eq(rows, cols), dtype)
 
+
+@_wraps(onp.identity)
+def identity(n, dtype=None):
+  return eye(n, dtype=dtype)
+
+
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
   # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 788c862da..f670b4026 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -734,6 +734,12 @@ def allclose(a, b, rtol=1e-05, atol=1e-08):
   return all(isclose(a, b, rtol, atol))
 
 
+@_wraps(onp.count_nonzero)
+def count_nonzero(a, axis=None):
+  return sum(lax.ne(a, _constant_like(a, 0)), axis=axis,
+             dtype=xla_bridge.canonicalize_dtype(onp.int_))
+
+
 ### Array-creation functions
 
 
@@ -863,6 +869,9 @@ def ones(shape, dtype=onp.dtype(""float64"")):
 @_wraps(onp.eye)
 def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
   M = N if M is None else M
+  if N < 0 or M < 0:
+    msg = ""negative dimensions are not allowed, got {} and {}""
+    raise ValueError(msg.format(N, M))
   if k is None:
     return lax.broadcasted_eye(dtype, (N, M), (0, 1))
   else:
@@ -874,6 +883,12 @@ def eye(N, M=None, k=None, dtype=onp.dtype(""float64"")):
     cols = lax.broadcasted_iota(k_dtype, (N, M), 1)
     return lax.convert_element_type(lax.eq(rows, cols), dtype)
 
+
+@_wraps(onp.identity)
+def identity(n, dtype=None):
+  return eye(n, dtype=dtype)
+
+
 @_wraps(onp.arange)
 def arange(*args, **kwargs):
   # attempt to generate a lazy IotaConstant, otherwise fall back to raw numpy",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,4fc73205b2d1888496e582d5adaf0fc3825f6a7f,bf73c1e282dcf3e845cfb6289ae0749ec80485b3,"Implement np.{diagonal,count_nonzero}.
Fix shape error if a scalar is passed to reducers. Add test for scalar reductions.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index fb2fb79ab..826d53c9f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -50,7 +50,7 @@ unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
-
+all_dtypes = numeric_dtypes + bool_dtypes
 
 OpRecord = collections.namedtuple(
   ""OpRecord"",
@@ -239,7 +239,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for rec in JAX_REDUCER_RECORDS
       for shape in rec.shapes for dtype in rec.dtypes
       for out_dtype in [None] + rec.dtypes
-      for axis in range(-len(shape), len(shape))
+      for axis in set(range(-len(shape), len(shape))) | set([None])
       for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, out_dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, dtype=out_dtype, keepdims=keepdims)
@@ -257,7 +257,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""axis"": axis, ""keepdims"": keepdims}
       for rec in JAX_REDUCER_NO_DTYPE_RECORDS
       for shape in rec.shapes for dtype in rec.dtypes
-      for axis in range(-len(shape), len(shape))
+      for axis in set(range(-len(shape), len(shape))) | set([None])
       for keepdims in [False, True]))
   def testReducerNoDtype(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
@@ -266,6 +266,20 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for shape in all_shapes for dtype in all_dtypes
+      for axis in set(range(-len(shape), len(shape))) | set([None])))
+  def testCountNonzero(self, shape, dtype, axis):
+    rng = jtu.rand_some_zero()
+    onp_fun = lambda x: onp.count_nonzero(x, axis)
+    lnp_fun = lambda x: lnp.count_nonzero(x, axis)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}"".format(
           rec.test_name.capitalize(),
@@ -515,6 +529,18 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_n={}"".format(onp.dtype(dtype).name, n),
+       ""dtype"": dtype, ""n"": n}
+      for dtype in default_dtypes
+      for n in list(range(4))))
+  def testIdentity(self, n, dtype):
+    onp_fun = lambda: onp.identity(n, dtype)
+    lnp_fun = lambda: lnp.identity(n, dtype)
+    args_maker = lambda: []
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_dtype_{}_offset={}_axis1={}_axis2={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index fb2fb79ab..826d53c9f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -50,7 +50,7 @@ unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
 numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
-
+all_dtypes = numeric_dtypes + bool_dtypes
 
 OpRecord = collections.namedtuple(
   ""OpRecord"",
@@ -239,7 +239,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       for rec in JAX_REDUCER_RECORDS
       for shape in rec.shapes for dtype in rec.dtypes
       for out_dtype in [None] + rec.dtypes
-      for axis in range(-len(shape), len(shape))
+      for axis in set(range(-len(shape), len(shape))) | set([None])
       for keepdims in [False, True]))
   def testReducer(self, onp_op, lnp_op, rng, shape, dtype, out_dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, dtype=out_dtype, keepdims=keepdims)
@@ -257,7 +257,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""axis"": axis, ""keepdims"": keepdims}
       for rec in JAX_REDUCER_NO_DTYPE_RECORDS
       for shape in rec.shapes for dtype in rec.dtypes
-      for axis in range(-len(shape), len(shape))
+      for axis in set(range(-len(shape), len(shape))) | set([None])
       for keepdims in [False, True]))
   def testReducerNoDtype(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
     onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
@@ -266,6 +266,20 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for shape in all_shapes for dtype in all_dtypes
+      for axis in set(range(-len(shape), len(shape))) | set([None])))
+  def testCountNonzero(self, shape, dtype, axis):
+    rng = jtu.rand_some_zero()
+    onp_fun = lambda x: onp.count_nonzero(x, axis)
+    lnp_fun = lambda x: lnp.count_nonzero(x, axis)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""{}_inshape={}_axis={}"".format(
           rec.test_name.capitalize(),
@@ -515,6 +529,18 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_n={}"".format(onp.dtype(dtype).name, n),
+       ""dtype"": dtype, ""n"": n}
+      for dtype in default_dtypes
+      for n in list(range(4))))
+  def testIdentity(self, n, dtype):
+    onp_fun = lambda: onp.identity(n, dtype)
+    lnp_fun = lambda: lnp.identity(n, dtype)
+    args_maker = lambda: []
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_dtype_{}_offset={}_axis1={}_axis2={}"".format(
           jtu.format_shape_dtype_string(shape, dtype),",No
jax/interpreters/xla.py,jax/interpreters/xla.py,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 23e32fa1d..a0cf754c2 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -213,6 +213,7 @@ pytype_aval_mappings = {}
 def abstractify_tuple(tup):
   return AbstractTuple(tuple(map(abstractify, tup)))
 pytype_aval_mappings[JaxTuple] = abstractify_tuple
+pytype_aval_mappings[AbstractTuple] = abstractify_tuple
 
 for t in array_types:
   pytype_aval_mappings[t] = make_shaped_array
@@ -321,6 +322,9 @@ canonicalize_dtype_handlers[DeviceArray] = identity
 xb.register_constant_handler(DeviceArray,
                              lambda c, val: c.Constant(onp.asarray(val)))
 
+pytype_aval_mappings[ConcreteArray] = make_shaped_array
+pytype_aval_mappings[ShapedArray] = lambda x: x
+
 
 class DeviceConstant(DeviceArray):
   @staticmethod","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 23e32fa1d..a0cf754c2 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -213,6 +213,7 @@ pytype_aval_mappings = {}
 def abstractify_tuple(tup):
   return AbstractTuple(tuple(map(abstractify, tup)))
 pytype_aval_mappings[JaxTuple] = abstractify_tuple
+pytype_aval_mappings[AbstractTuple] = abstractify_tuple
 
 for t in array_types:
   pytype_aval_mappings[t] = make_shaped_array
@@ -321,6 +322,9 @@ canonicalize_dtype_handlers[DeviceArray] = identity
 xb.register_constant_handler(DeviceArray,
                              lambda c, val: c.Constant(onp.asarray(val)))
 
+pytype_aval_mappings[ConcreteArray] = make_shaped_array
+pytype_aval_mappings[ShapedArray] = lambda x: x
+
 
 class DeviceConstant(DeviceArray):
   @staticmethod",No
jax/lax.py,jax/lax.py,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/jax/lax.py b/jax/lax.py
index d22f3f9a8..4d992635a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -570,7 +570,8 @@ def fori_loop(lower, upper, body_fun, init_val):
   # The `lt` and `add` functions are added to the namespace programmatically.
   _, _, result = _while_loop(
       lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
-      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
+      lambda upper_i_x: (upper_i_x[0],
+                         add(upper_i_x[1], onp.array(1, _dtype(upper_i_x[1]))),
                          body_fun(upper_i_x[1], upper_i_x[2])),
       (upper, lower, init_val))
   return result
@@ -2681,7 +2682,6 @@ def subvals(lst, replace):
 def _abstractify(x):
   # abstractify wrapper used internally for primitives like _while_loop
   if isinstance(x, core.Tracer):
-    # TODO(mattjj,dougalm): check that it's at least ShapedArray
-    return pe.PartialVal((x.aval, core.unit))
+    return pe.PartialVal((xla.abstractify(x.aval), core.unit))
   else:
     return pe.PartialVal((xla.abstractify(x), core.unit))","diff --git a/jax/lax.py b/jax/lax.py
index d22f3f9a8..4d992635a 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -570,7 +570,8 @@ def fori_loop(lower, upper, body_fun, init_val):
   # The `lt` and `add` functions are added to the namespace programmatically.
   _, _, result = _while_loop(
       lambda upper_i_x: lt(upper_i_x[1], upper_i_x[0]),
-      lambda upper_i_x: (upper_i_x[0], add(upper_i_x[1], 1),
+      lambda upper_i_x: (upper_i_x[0],
+                         add(upper_i_x[1], onp.array(1, _dtype(upper_i_x[1]))),
                          body_fun(upper_i_x[1], upper_i_x[2])),
       (upper, lower, init_val))
   return result
@@ -2681,7 +2682,6 @@ def subvals(lst, replace):
 def _abstractify(x):
   # abstractify wrapper used internally for primitives like _while_loop
   if isinstance(x, core.Tracer):
-    # TODO(mattjj,dougalm): check that it's at least ShapedArray
-    return pe.PartialVal((x.aval, core.unit))
+    return pe.PartialVal((xla.abstractify(x.aval), core.unit))
   else:
     return pe.PartialVal((xla.abstractify(x), core.unit))",No
jax/lax_linalg.py,jax/lax_linalg.py,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index bf823e4cb..f34c2db3a 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -28,12 +28,14 @@ from jax.abstract_arrays import ShapedArray
 from jax.core import Primitive
 from jax.lax import (standard_primitive, standard_unop, binop_dtype_rule,
                      _float, _complex, _input_dtype)
-import jaxlib
+from jaxlib import lapack
 
 # traceables
 
 def cholesky(x): return cholesky_p.bind(x)
 
+def lu(x): return lu_p.bind(x)
+
 def qr(x, full_matrices=True):
   q, r = qr_p.bind(x, full_matrices=full_matrices)
   return q, r
@@ -76,15 +78,14 @@ def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
   if len(shape.dimensions()) == 2 and (
     shape.element_type() == np.float32 or shape.element_type() == np.float64):
-    return c.GetTupleElement(jaxlib.lapack.jax_potrf(c, operand, lower=True), 0)
+    return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
     # unsupported types.
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-if hasattr(jaxlib, ""lapack""):
-  xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
+xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -140,11 +141,11 @@ def triangular_solve_cpu_translation_rule(
     c, a, b, left_side, lower, transpose_a, conjugate_a):
   shape = c.GetShape(a)
   if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return jaxlib.lapack.jax_trsm(
+    return lapack.jax_trsm(
       c, c.ConstantF32Scalar(1.0), a, b, left_side, lower, transpose_a,
       conjugate_a)
   elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return jaxlib.lapack.jax_trsm(
+    return lapack.jax_trsm(
       c, c.ConstantF64Scalar(1.0), a, b, left_side, lower, transpose_a,
       conjugate_a)
   else:
@@ -153,10 +154,79 @@ def triangular_solve_cpu_translation_rule(
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-if hasattr(jaxlib, ""lapack""):
-  xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
+xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
+
+
+# LU decomposition
+
+# Computes a pivoted LU decomposition such that
+# PA = LU
+# In the style of LAPACK, LU are stored in the same matrix.
+# TODO(phawkins): add a mechanism to report errors for singular matrices.
+
+def lu_impl(operand):
+  lu, pivot = xla.apply_primitive(lu_p, operand)
+  return core.pack((lu, pivot))
+
+def lu_translation_rule(c, operand):
+  raise NotImplementedError(
+    ""LU decomposition is only implemented on the CPU backend"")
+
+def lu_abstract_eval(operand):
+  if isinstance(operand, ShapedArray):
+    if operand.ndim < 2:
+      raise ValueError(""Argument to LU decomposition must have ndims >= 2"")
+
+    batch_dims = operand.shape[:-2]
+    m = operand.shape[-2]
+    n = operand.shape[-1]
+    pivot = ShapedArray(batch_dims + (min(m, n),), np.int32)
+  else:
+    pivot = operand
+  return core.AbstractTuple((operand, pivot))
+
+lu_p = Primitive('lu')
+lu_p.def_impl(lu_impl)
+lu_p.def_abstract_eval(lu_abstract_eval)
+xla.translations[lu_p] = lu_translation_rule
+
+def lu_cpu_translation_rule(c, operand):
+  shape = c.GetShape(operand)
+  if len(shape.dimensions()) == 2 and (
+    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+    out = lapack.jax_getrf(c, operand)
+    lu = c.GetTupleElement(out, 0)
+    # Subtract 1 from the pivot to get 0-based indices.
+    pivot = c.Sub(c.GetTupleElement(out, 1), c.ConstantS32Scalar(1))
+    # Throw away the `info` value, because we have no way to report errors.
+    return c.Tuple(lu, pivot)
+  else:
+    raise NotImplementedError(""Only unbatched LU decomposition is implemented"")
+
+xla.backend_specific_translations['Host'][lu_p] = lu_cpu_translation_rule
 
 
+def lu_pivots_to_permutation(swaps, k):
+  """"""Converts the pivots (row swaps) returned by LU to a permutation.""""""
+
+  def body_fn(i, loop_carry):
+    swaps, permutation = loop_carry
+    j = swaps[i]
+    x, y = np.ravel(permutation[i]), np.ravel(permutation[j])
+    permutation = lax.dynamic_update_index_in_dim(permutation, y, i, axis=0)
+    permutation = lax.dynamic_update_index_in_dim(permutation, x, j, axis=0)
+    return swaps, permutation
+
+  n, = np.shape(swaps)
+  permutation = np.arange(k)
+  _, permutation = lax.fori_loop(onp.array(0, onp.int32), onp.array(n, onp.int32),
+                                 body_fn, (swaps, permutation))
+  return permutation
+
+
+
+# QR decomposition
+
 def qr_impl(operand, full_matrices):
   q, r = xla.apply_primitive(qr_p, operand, full_matrices=full_matrices)
   return core.pack((q, r))
@@ -179,9 +249,6 @@ def qr_abstract_eval(operand, full_matrices):
     r = operand
   return core.AbstractTuple((q, r))
 
-def qr_dtype_rule(operand, full_matrices=True):
-  return operand.dtype
-
 def qr_jvp_rule(primals, tangents, full_matrices):
   # See j-towns.github.io/papers/qr-derivative.pdf for a terse derivation.
   x, = primals","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index bf823e4cb..f34c2db3a 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -28,12 +28,14 @@ from jax.abstract_arrays import ShapedArray
 from jax.core import Primitive
 from jax.lax import (standard_primitive, standard_unop, binop_dtype_rule,
                      _float, _complex, _input_dtype)
-import jaxlib
+from jaxlib import lapack
 
 # traceables
 
 def cholesky(x): return cholesky_p.bind(x)
 
+def lu(x): return lu_p.bind(x)
+
 def qr(x, full_matrices=True):
   q, r = qr_p.bind(x, full_matrices=full_matrices)
   return q, r
@@ -76,15 +78,14 @@ def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
   if len(shape.dimensions()) == 2 and (
     shape.element_type() == np.float32 or shape.element_type() == np.float64):
-    return c.GetTupleElement(jaxlib.lapack.jax_potrf(c, operand, lower=True), 0)
+    return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
     # unsupported types.
     # TODO(phawkins): support LAPACK primitives in batched mode.
     return c.Cholesky(operand)
 
-if hasattr(jaxlib, ""lapack""):
-  xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
+xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
 triangular_solve_dtype_rule = partial(
@@ -140,11 +141,11 @@ def triangular_solve_cpu_translation_rule(
     c, a, b, left_side, lower, transpose_a, conjugate_a):
   shape = c.GetShape(a)
   if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return jaxlib.lapack.jax_trsm(
+    return lapack.jax_trsm(
       c, c.ConstantF32Scalar(1.0), a, b, left_side, lower, transpose_a,
       conjugate_a)
   elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return jaxlib.lapack.jax_trsm(
+    return lapack.jax_trsm(
       c, c.ConstantF64Scalar(1.0), a, b, left_side, lower, transpose_a,
       conjugate_a)
   else:
@@ -153,10 +154,79 @@ def triangular_solve_cpu_translation_rule(
     # TODO(phawkins): support BLAS primitives in batched mode.
     return c.TriangularSolve(a, b, left_side, lower, transpose_a, conjugate_a)
 
-if hasattr(jaxlib, ""lapack""):
-  xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
+xla.backend_specific_translations['Host'][triangular_solve_p] = triangular_solve_cpu_translation_rule
 
 
+# LU decomposition
+
+# Computes a pivoted LU decomposition such that
+# PA = LU
+# In the style of LAPACK, LU are stored in the same matrix.
+# TODO(phawkins): add a mechanism to report errors for singular matrices.
+
+def lu_impl(operand):
+  lu, pivot = xla.apply_primitive(lu_p, operand)
+  return core.pack((lu, pivot))
+
+def lu_translation_rule(c, operand):
+  raise NotImplementedError(
+    ""LU decomposition is only implemented on the CPU backend"")
+
+def lu_abstract_eval(operand):
+  if isinstance(operand, ShapedArray):
+    if operand.ndim < 2:
+      raise ValueError(""Argument to LU decomposition must have ndims >= 2"")
+
+    batch_dims = operand.shape[:-2]
+    m = operand.shape[-2]
+    n = operand.shape[-1]
+    pivot = ShapedArray(batch_dims + (min(m, n),), np.int32)
+  else:
+    pivot = operand
+  return core.AbstractTuple((operand, pivot))
+
+lu_p = Primitive('lu')
+lu_p.def_impl(lu_impl)
+lu_p.def_abstract_eval(lu_abstract_eval)
+xla.translations[lu_p] = lu_translation_rule
+
+def lu_cpu_translation_rule(c, operand):
+  shape = c.GetShape(operand)
+  if len(shape.dimensions()) == 2 and (
+    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+    out = lapack.jax_getrf(c, operand)
+    lu = c.GetTupleElement(out, 0)
+    # Subtract 1 from the pivot to get 0-based indices.
+    pivot = c.Sub(c.GetTupleElement(out, 1), c.ConstantS32Scalar(1))
+    # Throw away the `info` value, because we have no way to report errors.
+    return c.Tuple(lu, pivot)
+  else:
+    raise NotImplementedError(""Only unbatched LU decomposition is implemented"")
+
+xla.backend_specific_translations['Host'][lu_p] = lu_cpu_translation_rule
+
+
+def lu_pivots_to_permutation(swaps, k):
+  """"""Converts the pivots (row swaps) returned by LU to a permutation.""""""
+
+  def body_fn(i, loop_carry):
+    swaps, permutation = loop_carry
+    j = swaps[i]
+    x, y = np.ravel(permutation[i]), np.ravel(permutation[j])
+    permutation = lax.dynamic_update_index_in_dim(permutation, y, i, axis=0)
+    permutation = lax.dynamic_update_index_in_dim(permutation, x, j, axis=0)
+    return swaps, permutation
+
+  n, = np.shape(swaps)
+  permutation = np.arange(k)
+  _, permutation = lax.fori_loop(onp.array(0, onp.int32), onp.array(n, onp.int32),
+                                 body_fn, (swaps, permutation))
+  return permutation
+
+
+
+# QR decomposition
+
 def qr_impl(operand, full_matrices):
   q, r = xla.apply_primitive(qr_p, operand, full_matrices=full_matrices)
   return core.pack((q, r))
@@ -179,9 +249,6 @@ def qr_abstract_eval(operand, full_matrices):
     r = operand
   return core.AbstractTuple((q, r))
 
-def qr_dtype_rule(operand, full_matrices=True):
-  return operand.dtype
-
 def qr_jvp_rule(primals, tangents, full_matrices):
   # See j-towns.github.io/papers/qr-derivative.pdf for a terse derivation.
   x, = primals",Yes
jax/numpy/linalg.py,jax/numpy/linalg.py,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 4dcb3d8e6..2eca96758 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -20,6 +20,7 @@ from __future__ import print_function
 import numpy as onp
 import warnings
 
+from .. import lax
 from .. import lax_linalg
 from .lax_numpy import _not_implemented
 from .lax_numpy import _wraps
@@ -35,6 +36,19 @@ def cholesky(a):
   warnings.warn(_EXPERIMENTAL_WARNING)
   return lax_linalg.cholesky(a)
 
+
+@_wraps(onp.linalg.det)
+def det(a):
+  dtype = lax._dtype(a)
+  a_shape = np.shape(a)
+  if len(a_shape) != 2 or a_shape[-1] != a_shape[-2]:
+    msg = ""Argument to det() must be a square matrix, got {}""
+    raise ValueError(msg.format(a_shape))
+  lu, pivot = lax_linalg.lu(a)
+  parity = np.count_nonzero(pivot != np.arange(a_shape[-1])) % 2
+  return np.prod(np.diagonal(lu)) * np.array(-2 * parity + 1, dtype=dtype)
+
+
 @_wraps(onp.linalg.inv)
 def inv(a):
   warnings.warn(_EXPERIMENTAL_WARNING)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 4dcb3d8e6..2eca96758 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -20,6 +20,7 @@ from __future__ import print_function
 import numpy as onp
 import warnings
 
+from .. import lax
 from .. import lax_linalg
 from .lax_numpy import _not_implemented
 from .lax_numpy import _wraps
@@ -35,6 +36,19 @@ def cholesky(a):
   warnings.warn(_EXPERIMENTAL_WARNING)
   return lax_linalg.cholesky(a)
 
+
+@_wraps(onp.linalg.det)
+def det(a):
+  dtype = lax._dtype(a)
+  a_shape = np.shape(a)
+  if len(a_shape) != 2 or a_shape[-1] != a_shape[-2]:
+    msg = ""Argument to det() must be a square matrix, got {}""
+    raise ValueError(msg.format(a_shape))
+  lu, pivot = lax_linalg.lu(a)
+  parity = np.count_nonzero(pivot != np.arange(a_shape[-1])) % 2
+  return np.prod(np.diagonal(lu)) * np.array(-2 * parity + 1, dtype=dtype)
+
+
 @_wraps(onp.linalg.inv)
 def inv(a):
   warnings.warn(_EXPERIMENTAL_WARNING)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 060238eec..30056a8dc 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -20,6 +20,7 @@ import warnings
 
 import scipy.linalg
 
+from .. import lax
 from .. import lax_linalg
 from ..numpy.lax_numpy import _wraps
 from ..numpy import lax_numpy as np
@@ -36,6 +37,13 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   return l if lower else np.conj(l.T)
 
 
+@_wraps(scipy.linalg.det)
+def det(a, overwrite_a=False, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
+  del overwrite_a, check_finite
+  return np_linalg.det(a)
+
+
 @_wraps(scipy.linalg.inv)
 def inv(a, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
@@ -43,6 +51,29 @@ def inv(a, overwrite_a=False, check_finite=True):
   return np_linalg.inv(a)
 
 
+@_wraps(scipy.linalg.lu_factor)
+def lu_factor(a, overwrite_a=False, check_finite=True):
+  del overwrite_a, check_finite
+  return lax_linalg.lu(a)
+
+
+@_wraps(scipy.linalg.lu)
+def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
+  del overwrite_a, check_finite
+  dtype = lax._dtype(a)
+  m, n = np.shape(a)
+  lu, pivots = lax_linalg.lu(a)
+  permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
+  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  k = min(m, n)
+  l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
+  u = np.triu(lu)[:k, :]
+  if permute_l:
+    return np.matmul(p, l), u
+  else:
+    return p, l, u
+
+
 @_wraps(scipy.linalg.qr)
 def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
        check_finite=True):","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 060238eec..30056a8dc 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -20,6 +20,7 @@ import warnings
 
 import scipy.linalg
 
+from .. import lax
 from .. import lax_linalg
 from ..numpy.lax_numpy import _wraps
 from ..numpy import lax_numpy as np
@@ -36,6 +37,13 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   return l if lower else np.conj(l.T)
 
 
+@_wraps(scipy.linalg.det)
+def det(a, overwrite_a=False, check_finite=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
+  del overwrite_a, check_finite
+  return np_linalg.det(a)
+
+
 @_wraps(scipy.linalg.inv)
 def inv(a, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
@@ -43,6 +51,29 @@ def inv(a, overwrite_a=False, check_finite=True):
   return np_linalg.inv(a)
 
 
+@_wraps(scipy.linalg.lu_factor)
+def lu_factor(a, overwrite_a=False, check_finite=True):
+  del overwrite_a, check_finite
+  return lax_linalg.lu(a)
+
+
+@_wraps(scipy.linalg.lu)
+def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
+  del overwrite_a, check_finite
+  dtype = lax._dtype(a)
+  m, n = np.shape(a)
+  lu, pivots = lax_linalg.lu(a)
+  permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
+  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  k = min(m, n)
+  l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
+  u = np.triu(lu)[:k, :]
+  if permute_l:
+    return np.matmul(p, l), u
+  else:
+    return p, l, u
+
+
 @_wraps(scipy.linalg.qr)
 def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
        check_finite=True):",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 787793f21..09bc4a3ea 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -25,7 +25,7 @@ from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
 from scipy.linalg.cython_blas cimport strsm, dtrsm
-from scipy.linalg.cython_lapack cimport spotrf, dpotrf
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
@@ -145,6 +145,72 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
           Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
       ))
 
+
+# ?getrf: LU decomposition
+
+cdef void lapack_sgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float* a_in = <float*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float* a_out = <float*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float))
+
+  sgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_sgetrf"", <void*>(lapack_sgetrf))
+
+
+cdef void lapack_dgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double* a_in = <double*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double* a_out = <double*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double))
+
+  dgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_dgetrf"", <void*>(lapack_dgetrf))
+
+
+def jax_getrf(c, a):
+  assert sizeof(int32_t) == sizeof(int)
+
+  a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
+  m, n = a_shape.dimensions()
+  if dtype == np.float32:
+    fn = b""lapack_sgetrf""
+  elif dtype == np.float64:
+    fn = b""lapack_dgetrf""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+          Shape.array_shape(np.int32, (min(m, n),), (0,)),
+          Shape.array_shape(np.int32, (), ()),
+      )),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+      ))
+
+
+
 # ?potrf: Cholesky decomposition
 
 cdef void lapack_spotrf(void* out_tuple, void** data) nogil:","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 787793f21..09bc4a3ea 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -25,7 +25,7 @@ from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
 from scipy.linalg.cython_blas cimport strsm, dtrsm
-from scipy.linalg.cython_lapack cimport spotrf, dpotrf
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
@@ -145,6 +145,72 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
           Shape.array_shape(dtype, b_shape.dimensions(), (0, 1)),
       ))
 
+
+# ?getrf: LU decomposition
+
+cdef void lapack_sgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float* a_in = <float*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float* a_out = <float*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float))
+
+  sgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_sgetrf"", <void*>(lapack_sgetrf))
+
+
+cdef void lapack_dgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double* a_in = <double*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double* a_out = <double*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double))
+
+  dgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_dgetrf"", <void*>(lapack_dgetrf))
+
+
+def jax_getrf(c, a):
+  assert sizeof(int32_t) == sizeof(int)
+
+  a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
+  m, n = a_shape.dimensions()
+  if dtype == np.float32:
+    fn = b""lapack_sgetrf""
+  elif dtype == np.float64:
+    fn = b""lapack_dgetrf""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+          Shape.array_shape(np.int32, (min(m, n),), (0,)),
+          Shape.array_shape(np.int32, (), ()),
+      )),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+      ))
+
+
+
 # ?potrf: Cholesky decomposition
 
 cdef void lapack_spotrf(void* out_tuple, void** data) nogil:",No
tests/linalg_test.py,tests/linalg_test.py,dfdc2e380634001fcdb8a7720457402617822420,5cf642e32626e103084451243b789652736a9354,"Add LU decomposition implementation backed by LAPACK on the CPU platform.

Implement np.linalg.det, and scipy.linalg.{lu,lu_factor,det}.

Add missing abstractification to loop arguments.
Implement XLA abstractification rules for AbstractTuple, ConcreteArray, and ShapedArray.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 1cc6a23cd..9f55b64dc 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -21,16 +21,19 @@ from functools import partial
 import itertools
 
 import numpy as onp
+import scipy as osp
 
 from absl.testing import absltest
 from absl.testing import parameterized
 
 from jax import jvp
 from jax import numpy as np
-from jax import scipy
+from jax import scipy as jsp
 from jax import test_util as jtu
 from jax.lib import xla_bridge
 
+from jaxlib import lapack
+
 from jax.config import config
 config.parse_flags_with_absl()
 
@@ -60,6 +63,21 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
+       ""n"": n, ""dtype"": dtype, ""rng"": rng}
+      for n in [0, 4, 5, 200]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testDet(self, n, dtype, rng):
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    self._CheckAgainstNumpy(onp.linalg.det, np.linalg.det, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.det, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), full_matrices),
@@ -141,6 +159,35 @@ class NumpyLinalgTest(jtu.JaxTestCase):
 
 class ScipyLinalgTest(jtu.JaxTestCase):
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 5), (10, 5), (50, 50)]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testLu(self, shape, dtype, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+
+    self._CheckAgainstNumpy(osp.linalg.lu, jsp.linalg.lu, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
+       ""n"": n, ""dtype"": dtype, ""rng"": rng}
+      for n in [1, 4, 5, 200]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testLuFactor(self, n, dtype, rng):
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    self._CheckAgainstNumpy(osp.linalg.lu_factor, jsp.linalg.lu_factor,
+                            args_maker, check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(jsp.linalg.lu_factor, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
@@ -158,7 +205,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       ]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
-  def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
+  def testSolveTriangular(self, lower, transpose_a, lhs_shape,
                                  rhs_shape, dtype, rng):
     k = rng(lhs_shape, dtype)
     l = onp.linalg.cholesky(onp.matmul(k, T(k))
@@ -175,7 +222,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
 
     # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
     # But it seems like an inevitable extension so we support it.
-    ans = scipy.linalg.solve_triangular(
+    ans = jsp.linalg.solve_triangular(
         l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
 
     self.assertAllClose(onp_ans, ans, check_dtypes=True)
@@ -197,13 +244,13 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       ]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
-  def testSolveTriangularBlockedGrad(self, lower, transpose_a, lhs_shape,
+  def testSolveTriangularGrad(self, lower, transpose_a, lhs_shape,
                                      rhs_shape, dtype, rng):
     # TODO(frostig): change ensemble to support a bigger rtol
     A = np.tril(rng(lhs_shape, dtype) + 5 * onp.eye(lhs_shape[-1], dtype=dtype))
     A = A if lower else T(A)
     B = rng(rhs_shape, dtype)
-    f = partial(scipy.linalg.solve_triangular, lower=lower,
+    f = partial(jsp.linalg.solve_triangular, lower=lower,
                 trans=1 if transpose_a else 0)
     jtu.check_grads(f, (A, B), 2, rtol=1e-3)
 ","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 1cc6a23cd..9f55b64dc 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -21,16 +21,19 @@ from functools import partial
 import itertools
 
 import numpy as onp
+import scipy as osp
 
 from absl.testing import absltest
 from absl.testing import parameterized
 
 from jax import jvp
 from jax import numpy as np
-from jax import scipy
+from jax import scipy as jsp
 from jax import test_util as jtu
 from jax.lib import xla_bridge
 
+from jaxlib import lapack
+
 from jax.config import config
 config.parse_flags_with_absl()
 
@@ -60,6 +63,21 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
+       ""n"": n, ""dtype"": dtype, ""rng"": rng}
+      for n in [0, 4, 5, 200]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testDet(self, n, dtype, rng):
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    self._CheckAgainstNumpy(onp.linalg.det, np.linalg.det, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.det, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), full_matrices),
@@ -141,6 +159,35 @@ class NumpyLinalgTest(jtu.JaxTestCase):
 
 class ScipyLinalgTest(jtu.JaxTestCase):
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 5), (10, 5), (50, 50)]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testLu(self, shape, dtype, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+
+    self._CheckAgainstNumpy(osp.linalg.lu, jsp.linalg.lu, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
+       ""n"": n, ""dtype"": dtype, ""rng"": rng}
+      for n in [1, 4, 5, 200]
+      for dtype in float_types()
+      for rng in [jtu.rand_default()]))
+  def testLuFactor(self, n, dtype, rng):
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    self._CheckAgainstNumpy(osp.linalg.lu_factor, jsp.linalg.lu_factor,
+                            args_maker, check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(jsp.linalg.lu_factor, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_transposea={}"".format(
@@ -158,7 +205,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       ]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
-  def testSolveTriangularBlocked(self, lower, transpose_a, lhs_shape,
+  def testSolveTriangular(self, lower, transpose_a, lhs_shape,
                                  rhs_shape, dtype, rng):
     k = rng(lhs_shape, dtype)
     l = onp.linalg.cholesky(onp.matmul(k, T(k))
@@ -175,7 +222,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
 
     # The standard scipy.linalg.solve_triangular doesn't support broadcasting.
     # But it seems like an inevitable extension so we support it.
-    ans = scipy.linalg.solve_triangular(
+    ans = jsp.linalg.solve_triangular(
         l if lower else T(l), b, trans=1 if transpose_a else 0, lower=lower)
 
     self.assertAllClose(onp_ans, ans, check_dtypes=True)
@@ -197,13 +244,13 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       ]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
-  def testSolveTriangularBlockedGrad(self, lower, transpose_a, lhs_shape,
+  def testSolveTriangularGrad(self, lower, transpose_a, lhs_shape,
                                      rhs_shape, dtype, rng):
     # TODO(frostig): change ensemble to support a bigger rtol
     A = np.tril(rng(lhs_shape, dtype) + 5 * onp.eye(lhs_shape[-1], dtype=dtype))
     A = A if lower else T(A)
     B = rng(rhs_shape, dtype)
-    f = partial(scipy.linalg.solve_triangular, lower=lower,
+    f = partial(jsp.linalg.solve_triangular, lower=lower,
                 trans=1 if transpose_a else 0)
     jtu.check_grads(f, (A, B), 2, rtol=1e-3)
 ",No
jax/lax_linalg.py,jax/lax_linalg.py,1815f17a5bd74d47d1689fc3cafc51a4badd734e,dfdc2e380634001fcdb8a7720457402617822420,Be more defensive about old jaxlib versions and non-CPU devices in LU decomposition usage.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index f34c2db3a..7219d9a17 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -203,7 +203,10 @@ def lu_cpu_translation_rule(c, operand):
   else:
     raise NotImplementedError(""Only unbatched LU decomposition is implemented"")
 
-xla.backend_specific_translations['Host'][lu_p] = lu_cpu_translation_rule
+# TODO(phawkins): The hasattr() test here is to avoid incompatibilities between
+# jax and an older jaxlib. Remove after a jaxlib release includes jax_getrf.
+if hasattr(lapack, ""jax_getrf""):
+  xla.backend_specific_translations['Host'][lu_p] = lu_cpu_translation_rule
 
 
 def lu_pivots_to_permutation(swaps, k):","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index f34c2db3a..7219d9a17 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -203,7 +203,10 @@ def lu_cpu_translation_rule(c, operand):
   else:
     raise NotImplementedError(""Only unbatched LU decomposition is implemented"")
 
-xla.backend_specific_translations['Host'][lu_p] = lu_cpu_translation_rule
+# TODO(phawkins): The hasattr() test here is to avoid incompatibilities between
+# jax and an older jaxlib. Remove after a jaxlib release includes jax_getrf.
+if hasattr(lapack, ""jax_getrf""):
+  xla.backend_specific_translations['Host'][lu_p] = lu_cpu_translation_rule
 
 
 def lu_pivots_to_permutation(swaps, k):",No
tests/linalg_test.py,tests/linalg_test.py,1815f17a5bd74d47d1689fc3cafc51a4badd734e,dfdc2e380634001fcdb8a7720457402617822420,Be more defensive about old jaxlib versions and non-CPU devices in LU decomposition usage.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 9f55b64dc..2674fbe61 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -63,6 +63,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
@@ -70,7 +71,10 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for n in [0, 4, 5, 200]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testDet(self, n, dtype, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
     self._CheckAgainstNumpy(onp.linalg.det, np.linalg.det, args_maker,
@@ -159,6 +163,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
 
 class ScipyLinalgTest(jtu.JaxTestCase):
 
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
@@ -166,13 +171,18 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for shape in [(1, 1), (4, 5), (10, 5), (50, 50)]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLu(self, shape, dtype, rng):
+    # TODO(phawkins): remove this after a jaxlib release.
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng(shape, dtype)]
 
     self._CheckAgainstNumpy(osp.linalg.lu, jsp.linalg.lu, args_maker,
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
 
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
@@ -180,7 +190,11 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for n in [1, 4, 5, 200]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuFactor(self, n, dtype, rng):
+    # TODO(phawkins): remove this after a jaxlib release.
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
     self._CheckAgainstNumpy(osp.linalg.lu_factor, jsp.linalg.lu_factor,","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 9f55b64dc..2674fbe61 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -63,6 +63,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
@@ -70,7 +71,10 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for n in [0, 4, 5, 200]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testDet(self, n, dtype, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
     self._CheckAgainstNumpy(onp.linalg.det, np.linalg.det, args_maker,
@@ -159,6 +163,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
 
 class ScipyLinalgTest(jtu.JaxTestCase):
 
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
@@ -166,13 +171,18 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for shape in [(1, 1), (4, 5), (10, 5), (50, 50)]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLu(self, shape, dtype, rng):
+    # TODO(phawkins): remove this after a jaxlib release.
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng(shape, dtype)]
 
     self._CheckAgainstNumpy(osp.linalg.lu, jsp.linalg.lu, args_maker,
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
 
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
@@ -180,7 +190,11 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for n in [1, 4, 5, 200]
       for dtype in float_types()
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuFactor(self, n, dtype, rng):
+    # TODO(phawkins): remove this after a jaxlib release.
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
     self._CheckAgainstNumpy(osp.linalg.lu_factor, jsp.linalg.lu_factor,",No
jax/lax_linalg.py,jax/lax_linalg.py,b68c93d37f44432a58586d6038de30f5de74ba51,3db941f183b75b13c88ad8ab9e987d12af57748c,"Implement np.linalg.slogdet.

Change implementation of np.linalg.logdet to call np.linalg.slogdet.

Add support for complex64 LU decomposition.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 7219d9a17..5f41a9ec8 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -190,10 +190,11 @@ lu_p.def_impl(lu_impl)
 lu_p.def_abstract_eval(lu_abstract_eval)
 xla.translations[lu_p] = lu_translation_rule
 
+_lu_cpu_types = {np.float32, np.float64, np.complex64}
+
 def lu_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and (
-    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+  if len(shape.dimensions()) == 2 and shape.element_type().type in _lu_cpu_types:
     out = lapack.jax_getrf(c, operand)
     lu = c.GetTupleElement(out, 0)
     # Subtract 1 from the pivot to get 0-based indices.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 7219d9a17..5f41a9ec8 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -190,10 +190,11 @@ lu_p.def_impl(lu_impl)
 lu_p.def_abstract_eval(lu_abstract_eval)
 xla.translations[lu_p] = lu_translation_rule
 
+_lu_cpu_types = {np.float32, np.float64, np.complex64}
+
 def lu_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and (
-    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+  if len(shape.dimensions()) == 2 and shape.element_type().type in _lu_cpu_types:
     out = lapack.jax_getrf(c, operand)
     lu = c.GetTupleElement(out, 0)
     # Subtract 1 from the pivot to get 0-based indices.",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,b68c93d37f44432a58586d6038de30f5de74ba51,3db941f183b75b13c88ad8ab9e987d12af57748c,"Implement np.linalg.slogdet.

Change implementation of np.linalg.logdet to call np.linalg.slogdet.

Add support for complex64 LU decomposition.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f670b4026..f990fbdd5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -992,9 +992,12 @@ diag_indices = onp.diag_indices
 @_wraps(onp.diagonal)
 def diagonal(a, offset=0, axis1=0, axis2=1):
   a_shape = shape(a)
+  a_ndims = len(a_shape)
 
   # Move the two dimensions to the end.
-  perm = [i for i in range(len(a_shape)) if i != axis1 and i != axis2]
+  axis1 %= a_ndims
+  axis2 %= a_ndims
+  perm = [i for i in range(a_ndims) if i != axis1 and i != axis2]
   perm = perm + [axis1, axis2]
   a = lax.transpose(a, perm)
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f670b4026..f990fbdd5 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -992,9 +992,12 @@ diag_indices = onp.diag_indices
 @_wraps(onp.diagonal)
 def diagonal(a, offset=0, axis1=0, axis2=1):
   a_shape = shape(a)
+  a_ndims = len(a_shape)
 
   # Move the two dimensions to the end.
-  perm = [i for i in range(len(a_shape)) if i != axis1 and i != axis2]
+  axis1 %= a_ndims
+  axis2 %= a_ndims
+  perm = [i for i in range(a_ndims) if i != axis1 and i != axis2]
   perm = perm + [axis1, axis2]
   a = lax.transpose(a, perm)
 ",No
jax/numpy/linalg.py,jax/numpy/linalg.py,b68c93d37f44432a58586d6038de30f5de74ba51,3db941f183b75b13c88ad8ab9e987d12af57748c,"Implement np.linalg.slogdet.

Change implementation of np.linalg.logdet to call np.linalg.slogdet.

Add support for complex64 LU decomposition.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 2eca96758..8d071829f 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -37,16 +37,35 @@ def cholesky(a):
   return lax_linalg.cholesky(a)
 
 
-@_wraps(onp.linalg.det)
-def det(a):
+@_wraps(onp.linalg.slogdet)
+def slogdet(a):
   dtype = lax._dtype(a)
   a_shape = np.shape(a)
-  if len(a_shape) != 2 or a_shape[-1] != a_shape[-2]:
-    msg = ""Argument to det() must be a square matrix, got {}""
+  if len(a_shape) < 2 or a_shape[-1] != a_shape[-2]:
+    msg = ""Argument to slogdet() must have shape [..., n, n], got {}""
     raise ValueError(msg.format(a_shape))
   lu, pivot = lax_linalg.lu(a)
-  parity = np.count_nonzero(pivot != np.arange(a_shape[-1])) % 2
-  return np.prod(np.diagonal(lu)) * np.array(-2 * parity + 1, dtype=dtype)
+  diag = np.diagonal(lu, axis1=-2, axis2=-1)
+  is_zero = np.any(diag == 0, axis=-1)
+  parity = np.count_nonzero(pivot != np.arange(a_shape[-1]), axis=-1)
+  if np.iscomplexobj(a):
+    sign = np.prod(diag / np.abs(diag))
+  else:
+    sign = 1
+    parity = parity + np.count_nonzero(diag < 0)
+  sign = np.where(is_zero,
+                  np.array(0, dtype=dtype),
+                  sign * np.array(-2 * (parity % 2) + 1, dtype=dtype))
+  logdet = np.where(
+      is_zero, np.array(-np.inf, dtype=dtype),
+      np.sum(np.log(np.abs(diag)), axis=-1))
+  return sign, logdet
+
+
+@_wraps(onp.linalg.det)
+def det(a):
+  sign, logdet = slogdet(a)
+  return sign * np.exp(logdet)
 
 
 @_wraps(onp.linalg.inv)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 2eca96758..8d071829f 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -37,16 +37,35 @@ def cholesky(a):
   return lax_linalg.cholesky(a)
 
 
-@_wraps(onp.linalg.det)
-def det(a):
+@_wraps(onp.linalg.slogdet)
+def slogdet(a):
   dtype = lax._dtype(a)
   a_shape = np.shape(a)
-  if len(a_shape) != 2 or a_shape[-1] != a_shape[-2]:
-    msg = ""Argument to det() must be a square matrix, got {}""
+  if len(a_shape) < 2 or a_shape[-1] != a_shape[-2]:
+    msg = ""Argument to slogdet() must have shape [..., n, n], got {}""
     raise ValueError(msg.format(a_shape))
   lu, pivot = lax_linalg.lu(a)
-  parity = np.count_nonzero(pivot != np.arange(a_shape[-1])) % 2
-  return np.prod(np.diagonal(lu)) * np.array(-2 * parity + 1, dtype=dtype)
+  diag = np.diagonal(lu, axis1=-2, axis2=-1)
+  is_zero = np.any(diag == 0, axis=-1)
+  parity = np.count_nonzero(pivot != np.arange(a_shape[-1]), axis=-1)
+  if np.iscomplexobj(a):
+    sign = np.prod(diag / np.abs(diag))
+  else:
+    sign = 1
+    parity = parity + np.count_nonzero(diag < 0)
+  sign = np.where(is_zero,
+                  np.array(0, dtype=dtype),
+                  sign * np.array(-2 * (parity % 2) + 1, dtype=dtype))
+  logdet = np.where(
+      is_zero, np.array(-np.inf, dtype=dtype),
+      np.sum(np.log(np.abs(diag)), axis=-1))
+  return sign, logdet
+
+
+@_wraps(onp.linalg.det)
+def det(a):
+  sign, logdet = slogdet(a)
+  return sign * np.exp(logdet)
 
 
 @_wraps(onp.linalg.inv)",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,b68c93d37f44432a58586d6038de30f5de74ba51,3db941f183b75b13c88ad8ab9e987d12af57748c,"Implement np.linalg.slogdet.

Change implementation of np.linalg.logdet to call np.linalg.slogdet.

Add support for complex64 LU decomposition.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 09bc4a3ea..ccff5645d 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -25,7 +25,7 @@ from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
 from scipy.linalg.cython_blas cimport strsm, dtrsm
-from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, spotrf, dpotrf
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
@@ -182,6 +182,23 @@ cdef void lapack_dgetrf(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_dgetrf"", <void*>(lapack_dgetrf))
 
 
+cdef void lapack_cgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float complex* a_in = <float complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float complex))
+
+  cgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_cgetrf"", <void*>(lapack_cgetrf))
+
+
 def jax_getrf(c, a):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -192,6 +209,8 @@ def jax_getrf(c, a):
     fn = b""lapack_sgetrf""
   elif dtype == np.float64:
     fn = b""lapack_dgetrf""
+  elif dtype == np.complex64:
+    fn = b""lapack_cgetrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 09bc4a3ea..ccff5645d 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -25,7 +25,7 @@ from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
 from scipy.linalg.cython_blas cimport strsm, dtrsm
-from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, spotrf, dpotrf
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
@@ -182,6 +182,23 @@ cdef void lapack_dgetrf(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_dgetrf"", <void*>(lapack_dgetrf))
 
 
+cdef void lapack_cgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float complex* a_in = <float complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float complex))
+
+  cgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_cgetrf"", <void*>(lapack_cgetrf))
+
+
 def jax_getrf(c, a):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -192,6 +209,8 @@ def jax_getrf(c, a):
     fn = b""lapack_sgetrf""
   elif dtype == np.float64:
     fn = b""lapack_dgetrf""
+  elif dtype == np.complex64:
+    fn = b""lapack_cgetrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,b68c93d37f44432a58586d6038de30f5de74ba51,3db941f183b75b13c88ad8ab9e987d12af57748c,"Implement np.linalg.slogdet.

Change implementation of np.linalg.logdet to call np.linalg.slogdet.

Add support for complex64 LU decomposition.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 826d53c9f..009842661 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -520,7 +520,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""axis2"": axis2, ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
       for shape in [shape for shape in all_shapes if len(shape) >= 2]
-      for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
+      for axis1 in range(-len(shape), len(shape))
+      for axis2 in [a for a in range(-len(shape), len(shape))
+                    if a % len(shape) != axis1 % len(shape)]
       for offset in list(range(-4, 4))))
   def testDiagonal(self, shape, dtype, offset, axis1, axis2, rng):
     onp_fun = lambda arg: onp.diagonal(arg, offset, axis1, axis2)","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 826d53c9f..009842661 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -520,7 +520,9 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""axis2"": axis2, ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
       for shape in [shape for shape in all_shapes if len(shape) >= 2]
-      for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
+      for axis1 in range(-len(shape), len(shape))
+      for axis2 in [a for a in range(-len(shape), len(shape))
+                    if a % len(shape) != axis1 % len(shape)]
       for offset in list(range(-4, 4))))
   def testDiagonal(self, shape, dtype, offset, axis1, axis2, rng):
     onp_fun = lambda arg: onp.diagonal(arg, offset, axis1, axis2)",No
tests/linalg_test.py,tests/linalg_test.py,b68c93d37f44432a58586d6038de30f5de74ba51,3db941f183b75b13c88ad8ab9e987d12af57748c,"Implement np.linalg.slogdet.

Change implementation of np.linalg.logdet to call np.linalg.slogdet.

Add support for complex64 LU decomposition.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2674fbe61..8326b7ca4 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -44,6 +44,9 @@ def float_types():
   return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
              for dtype in [onp.float32, onp.float64])
 
+def complex_types():
+  return {onp.complex64}
+
 
 class NumpyLinalgTest(jtu.JaxTestCase):
 
@@ -68,8 +71,8 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
        ""n"": n, ""dtype"": dtype, ""rng"": rng}
-      for n in [0, 4, 5, 200]
-      for dtype in float_types()
+      for n in [0, 4, 5, 50]
+      for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testDet(self, n, dtype, rng):
@@ -81,6 +84,22 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.det, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
+       ""n"": n, ""dtype"": dtype, ""rng"": rng}
+      for n in [0, 4, 10, 200]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSlogdet(self, n, dtype, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    self._CheckAgainstNumpy(onp.linalg.slogdet, np.linalg.slogdet, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.slogdet, args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
@@ -169,7 +188,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
        ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
       for shape in [(1, 1), (4, 5), (10, 5), (50, 50)]
-      for dtype in float_types()
+      for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLu(self, shape, dtype, rng):
@@ -188,7 +207,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
        ""n"": n, ""dtype"": dtype, ""rng"": rng}
       for n in [1, 4, 5, 200]
-      for dtype in float_types()
+      for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuFactor(self, n, dtype, rng):","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2674fbe61..8326b7ca4 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -44,6 +44,9 @@ def float_types():
   return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
              for dtype in [onp.float32, onp.float64])
 
+def complex_types():
+  return {onp.complex64}
+
 
 class NumpyLinalgTest(jtu.JaxTestCase):
 
@@ -68,8 +71,8 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
        ""n"": n, ""dtype"": dtype, ""rng"": rng}
-      for n in [0, 4, 5, 200]
-      for dtype in float_types()
+      for n in [0, 4, 5, 50]
+      for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testDet(self, n, dtype, rng):
@@ -81,6 +84,22 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.det, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
+       ""n"": n, ""dtype"": dtype, ""rng"": rng}
+      for n in [0, 4, 10, 200]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSlogdet(self, n, dtype, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    self._CheckAgainstNumpy(onp.linalg.slogdet, np.linalg.slogdet, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.slogdet, args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
@@ -169,7 +188,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
        ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
       for shape in [(1, 1), (4, 5), (10, 5), (50, 50)]
-      for dtype in float_types()
+      for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLu(self, shape, dtype, rng):
@@ -188,7 +207,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
        ""n"": n, ""dtype"": dtype, ""rng"": rng}
       for n in [1, 4, 5, 200]
-      for dtype in float_types()
+      for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuFactor(self, n, dtype, rng):",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,bd50c5b6b5e6dcc68e607c5934189d56adee18e0,c8c8fa1a4ffebe589f4ad90359251e2b5d70e08a,"Implement np.{isposinf,isneginf,nansum,nanprod,nanmin,nanmax,nan_to_num}.

No tests until we figure about a story about fast-math semantics.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f990fbdd5..da41b4829 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -603,6 +603,7 @@ around = round
 # To disable fast math mode on CPU, set the environment variable
 # XLA_FLAGS=--xla_cpu_enable_fast_math=false.
 
+@_wraps(onp.isfinite)
 def isfinite(x):
   dtype = _dtype(x)
   if issubdtype(dtype, floating):
@@ -612,6 +613,7 @@ def isfinite(x):
   else:
     return full_like(x, True, dtype=bool_)
 
+@_wraps(onp.isinf)
 def isinf(x):
   dtype = _dtype(x)
   if issubdtype(dtype, floating):
@@ -622,10 +624,33 @@ def isinf(x):
   else:
     return full_like(x, False, dtype=bool_)
 
+def _isposneginf(infinity, x):
+  dtype = _dtype(x)
+  if issubdtype(dtype, floating):
+    return lax.eq(x, infinity)
+  elif issubdtype(dtype, complexfloating):
+    raise ValueError(""isposinf/isneginf are not well defined for complex types"")
+  else:
+    return full_like(x, False, dtype=bool_)
+
+isposinf = _wraps(onp.isposinf)(partial(_isposneginf, inf))
+isneginf = _wraps(onp.isneginf)(partial(_isposneginf, -inf))
+
+@_wraps(onp.isnan)
 def isnan(x):
   return lax.bitwise_and(lax.bitwise_not(isfinite(x)),
                          lax.bitwise_not(isinf(x)))
 
+@_wraps(onp.nan_to_num)
+def nan_to_num(x, copy=True):
+  del copy
+  if iscomplexobj(x):
+    raise ValueError(""nan_to_num is not well defined for complex types"")
+  info = finfo(xla_bridge.canonicalize_dtype(_dtype(x)))
+  x = where(isnan(x), _constant_like(x, 0), x)
+  x = where(isposinf(x), _constant_like(x, info.max), x)
+  x = where(isneginf(x), _constant_like(x, info.min), x)
+  return x
 
 ### Reducers
 
@@ -740,6 +765,24 @@ def count_nonzero(a, axis=None):
              dtype=xla_bridge.canonicalize_dtype(onp.int_))
 
 
+def _make_nan_reduction(onp_reduction, np_reduction, init_val, nan_if_all_nan):
+  @_wraps(onp_reduction)
+  def nan_reduction(a, axis=None, out=None, keepdims=False, **kwargs):
+    out = np_reduction(where(isnan(a), _reduction_init_val(a, init_val), a),
+                       axis=axis, out=out, keepdims=keepdims, **kwargs)
+    if nan_if_all_nan:
+      return where(all(isnan(a), axis=axis, keepdims=keepdims),
+                   _constant_like(a, nan), out)
+    else:
+      return out
+
+  return nan_reduction
+
+nanmin = _make_nan_reduction(onp.nanmin, min, inf, nan_if_all_nan=True)
+nanmax = _make_nan_reduction(onp.nanmax, max, -inf, nan_if_all_nan=True)
+nansum = _make_nan_reduction(onp.nansum, sum, 0, nan_if_all_nan=False)
+nanprod = _make_nan_reduction(onp.nanprod, prod, 1, nan_if_all_nan=False)
+
 ### Array-creation functions
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index f990fbdd5..da41b4829 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -603,6 +603,7 @@ around = round
 # To disable fast math mode on CPU, set the environment variable
 # XLA_FLAGS=--xla_cpu_enable_fast_math=false.
 
+@_wraps(onp.isfinite)
 def isfinite(x):
   dtype = _dtype(x)
   if issubdtype(dtype, floating):
@@ -612,6 +613,7 @@ def isfinite(x):
   else:
     return full_like(x, True, dtype=bool_)
 
+@_wraps(onp.isinf)
 def isinf(x):
   dtype = _dtype(x)
   if issubdtype(dtype, floating):
@@ -622,10 +624,33 @@ def isinf(x):
   else:
     return full_like(x, False, dtype=bool_)
 
+def _isposneginf(infinity, x):
+  dtype = _dtype(x)
+  if issubdtype(dtype, floating):
+    return lax.eq(x, infinity)
+  elif issubdtype(dtype, complexfloating):
+    raise ValueError(""isposinf/isneginf are not well defined for complex types"")
+  else:
+    return full_like(x, False, dtype=bool_)
+
+isposinf = _wraps(onp.isposinf)(partial(_isposneginf, inf))
+isneginf = _wraps(onp.isneginf)(partial(_isposneginf, -inf))
+
+@_wraps(onp.isnan)
 def isnan(x):
   return lax.bitwise_and(lax.bitwise_not(isfinite(x)),
                          lax.bitwise_not(isinf(x)))
 
+@_wraps(onp.nan_to_num)
+def nan_to_num(x, copy=True):
+  del copy
+  if iscomplexobj(x):
+    raise ValueError(""nan_to_num is not well defined for complex types"")
+  info = finfo(xla_bridge.canonicalize_dtype(_dtype(x)))
+  x = where(isnan(x), _constant_like(x, 0), x)
+  x = where(isposinf(x), _constant_like(x, info.max), x)
+  x = where(isneginf(x), _constant_like(x, info.min), x)
+  return x
 
 ### Reducers
 
@@ -740,6 +765,24 @@ def count_nonzero(a, axis=None):
              dtype=xla_bridge.canonicalize_dtype(onp.int_))
 
 
+def _make_nan_reduction(onp_reduction, np_reduction, init_val, nan_if_all_nan):
+  @_wraps(onp_reduction)
+  def nan_reduction(a, axis=None, out=None, keepdims=False, **kwargs):
+    out = np_reduction(where(isnan(a), _reduction_init_val(a, init_val), a),
+                       axis=axis, out=out, keepdims=keepdims, **kwargs)
+    if nan_if_all_nan:
+      return where(all(isnan(a), axis=axis, keepdims=keepdims),
+                   _constant_like(a, nan), out)
+    else:
+      return out
+
+  return nan_reduction
+
+nanmin = _make_nan_reduction(onp.nanmin, min, inf, nan_if_all_nan=True)
+nanmax = _make_nan_reduction(onp.nanmax, max, -inf, nan_if_all_nan=True)
+nansum = _make_nan_reduction(onp.nansum, sum, 0, nan_if_all_nan=False)
+nanprod = _make_nan_reduction(onp.nanprod, prod, 1, nan_if_all_nan=False)
+
 ### Array-creation functions
 
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,0a75007c1924ef66d913e050d02464b1f2becb16,aae556bf6eaaf0e31242b818015aeecf25ca147a,Implement np.kron.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index da41b4829..db9ffe176 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1346,6 +1346,41 @@ def outer(a, b, out=None):
   return ravel(a)[:, None] * ravel(b)
 
 
+@_wraps(onp.kron)
+def kron(a, b):
+  a_shape = shape(a)
+  b_shape = shape(b)
+  a_ndims = len(a_shape)
+  b_ndims = len(b_shape)
+  a = array(a)
+  b = array(b)
+  d = _min(a_ndims, b_ndims)
+  if d == 0:
+    return a * b
+  a_broadcast_dims = list(range(a_ndims - d, a_ndims + d, 2))
+  a_broadcast_shape = onp.ones(a_ndims + d, dtype=onp.int64)
+  a_broadcast_shape[:-2*d] = a_shape[:-d]
+  a_broadcast_shape[a_broadcast_dims] = a_shape[-d:]
+
+  b_broadcast_dims = list(range(b_ndims -d + 1, b_ndims + d + 1, 2))
+  b_broadcast_shape = onp.ones(b_ndims + d, dtype=onp.int64)
+  b_broadcast_shape[:-2*d] = b_shape[:-d]
+  b_broadcast_shape[b_broadcast_dims] = b_shape[-d:]
+
+  if a_ndims > b_ndims:
+    out_shape = onp.array(a_shape, dtype=onp.int64)
+    out_shape[-d:] *= onp.array(b_shape, dtype=onp.int64)
+  else:
+    out_shape = onp.array(b_shape, dtype=onp.int64)
+    out_shape[-d:] *= onp.array(a_shape, dtype=onp.int64)
+
+  a_broadcast = lax.broadcast_in_dim(
+    a, a_broadcast_shape, list(range(a_ndims - d)) + a_broadcast_dims)
+  b_broadcast = lax.broadcast_in_dim(
+    b, b_broadcast_shape, list(range(b_ndims - d)) + b_broadcast_dims)
+  return lax.reshape(a_broadcast * b_broadcast, out_shape)
+
+
 ### Misc
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index da41b4829..db9ffe176 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1346,6 +1346,41 @@ def outer(a, b, out=None):
   return ravel(a)[:, None] * ravel(b)
 
 
+@_wraps(onp.kron)
+def kron(a, b):
+  a_shape = shape(a)
+  b_shape = shape(b)
+  a_ndims = len(a_shape)
+  b_ndims = len(b_shape)
+  a = array(a)
+  b = array(b)
+  d = _min(a_ndims, b_ndims)
+  if d == 0:
+    return a * b
+  a_broadcast_dims = list(range(a_ndims - d, a_ndims + d, 2))
+  a_broadcast_shape = onp.ones(a_ndims + d, dtype=onp.int64)
+  a_broadcast_shape[:-2*d] = a_shape[:-d]
+  a_broadcast_shape[a_broadcast_dims] = a_shape[-d:]
+
+  b_broadcast_dims = list(range(b_ndims -d + 1, b_ndims + d + 1, 2))
+  b_broadcast_shape = onp.ones(b_ndims + d, dtype=onp.int64)
+  b_broadcast_shape[:-2*d] = b_shape[:-d]
+  b_broadcast_shape[b_broadcast_dims] = b_shape[-d:]
+
+  if a_ndims > b_ndims:
+    out_shape = onp.array(a_shape, dtype=onp.int64)
+    out_shape[-d:] *= onp.array(b_shape, dtype=onp.int64)
+  else:
+    out_shape = onp.array(b_shape, dtype=onp.int64)
+    out_shape[-d:] *= onp.array(a_shape, dtype=onp.int64)
+
+  a_broadcast = lax.broadcast_in_dim(
+    a, a_broadcast_shape, list(range(a_ndims - d)) + a_broadcast_dims)
+  b_broadcast = lax.broadcast_in_dim(
+    b, b_broadcast_shape, list(range(b_ndims - d)) + b_broadcast_dims)
+  return lax.reshape(a_broadcast * b_broadcast, out_shape)
+
+
 ### Misc
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,0a75007c1924ef66d913e050d02464b1f2becb16,aae556bf6eaaf0e31242b818015aeecf25ca147a,Implement np.kron.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 009842661..c63baaf6f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -109,6 +109,7 @@ JAX_COMPOUND_OP_RECORDS = [
               test_name=""expm1_large""),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""kron"", 2, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
     op_record(""outer"", 2, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 009842661..c63baaf6f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -109,6 +109,7 @@ JAX_COMPOUND_OP_RECORDS = [
               test_name=""expm1_large""),
     op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""kron"", 2, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
     op_record(""outer"", 2, default_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,9888b33dc0d166158fb1c40a172c5a0b07cc568d,0a75007c1924ef66d913e050d02464b1f2becb16,Lower learning rate in NN notebook to fix divergence,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index ec81e583d..470770078 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -128,7 +128,7 @@
         ""\n"",
         ""layer_sizes = [784, 512, 512, 10]\n"",
         ""param_scale = 0.1\n"",
-        ""step_size = 0.001\n"",
+        ""step_size = 0.0001\n"",
         ""num_epochs = 10\n"",
         ""batch_size = 128\n"",
         ""n_targets = 10\n"",","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index ec81e583d..470770078 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -128,7 +128,7 @@
         ""\n"",
         ""layer_sizes = [784, 512, 512, 10]\n"",
         ""param_scale = 0.1\n"",
-        ""step_size = 0.001\n"",
+        ""step_size = 0.0001\n"",
         ""num_epochs = 10\n"",
         ""batch_size = 128\n"",
         ""n_targets = 10\n"",",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,12a36f4c2850cd937f0d18aae3aaa6ebd65c783a,aae556bf6eaaf0e31242b818015aeecf25ca147a,Lower learning rate in NN example notebook to fix divergence.,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index ec81e583d..470770078 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -128,7 +128,7 @@
         ""\n"",
         ""layer_sizes = [784, 512, 512, 10]\n"",
         ""param_scale = 0.1\n"",
-        ""step_size = 0.001\n"",
+        ""step_size = 0.0001\n"",
         ""num_epochs = 10\n"",
         ""batch_size = 128\n"",
         ""n_targets = 10\n"",","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index ec81e583d..470770078 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -128,7 +128,7 @@
         ""\n"",
         ""layer_sizes = [784, 512, 512, 10]\n"",
         ""param_scale = 0.1\n"",
-        ""step_size = 0.001\n"",
+        ""step_size = 0.0001\n"",
         ""num_epochs = 10\n"",
         ""batch_size = 128\n"",
         ""n_targets = 10\n"",",No
jax/interpreters/batching.py,jax/interpreters/batching.py,43f2e2a70a1d6e976d312561179a851a93aa0532,c8c8fa1a4ffebe589f4ad90359251e2b5d70e08a,cover unimplemented add_jaxvals_p batching case,"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 725e5b038..c7f5ffe13 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -229,40 +229,49 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
 def add_batched(batched_args, batch_dims):
-  xs, ys = batched_args
   bdx, bdy = batch_dims
   if bdx == bdy:
+    xs, ys = batched_args
     return add_jaxvals_p.bind(xs, ys), bdx
   else:
-    raise NotImplementedError  # TODO(mattjj)
+    xs, ys = map(bdim_at_front, batched_args, batch_dims)
+    return add_jaxvals_p.bind(xs, ys), 0
 primitive_batchers[add_jaxvals_p] = add_batched
 
 
 ### util
 
+# These utilities depend on primitives for things like broadcasting, reshaping,
+# and transposition on arrays. To avoid a circular import from depending on
+# lax.py, these functions use method dispatch on their arguments, which could be
+# DeviceArrays, numpy.ndarrays, or traced versions of those. This strategy
+# almost works, except for broadcast, for which raw numpy.ndarrays don't have a
+# method. To handle that case, the `broadcast` function uses a try/except.
+
 
 def bdim_at_front(x, bdim, broadcast_size=1):
   if bdim is None:
-    return broadcast(x, broadcast_size) if onp.ndim(x) else x
+    return broadcast(x, broadcast_size)
   else:
     return move_dim_to_front(x, bdim)
 
 def move_dim_to_front(x, dim):
-  assert 0 <= dim < onp.ndim(x)
-  if dim == 0:
-    return x
-  else:
-    perm = (dim,) + tuple(range(dim)) + tuple(range(dim + 1, onp.ndim(x)))
-    return x.transpose(perm)
-
-def handle_scalar_broadcasting(nd, x, bdim):
-  if bdim is None or nd == onp.ndim(x):
-    return x
+  aval = get_aval(x)
+  if type(aval) is AbstractTuple:
+    return pack(map(partial(move_dim_to_front, dim=dim), x))
+  elif isinstance(aval, ShapedArray):
+    assert 0 <= dim < onp.ndim(x)
+    if dim == 0:
+      return x
+    else:
+      perm = (dim,) + tuple(range(dim)) + tuple(range(dim + 1, onp.ndim(x)))
+      return x.transpose(perm)
   else:
-    return x.reshape(x.shape + (1,) * (nd - x.ndim))
+    raise TypeError(type(x))
 
 def dimsize(dim, x):
-  if type(x) is JaxTuple:
+  aval = get_aval(x)
+  if type(aval) is AbstractTuple:
     return reduce(set.union, map(partial(dimsize, dim), x))
   elif type(dim) is int:
     return {x.shape[dim]}
@@ -298,8 +307,26 @@ def moveaxis(sz, dst, src, x):
     raise TypeError(type(aval))
 
 def broadcast(x, sz):
-  try:
-    return x.broadcast((sz,))
-  except AttributeError:
-    assert not isinstance(x, Tracer)
-    return onp.broadcast_to(x, (sz,) + onp.shape(x))
+  aval = get_aval(x)
+  if type(aval) is AbstractTuple:
+    return pack(map(partial(broadcast, sz=sz), x))
+  elif isinstance(aval, ShapedArray):
+    # for scalars, don't actually broadcast
+    if not onp.ndim(x):
+      return x
+
+    # See comment at the top of this section about this try/except.
+    try:
+      return x.broadcast((sz,))
+    except AttributeError:
+      assert not isinstance(x, Tracer)
+      return onp.broadcast_to(x, (sz,) + onp.shape(x))
+  else:
+    raise TypeError(type(x))
+
+def handle_scalar_broadcasting(nd, x, bdim):
+  assert isinstance(get_aval(x), ShapedArray)
+  if bdim is None or nd == onp.ndim(x):
+    return x
+  else:
+    return x.reshape(x.shape + (1,) * (nd - x.ndim))","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 725e5b038..c7f5ffe13 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -229,40 +229,49 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
 def add_batched(batched_args, batch_dims):
-  xs, ys = batched_args
   bdx, bdy = batch_dims
   if bdx == bdy:
+    xs, ys = batched_args
     return add_jaxvals_p.bind(xs, ys), bdx
   else:
-    raise NotImplementedError  # TODO(mattjj)
+    xs, ys = map(bdim_at_front, batched_args, batch_dims)
+    return add_jaxvals_p.bind(xs, ys), 0
 primitive_batchers[add_jaxvals_p] = add_batched
 
 
 ### util
 
+# These utilities depend on primitives for things like broadcasting, reshaping,
+# and transposition on arrays. To avoid a circular import from depending on
+# lax.py, these functions use method dispatch on their arguments, which could be
+# DeviceArrays, numpy.ndarrays, or traced versions of those. This strategy
+# almost works, except for broadcast, for which raw numpy.ndarrays don't have a
+# method. To handle that case, the `broadcast` function uses a try/except.
+
 
 def bdim_at_front(x, bdim, broadcast_size=1):
   if bdim is None:
-    return broadcast(x, broadcast_size) if onp.ndim(x) else x
+    return broadcast(x, broadcast_size)
   else:
     return move_dim_to_front(x, bdim)
 
 def move_dim_to_front(x, dim):
-  assert 0 <= dim < onp.ndim(x)
-  if dim == 0:
-    return x
+  aval = get_aval(x)
+  if type(aval) is AbstractTuple:
+    return pack(map(partial(move_dim_to_front, dim=dim), x))
+  elif isinstance(aval, ShapedArray):
+    assert 0 <= dim < onp.ndim(x)
+    if dim == 0:
+      return x
+    else:
+      perm = (dim,) + tuple(range(dim)) + tuple(range(dim + 1, onp.ndim(x)))
+      return x.transpose(perm)
   else:
-    perm = (dim,) + tuple(range(dim)) + tuple(range(dim + 1, onp.ndim(x)))
-    return x.transpose(perm)
-
-def handle_scalar_broadcasting(nd, x, bdim):
-  if bdim is None or nd == onp.ndim(x):
-    return x
-  else:
-    return x.reshape(x.shape + (1,) * (nd - x.ndim))
+    raise TypeError(type(x))
 
 def dimsize(dim, x):
-  if type(x) is JaxTuple:
+  aval = get_aval(x)
+  if type(aval) is AbstractTuple:
     return reduce(set.union, map(partial(dimsize, dim), x))
   elif type(dim) is int:
     return {x.shape[dim]}
@@ -298,8 +307,26 @@ def moveaxis(sz, dst, src, x):
     raise TypeError(type(aval))
 
 def broadcast(x, sz):
-  try:
-    return x.broadcast((sz,))
-  except AttributeError:
-    assert not isinstance(x, Tracer)
-    return onp.broadcast_to(x, (sz,) + onp.shape(x))
+  aval = get_aval(x)
+  if type(aval) is AbstractTuple:
+    return pack(map(partial(broadcast, sz=sz), x))
+  elif isinstance(aval, ShapedArray):
+    # for scalars, don't actually broadcast
+    if not onp.ndim(x):
+      return x
+
+    # See comment at the top of this section about this try/except.
+    try:
+      return x.broadcast((sz,))
+    except AttributeError:
+      assert not isinstance(x, Tracer)
+      return onp.broadcast_to(x, (sz,) + onp.shape(x))
+  else:
+    raise TypeError(type(x))
+
+def handle_scalar_broadcasting(nd, x, bdim):
+  assert isinstance(get_aval(x), ShapedArray)
+  if bdim is None or nd == onp.ndim(x):
+    return x
+  else:
+    return x.reshape(x.shape + (1,) * (nd - x.ndim))",Yes
tests/batching_test.py,tests/batching_test.py,43f2e2a70a1d6e976d312561179a851a93aa0532,c8c8fa1a4ffebe589f4ad90359251e2b5d70e08a,cover unimplemented add_jaxvals_p batching case,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 4c15cc087..aeb35681d 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -24,7 +24,7 @@ import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
 from jax import lax
-from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev, hessian
 from jax.api import vmap
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
@@ -291,6 +291,21 @@ class BatchingTest(jtu.JaxTestCase):
     expected = np.array([True, False])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  def testHessian(self):
+    # test based on code from sindhwani@google
+    def fun(x, t):
+      return np.sum(np.power(np.maximum(x, 0.0), 2)) + t
+
+    x = onp.array([-1., -0.5, 0., 0.5, 1.0])
+
+    ans = hessian(lambda x: fun(x, 0.0))(x)
+    expected = onp.array([[0., 0., 0., 0., 0.],
+                          [0., 0., 0., 0., 0.],
+                          [0., 0.,0.5, 0., 0.],
+                          [0., 0., 0., 2., 0.],
+                          [0., 0., 0., 0., 2.]])
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 4c15cc087..aeb35681d 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -24,7 +24,7 @@ import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
 from jax import lax
-from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev, hessian
 from jax.api import vmap
 from jax.core import unit
 from jax.interpreters import partial_eval as pe
@@ -291,6 +291,21 @@ class BatchingTest(jtu.JaxTestCase):
     expected = np.array([True, False])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  def testHessian(self):
+    # test based on code from sindhwani@google
+    def fun(x, t):
+      return np.sum(np.power(np.maximum(x, 0.0), 2)) + t
+
+    x = onp.array([-1., -0.5, 0., 0.5, 1.0])
+
+    ans = hessian(lambda x: fun(x, 0.0))(x)
+    expected = onp.array([[0., 0., 0., 0., 0.],
+                          [0., 0., 0., 0., 0.],
+                          [0., 0.,0.5, 0., 0.],
+                          [0., 0., 0., 2., 0.],
+                          [0., 0., 0., 0., 2.]])
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/util.py,jax/util.py,8127392a18b5e8649f26183b7a73f37373fb28d7,e05ac8d5de0503dc2d3aad69059dd08d72a145c9,"Use get() rather than a try-catch block in memoized function lookup.

Currently backtraces often look like this:
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/p/jax/jax/util.py in memoized_fun(*args, **kwargs)
    133     try:
--> 134       return cache[key]
    135     except KeyError:

KeyError: ((lu, ShapedArray(int32[2,2])), ())

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
~/p/jax/jax/util.py in memoized_fun(*args, **kwargs)
    133     try:
--> 134       return cache[key]
    135     except KeyError:

KeyError: ((lu, xla_client.Shape(_dtype=dtype('int32'), _dimensions=(2, 2), _is_tuple=False, _minor_to_major=None)), ())

During handling of the above exception, another exception occurred:

NotImplementedError                       Traceback (most recent call last)
<ipython-input-26-d6c00d50e3c9> in <module>
```

The ""during handling of the above exception..."" message is mostly a distraction for the user that occurs because we perform the memoized function evaluation inside a `catch` block. By performing the function evaluation outside the catch block, we can get better backtraces without the distraction of the KeyError exception.

```","diff --git a/jax/util.py b/jax/util.py
index cfb8c3f8b..ed6fd0b12 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -126,20 +126,24 @@ def split_merge(predicate, xs):
   return lhs, rhs, merge
 
 
+class _MemoizeNoEntry(object):
+  pass
+
+_NO_MEMO_ENTRY = _MemoizeNoEntry
+
 def memoize(fun):
   cache = {}
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))
     try:
-      return cache[key]
-    except KeyError:
-      ans = cache[key] = fun(*args, **kwargs)
-      return ans
+      ans = cache.get(key, _NO_MEMO_ENTRY)
+      if ans != _NO_MEMO_ENTRY:
+        return ans
     except TypeError:
-      if allow_memoize_hash_failures:
-        return fun(*args, **kwargs)
-      else:
+      if not allow_memoize_hash_failures:
         raise
+    ans = cache[key] = fun(*args, **kwargs)
+    return ans
   return memoized_fun
 
 ","diff --git a/jax/util.py b/jax/util.py
index cfb8c3f8b..ed6fd0b12 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -126,20 +126,24 @@ def split_merge(predicate, xs):
   return lhs, rhs, merge
 
 
+class _MemoizeNoEntry(object):
+  pass
+
+_NO_MEMO_ENTRY = _MemoizeNoEntry
+
 def memoize(fun):
   cache = {}
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))
     try:
-      return cache[key]
-    except KeyError:
-      ans = cache[key] = fun(*args, **kwargs)
-      return ans
+      ans = cache.get(key, _NO_MEMO_ENTRY)
+      if ans != _NO_MEMO_ENTRY:
+        return ans
     except TypeError:
-      if allow_memoize_hash_failures:
-        return fun(*args, **kwargs)
-      else:
+      if not allow_memoize_hash_failures:
         raise
+    ans = cache[key] = fun(*args, **kwargs)
+    return ans
   return memoized_fun
 
 ",No
jax/numpy/linalg.py,jax/numpy/linalg.py,a4386457e2e8c644e227d740f3e5bd62cbaabb8e,df59c5122c439cca0e0663f4f664956273f900cd,"Fix test failures due to type mismatches in linear algebra tests.

Minor code cleanups.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 8d071829f..1ec612423 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -46,12 +46,12 @@ def slogdet(a):
     raise ValueError(msg.format(a_shape))
   lu, pivot = lax_linalg.lu(a)
   diag = np.diagonal(lu, axis1=-2, axis2=-1)
-  is_zero = np.any(diag == 0, axis=-1)
+  is_zero = np.any(diag == np.array(0, dtype=dtype), axis=-1)
   parity = np.count_nonzero(pivot != np.arange(a_shape[-1]), axis=-1)
   if np.iscomplexobj(a):
     sign = np.prod(diag / np.abs(diag))
   else:
-    sign = 1
+    sign = np.array(1, dtype=dtype)
     parity = parity + np.count_nonzero(diag < 0)
   sign = np.where(is_zero,
                   np.array(0, dtype=dtype),
@@ -59,7 +59,7 @@ def slogdet(a):
   logdet = np.where(
       is_zero, np.array(-np.inf, dtype=dtype),
       np.sum(np.log(np.abs(diag)), axis=-1))
-  return sign, logdet
+  return sign, np.real(logdet)
 
 
 @_wraps(onp.linalg.det)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 8d071829f..1ec612423 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -46,12 +46,12 @@ def slogdet(a):
     raise ValueError(msg.format(a_shape))
   lu, pivot = lax_linalg.lu(a)
   diag = np.diagonal(lu, axis1=-2, axis2=-1)
-  is_zero = np.any(diag == 0, axis=-1)
+  is_zero = np.any(diag == np.array(0, dtype=dtype), axis=-1)
   parity = np.count_nonzero(pivot != np.arange(a_shape[-1]), axis=-1)
   if np.iscomplexobj(a):
     sign = np.prod(diag / np.abs(diag))
   else:
-    sign = 1
+    sign = np.array(1, dtype=dtype)
     parity = parity + np.count_nonzero(diag < 0)
   sign = np.where(is_zero,
                   np.array(0, dtype=dtype),
@@ -59,7 +59,7 @@ def slogdet(a):
   logdet = np.where(
       is_zero, np.array(-np.inf, dtype=dtype),
       np.sum(np.log(np.abs(diag)), axis=-1))
-  return sign, logdet
+  return sign, np.real(logdet)
 
 
 @_wraps(onp.linalg.det)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,a4386457e2e8c644e227d740f3e5bd62cbaabb8e,df59c5122c439cca0e0663f4f664956273f900cd,"Fix test failures due to type mismatches in linear algebra tests.

Minor code cleanups.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 30056a8dc..75f47ea4b 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -64,7 +64,7 @@ def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
   m, n = np.shape(a)
   lu, pivots = lax_linalg.lu(a)
   permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
-  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  p = np.real(np.array(permutation == np.arange(m)[:, None], dtype=dtype))
   k = min(m, n)
   l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
   u = np.triu(lu)[:k, :]","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 30056a8dc..75f47ea4b 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -64,7 +64,7 @@ def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
   m, n = np.shape(a)
   lu, pivots = lax_linalg.lu(a)
   permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
-  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  p = np.real(np.array(permutation == np.arange(m)[:, None], dtype=dtype))
   k = min(m, n)
   l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
   u = np.triu(lu)[:k, :]",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,a4386457e2e8c644e227d740f3e5bd62cbaabb8e,df59c5122c439cca0e0663f4f664956273f900cd,"Fix test failures due to type mismatches in linear algebra tests.

Minor code cleanups.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index ccff5645d..c8e52cd60 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -29,7 +29,8 @@ from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
-from jaxlib.xla_client import Shape
+
+Shape = xla_client.Shape
 
 
 cdef register_cpu_custom_call_target(fn_name, void* fn):","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index ccff5645d..c8e52cd60 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -29,7 +29,8 @@ from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, spotrf, dpotrf
 
 import numpy as np
 from jaxlib import xla_client
-from jaxlib.xla_client import Shape
+
+Shape = xla_client.Shape
 
 
 cdef register_cpu_custom_call_target(fn_name, void* fn):",No
tests/linalg_test.py,tests/linalg_test.py,a4386457e2e8c644e227d740f3e5bd62cbaabb8e,df59c5122c439cca0e0663f4f664956273f900cd,"Fix test failures due to type mismatches in linear algebra tests.

Minor code cleanups.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 8326b7ca4..2a1908265 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -197,7 +197,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng(shape, dtype)]
 
-    self._CheckAgainstNumpy(osp.linalg.lu, jsp.linalg.lu, args_maker,
+    self._CheckAgainstNumpy(jsp.linalg.lu, osp.linalg.lu, args_maker,
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
 
@@ -216,7 +216,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
-    self._CheckAgainstNumpy(osp.linalg.lu_factor, jsp.linalg.lu_factor,
+    self._CheckAgainstNumpy(jsp.linalg.lu_factor, osp.linalg.lu_factor,
                             args_maker, check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu_factor, args_maker, check_dtypes=True)
 ","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 8326b7ca4..2a1908265 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -197,7 +197,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng(shape, dtype)]
 
-    self._CheckAgainstNumpy(osp.linalg.lu, jsp.linalg.lu, args_maker,
+    self._CheckAgainstNumpy(jsp.linalg.lu, osp.linalg.lu, args_maker,
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
 
@@ -216,7 +216,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
-    self._CheckAgainstNumpy(osp.linalg.lu_factor, jsp.linalg.lu_factor,
+    self._CheckAgainstNumpy(jsp.linalg.lu_factor, osp.linalg.lu_factor,
                             args_maker, check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu_factor, args_maker, check_dtypes=True)
 ",No
jax/lax_linalg.py,jax/lax_linalg.py,06135fa6f5631e71a4304b323cb315a11ceee0b2,54772e1b9b1a620a42b782a8e2712daf9d42aede,"Implement numpy.linalg.solve and scipy.linalg.solve.

Make Cholesky and TriangularSolve work for complex numbers on CPU. The HLO implementations are broken for complex numbers on GPU/TPU, so no tests enabled for these yet.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 5f41a9ec8..f50933452 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -55,6 +55,8 @@ def _T(x):
 
 # primitives
 
+_cpu_lapack_types = {np.float32, np.float64, np.complex64}
+
 
 def cholesky_jvp_rule(primals, tangents):
   x, = primals
@@ -70,14 +72,14 @@ def cholesky_jvp_rule(primals, tangents):
       L, tmp, left_side=True, transpose_a=False, lower=True)))
   return L, L_dot
 
-cholesky_p = standard_unop(_float, 'cholesky')
+cholesky_p = standard_unop(_float | _complex, 'cholesky')
 ad.primitive_jvps[cholesky_p] = cholesky_jvp_rule
 
 
 def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and (
-    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
@@ -140,14 +142,11 @@ ad.primitive_transposes[triangular_solve_p] = triangular_solve_transpose_rule
 def triangular_solve_cpu_translation_rule(
     c, a, b, left_side, lower, transpose_a, conjugate_a):
   shape = c.GetShape(a)
-  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
-    return lapack.jax_trsm(
-      c, c.ConstantF32Scalar(1.0), a, b, left_side, lower, transpose_a,
-      conjugate_a)
-  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     return lapack.jax_trsm(
-      c, c.ConstantF64Scalar(1.0), a, b, left_side, lower, transpose_a,
-      conjugate_a)
+      c, c.Constant(onp.array(1, dtype=dtype)), a, b, left_side, lower,
+                    transpose_a, conjugate_a)
   else:
     # Fall back to the HLO implementation for batched triangular_solve or
     # unsupported types.
@@ -190,11 +189,10 @@ lu_p.def_impl(lu_impl)
 lu_p.def_abstract_eval(lu_abstract_eval)
 xla.translations[lu_p] = lu_translation_rule
 
-_lu_cpu_types = {np.float32, np.float64, np.complex64}
-
 def lu_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and shape.element_type().type in _lu_cpu_types:
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     out = lapack.jax_getrf(c, operand)
     lu = c.GetTupleElement(out, 0)
     # Subtract 1 from the pivot to get 0-based indices.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 5f41a9ec8..f50933452 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -55,6 +55,8 @@ def _T(x):
 
 # primitives
 
+_cpu_lapack_types = {np.float32, np.float64, np.complex64}
+
 
 def cholesky_jvp_rule(primals, tangents):
   x, = primals
@@ -70,14 +72,14 @@ def cholesky_jvp_rule(primals, tangents):
       L, tmp, left_side=True, transpose_a=False, lower=True)))
   return L, L_dot
 
-cholesky_p = standard_unop(_float, 'cholesky')
+cholesky_p = standard_unop(_float | _complex, 'cholesky')
 ad.primitive_jvps[cholesky_p] = cholesky_jvp_rule
 
 
 def cholesky_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and (
-    shape.element_type() == np.float32 or shape.element_type() == np.float64):
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     return c.GetTupleElement(lapack.jax_potrf(c, operand, lower=True), 0)
   else:
     # Fall back to the HLO implementation for batched Cholesky decomposition or
@@ -140,14 +142,11 @@ ad.primitive_transposes[triangular_solve_p] = triangular_solve_transpose_rule
 def triangular_solve_cpu_translation_rule(
     c, a, b, left_side, lower, transpose_a, conjugate_a):
   shape = c.GetShape(a)
-  if len(shape.dimensions()) == 2 and shape.element_type() == np.float32:
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     return lapack.jax_trsm(
-      c, c.ConstantF32Scalar(1.0), a, b, left_side, lower, transpose_a,
-      conjugate_a)
-  elif len(shape.dimensions()) == 2 and shape.element_type() == np.float64:
-    return lapack.jax_trsm(
-      c, c.ConstantF64Scalar(1.0), a, b, left_side, lower, transpose_a,
-      conjugate_a)
+      c, c.Constant(onp.array(1, dtype=dtype)), a, b, left_side, lower,
+                    transpose_a, conjugate_a)
   else:
     # Fall back to the HLO implementation for batched triangular_solve or
     # unsupported types.
@@ -190,11 +189,10 @@ lu_p.def_impl(lu_impl)
 lu_p.def_abstract_eval(lu_abstract_eval)
 xla.translations[lu_p] = lu_translation_rule
 
-_lu_cpu_types = {np.float32, np.float64, np.complex64}
-
 def lu_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)
-  if len(shape.dimensions()) == 2 and shape.element_type().type in _lu_cpu_types:
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     out = lapack.jax_getrf(c, operand)
     lu = c.GetTupleElement(out, 0)
     # Subtract 1 from the pivot to get 0-based indices.",Yes
jax/numpy/linalg.py,jax/numpy/linalg.py,06135fa6f5631e71a4304b323cb315a11ceee0b2,54772e1b9b1a620a42b782a8e2712daf9d42aede,"Implement numpy.linalg.solve and scipy.linalg.solve.

Make Cholesky and TriangularSolve work for complex numbers on CPU. The HLO implementations are broken for complex numbers on GPU/TPU, so no tests enabled for these yet.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 1ec612423..6f87e196e 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -92,6 +92,42 @@ def qr(a, mode=""reduced""):
     return r
   return q, r
 
+
+@_wraps(onp.linalg.solve)
+def solve(a, b):
+  a_shape = np.shape(a)
+  b_shape = np.shape(b)
+  a_ndims = len(a_shape)
+  b_ndims = len(b_shape)
+  if not (a_ndims >= 2 and a_shape[-1] == a_shape[-2] and
+          (a_ndims == b_ndims or a_ndims == b_ndims + 1)):
+    msg = (""The arguments to solve must have shapes a=[..., m, m] and ""
+           ""b=[..., m, k] or b=[..., m]; got a={} and b={}"")
+    raise ValueError(msg.format(a_shape, b_shape))
+  lu, pivots = lax_linalg.lu(a)
+  dtype = lax._dtype(a)
+
+  # TODO(phawkins): add unit_diagonal support to solve_triangular, use it here
+  # instead of explicit masking of l.
+  m = a_shape[-1]
+  l = np.tril(lu, -1)[:, :m] + np.eye(m, m, dtype=dtype)
+
+  # TODO(phawkins): triangular_solve only supports matrices on the RHS, so we
+  # add a dummy dimension. Extend it to support vectors and simplify this.
+  x = b if a_ndims == b_ndims else b[..., None]
+
+  # TODO(phawkins): use a gather rather than a matrix multiplication here.
+  permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
+  p = np.array(permutation[:, None] == np.arange(m), dtype=dtype)
+  x = np.matmul(p, x)
+
+  x = lax_linalg.triangular_solve(l, x, left_side=True, lower=True)
+  x = lax_linalg.triangular_solve(lu, x, left_side=True, lower=False)
+
+  return x[..., 0] if a_ndims != b_ndims else x
+
+
+
 for func in get_module_functions(onp.linalg):
   if func.__name__ not in globals():
     globals()[func.__name__] = _not_implemented(func)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 1ec612423..6f87e196e 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -92,6 +92,42 @@ def qr(a, mode=""reduced""):
     return r
   return q, r
 
+
+@_wraps(onp.linalg.solve)
+def solve(a, b):
+  a_shape = np.shape(a)
+  b_shape = np.shape(b)
+  a_ndims = len(a_shape)
+  b_ndims = len(b_shape)
+  if not (a_ndims >= 2 and a_shape[-1] == a_shape[-2] and
+          (a_ndims == b_ndims or a_ndims == b_ndims + 1)):
+    msg = (""The arguments to solve must have shapes a=[..., m, m] and ""
+           ""b=[..., m, k] or b=[..., m]; got a={} and b={}"")
+    raise ValueError(msg.format(a_shape, b_shape))
+  lu, pivots = lax_linalg.lu(a)
+  dtype = lax._dtype(a)
+
+  # TODO(phawkins): add unit_diagonal support to solve_triangular, use it here
+  # instead of explicit masking of l.
+  m = a_shape[-1]
+  l = np.tril(lu, -1)[:, :m] + np.eye(m, m, dtype=dtype)
+
+  # TODO(phawkins): triangular_solve only supports matrices on the RHS, so we
+  # add a dummy dimension. Extend it to support vectors and simplify this.
+  x = b if a_ndims == b_ndims else b[..., None]
+
+  # TODO(phawkins): use a gather rather than a matrix multiplication here.
+  permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
+  p = np.array(permutation[:, None] == np.arange(m), dtype=dtype)
+  x = np.matmul(p, x)
+
+  x = lax_linalg.triangular_solve(l, x, left_side=True, lower=True)
+  x = lax_linalg.triangular_solve(lu, x, left_side=True, lower=False)
+
+  return x[..., 0] if a_ndims != b_ndims else x
+
+
+
 for func in get_module_functions(onp.linalg):
   if func.__name__ not in globals():
     globals()[func.__name__] = _not_implemented(func)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,06135fa6f5631e71a4304b323cb315a11ceee0b2,54772e1b9b1a620a42b782a8e2712daf9d42aede,"Implement numpy.linalg.solve and scipy.linalg.solve.

Make Cholesky and TriangularSolve work for complex numbers on CPU. The HLO implementations are broken for complex numbers on GPU/TPU, so no tests enabled for these yet.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 75f47ea4b..23e3c8b4b 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -29,11 +29,13 @@ from ..numpy import linalg as np_linalg
 
 _EXPERIMENTAL_WARNING = ""scipy.linalg support is experimental and may cause silent failures or wrong outputs""
 
+_T = lambda x: np.swapaxes(x, -1, -2)
+
 @_wraps(scipy.linalg.cholesky)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
-  l = lax_linalg.cholesky(a)
+  l = lax_linalg.cholesky(a if lower else np.conj(a.T))
   return l if lower else np.conj(l.T)
 
 
@@ -60,9 +62,9 @@ def lu_factor(a, overwrite_a=False, check_finite=True):
 @_wraps(scipy.linalg.lu)
 def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
+  lu, pivots = lax_linalg.lu(a)
   dtype = lax._dtype(a)
   m, n = np.shape(a)
-  lu, pivots = lax_linalg.lu(a)
   permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
   p = np.real(np.array(permutation == np.arange(m)[:, None], dtype=dtype))
   k = min(m, n)
@@ -93,6 +95,32 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
     return r
   return q, r
 
+@_wraps(scipy.linalg.solve)
+def solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False,
+          debug=False, check_finite=True):
+  del overwrite_a, overwrite_b, debug, check_finite
+  if not sym_pos:
+    return np_linalg.solve(a, b)
+
+  a_shape = np.shape(a)
+  b_shape = np.shape(b)
+  a_ndims = len(a_shape)
+  b_ndims = len(b_shape)
+  if not (a_ndims >= 2 and a_shape[-1] == a_shape[-2] and
+          (a_ndims == b_ndims or a_ndims == b_ndims + 1)):
+    msg = (""The arguments to solve must have shapes a=[..., m, m] and ""
+           ""b=[..., m, k] or b=[..., m]; got a={} and b={}"")
+
+  # TODO(phawkins): triangular_solve only supports matrices on the RHS, so we
+  # add a dummy dimension. Extend it to support vectors and simplify this.
+  b = b if a_ndims == b_ndims else b[..., None]
+  lu = lax_linalg.cholesky(a if lower else np.conj(_T(a)))
+  b = lax_linalg.triangular_solve(lu, b, left_side=True, lower=True)
+  b = lax_linalg.triangular_solve(lu, b, left_side=True, lower=True,
+                                  transpose_a=True, conjugate_a=True)
+  return b[..., 0] if a_ndims != b_ndims else b
+
+
 
 @_wraps(scipy.linalg.solve_triangular)
 def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 75f47ea4b..23e3c8b4b 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -29,11 +29,13 @@ from ..numpy import linalg as np_linalg
 
 _EXPERIMENTAL_WARNING = ""scipy.linalg support is experimental and may cause silent failures or wrong outputs""
 
+_T = lambda x: np.swapaxes(x, -1, -2)
+
 @_wraps(scipy.linalg.cholesky)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
-  l = lax_linalg.cholesky(a)
+  l = lax_linalg.cholesky(a if lower else np.conj(a.T))
   return l if lower else np.conj(l.T)
 
 
@@ -60,9 +62,9 @@ def lu_factor(a, overwrite_a=False, check_finite=True):
 @_wraps(scipy.linalg.lu)
 def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
+  lu, pivots = lax_linalg.lu(a)
   dtype = lax._dtype(a)
   m, n = np.shape(a)
-  lu, pivots = lax_linalg.lu(a)
   permutation = lax_linalg.lu_pivots_to_permutation(pivots, m)
   p = np.real(np.array(permutation == np.arange(m)[:, None], dtype=dtype))
   k = min(m, n)
@@ -93,6 +95,32 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
     return r
   return q, r
 
+@_wraps(scipy.linalg.solve)
+def solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False,
+          debug=False, check_finite=True):
+  del overwrite_a, overwrite_b, debug, check_finite
+  if not sym_pos:
+    return np_linalg.solve(a, b)
+
+  a_shape = np.shape(a)
+  b_shape = np.shape(b)
+  a_ndims = len(a_shape)
+  b_ndims = len(b_shape)
+  if not (a_ndims >= 2 and a_shape[-1] == a_shape[-2] and
+          (a_ndims == b_ndims or a_ndims == b_ndims + 1)):
+    msg = (""The arguments to solve must have shapes a=[..., m, m] and ""
+           ""b=[..., m, k] or b=[..., m]; got a={} and b={}"")
+
+  # TODO(phawkins): triangular_solve only supports matrices on the RHS, so we
+  # add a dummy dimension. Extend it to support vectors and simplify this.
+  b = b if a_ndims == b_ndims else b[..., None]
+  lu = lax_linalg.cholesky(a if lower else np.conj(_T(a)))
+  b = lax_linalg.triangular_solve(lu, b, left_side=True, lower=True)
+  b = lax_linalg.triangular_solve(lu, b, left_side=True, lower=True,
+                                  transpose_a=True, conjugate_a=True)
+  return b[..., 0] if a_ndims != b_ndims else b
+
+
 
 @_wraps(scipy.linalg.solve_triangular)
 def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,06135fa6f5631e71a4304b323cb315a11ceee0b2,54772e1b9b1a620a42b782a8e2712daf9d42aede,"Implement numpy.linalg.solve and scipy.linalg.solve.

Make Cholesky and TriangularSolve work for complex numbers on CPU. The HLO implementations are broken for complex numbers on GPU/TPU, so no tests enabled for these yet.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index c8e52cd60..9f71fbd2b 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -24,8 +24,9 @@ from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
-from scipy.linalg.cython_blas cimport strsm, dtrsm
-from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, spotrf, dpotrf
+from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
+from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
 
 import numpy as np
 from jaxlib import xla_client
@@ -102,6 +103,35 @@ cdef void blas_dtrsm(void* out, void** data) nogil:
 register_cpu_custom_call_target(b""blas_dtrsm"", <void*>(blas_dtrsm))
 
 
+cdef void blas_ctrsm(void* out, void** data) nogil:
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
+  cdef float complex* alpha = <float complex*>(data[6])
+  cdef float complex* a = <float complex*>(data[7])
+  cdef float complex* b = <float complex*>(data[8])
+
+  cdef float complex* x = <float complex*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(float complex))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m
+  cdef int ldb = m if left_side else n
+  ctrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_ctrsm"", <void*>(blas_ctrsm))
+
 def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
              conj_a=False, diag=False):
   b_shape = c.GetShape(b)
@@ -118,17 +148,22 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
 
   if dtype == np.float32:
     fn = b""blas_strsm""
-  elif dtype == np.float64: 
+  elif dtype == np.float64:
     fn = b""blas_dtrsm""
+  elif dtype == np.complex64:
+    fn = b""blas_ctrsm""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
+  if conj_a and not trans_a:
+    raise NotImplementedError(""Conjugation without transposition not supported"")
+
   return c.CustomCall(
       fn,
       operands=(
         c.ConstantS32Scalar(int(left_side)),
         c.ConstantS32Scalar(int(lower)),
-        c.ConstantS32Scalar(1 if trans_a else 0),
+        c.ConstantS32Scalar((2 if conj_a else 1) if trans_a else 0),
         c.ConstantS32Scalar(int(diag)),
         c.ConstantS32Scalar(m),
         c.ConstantS32Scalar(n),
@@ -290,6 +325,35 @@ cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_dpotrf"", <void*>(lapack_dpotrf))
 
+
+cdef void lapack_cpotrf(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float complex* a_in = <float complex*>(data[2])
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef int* info = <int*>(out[1])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(float complex))
+
+  cpotrf(&uplo, &n, a_out, &n, info)
+
+  # cpotrf leaves junk in the part of the triangle that is not written; zero it.
+  cdef int i
+  cdef int j
+  if lower:
+    for i in range(n):
+      for j in range(i):
+        a_out[i * n + j] = 0
+  else:
+    for i in range(n):
+      for j in range(i, n):
+        a_out[i * n + j] = 0
+
+register_cpu_custom_call_target(b""lapack_cpotrf"", <void*>(lapack_cpotrf))
+
 def jax_potrf(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -300,8 +364,10 @@ def jax_potrf(c, a, lower=False):
     raise ValueError(""potrf expects a square matrix, got {}"".format(a_shape))
   if dtype == np.float32:
     fn = b""lapack_spotrf""
-  elif dtype == np.float64: 
+  elif dtype == np.float64:
     fn = b""lapack_dpotrf""
+  elif dtype == np.complex64:
+    fn = b""lapack_cpotrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index c8e52cd60..9f71fbd2b 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -24,8 +24,9 @@ from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
-from scipy.linalg.cython_blas cimport strsm, dtrsm
-from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, spotrf, dpotrf
+from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
+from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
 
 import numpy as np
 from jaxlib import xla_client
@@ -102,6 +103,35 @@ cdef void blas_dtrsm(void* out, void** data) nogil:
 register_cpu_custom_call_target(b""blas_dtrsm"", <void*>(blas_dtrsm))
 
 
+cdef void blas_ctrsm(void* out, void** data) nogil:
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
+  cdef float complex* alpha = <float complex*>(data[6])
+  cdef float complex* a = <float complex*>(data[7])
+  cdef float complex* b = <float complex*>(data[8])
+
+  cdef float complex* x = <float complex*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(float complex))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m
+  cdef int ldb = m if left_side else n
+  ctrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_ctrsm"", <void*>(blas_ctrsm))
+
 def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
              conj_a=False, diag=False):
   b_shape = c.GetShape(b)
@@ -118,17 +148,22 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
 
   if dtype == np.float32:
     fn = b""blas_strsm""
-  elif dtype == np.float64: 
+  elif dtype == np.float64:
     fn = b""blas_dtrsm""
+  elif dtype == np.complex64:
+    fn = b""blas_ctrsm""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
+  if conj_a and not trans_a:
+    raise NotImplementedError(""Conjugation without transposition not supported"")
+
   return c.CustomCall(
       fn,
       operands=(
         c.ConstantS32Scalar(int(left_side)),
         c.ConstantS32Scalar(int(lower)),
-        c.ConstantS32Scalar(1 if trans_a else 0),
+        c.ConstantS32Scalar((2 if conj_a else 1) if trans_a else 0),
         c.ConstantS32Scalar(int(diag)),
         c.ConstantS32Scalar(m),
         c.ConstantS32Scalar(n),
@@ -290,6 +325,35 @@ cdef void lapack_dpotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_dpotrf"", <void*>(lapack_dpotrf))
 
+
+cdef void lapack_cpotrf(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float complex* a_in = <float complex*>(data[2])
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef int* info = <int*>(out[1])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(float complex))
+
+  cpotrf(&uplo, &n, a_out, &n, info)
+
+  # cpotrf leaves junk in the part of the triangle that is not written; zero it.
+  cdef int i
+  cdef int j
+  if lower:
+    for i in range(n):
+      for j in range(i):
+        a_out[i * n + j] = 0
+  else:
+    for i in range(n):
+      for j in range(i, n):
+        a_out[i * n + j] = 0
+
+register_cpu_custom_call_target(b""lapack_cpotrf"", <void*>(lapack_cpotrf))
+
 def jax_potrf(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -300,8 +364,10 @@ def jax_potrf(c, a, lower=False):
     raise ValueError(""potrf expects a square matrix, got {}"".format(a_shape))
   if dtype == np.float32:
     fn = b""lapack_spotrf""
-  elif dtype == np.float64: 
+  elif dtype == np.float64:
     fn = b""lapack_dpotrf""
+  elif dtype == np.complex64:
+    fn = b""lapack_cpotrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ",No
tests/linalg_test.py,tests/linalg_test.py,06135fa6f5631e71a4304b323cb315a11ceee0b2,54772e1b9b1a620a42b782a8e2712daf9d42aede,"Implement numpy.linalg.solve and scipy.linalg.solve.

Make Cholesky and TriangularSolve work for complex numbers on CPU. The HLO implementations are broken for complex numbers on GPU/TPU, so no tests enabled for these yet.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2a1908265..4d42334e8 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -60,13 +60,12 @@ class NumpyLinalgTest(jtu.JaxTestCase):
   def testCholesky(self, shape, dtype, rng):
     def args_maker():
       a = rng(shape, dtype)
-      return [onp.matmul(a, T(a))]
+      return [onp.matmul(a, np.conj(T(a)))]
 
     self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
-  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
@@ -74,6 +73,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for n in [0, 4, 5, 50]
       for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testDet(self, n, dtype, rng):
     if not hasattr(lapack, ""jax_getrf""):
@@ -155,6 +155,31 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     if not full_matrices and m >= n:
         jtu.check_jvp(np.linalg.qr, partial(jvp, np.linalg.qr), (a,))
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((1, 1), (1, 1)),
+          ((4, 4), (4,)),
+          ((8, 8), (8, 4)),
+      ]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSolve(self, lhs_shape, rhs_shape, dtype, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    self._CheckAgainstNumpy(onp.linalg.solve, np.linalg.solve, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.solve, args_maker, check_dtypes=True)
+
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
@@ -221,6 +246,46 @@ class ScipyLinalgTest(jtu.JaxTestCase):
     self._CompileAndCheck(jsp.linalg.lu_factor, args_maker, check_dtypes=True)
 
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_sym_pos={}_lower={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           sym_pos, lower),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""sym_pos"": sym_pos, ""lower"": lower, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((1, 1), (1, 1)),
+          ((4, 4), (4,)),
+          ((8, 8), (8, 4)),
+      ]
+      for sym_pos, lower in [
+        (False, False),
+        (True, False),
+        (True, True),
+      ]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSolve(self, lhs_shape, rhs_shape, dtype, sym_pos, lower, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    osp_fun = lambda lhs, rhs: osp.linalg.solve(lhs, rhs, sym_pos=sym_pos, lower=lower)
+    jsp_fun = lambda lhs, rhs: jsp.linalg.solve(lhs, rhs, sym_pos=sym_pos, lower=lower)
+
+    def args_maker():
+      a = rng(lhs_shape, dtype)
+      if sym_pos:
+        a = onp.matmul(a, onp.conj(T(a)))
+        a = onp.tril(a) if lower else onp.triu(a)
+      return [a, rng(rhs_shape, dtype)]
+
+    self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(jsp_fun, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_transposea={}"".format(","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2a1908265..4d42334e8 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -60,13 +60,12 @@ class NumpyLinalgTest(jtu.JaxTestCase):
   def testCholesky(self, shape, dtype, rng):
     def args_maker():
       a = rng(shape, dtype)
-      return [onp.matmul(a, T(a))]
+      return [onp.matmul(a, np.conj(T(a)))]
 
     self._CheckAgainstNumpy(onp.linalg.cholesky, np.linalg.cholesky, args_maker,
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.cholesky, args_maker, check_dtypes=True)
 
-  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_n={}"".format(jtu.format_shape_dtype_string((n,n), dtype)),
@@ -74,6 +73,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for n in [0, 4, 5, 50]
       for dtype in float_types() | complex_types()
       for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testDet(self, n, dtype, rng):
     if not hasattr(lapack, ""jax_getrf""):
@@ -155,6 +155,31 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     if not full_matrices and m >= n:
         jtu.check_jvp(np.linalg.qr, partial(jvp, np.linalg.qr), (a,))
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((1, 1), (1, 1)),
+          ((4, 4), (4,)),
+          ((8, 8), (8, 4)),
+      ]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSolve(self, lhs_shape, rhs_shape, dtype, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    self._CheckAgainstNumpy(onp.linalg.solve, np.linalg.solve, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(np.linalg.solve, args_maker, check_dtypes=True)
+
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
@@ -221,6 +246,46 @@ class ScipyLinalgTest(jtu.JaxTestCase):
     self._CompileAndCheck(jsp.linalg.lu_factor, args_maker, check_dtypes=True)
 
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_sym_pos={}_lower={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           sym_pos, lower),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""sym_pos"": sym_pos, ""lower"": lower, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((1, 1), (1, 1)),
+          ((4, 4), (4,)),
+          ((8, 8), (8, 4)),
+      ]
+      for sym_pos, lower in [
+        (False, False),
+        (True, False),
+        (True, True),
+      ]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSolve(self, lhs_shape, rhs_shape, dtype, sym_pos, lower, rng):
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    osp_fun = lambda lhs, rhs: osp.linalg.solve(lhs, rhs, sym_pos=sym_pos, lower=lower)
+    jsp_fun = lambda lhs, rhs: jsp.linalg.solve(lhs, rhs, sym_pos=sym_pos, lower=lower)
+
+    def args_maker():
+      a = rng(lhs_shape, dtype)
+      if sym_pos:
+        a = onp.matmul(a, onp.conj(T(a)))
+        a = onp.tril(a) if lower else onp.triu(a)
+      return [a, rng(rhs_shape, dtype)]
+
+    self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker,
+                            check_dtypes=True, tol=1e-3)
+    self._CompileAndCheck(jsp_fun, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":
        ""_lhs={}_rhs={}_lower={}_transposea={}"".format(",No
jax/scipy/linalg.py,jax/scipy/linalg.py,b1ff7b4ff9b61e10482c35a2f23020484b029531,06135fa6f5631e71a4304b323cb315a11ceee0b2,Fix missing `raise` in error path.,"diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 23e3c8b4b..088d10751 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -110,6 +110,7 @@ def solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False
           (a_ndims == b_ndims or a_ndims == b_ndims + 1)):
     msg = (""The arguments to solve must have shapes a=[..., m, m] and ""
            ""b=[..., m, k] or b=[..., m]; got a={} and b={}"")
+    raise ValueError(msg.format(a_shape, b_shape))
 
   # TODO(phawkins): triangular_solve only supports matrices on the RHS, so we
   # add a dummy dimension. Extend it to support vectors and simplify this.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 23e3c8b4b..088d10751 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -110,6 +110,7 @@ def solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False
           (a_ndims == b_ndims or a_ndims == b_ndims + 1)):
     msg = (""The arguments to solve must have shapes a=[..., m, m] and ""
            ""b=[..., m, k] or b=[..., m]; got a={} and b={}"")
+    raise ValueError(msg.format(a_shape, b_shape))
 
   # TODO(phawkins): triangular_solve only supports matrices on the RHS, so we
   # add a dummy dimension. Extend it to support vectors and simplify this.",No
tests/linalg_test.py,tests/linalg_test.py,87824ee03e08461f5aedf0324c46f23fcd6485c8,54772e1b9b1a620a42b782a8e2712daf9d42aede,skip numerically unstable test,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2a1908265..2122bf62c 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -280,6 +280,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
   def testSolveTriangularGrad(self, lower, transpose_a, lhs_shape,
                                      rhs_shape, dtype, rng):
     # TODO(frostig): change ensemble to support a bigger rtol
+    self.skipTest(""rtol does not cover all devices and precision modes"")
     A = np.tril(rng(lhs_shape, dtype) + 5 * onp.eye(lhs_shape[-1], dtype=dtype))
     A = A if lower else T(A)
     B = rng(rhs_shape, dtype)","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 2a1908265..2122bf62c 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -280,6 +280,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
   def testSolveTriangularGrad(self, lower, transpose_a, lhs_shape,
                                      rhs_shape, dtype, rng):
     # TODO(frostig): change ensemble to support a bigger rtol
+    self.skipTest(""rtol does not cover all devices and precision modes"")
     A = np.tril(rng(lhs_shape, dtype) + 5 * onp.eye(lhs_shape[-1], dtype=dtype))
     A = A if lower else T(A)
     B = rng(rhs_shape, dtype)",No
jax/lax.py,jax/lax.py,d48e7ef43d933e9b56dff7bd026f5dd576930543,648fd43619db82132ca6e8c44e4c2c0a32ac3406,"add batching (vmap) rule for lax.dynamic_slice

fixes #165","diff --git a/jax/lax.py b/jax/lax.py
index 4d992635a..5a55d5be5 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1752,11 +1752,37 @@ def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
   zeros = broadcast(_const(t, 0), operand_shape)
   return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]
 
+def dynamic_slice_batching_rule(batched_args, batch_dims, slice_sizes,
+                                **unused_kwargs):
+  operand, start_indices = batched_args
+  op_bdim, idx_bdim = batch_dims
+
+  if idx_bdim is None:
+    new_start_indices = concatenate(
+        [start_indices[:op_bdim], _zeros(start_indices, shape=(1,)),
+         start_indices[op_bdim:]], 0)
+    new_slice_sizes = list(slice_sizes)
+    new_slice_sizes.insert(op_bdim, operand.shape[op_bdim])
+    out = dynamic_slice(operand, new_start_indices, new_slice_sizes)
+    return out, op_bdim
+  else:
+    # TODO(mattjj): add support for Gather HLO, use it here
+    start_indices = batching.bdim_at_front(start_indices, idx_bdim)
+    if op_bdim is None:
+      out = concatenate([dynamic_slice(operand, idx, slice_sizes)
+                         for idx in start_indices], 0)
+    else:
+      operand = batching.bdim_at_front(operand, op_bdim)
+      out = concatenate([dynamic_slice(op, idx, slice_sizes)
+                         for op, idx in zip(operand, start_indices)], 0)
+    return out, 0
+
 dynamic_slice_p = standard_primitive(
     dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
     dynamic_slice_translation_rule)
 ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
 ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule
+batching.primitive_batchers[dynamic_slice_p] = dynamic_slice_batching_rule
 
 
 def dynamic_update_slice_shape_rule(operand, update, start_indices,","diff --git a/jax/lax.py b/jax/lax.py
index 4d992635a..5a55d5be5 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1752,11 +1752,37 @@ def dynamic_slice_transpose_rule(t, operand, start_indices, slice_sizes,
   zeros = broadcast(_const(t, 0), operand_shape)
   return [dynamic_update_slice(zeros, t, start_indices), ad_util.zero]
 
+def dynamic_slice_batching_rule(batched_args, batch_dims, slice_sizes,
+                                **unused_kwargs):
+  operand, start_indices = batched_args
+  op_bdim, idx_bdim = batch_dims
+
+  if idx_bdim is None:
+    new_start_indices = concatenate(
+        [start_indices[:op_bdim], _zeros(start_indices, shape=(1,)),
+         start_indices[op_bdim:]], 0)
+    new_slice_sizes = list(slice_sizes)
+    new_slice_sizes.insert(op_bdim, operand.shape[op_bdim])
+    out = dynamic_slice(operand, new_start_indices, new_slice_sizes)
+    return out, op_bdim
+  else:
+    # TODO(mattjj): add support for Gather HLO, use it here
+    start_indices = batching.bdim_at_front(start_indices, idx_bdim)
+    if op_bdim is None:
+      out = concatenate([dynamic_slice(operand, idx, slice_sizes)
+                         for idx in start_indices], 0)
+    else:
+      operand = batching.bdim_at_front(operand, op_bdim)
+      out = concatenate([dynamic_slice(op, idx, slice_sizes)
+                         for op, idx in zip(operand, start_indices)], 0)
+    return out, 0
+
 dynamic_slice_p = standard_primitive(
     dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice',
     dynamic_slice_translation_rule)
 ad.defjvp(dynamic_slice_p, dynamic_slice_jvp_rule, None)
 ad.primitive_transposes[dynamic_slice_p] = dynamic_slice_transpose_rule
+batching.primitive_batchers[dynamic_slice_p] = dynamic_slice_batching_rule
 
 
 def dynamic_update_slice_shape_rule(operand, update, start_indices,",No
tests/batching_test.py,tests/batching_test.py,d48e7ef43d933e9b56dff7bd026f5dd576930543,648fd43619db82132ca6e8c44e4c2c0a32ac3406,"add batching (vmap) rule for lax.dynamic_slice

fixes #165","diff --git a/tests/batching_test.py b/tests/batching_test.py
index aeb35681d..611ef8e9a 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -306,6 +306,26 @@ class BatchingTest(jtu.JaxTestCase):
                           [0., 0., 0., 0., 2.]])
     self.assertAllClose(ans, expected, check_dtypes=False)
 
+  def testDynamicSlice(self):
+    # test dynamic_slice via numpy indexing syntax
+    x = onp.arange(30).reshape((10, 3))
+
+    ans = vmap(lambda x, i: x[i], in_axes=(0, None))(x, 1)
+    expected = x[:, 1]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+
+    idx = onp.array([0, 1, 2, 1, 0] * 2)
+    ans = vmap(lambda x, i: x[i], in_axes=(0, 0))(x, idx)
+    expected = x[onp.arange(10), idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+    x = onp.arange(3)
+    idx = onp.array([0, 1, 2, 1, 0] * 2)
+    ans = vmap(lambda x, i: x[i], in_axes=(None, 0))(x, idx)
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index aeb35681d..611ef8e9a 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -306,6 +306,26 @@ class BatchingTest(jtu.JaxTestCase):
                           [0., 0., 0., 0., 2.]])
     self.assertAllClose(ans, expected, check_dtypes=False)
 
+  def testDynamicSlice(self):
+    # test dynamic_slice via numpy indexing syntax
+    x = onp.arange(30).reshape((10, 3))
+
+    ans = vmap(lambda x, i: x[i], in_axes=(0, None))(x, 1)
+    expected = x[:, 1]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+
+    idx = onp.array([0, 1, 2, 1, 0] * 2)
+    ans = vmap(lambda x, i: x[i], in_axes=(0, 0))(x, idx)
+    expected = x[onp.arange(10), idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+    x = onp.arange(3)
+    idx = onp.array([0, 1, 2, 1, 0] * 2)
+    ans = vmap(lambda x, i: x[i], in_axes=(None, 0))(x, idx)
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6d6b5263fea9d2549becc6b411d78ca348cc361d,fd3645ab89e0acc95d6df2908cedea6eac1de59e,"add non-advanced boolean indexing support

also don't sub-sample indexing tests (run them all)
fixes #166","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index db9ffe176..5d4597537 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1438,7 +1438,7 @@ def _rewriting_take(arr, idx, axis=0):
 
   # Handle slice index (only static, otherwise an error is raised)
   elif isinstance(idx, slice):
-    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
+    if not _all(elt is None or type(core.get_aval(elt)) is ConcreteArray
                 for elt in (idx.start, idx.stop, idx.step)):
       msg = (""Array slice indices must have static start/stop/step to be used ""
              ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
@@ -1448,6 +1448,27 @@ def _rewriting_take(arr, idx, axis=0):
       result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
       return lax.rev(result, [axis]) if needs_rev else result
 
+  # Handle non-advanced bool index (only static, otherwise an error is raised)
+  elif (isinstance(abstract_idx, ShapedArray) and onp.issubdtype(abstract_idx.dtype, onp.bool_)
+        or isinstance(idx, list) and _all(not _shape(e) and onp.issubdtype(_dtype(e), onp.bool_)
+                                          for e in idx)):
+    if isinstance(idx, list):
+      idx = array(idx)
+      abstract_idx = core.get_aval(idx)
+
+    if not type(abstract_idx) is ConcreteArray:
+      msg = (""Array boolean indices must be static (e.g. no dependence on an ""
+             ""argument to a jit or vmap function)."")
+      raise IndexError(msg)
+    else:
+      if idx.ndim > arr.ndim or idx.shape != arr.shape[:idx.ndim]:
+        msg = ""Boolean index shape did not match indexed array shape prefix.""
+        raise IndexError(msg)
+      else:
+        reshaped_arr = arr.reshape((-1,) + arr.shape[idx.ndim:])
+        int_idx, = onp.where(idx.ravel())
+        return lax.index_take(reshaped_arr, (int_idx,), (0,))
+
   # Handle non-advanced tuple indices by recursing once
   elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
     canonical_idx = _canonicalize_tuple_index(arr, idx)
@@ -1487,10 +1508,11 @@ def _rewriting_take(arr, idx, axis=0):
   # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
   elif _is_advanced_int_indexer(idx):
     canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
-    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
+    idx_noadvanced = [slice(None) if _is_int_arraylike(e) else e
+                      for e in canonical_idx]
     arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))
 
-    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
+    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int_arraylike(e))
     idx_advanced, axes = zip(*advanced_pairs)
     idx_advanced = broadcast_arrays(*idx_advanced)
 
@@ -1522,11 +1544,11 @@ def _is_advanced_int_indexer(idx):
   # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
   if isinstance(idx, (tuple, list)):
     # We assume this check comes *after* the check for non-advanced tuple index,
-    # and hence we already know at least one element is a sequence
-    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
-                for e in idx)
+    # and hence we already know at least one element is a sequence if it's a tuple
+    return _all(e is None or e is Ellipsis or isinstance(e, slice)
+                or _is_int_arraylike(e) for e in idx)
   else:
-    return _is_int(idx)
+    return _is_int_arraylike(idx)
 
 
 def _is_advanced_int_indexer_without_slices(idx):
@@ -1539,11 +1561,11 @@ def _is_advanced_int_indexer_without_slices(idx):
       return True
 
 
-def _is_int(x):
+def _is_int_arraylike(x):
   """"""Returns True if x is array-like with integer dtype, False otherwise.""""""
   return (isinstance(x, int) and not isinstance(x, bool)
           or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
-          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))
+          or isinstance(x, (list, tuple)) and _all(_is_int_arraylike(e) for e in x))
 
 
 def _canonicalize_tuple_index(arr, idx):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index db9ffe176..5d4597537 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1438,7 +1438,7 @@ def _rewriting_take(arr, idx, axis=0):
 
   # Handle slice index (only static, otherwise an error is raised)
   elif isinstance(idx, slice):
-    if not _all(elt is None or isinstance(core.get_aval(elt), ConcreteArray)
+    if not _all(elt is None or type(core.get_aval(elt)) is ConcreteArray
                 for elt in (idx.start, idx.stop, idx.step)):
       msg = (""Array slice indices must have static start/stop/step to be used ""
              ""with Numpy indexing syntax. Try lax.dynamic_slice instead."")
@@ -1448,6 +1448,27 @@ def _rewriting_take(arr, idx, axis=0):
       result = lax.slice_in_dim(arr, start, limit, stride, axis=axis)
       return lax.rev(result, [axis]) if needs_rev else result
 
+  # Handle non-advanced bool index (only static, otherwise an error is raised)
+  elif (isinstance(abstract_idx, ShapedArray) and onp.issubdtype(abstract_idx.dtype, onp.bool_)
+        or isinstance(idx, list) and _all(not _shape(e) and onp.issubdtype(_dtype(e), onp.bool_)
+                                          for e in idx)):
+    if isinstance(idx, list):
+      idx = array(idx)
+      abstract_idx = core.get_aval(idx)
+
+    if not type(abstract_idx) is ConcreteArray:
+      msg = (""Array boolean indices must be static (e.g. no dependence on an ""
+             ""argument to a jit or vmap function)."")
+      raise IndexError(msg)
+    else:
+      if idx.ndim > arr.ndim or idx.shape != arr.shape[:idx.ndim]:
+        msg = ""Boolean index shape did not match indexed array shape prefix.""
+        raise IndexError(msg)
+      else:
+        reshaped_arr = arr.reshape((-1,) + arr.shape[idx.ndim:])
+        int_idx, = onp.where(idx.ravel())
+        return lax.index_take(reshaped_arr, (int_idx,), (0,))
+
   # Handle non-advanced tuple indices by recursing once
   elif isinstance(idx, tuple) and _all(onp.ndim(elt) == 0 for elt in idx):
     canonical_idx = _canonicalize_tuple_index(arr, idx)
@@ -1487,10 +1508,11 @@ def _rewriting_take(arr, idx, axis=0):
   # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
   elif _is_advanced_int_indexer(idx):
     canonical_idx = _canonicalize_tuple_index(arr, tuple(idx))
-    idx_noadvanced = [slice(None) if _is_int(e) else e for e in canonical_idx]
+    idx_noadvanced = [slice(None) if _is_int_arraylike(e) else e
+                      for e in canonical_idx]
     arr_sliced = _rewriting_take(arr, tuple(idx_noadvanced))
 
-    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int(e))
+    advanced_pairs = ((e, i) for i, e in enumerate(canonical_idx) if _is_int_arraylike(e))
     idx_advanced, axes = zip(*advanced_pairs)
     idx_advanced = broadcast_arrays(*idx_advanced)
 
@@ -1522,11 +1544,11 @@ def _is_advanced_int_indexer(idx):
   # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing
   if isinstance(idx, (tuple, list)):
     # We assume this check comes *after* the check for non-advanced tuple index,
-    # and hence we already know at least one element is a sequence
-    return _all(e is None or e is Ellipsis or isinstance(e, slice) or _is_int(e)
-                for e in idx)
+    # and hence we already know at least one element is a sequence if it's a tuple
+    return _all(e is None or e is Ellipsis or isinstance(e, slice)
+                or _is_int_arraylike(e) for e in idx)
   else:
-    return _is_int(idx)
+    return _is_int_arraylike(idx)
 
 
 def _is_advanced_int_indexer_without_slices(idx):
@@ -1539,11 +1561,11 @@ def _is_advanced_int_indexer_without_slices(idx):
       return True
 
 
-def _is_int(x):
+def _is_int_arraylike(x):
   """"""Returns True if x is array-like with integer dtype, False otherwise.""""""
   return (isinstance(x, int) and not isinstance(x, bool)
           or onp.issubdtype(getattr(x, ""dtype"", None), onp.integer)
-          or isinstance(x, (list, tuple)) and _all(_is_int(e) for e in x))
+          or isinstance(x, (list, tuple)) and _all(_is_int_arraylike(e) for e in x))
 
 
 def _canonicalize_tuple_index(arr, idx):",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,6d6b5263fea9d2549becc6b411d78ca348cc361d,fd3645ab89e0acc95d6df2908cedea6eac1de59e,"add non-advanced boolean indexing support

also don't sub-sample indexing tests (run them all)
fixes #166","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index c6f034ed0..42cfb66a9 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -62,11 +62,10 @@ def check_grads(f, args, order, atol=None, rtol=None, eps=None):
 class IndexingTest(jtu.JaxTestCase):
   """"""Tests for Numpy indexing translation rules.""""""
 
-  @parameterized.named_parameters(jtu.cases_from_list({
-      ""testcase_name"":
-          ""{}_inshape={}_indexer={}"".format(
-              name, jtu.format_shape_dtype_string( shape, dtype), indexer),
-      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  @parameterized.named_parameters({
+      ""testcase_name"": ""{}_inshape={}_indexer={}"".format(
+          name, jtu.format_shape_dtype_string( shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
   } for name, index_specs in [
       (""OneIntIndex"", [
           IndexSpec(shape=(3,), indexer=1),
@@ -154,14 +153,14 @@ class IndexingTest(jtu.JaxTestCase):
           IndexSpec(shape=(3, 4), indexer=()),
       ]),
   ] for shape, indexer in index_specs for dtype in all_dtypes
-                                  for rng in [jtu.rand_default()]))
+                                  for rng in [jtu.rand_default()])
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype)]
     fun = lambda x: x[indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list({
+  @parameterized.named_parameters({
       ""testcase_name"":
           ""{}_inshape={}_indexer={}"".format(name,
                                             jtu.format_shape_dtype_string(
@@ -233,7 +232,7 @@ class IndexingTest(jtu.JaxTestCase):
       #   IndexSpec(shape=(3, 4), indexer=()),
       #   ]),
   ] for shape, indexer in index_specs for dtype in float_dtypes
-                                  for rng in [jtu.rand_default()]))
+                                  for rng in [jtu.rand_default()])
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -257,7 +256,7 @@ class IndexingTest(jtu.JaxTestCase):
     else:
       return idx, lambda x: x
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -280,7 +279,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -292,7 +291,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self.assertRaises(IndexError, lambda: fun(*args_maker()))
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -312,7 +311,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -324,7 +323,7 @@ class IndexingTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
   @skip
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -346,7 +345,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
     # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -360,7 +359,7 @@ class IndexingTest(jtu.JaxTestCase):
     arr = rng(shape, dtype)
     check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -412,13 +411,13 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype), indexer]
     fun = lambda x, idx: x[idx]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -470,14 +469,14 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     arg = rng(shape, dtype)
     fun = lambda x: x[indexer]**2
     check_grads(fun, (arg,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -533,7 +532,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
                             for e in indexer]
@@ -588,6 +587,49 @@ class IndexingTest(jtu.JaxTestCase):
 
     self.assertAllClose(a1, a2, check_dtypes=True)
 
+  def testBooleanIndexingArray1D(self):
+    idx = onp.array([True, True, False])
+    x = api.device_put(onp.arange(3))
+    ans = x[idx]
+    expected = onp.arange(3)[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingList1D(self):
+    idx = [True, True, False]
+    x = api.device_put(onp.arange(3))
+    ans = x[idx]
+    expected = onp.arange(3)[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingArray2DBroadcast(self):
+    idx = onp.array([True, True, False, True])
+    x = onp.arange(8).reshape(4, 2)
+    ans = api.device_put(x)[idx]
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingList2DBroadcast(self):
+    idx = [True, True, False, True]
+    x = onp.arange(8).reshape(4, 2)
+    ans = api.device_put(x)[idx]
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingArray2D(self):
+    idx = onp.array([[True, False],
+                     [False, True],
+                     [False, False],
+                     [True, True]])
+    x = onp.arange(8).reshape(4, 2)
+    ans = api.device_put(x)[idx]
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingDynamicShapeError(self):
+    x = onp.zeros(3)
+    i = onp.array([True, True, False])
+    self.assertRaises(IndexError, lambda: api.jit(lambda x, i: x[i])(x, i))
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index c6f034ed0..42cfb66a9 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -62,11 +62,10 @@ def check_grads(f, args, order, atol=None, rtol=None, eps=None):
 class IndexingTest(jtu.JaxTestCase):
   """"""Tests for Numpy indexing translation rules.""""""
 
-  @parameterized.named_parameters(jtu.cases_from_list({
-      ""testcase_name"":
-          ""{}_inshape={}_indexer={}"".format(
-              name, jtu.format_shape_dtype_string( shape, dtype), indexer),
-      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  @parameterized.named_parameters({
+      ""testcase_name"": ""{}_inshape={}_indexer={}"".format(
+          name, jtu.format_shape_dtype_string( shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
   } for name, index_specs in [
       (""OneIntIndex"", [
           IndexSpec(shape=(3,), indexer=1),
@@ -154,14 +153,14 @@ class IndexingTest(jtu.JaxTestCase):
           IndexSpec(shape=(3, 4), indexer=()),
       ]),
   ] for shape, indexer in index_specs for dtype in all_dtypes
-                                  for rng in [jtu.rand_default()]))
+                                  for rng in [jtu.rand_default()])
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype)]
     fun = lambda x: x[indexer]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list({
+  @parameterized.named_parameters({
       ""testcase_name"":
           ""{}_inshape={}_indexer={}"".format(name,
                                             jtu.format_shape_dtype_string(
@@ -233,7 +232,7 @@ class IndexingTest(jtu.JaxTestCase):
       #   IndexSpec(shape=(3, 4), indexer=()),
       #   ]),
   ] for shape, indexer in index_specs for dtype in float_dtypes
-                                  for rng in [jtu.rand_default()]))
+                                  for rng in [jtu.rand_default()])
   @jtu.skip_on_devices(""tpu"")
   def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -257,7 +256,7 @@ class IndexingTest(jtu.JaxTestCase):
     else:
       return idx, lambda x: x
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -280,7 +279,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -292,7 +291,7 @@ class IndexingTest(jtu.JaxTestCase):
     args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
     self.assertRaises(IndexError, lambda: fun(*args_maker()))
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -312,7 +311,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
     unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
 
@@ -324,7 +323,7 @@ class IndexingTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
   @skip
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -346,7 +345,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
     # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
@@ -360,7 +359,7 @@ class IndexingTest(jtu.JaxTestCase):
     arr = rng(shape, dtype)
     check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -412,13 +411,13 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     args_maker = lambda: [rng(shape, dtype), indexer]
     fun = lambda x, idx: x[idx]
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -470,14 +469,14 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in float_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
     tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
     arg = rng(shape, dtype)
     fun = lambda x: x[indexer]**2
     check_grads(fun, (arg,), 2, tol, tol, tol)
 
-  @parameterized.named_parameters(jtu.cases_from_list(
+  @parameterized.named_parameters(
       {""testcase_name"": ""{}_inshape={}_indexer={}""
        .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
        ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
@@ -533,7 +532,7 @@ class IndexingTest(jtu.JaxTestCase):
       ]
       for shape, indexer in index_specs
       for dtype in all_dtypes
-      for rng in [jtu.rand_default()]))
+      for rng in [jtu.rand_default()])
   def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
     indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
                             for e in indexer]
@@ -588,6 +587,49 @@ class IndexingTest(jtu.JaxTestCase):
 
     self.assertAllClose(a1, a2, check_dtypes=True)
 
+  def testBooleanIndexingArray1D(self):
+    idx = onp.array([True, True, False])
+    x = api.device_put(onp.arange(3))
+    ans = x[idx]
+    expected = onp.arange(3)[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingList1D(self):
+    idx = [True, True, False]
+    x = api.device_put(onp.arange(3))
+    ans = x[idx]
+    expected = onp.arange(3)[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingArray2DBroadcast(self):
+    idx = onp.array([True, True, False, True])
+    x = onp.arange(8).reshape(4, 2)
+    ans = api.device_put(x)[idx]
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingList2DBroadcast(self):
+    idx = [True, True, False, True]
+    x = onp.arange(8).reshape(4, 2)
+    ans = api.device_put(x)[idx]
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingArray2D(self):
+    idx = onp.array([[True, False],
+                     [False, True],
+                     [False, False],
+                     [True, True]])
+    x = onp.arange(8).reshape(4, 2)
+    ans = api.device_put(x)[idx]
+    expected = x[idx]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testBooleanIndexingDynamicShapeError(self):
+    x = onp.zeros(3)
+    i = onp.array([True, True, False])
+    self.assertRaises(IndexError, lambda: api.jit(lambda x, i: x[i])(x, i))
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
tests/scipy_stats_test.py,tests/scipy_stats_test.py,b900339b879cba9737089abbd5fc061969ef5a60,6d6b5263fea9d2549becc6b411d78ca348cc361d,loosen scipy test tol to fix benign failure,"diff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py
index 7d7338f62..b0edb80c5 100644
--- a/tests/scipy_stats_test.py
+++ b/tests/scipy_stats_test.py
@@ -50,7 +50,8 @@ class LaxBackedScipyStatsTests(jtu.JaxTestCase):
       x, a, b, loc, scale = map(rng, shapes, dtypes)
       return [x, onp.abs(a), onp.abs(b), loc, onp.abs(scale)]
 
-    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True,
+                            tol=1e-4)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
   @genNamedParametersNArgs(3, jtu.rand_default())","diff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py
index 7d7338f62..b0edb80c5 100644
--- a/tests/scipy_stats_test.py
+++ b/tests/scipy_stats_test.py
@@ -50,7 +50,8 @@ class LaxBackedScipyStatsTests(jtu.JaxTestCase):
       x, a, b, loc, scale = map(rng, shapes, dtypes)
       return [x, onp.abs(a), onp.abs(b), loc, onp.abs(scale)]
 
-    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True,
+                            tol=1e-4)
     self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
   @genNamedParametersNArgs(3, jtu.rand_default())",No
jax/experimental/stax.py,jax/experimental/stax.py,59142a2494537aa68e6d192416f1d4baa3457556,041ef32272e154d65dd28faf171df5f1cf0b9f06,"add error message if Dropout gets no rng key

closes #170 (though there's more work to be done here)","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index eab5c2c1d..cfe2f3284 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -217,6 +217,12 @@ def Dropout(rate, mode='train'):
   def init_fun(input_shape):
     return input_shape, ()
   def apply_fun(params, inputs, rng):
+    if rng is None:
+      msg = (""Dropout layer requires apply_fun to be called with a PRNG key ""
+             ""argument. That is, instead of `apply_fun(params, inputs)`, call ""
+             ""it like `apply_fun(params, inputs, key)` where `key` is a ""
+             ""jax.random.PRNGKey value."")
+      raise ValueError(msg)
     if mode == 'train':
       keep = random.bernoulli(rng, rate, inputs.shape)
       return np.where(keep, inputs / rate, 0)","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index eab5c2c1d..cfe2f3284 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -217,6 +217,12 @@ def Dropout(rate, mode='train'):
   def init_fun(input_shape):
     return input_shape, ()
   def apply_fun(params, inputs, rng):
+    if rng is None:
+      msg = (""Dropout layer requires apply_fun to be called with a PRNG key ""
+             ""argument. That is, instead of `apply_fun(params, inputs)`, call ""
+             ""it like `apply_fun(params, inputs, key)` where `key` is a ""
+             ""jax.random.PRNGKey value."")
+      raise ValueError(msg)
     if mode == 'train':
       keep = random.bernoulli(rng, rate, inputs.shape)
       return np.where(keep, inputs / rate, 0)",No
setup.py,setup.py,95a86cd0fb16162401ff87211137acde18575a56,59142a2494537aa68e6d192416f1d4baa3457556,bump version number for pypi,"diff --git a/setup.py b/setup.py
index f7cbef252..deb377b46 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.14',
+    version='0.1.15',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index f7cbef252..deb377b46 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.14',
+    version='0.1.15',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
tests/linalg_test.py,tests/linalg_test.py,0211934c2c14c48a14cc89a6b91cdfa9c282fdfc,59142a2494537aa68e6d192416f1d4baa3457556,skip linalg QR test on cpu,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 0ad1a9f39..4ff55d271 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -110,6 +110,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for dtype in float_types()
       for full_matrices in [False, True]
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""cpu"")
   def testQr(self, shape, dtype, full_matrices, rng):
     m, n = shape[-2:]
 ","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 0ad1a9f39..4ff55d271 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -110,6 +110,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       for dtype in float_types()
       for full_matrices in [False, True]
       for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""cpu"")
   def testQr(self, shape, dtype, full_matrices, rng):
     m, n = shape[-2:]
 ",No
jax/test_util.py,jax/test_util.py,b2d6ce175a0193ba0a7cb6a851f4f9910a1e1dc0,0211934c2c14c48a14cc89a6b91cdfa9c282fdfc,add a placeholder comment for lbr tests,"diff --git a/jax/test_util.py b/jax/test_util.py
index 65acfc2b1..8b36df0bf 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -35,6 +35,9 @@ from .config import flags
 from .util import partial
 from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce
 
+# lbr tests placeholder
+
+
 FLAGS = flags.FLAGS
 flags.DEFINE_enum(
     'jax_test_dut',","diff --git a/jax/test_util.py b/jax/test_util.py
index 65acfc2b1..8b36df0bf 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -35,6 +35,9 @@ from .config import flags
 from .util import partial
 from .tree_util import tree_multimap, tree_all, tree_map, tree_reduce
 
+# lbr tests placeholder
+
+
 FLAGS = flags.FLAGS
 flags.DEFINE_enum(
     'jax_test_dut',",No
jax/experimental/stax.py,jax/experimental/stax.py,b4246163ac7c4a6480187d05b5a4c476434f1126,95a86cd0fb16162401ff87211137acde18575a56,add stax.FanInConcat (fixes #174),"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index cfe2f3284..e9988f128 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -212,6 +212,18 @@ def FanInSum():
 FanInSum = FanInSum()
 
 
+def FanInConcat(axis=-1):
+  """"""Layer construction function for a fan-in concatenation layer.""""""
+  def init_fun(input_shape):
+    ax = axis % len(input_shape[0])
+    concat_size = sum(shape[ax] for shape in input_shape)
+    out_shape = input_shape[0][:ax] + (concat_size,) + input_shape[0][ax+1:]
+    return out_shape, ()
+  def apply_fun(params, inputs, rng=None):
+    return np.concatenate(inputs, axis)
+  return init_fun, apply_fun
+
+
 def Dropout(rate, mode='train'):
   """"""Layer construction function for a dropout layer with given rate.""""""
   def init_fun(input_shape):","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index cfe2f3284..e9988f128 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -212,6 +212,18 @@ def FanInSum():
 FanInSum = FanInSum()
 
 
+def FanInConcat(axis=-1):
+  """"""Layer construction function for a fan-in concatenation layer.""""""
+  def init_fun(input_shape):
+    ax = axis % len(input_shape[0])
+    concat_size = sum(shape[ax] for shape in input_shape)
+    out_shape = input_shape[0][:ax] + (concat_size,) + input_shape[0][ax+1:]
+    return out_shape, ()
+  def apply_fun(params, inputs, rng=None):
+    return np.concatenate(inputs, axis)
+  return init_fun, apply_fun
+
+
 def Dropout(rate, mode='train'):
   """"""Layer construction function for a dropout layer with given rate.""""""
   def init_fun(input_shape):",No
tests/stax_test.py,tests/stax_test.py,b4246163ac7c4a6480187d05b5a4c476434f1126,95a86cd0fb16162401ff87211137acde18575a56,add stax.FanInConcat (fixes #174),"diff --git a/tests/stax_test.py b/tests/stax_test.py
index 3be51aa77..cb3502502 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -30,9 +30,18 @@ from jax.config import config
 config.parse_flags_with_absl()
 
 
+def random_inputs(rng, input_shape):
+  if type(input_shape) is tuple:
+    return rng.randn(*input_shape).astype(onp.float32)
+  elif type(input_shape) is list:
+    return [random_inputs(rng, shape) for shape in input_shape]
+  else:
+    raise TypeError(type(input_shape))
+
+
 def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   result_shape, params = init_fun(input_shape)
-  inputs = onp.random.RandomState(0).randn(*input_shape).astype(""float32"")
+  inputs = random_inputs(onp.random.RandomState(0), input_shape)
   rng_key = random.PRNGKey(0)
   result = apply_fun(params, inputs, rng_key)
   test_case.assertEqual(result.shape, result_shape)
@@ -130,6 +139,26 @@ class StaxTest(jtu.JaxTestCase):
     init_fun, apply_fun = stax.Dropout(0.9)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(3, 4), (2, 5, 6, 1)]))
+  def testFanInSum(self, input_shape):
+    init_fun, apply_fun = stax.FanInSum
+    _CheckShapeAgreement(self, init_fun, apply_fun, [input_shape, input_shape])
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_inshapes={}_axis={}"".format(input_shapes, axis),
+       ""input_shapes"": input_shapes, ""axis"": axis}
+      for input_shapes, axis in [
+          ([(2, 3), (2, 1)], 1),
+          ([(2, 3), (2, 1)], -1),
+          ([(1, 2, 4), (1, 1, 4)], 1),
+      ]))
+  def testFanInConcat(self, input_shapes, axis):
+    init_fun, apply_fun = stax.FanInConcat(axis)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shapes)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 3be51aa77..cb3502502 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -30,9 +30,18 @@ from jax.config import config
 config.parse_flags_with_absl()
 
 
+def random_inputs(rng, input_shape):
+  if type(input_shape) is tuple:
+    return rng.randn(*input_shape).astype(onp.float32)
+  elif type(input_shape) is list:
+    return [random_inputs(rng, shape) for shape in input_shape]
+  else:
+    raise TypeError(type(input_shape))
+
+
 def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
   result_shape, params = init_fun(input_shape)
-  inputs = onp.random.RandomState(0).randn(*input_shape).astype(""float32"")
+  inputs = random_inputs(onp.random.RandomState(0), input_shape)
   rng_key = random.PRNGKey(0)
   result = apply_fun(params, inputs, rng_key)
   test_case.assertEqual(result.shape, result_shape)
@@ -130,6 +139,26 @@ class StaxTest(jtu.JaxTestCase):
     init_fun, apply_fun = stax.Dropout(0.9)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(3, 4), (2, 5, 6, 1)]))
+  def testFanInSum(self, input_shape):
+    init_fun, apply_fun = stax.FanInSum
+    _CheckShapeAgreement(self, init_fun, apply_fun, [input_shape, input_shape])
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_inshapes={}_axis={}"".format(input_shapes, axis),
+       ""input_shapes"": input_shapes, ""axis"": axis}
+      for input_shapes, axis in [
+          ([(2, 3), (2, 1)], 1),
+          ([(2, 3), (2, 1)], -1),
+          ([(1, 2, 4), (1, 1, 4)], 1),
+      ]))
+  def testFanInConcat(self, input_shapes, axis):
+    init_fun, apply_fun = stax.FanInConcat(axis)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shapes)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,9c49a9bfe690738d502a48e33d2f421c4694e6ab,95a86cd0fb16162401ff87211137acde18575a56,add np.append and np.polyval,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5d4597537..35caff5fb 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1070,6 +1070,27 @@ def diag(v, k=0):
     raise ValueError(""diag input must be 1d or 2d"")
 
 
+@_wraps(onp.polyval)
+def polyval(p, x):
+  if isinstance(p, onp.poly1d):
+    p = onp.asarray(p)
+  if isinstance(x, onp.poly1d):
+    y = 0
+  else:
+    y = zeros_like(x)
+  for i in range(len(p)):
+    y = y * x + p[i]
+  return y
+
+
+@_wraps(onp.append)
+def append(arr, values, axis=None):
+  if axis is None:
+    return concatenate([ravel(arr), ravel(values)], 0)
+  else:
+    return concatenate([arr, values], axis=axis)
+
+
 ### Tensor contraction operations
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5d4597537..35caff5fb 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1070,6 +1070,27 @@ def diag(v, k=0):
     raise ValueError(""diag input must be 1d or 2d"")
 
 
+@_wraps(onp.polyval)
+def polyval(p, x):
+  if isinstance(p, onp.poly1d):
+    p = onp.asarray(p)
+  if isinstance(x, onp.poly1d):
+    y = 0
+  else:
+    y = zeros_like(x)
+  for i in range(len(p)):
+    y = y * x + p[i]
+  return y
+
+
+@_wraps(onp.append)
+def append(arr, values, axis=None):
+  if axis is None:
+    return concatenate([ravel(arr), ravel(values)], 0)
+  else:
+    return concatenate([arr, values], axis=axis)
+
+
 ### Tensor contraction operations
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,9c49a9bfe690738d502a48e33d2f421c4694e6ab,95a86cd0fb16162401ff87211137acde18575a56,add np.append and np.polyval,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index c63baaf6f..082ea714f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -119,6 +119,7 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""polyval"", 2, numeric_dtypes, nonempty_array_shapes, jtu.rand_default(), []),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
@@ -451,6 +452,28 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
+          axis, "","".join(str(d) for d in base_shape),
+          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
+       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
+       ""rng"": jtu.rand_default()}
+      for dtypes in CombosWithReplacement(default_dtypes, 2)
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for axis in range(-len(base_shape)+1, len(base_shape))))
+  def testAppend(self, axis, base_shape, dtypes, rng):
+    wrapped_axis = axis % len(base_shape)
+    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
+    onp_fun = lambda arr, values: onp.append(arr, values, axis=axis)
+    lnp_fun = lambda arr, values: lnp.append(arr, values, axis=axis)
+
+    def args_maker():
+      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape=[{}]_axis={}_repeats={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, repeats),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index c63baaf6f..082ea714f 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -119,6 +119,7 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""polyval"", 2, numeric_dtypes, nonempty_array_shapes, jtu.rand_default(), []),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
@@ -451,6 +452,28 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
+          axis, "","".join(str(d) for d in base_shape),
+          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
+       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
+       ""rng"": jtu.rand_default()}
+      for dtypes in CombosWithReplacement(default_dtypes, 2)
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for axis in range(-len(base_shape)+1, len(base_shape))))
+  def testAppend(self, axis, base_shape, dtypes, rng):
+    wrapped_axis = axis % len(base_shape)
+    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
+    onp_fun = lambda arr, values: onp.append(arr, values, axis=axis)
+    lnp_fun = lambda arr, values: lnp.append(arr, values, axis=axis)
+
+    def args_maker():
+      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape=[{}]_axis={}_repeats={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), axis, repeats),",No
jax/test_util.py,jax/test_util.py,5d2fc7c05bc103b73e2c6fb52f59fdfc57b4e5a5,9c49a9bfe690738d502a48e33d2f421c4694e6ab,only test np.polyval on nonscalar array shapes,"diff --git a/jax/test_util.py b/jax/test_util.py
index 65acfc2b1..935a91992 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -458,7 +458,7 @@ class JaxTestCase(parameterized.TestCase):
   def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                          check_dtypes=False, tol=1e-5):
     args = args_maker()
-    lax_ans = lax_op(*args)
     numpy_ans = numpy_reference_op(*args)
+    lax_ans = lax_op(*args)
     self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                         atol=tol, rtol=tol)","diff --git a/jax/test_util.py b/jax/test_util.py
index 65acfc2b1..935a91992 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -458,7 +458,7 @@ class JaxTestCase(parameterized.TestCase):
   def _CheckAgainstNumpy(self, lax_op, numpy_reference_op, args_maker,
                          check_dtypes=False, tol=1e-5):
     args = args_maker()
-    lax_ans = lax_op(*args)
     numpy_ans = numpy_reference_op(*args)
+    lax_ans = lax_op(*args)
     self.assertAllClose(lax_ans, numpy_ans, check_dtypes=check_dtypes,
                         atol=tol, rtol=tol)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,5d2fc7c05bc103b73e2c6fb52f59fdfc57b4e5a5,9c49a9bfe690738d502a48e33d2f421c4694e6ab,only test np.polyval on nonscalar array shapes,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 082ea714f..dda5dc26a 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -35,7 +35,8 @@ from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
-nonempty_array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_nonscalar_array_shapes = [(4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_array_shapes = [()] + nonempty_nonscalar_array_shapes
 empty_array_shapes = [(0,), (0, 4), (3, 0),]
 
 scalar_shapes = [jtu.NUMPY_SCALAR_SHAPE]
@@ -119,7 +120,7 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""polyval"", 2, numeric_dtypes, nonempty_array_shapes, jtu.rand_default(), []),
+    op_record(""polyval"", 2, numeric_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 082ea714f..dda5dc26a 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -35,7 +35,8 @@ from jax.config import config
 config.parse_flags_with_absl()
 FLAGS = config.FLAGS
 
-nonempty_array_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_nonscalar_array_shapes = [(4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+nonempty_array_shapes = [()] + nonempty_nonscalar_array_shapes
 empty_array_shapes = [(0,), (0, 4), (3, 0),]
 
 scalar_shapes = [jtu.NUMPY_SCALAR_SHAPE]
@@ -119,7 +120,7 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""polyval"", 2, numeric_dtypes, nonempty_array_shapes, jtu.rand_default(), []),
+    op_record(""polyval"", 2, numeric_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),",No
jax/random.py,jax/random.py,072e6f78f1aebfa651574e47709d0f9f0e2e17a5,c5c6e6c5c7f86dad5ac70e974af7d814e723fdb7,replace PRNGKey class with uint32[2] array,"diff --git a/jax/random.py b/jax/random.py
index 60b6ba0fc..671e54bf4 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -29,41 +29,25 @@ from .api import jit
 from jax.lib import xla_bridge
 from jax import core
 
-class PRNGKey(object):
-  """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
-  __slots__ = [""keypair""]
-
-  def __init__(self, seed):
-    """"""Create a new PRNG key.
-
-    Args:
-      seed: a scalar integer value used to initialize the PRNG key.
-
-    Returns:
-      A new PRNGKey object.
-    """"""
-    convert = lambda key: lax.convert_element_type(key, onp.uint32)
-    if onp.shape(seed):
-      raise TypeError(""PRNGKey seed must be a scalar."")
-    if isinstance(seed, (int, onp.ndarray)):
-      # Special handling of raw integer values, which may have be 64bit even
-      # when jax_enable_x64=False and we don't want to drop the top 32 bits
-      k1 = convert(onp.bitwise_and(onp.right_shift(seed, 32), 0xFFFFFFFF))
-    else:
-      k1 = convert(lax.shift_right_logical(seed, 32))
-    k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
-    self.keypair = core.pack((k1, k2))
-
-  @classmethod
-  def from_keypair(cls, keypair):
-    """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
-    new = cls.__new__(cls)
-    new.keypair = core.pack(keypair)
-    return new
-
-
-tree_util.register_pytree_node(PRNGKey, lambda k: (k.keypair, None),
-                               lambda _, xs: PRNGKey.from_keypair(xs))
+
+def PRNGKey(seed):
+  if onp.shape(seed):
+    raise TypeError(""PRNGKey seed must be a scalar."")
+  convert = lambda k: lax.reshape(lax.convert_element_type(k, onp.uint32), [1])
+  if isinstance(seed, (int, onp.ndarray)):
+    # Special handling of raw integer values, which may have be 64bit even
+    # when jax_enable_x64=False and we don't want to drop the top 32 bits
+    k1 = convert(onp.bitwise_and(onp.right_shift(seed, 32), 0xFFFFFFFF))
+  else:
+    k1 = convert(lax.shift_right_logical(seed, 32))
+  k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
+  return lax.concatenate([k1, k2], 0)
+
+def is_prng_key(key):
+  try:
+    return key.shape == (2,) and key.dtype == onp.uint32
+  except AttributeError:
+    return False
 
 
 ### utilities
@@ -169,14 +153,15 @@ def split(key, num=2):
   Returns:
     A tuple of length `num` of new PRNGKey instances.
   """"""
-  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, num * 2))
-  bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
-  keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
-  return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)
+  counts = lax.tie_in(key, lax.iota(onp.uint32, num * 2))
+  bits = lax.reshape(threefry_2x32(key, counts), (num, 2))
+  return tuple(bits)
 
 
 def _random_bits(key, bit_width, shape):
   """"""Sample uniform random bits of given width and shape using PRNG key.""""""
+  if not is_prng_key(key):
+    raise TypeError(""_random_bits got invalid prng key."")
   if bit_width not in (32, 64):
     raise TypeError(""requires 32- or 64-bit field width."")
   max_count = (bit_width // 32) * onp.prod(shape)
@@ -184,8 +169,8 @@ def _random_bits(key, bit_width, shape):
     # TODO(mattjj): just split the key here
     raise TypeError(""requesting more random bits than a single call provides."")
 
-  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, max_count))
-  bits = threefry_2x32(key.keypair, counts)
+  counts = lax.tie_in(key, lax.iota(onp.uint32, max_count))
+  bits = threefry_2x32(key, counts)
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
     bits = (bits[0] << onp.uint64(32)) | bits[1]","diff --git a/jax/random.py b/jax/random.py
index 60b6ba0fc..671e54bf4 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -29,41 +29,25 @@ from .api import jit
 from jax.lib import xla_bridge
 from jax import core
 
-class PRNGKey(object):
-  """"""A pseudo-random number generator (PRNG) key for use with lax.random.""""""
-  __slots__ = [""keypair""]
 
-  def __init__(self, seed):
-    """"""Create a new PRNG key.
+def PRNGKey(seed):
+  if onp.shape(seed):
+    raise TypeError(""PRNGKey seed must be a scalar."")
+  convert = lambda k: lax.reshape(lax.convert_element_type(k, onp.uint32), [1])
+  if isinstance(seed, (int, onp.ndarray)):
+    # Special handling of raw integer values, which may have be 64bit even
+    # when jax_enable_x64=False and we don't want to drop the top 32 bits
+    k1 = convert(onp.bitwise_and(onp.right_shift(seed, 32), 0xFFFFFFFF))
+  else:
+    k1 = convert(lax.shift_right_logical(seed, 32))
+  k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
+  return lax.concatenate([k1, k2], 0)
 
-    Args:
-      seed: a scalar integer value used to initialize the PRNG key.
-
-    Returns:
-      A new PRNGKey object.
-    """"""
-    convert = lambda key: lax.convert_element_type(key, onp.uint32)
-    if onp.shape(seed):
-      raise TypeError(""PRNGKey seed must be a scalar."")
-    if isinstance(seed, (int, onp.ndarray)):
-      # Special handling of raw integer values, which may have be 64bit even
-      # when jax_enable_x64=False and we don't want to drop the top 32 bits
-      k1 = convert(onp.bitwise_and(onp.right_shift(seed, 32), 0xFFFFFFFF))
-    else:
-      k1 = convert(lax.shift_right_logical(seed, 32))
-    k2 = convert(lax.bitwise_and(seed, 0xFFFFFFFF))
-    self.keypair = core.pack((k1, k2))
-
-  @classmethod
-  def from_keypair(cls, keypair):
-    """"""Internal method to create a PRNGKey instance from a raw key pair.""""""
-    new = cls.__new__(cls)
-    new.keypair = core.pack(keypair)
-    return new
-
-
-tree_util.register_pytree_node(PRNGKey, lambda k: (k.keypair, None),
-                               lambda _, xs: PRNGKey.from_keypair(xs))
+def is_prng_key(key):
+  try:
+    return key.shape == (2,) and key.dtype == onp.uint32
+  except AttributeError:
+    return False
 
 
 ### utilities
@@ -169,14 +153,15 @@ def split(key, num=2):
   Returns:
     A tuple of length `num` of new PRNGKey instances.
   """"""
-  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, num * 2))
-  bits = lax.reshape(threefry_2x32(key.keypair, counts), (num, 2))
-  keypairs = (lax.index_in_dim(bits, i, keepdims=False) for i in range(num))
-  return tuple(PRNGKey.from_keypair((kp[0], kp[1])) for kp in keypairs)
+  counts = lax.tie_in(key, lax.iota(onp.uint32, num * 2))
+  bits = lax.reshape(threefry_2x32(key, counts), (num, 2))
+  return tuple(bits)
 
 
 def _random_bits(key, bit_width, shape):
   """"""Sample uniform random bits of given width and shape using PRNG key.""""""
+  if not is_prng_key(key):
+    raise TypeError(""_random_bits got invalid prng key."")
   if bit_width not in (32, 64):
     raise TypeError(""requires 32- or 64-bit field width."")
   max_count = (bit_width // 32) * onp.prod(shape)
@@ -184,8 +169,8 @@ def _random_bits(key, bit_width, shape):
     # TODO(mattjj): just split the key here
     raise TypeError(""requesting more random bits than a single call provides."")
 
-  counts = lax.tie_in(key.keypair, lax.iota(onp.uint32, max_count))
-  bits = threefry_2x32(key.keypair, counts)
+  counts = lax.tie_in(key, lax.iota(onp.uint32, max_count))
+  bits = threefry_2x32(key, counts)
   if bit_width == 64:
     bits = [lax.convert_element_type(x, onp.uint64) for x in np.split(bits, 2)]
     bits = (bits[0] << onp.uint64(32)) | bits[1]",Yes
tests/batching_test.py,tests/batching_test.py,4ed5df85e3b8145910f31b6d1010034bb1011a98,072e6f78f1aebfa651574e47709d0f9f0e2e17a5,add batching test for PRNGKeys and random sampling,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 611ef8e9a..07dd22abb 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -24,6 +24,7 @@ import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
 from jax import lax
+from jax import random
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev, hessian
 from jax.api import vmap
 from jax.core import unit
@@ -326,6 +327,14 @@ class BatchingTest(jtu.JaxTestCase):
     expected = x[idx]
     self.assertAllClose(ans, expected, check_dtypes=False)
 
+  def testRandom(self):
+    seeds = vmap(random.PRNGKey)(onp.arange(10))
+    ans = vmap(partial(random.normal, shape=(3, 2)))(seeds)
+    expected = onp.stack([random.normal(random.PRNGKey(seed), (3, 2))
+                          for seed in onp.arange(10)])
+    self.assertAllClose(ans, expected, check_dtypes=False)
+    assert len(onp.unique(ans)) == 10 * 3 * 2
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 611ef8e9a..07dd22abb 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -24,6 +24,7 @@ import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
 from jax import lax
+from jax import random
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr, jacfwd, jacrev, hessian
 from jax.api import vmap
 from jax.core import unit
@@ -326,6 +327,14 @@ class BatchingTest(jtu.JaxTestCase):
     expected = x[idx]
     self.assertAllClose(ans, expected, check_dtypes=False)
 
+  def testRandom(self):
+    seeds = vmap(random.PRNGKey)(onp.arange(10))
+    ans = vmap(partial(random.normal, shape=(3, 2)))(seeds)
+    expected = onp.stack([random.normal(random.PRNGKey(seed), (3, 2))
+                          for seed in onp.arange(10)])
+    self.assertAllClose(ans, expected, check_dtypes=False)
+    assert len(onp.unique(ans)) == 10 * 3 * 2
+
 
 if __name__ == '__main__':
   absltest.main()",No
README.md,README.md,d9a798f9dbf2f1e97ec78ee51f6a910a68c8e914,d2bfc9ad2aca9ab451a3afdacf38677261aca228,fix minor typos in readme.md,"diff --git a/README.md b/README.md
index daa85fa35..ab95e6e38 100644
--- a/README.md
+++ b/README.md
@@ -152,7 +152,7 @@ pip install --upgrade jax  # install jax
 The library package name must correspond to the version of the existing CUDA
 installation you want to use, with `cuda100` for CUDA 10.0, `cuda92` for CUDA
 9.2, and `cuda90` for CUDA 9.0. To find your CUDA and CUDNN versions, you can
-run command like these, depending on your CUDNN install path:
+run commands like these, depending on your CUDNN install path:
 
 ```bash
 nvcc --version
@@ -274,7 +274,7 @@ For automatic differentiation with `grad`, JAX has the same restrictions
 as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
 works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
 x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
-dicts freely: jax doesn't even see them. Using `np.dot(A, B)` rather than
+dicts freely: JAX doesn't even see them. Using `np.dot(A, B)` rather than
 `A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.
 
 For compiling your own functions with `jit` there are a few more requirements.","diff --git a/README.md b/README.md
index daa85fa35..ab95e6e38 100644
--- a/README.md
+++ b/README.md
@@ -152,7 +152,7 @@ pip install --upgrade jax  # install jax
 The library package name must correspond to the version of the existing CUDA
 installation you want to use, with `cuda100` for CUDA 10.0, `cuda92` for CUDA
 9.2, and `cuda90` for CUDA 9.0. To find your CUDA and CUDNN versions, you can
-run command like these, depending on your CUDNN install path:
+run commands like these, depending on your CUDNN install path:
 
 ```bash
 nvcc --version
@@ -274,7 +274,7 @@ For automatic differentiation with `grad`, JAX has the same restrictions
 as [Autograd](https://github.com/hips/autograd). Specifically, differentiation
 works with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =
 x`) or indexed in-place updating (`A[i] += b`). You can use lists, tuples, and
-dicts freely: jax doesn't even see them. Using `np.dot(A, B)` rather than
+dicts freely: JAX doesn't even see them. Using `np.dot(A, B)` rather than
 `A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.
 
 For compiling your own functions with `jit` there are a few more requirements.",No
jax/random.py,jax/random.py,a627cc80e8de84e3bc0db550d27f9fa4b432c8a4,d2bfc9ad2aca9ab451a3afdacf38677261aca228,"make random.split return something vmap-compatible

(in particular, return an array rather than a tuple, c.f. #181)","diff --git a/jax/random.py b/jax/random.py
index 671e54bf4..8b00c9ca7 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -154,8 +154,7 @@ def split(key, num=2):
     A tuple of length `num` of new PRNGKey instances.
   """"""
   counts = lax.tie_in(key, lax.iota(onp.uint32, num * 2))
-  bits = lax.reshape(threefry_2x32(key, counts), (num, 2))
-  return tuple(bits)
+  return lax.reshape(threefry_2x32(key, counts), (num, 2))
 
 
 def _random_bits(key, bit_width, shape):","diff --git a/jax/random.py b/jax/random.py
index 671e54bf4..8b00c9ca7 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -154,8 +154,7 @@ def split(key, num=2):
     A tuple of length `num` of new PRNGKey instances.
   """"""
   counts = lax.tie_in(key, lax.iota(onp.uint32, num * 2))
-  bits = lax.reshape(threefry_2x32(key, counts), (num, 2))
-  return tuple(bits)
+  return lax.reshape(threefry_2x32(key, counts), (num, 2))
 
 
 def _random_bits(key, bit_width, shape):",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,c4d32ca2cdcef0f1816bf1a5473184e853abdcfd,82fb71413f43821fb4969252d869fb6a140b6131,"support x[[0, 2, 4], [0, 2, 4]] indexing, fix #187","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 35caff5fb..69dbe2fca 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1510,7 +1510,7 @@ def _rewriting_take(arr, idx, axis=0):
   # Handle integer array indexing *without* ellipsis/slices/nones
   # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
   if _is_advanced_int_indexer_without_slices(idx):
-    if isinstance(idx, list):
+    if isinstance(idx, (tuple, list)):
       if _any(_shape(e) for e in idx):
         # At least one sequence element in the index list means broadcasting.
         idx = broadcast_arrays(*idx)
@@ -1521,7 +1521,7 @@ def _rewriting_take(arr, idx, axis=0):
       # The indexer is just a single integer array.
       idx = [idx]
 
-    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
+    flat_idx = tuple([mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx)])
     out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
     return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 35caff5fb..69dbe2fca 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1510,7 +1510,7 @@ def _rewriting_take(arr, idx, axis=0):
   # Handle integer array indexing *without* ellipsis/slices/nones
   # https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#integer-array-indexing
   if _is_advanced_int_indexer_without_slices(idx):
-    if isinstance(idx, list):
+    if isinstance(idx, (tuple, list)):
       if _any(_shape(e) for e in idx):
         # At least one sequence element in the index list means broadcasting.
         idx = broadcast_arrays(*idx)
@@ -1521,7 +1521,7 @@ def _rewriting_take(arr, idx, axis=0):
       # The indexer is just a single integer array.
       idx = [idx]
 
-    flat_idx = tuple(mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx))
+    flat_idx = tuple([mod(ravel(x), arr.shape[i]) for i, x in enumerate(idx)])
     out = lax.index_take(arr, flat_idx, tuple(range(len(idx))))
     return lax.reshape(out, idx[0].shape + _shape(arr)[len(idx):])
 ",No
tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,c4d32ca2cdcef0f1816bf1a5473184e853abdcfd,82fb71413f43821fb4969252d869fb6a140b6131,"support x[[0, 2, 4], [0, 2, 4]] indexing, fix #187","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 42cfb66a9..9386f3e03 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -398,6 +398,10 @@ class IndexingTest(jtu.JaxTestCase):
            [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
             IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
             ]),
+          (""TupleOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=([0, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=([[0], [-1]], [[2, 3, 0, 3]])),
+            ]),
           (""ListOfPythonIntsAndIntArrays"",
            [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
             IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
@@ -630,6 +634,15 @@ class IndexingTest(jtu.JaxTestCase):
     i = onp.array([True, True, False])
     self.assertRaises(IndexError, lambda: api.jit(lambda x, i: x[i])(x, i))
 
+  def testIssue187(self):
+    x = lnp.ones((5, 5))
+    x[[0, 2, 4], [0, 2, 4]]  # doesn't crash
+
+    x = onp.arange(25).reshape((5, 5))
+    ans = api.jit(lambda x: x[[0, 2, 4], [0, 2, 4]])(x)
+    expected = x[[0, 2, 4], [0, 2, 4]]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
index 42cfb66a9..9386f3e03 100644
--- a/tests/lax_numpy_indexing_test.py
+++ b/tests/lax_numpy_indexing_test.py
@@ -398,6 +398,10 @@ class IndexingTest(jtu.JaxTestCase):
            [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
             IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
             ]),
+          (""TupleOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=([0, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=([[0], [-1]], [[2, 3, 0, 3]])),
+            ]),
           (""ListOfPythonIntsAndIntArrays"",
            [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
             IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
@@ -630,6 +634,15 @@ class IndexingTest(jtu.JaxTestCase):
     i = onp.array([True, True, False])
     self.assertRaises(IndexError, lambda: api.jit(lambda x, i: x[i])(x, i))
 
+  def testIssue187(self):
+    x = lnp.ones((5, 5))
+    x[[0, 2, 4], [0, 2, 4]]  # doesn't crash
+
+    x = onp.arange(25).reshape((5, 5))
+    ans = api.jit(lambda x: x[[0, 2, 4], [0, 2, 4]])(x)
+    expected = x[[0, 2, 4], [0, 2, 4]]
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/lax_linalg.py,jax/lax_linalg.py,484db1e15fd188ef977e6c15bed12fa8f611bfe7,c5c6e6c5c7f86dad5ac70e974af7d814e723fdb7,Add SVD for float and double types,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index f50933452..0e3efe71c 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -40,6 +40,13 @@ def qr(x, full_matrices=True):
   q, r = qr_p.bind(x, full_matrices=full_matrices)
   return q, r
 
+def svd(x, full_matrices=True, compute_uv=True):
+  s, u, v = qr_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
+  if compute_uv:
+    return s, u, v
+  else:
+    return s
+
 def triangular_solve(a, b, left_side=False, lower=False, transpose_a=False,
                      conjugate_a=False):
   return triangular_solve_p.bind(
@@ -271,3 +278,36 @@ qr_p.def_impl(qr_impl)
 qr_p.def_abstract_eval(qr_abstract_eval)
 xla.translations[qr_p] = qr_translation_rule
 ad.primitive_jvps[qr_p] = qr_jvp_rule
+
+
+# SVD decomposition
+
+def svd_impl(operand, full_matrices, compute_uv):
+  s, u, vt = xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
+  return core.pack((s, u, vt))
+
+def svd_translation_rule(c, operand):
+  raise NotImplementedError(
+    ""SVD is only implemented on the CPU backend"")
+
+def svd_abstract_eval(operand, full_matrices, compute_uv):
+  if isinstance(operand, ShapedArray):
+    if operand.ndim < 2:
+      raise ValueError(""Argument to SVD must have ndims >= 2"")
+
+    batch_dims = operand.shape[:-2]
+    m = operand.shape[-2]
+    n = operand.shape[-1]
+    s = ShapedArray(batch_dims + (min(m, n),), operand.dtype)
+    u = ShapedArray(batch_dims + (m, m if full_matrices else min(m, n)), operand.dtype)
+    vt = ShapedArray(batch_dims + (n if full_matrices else min(m, n), n), operand.dtype)
+  else:
+    s = operand
+    u = operand
+    vt = operand
+  return core.AbstractTuple((s, u, vt))
+
+svd_p = Primitive('svd')
+svd_p.def_impl(svd_impl)
+svd_p.def_abstract_eval(svd_abstract_eval)
+xla.translations[svd_p] = svd_translation_rule","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index f50933452..0e3efe71c 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -40,6 +40,13 @@ def qr(x, full_matrices=True):
   q, r = qr_p.bind(x, full_matrices=full_matrices)
   return q, r
 
+def svd(x, full_matrices=True, compute_uv=True):
+  s, u, v = qr_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
+  if compute_uv:
+    return s, u, v
+  else:
+    return s
+
 def triangular_solve(a, b, left_side=False, lower=False, transpose_a=False,
                      conjugate_a=False):
   return triangular_solve_p.bind(
@@ -271,3 +278,36 @@ qr_p.def_impl(qr_impl)
 qr_p.def_abstract_eval(qr_abstract_eval)
 xla.translations[qr_p] = qr_translation_rule
 ad.primitive_jvps[qr_p] = qr_jvp_rule
+
+
+# SVD decomposition
+
+def svd_impl(operand, full_matrices, compute_uv):
+  s, u, vt = xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
+  return core.pack((s, u, vt))
+
+def svd_translation_rule(c, operand):
+  raise NotImplementedError(
+    ""SVD is only implemented on the CPU backend"")
+
+def svd_abstract_eval(operand, full_matrices, compute_uv):
+  if isinstance(operand, ShapedArray):
+    if operand.ndim < 2:
+      raise ValueError(""Argument to SVD must have ndims >= 2"")
+
+    batch_dims = operand.shape[:-2]
+    m = operand.shape[-2]
+    n = operand.shape[-1]
+    s = ShapedArray(batch_dims + (min(m, n),), operand.dtype)
+    u = ShapedArray(batch_dims + (m, m if full_matrices else min(m, n)), operand.dtype)
+    vt = ShapedArray(batch_dims + (n if full_matrices else min(m, n), n), operand.dtype)
+  else:
+    s = operand
+    u = operand
+    vt = operand
+  return core.AbstractTuple((s, u, vt))
+
+svd_p = Primitive('svd')
+svd_p.def_impl(svd_impl)
+svd_p.def_abstract_eval(svd_abstract_eval)
+xla.translations[svd_p] = svd_translation_rule",No
jax/numpy/linalg.py,jax/numpy/linalg.py,484db1e15fd188ef977e6c15bed12fa8f611bfe7,c5c6e6c5c7f86dad5ac70e974af7d814e723fdb7,Add SVD for float and double types,"diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 6f87e196e..2b05ce82d 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -37,6 +37,12 @@ def cholesky(a):
   return lax_linalg.cholesky(a)
 
 
+@_wraps(onp.linalg.svd)
+def svd(a, full_matrices=True, compute_uv=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
+  return lax_linalg.svd(a, full_matrices, compute_uv)
+
+
 @_wraps(onp.linalg.slogdet)
 def slogdet(a):
   dtype = lax._dtype(a)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 6f87e196e..2b05ce82d 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -37,6 +37,12 @@ def cholesky(a):
   return lax_linalg.cholesky(a)
 
 
+@_wraps(onp.linalg.svd)
+def svd(a, full_matrices=True, compute_uv=True):
+  warnings.warn(_EXPERIMENTAL_WARNING)
+  return lax_linalg.svd(a, full_matrices, compute_uv)
+
+
 @_wraps(onp.linalg.slogdet)
 def slogdet(a):
   dtype = lax._dtype(a)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,484db1e15fd188ef977e6c15bed12fa8f611bfe7,c5c6e6c5c7f86dad5ac70e974af7d814e723fdb7,Add SVD for float and double types,"diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 088d10751..388d030e8 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -39,6 +39,13 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   return l if lower else np.conj(l.T)
 
 
+@_wraps(scipy.linalg.svd)
+def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd'):
+  warnings.warn(_EXPERIMENTAL_WARNING)
+  del overwrite_a, check_finite, lapack_driver
+  return lax_linalg.svd(a)
+
+
 @_wraps(scipy.linalg.det)
 def det(a, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 088d10751..388d030e8 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -39,6 +39,13 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   return l if lower else np.conj(l.T)
 
 
+@_wraps(scipy.linalg.svd)
+def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd'):
+  warnings.warn(_EXPERIMENTAL_WARNING)
+  del overwrite_a, check_finite, lapack_driver
+  return lax_linalg.svd(a)
+
+
 @_wraps(scipy.linalg.det)
 def det(a, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,484db1e15fd188ef977e6c15bed12fa8f611bfe7,c5c6e6c5c7f86dad5ac70e974af7d814e723fdb7,Add SVD for float and double types,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9f71fbd2b..3b689020d 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -27,6 +27,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
 
 import numpy as np
 from jaxlib import xla_client
@@ -383,3 +384,125 @@ def jax_potrf(c, a, lower=False):
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
       ))
+
+
+
+# ?gesdd: SVD decomposition
+
+cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt = (<int32_t*>(data[0]))[0]
+  cdef int m = (<int32_t*>(data[1]))[0]
+  cdef int n = (<int32_t*>(data[2]))[0]
+  cdef float* a_in = <float*>(data[3])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float* a_out = <float*>(out[0])
+  cdef float* s = <float*>(out[1])
+  cdef float* u = <float*>(out[2])
+  cdef float* vt = <float*>(out[3])
+  cdef int* info = <int*>(out[4])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_some == 1:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+
+  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+
+  # First perform a workspace query to get the optimal lwork
+  cdef float wkopt = 0
+  cdef int lwork = -1
+  sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
+  lwork = <int> wkopt
+
+  # Now get the actual SVD
+  cdef work = <float *> malloc(lwork * sizeof(float))
+  sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+
+register_cpu_custom_call_target(b""lapack_sgesdd"", <void*>(lapack_sgesdd))
+
+
+cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef double* a_in = <double*>(data[4])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double* a_out = <double*>(out[0])
+  cdef double* s = <double*>(out[1])
+  cdef double* u = <double*>(out[2])
+  cdef double* vt = <double*>(out[3])
+  cdef int* info = <int*>(out[4])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_some == 1:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+
+  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+
+  # First perform a workspace query to get the optimal lwork
+  cdef double wkopt = 0
+  cdef int lwork = -1
+  dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
+  lwork = <int> wkopt
+
+  # Now get the actual SVD
+  cdef work = <double *> malloc(lwork * sizeof(double))
+  dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+
+register_cpu_custom_call_target(b""lapack_dgesdd"", <void*>(lapack_dgesdd))
+
+def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
+  assert sizeof(int32_t) == sizeof(int)
+
+  a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
+  m, n = a_shape.dimensions()
+  if dtype == np.float32:
+    fn = b""lapack_sgesdd""
+  elif dtype == np.float64:
+    fn = b""lapack_dgesdd""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(c.ConstantS32Scalar(int(full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
+                c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+          Shape.array_shape(dtype, (min(m, n),), (0,)),
+          Shape.array_shape(dtype, (m, m if full_matrices else min(m, n)), (0, 1)),
+          Shape.array_shape(dtype, (n if full_matrices else min(m, n), n), (0, 1)),
+          Shape.array_shape(np.int32, (), ()),
+      )),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+      ))","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9f71fbd2b..3b689020d 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -27,6 +27,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
 
 import numpy as np
 from jaxlib import xla_client
@@ -383,3 +384,125 @@ def jax_potrf(c, a, lower=False):
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
       ))
+
+
+
+# ?gesdd: SVD decomposition
+
+cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt = (<int32_t*>(data[0]))[0]
+  cdef int m = (<int32_t*>(data[1]))[0]
+  cdef int n = (<int32_t*>(data[2]))[0]
+  cdef float* a_in = <float*>(data[3])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float* a_out = <float*>(out[0])
+  cdef float* s = <float*>(out[1])
+  cdef float* u = <float*>(out[2])
+  cdef float* vt = <float*>(out[3])
+  cdef int* info = <int*>(out[4])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_some == 1:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+
+  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+
+  # First perform a workspace query to get the optimal lwork
+  cdef float wkopt = 0
+  cdef int lwork = -1
+  sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
+  lwork = <int> wkopt
+
+  # Now get the actual SVD
+  cdef work = <float *> malloc(lwork * sizeof(float))
+  sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+
+register_cpu_custom_call_target(b""lapack_sgesdd"", <void*>(lapack_sgesdd))
+
+
+cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef double* a_in = <double*>(data[4])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double* a_out = <double*>(out[0])
+  cdef double* s = <double*>(out[1])
+  cdef double* u = <double*>(out[2])
+  cdef double* vt = <double*>(out[3])
+  cdef int* info = <int*>(out[4])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_some == 1:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+
+  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+
+  # First perform a workspace query to get the optimal lwork
+  cdef double wkopt = 0
+  cdef int lwork = -1
+  dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
+  lwork = <int> wkopt
+
+  # Now get the actual SVD
+  cdef work = <double *> malloc(lwork * sizeof(double))
+  dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+
+register_cpu_custom_call_target(b""lapack_dgesdd"", <void*>(lapack_dgesdd))
+
+def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
+  assert sizeof(int32_t) == sizeof(int)
+
+  a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
+  m, n = a_shape.dimensions()
+  if dtype == np.float32:
+    fn = b""lapack_sgesdd""
+  elif dtype == np.float64:
+    fn = b""lapack_dgesdd""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(c.ConstantS32Scalar(int(full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
+                c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+          Shape.array_shape(dtype, (min(m, n),), (0,)),
+          Shape.array_shape(dtype, (m, m if full_matrices else min(m, n)), (0, 1)),
+          Shape.array_shape(dtype, (n if full_matrices else min(m, n), n), (0, 1)),
+          Shape.array_shape(np.int32, (), ()),
+      )),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (m, n), (0, 1)),
+      ))",No
.travis.yml,.travis.yml,77f0fca7fa3ec21ef6a6986ae6427955ae303866,8ada14e96b8c5ec7c0c60099460003be02aa2541,"fix np.polyval complex128 bug in x64=True mode

Also enable JAX_ENABLE_X64=True tests on Travis to avoid future such
issues. (Internal tests catch things like this, but we don't run those
automatically.)","diff --git a/.travis.yml b/.travis.yml
index 519d1a035..5c904540a 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -7,7 +7,8 @@ python:
   - ""2.7""
   - ""3.6""
 env:
-  - DEPS=""pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy scipy""
+  - JAX_ENABLE_X64=0 JAX_NUM_GENERATED_CASES=100
+  - JAX_ENABLE_X64=1 JAX_NUM_GENERATED_CASES=100
 before_install:
   - if [[ ""$TRAVIS_PYTHON_VERSION"" == ""2.7"" ]]; then
       wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;
@@ -19,8 +20,8 @@ before_install:
   - conda update --yes conda
   - conda config --add channels conda-forge
 install:
-  - conda install --yes python=$TRAVIS_PYTHON_VERSION $DEPS
+  - conda install --yes python=$TRAVIS_PYTHON_VERSION pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy scipy
   - pip install jaxlib
   - pip install -v .
 script:
-  - JAX_NUM_GENERATED_CASES=100 nosetests tests examples
+  - nosetests tests examples","diff --git a/.travis.yml b/.travis.yml
index 519d1a035..5c904540a 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -7,7 +7,8 @@ python:
   - ""2.7""
   - ""3.6""
 env:
-  - DEPS=""pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy scipy""
+  - JAX_ENABLE_X64=0 JAX_NUM_GENERATED_CASES=100
+  - JAX_ENABLE_X64=1 JAX_NUM_GENERATED_CASES=100
 before_install:
   - if [[ ""$TRAVIS_PYTHON_VERSION"" == ""2.7"" ]]; then
       wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh -O miniconda.sh;
@@ -19,8 +20,8 @@ before_install:
   - conda update --yes conda
   - conda config --add channels conda-forge
 install:
-  - conda install --yes python=$TRAVIS_PYTHON_VERSION $DEPS
+  - conda install --yes python=$TRAVIS_PYTHON_VERSION pip nose six protobuf>=3.6.0 absl-py opt_einsum numpy scipy
   - pip install jaxlib
   - pip install -v .
 script:
-  - JAX_NUM_GENERATED_CASES=100 nosetests tests examples
+  - nosetests tests examples",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,77f0fca7fa3ec21ef6a6986ae6427955ae303866,8ada14e96b8c5ec7c0c60099460003be02aa2541,"fix np.polyval complex128 bug in x64=True mode

Also enable JAX_ENABLE_X64=True tests on Travis to avoid future such
issues. (Internal tests catch things like this, but we don't run those
automatically.)","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 6d5cbb44d..363d3ae11 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -34,7 +34,9 @@ from jaxlib import xla_client
 
 
 FLAGS = flags.FLAGS
-flags.DEFINE_bool('jax_enable_x64', False, 'Enable 64-bit types to be used.')
+flags.DEFINE_bool('jax_enable_x64',
+                  os.getenv('JAX_ENABLE_X64', False),
+                  'Enable 64-bit types to be used.')
 flags.DEFINE_string('jax_dump_hlo_graph', None, 'Regexp of HLO graphs to dump.')
 flags.DEFINE_bool('jax_hlo_profile', False, 'Enables HLO profiling mode.')
 flags.DEFINE_string('jax_dump_hlo_unoptimized', None,
@@ -216,6 +218,11 @@ _dtype_to_32bit_dtype = {
 def canonicalize_dtype(dtype):
   """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
   dtype = onp.dtype(dtype)
+
+  # special rule for complex128, which XLA doesn't support
+  if dtype == onp.complex128:
+    dtype = onp.dtype('complex64')
+
   if FLAGS.jax_enable_x64:
     return str(dtype)
   else:","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 6d5cbb44d..363d3ae11 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -34,7 +34,9 @@ from jaxlib import xla_client
 
 
 FLAGS = flags.FLAGS
-flags.DEFINE_bool('jax_enable_x64', False, 'Enable 64-bit types to be used.')
+flags.DEFINE_bool('jax_enable_x64',
+                  os.getenv('JAX_ENABLE_X64', False),
+                  'Enable 64-bit types to be used.')
 flags.DEFINE_string('jax_dump_hlo_graph', None, 'Regexp of HLO graphs to dump.')
 flags.DEFINE_bool('jax_hlo_profile', False, 'Enables HLO profiling mode.')
 flags.DEFINE_string('jax_dump_hlo_unoptimized', None,
@@ -216,6 +218,11 @@ _dtype_to_32bit_dtype = {
 def canonicalize_dtype(dtype):
   """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
   dtype = onp.dtype(dtype)
+
+  # special rule for complex128, which XLA doesn't support
+  if dtype == onp.complex128:
+    dtype = onp.dtype('complex64')
+
   if FLAGS.jax_enable_x64:
     return str(dtype)
   else:",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,77f0fca7fa3ec21ef6a6986ae6427955ae303866,8ada14e96b8c5ec7c0c60099460003be02aa2541,"fix np.polyval complex128 bug in x64=True mode

Also enable JAX_ENABLE_X64=True tests on Travis to avoid future such
issues. (Internal tests catch things like this, but we don't run those
automatically.)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 69dbe2fca..7fc3aaaf7 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -149,8 +149,7 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    from_dtypes = (_dtype(x) for x in args)
-    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
+    to_dtype = xla_bridge.canonicalize_dtype(result_type(*args))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 69dbe2fca..7fc3aaaf7 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -149,8 +149,7 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    from_dtypes = (_dtype(x) for x in args)
-    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
+    to_dtype = xla_bridge.canonicalize_dtype(result_type(*args))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]
 ",No
jax/test_util.py,jax/test_util.py,77f0fca7fa3ec21ef6a6986ae6427955ae303866,8ada14e96b8c5ec7c0c60099460003be02aa2541,"fix np.polyval complex128 bug in x64=True mode

Also enable JAX_ENABLE_X64=True tests on Travis to avoid future such
issues. (Internal tests catch things like this, but we don't run those
automatically.)","diff --git a/jax/test_util.py b/jax/test_util.py
index 754a7b88b..34f633d2a 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -408,8 +408,17 @@ class JaxTestCase(parameterized.TestCase):
       self.assertDtypesMatch(x, y)
 
   def assertDtypesMatch(self, x, y):
+    # special rule for complex128, which XLA doesn't support
+    def c128_to_c64(dtype):
+      if dtype == onp.complex128:
+        return onp.complex64
+      else:
+        return dtype
+
     if FLAGS.jax_enable_x64:
-      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)
+      x_dtype = c128_to_c64(onp.asarray(x).dtype)
+      y_dtype = c128_to_c64(onp.asarray(y).dtype)
+      self.assertEqual(x_dtype, y_dtype)
 
   def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
     """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""","diff --git a/jax/test_util.py b/jax/test_util.py
index 754a7b88b..34f633d2a 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -408,8 +408,17 @@ class JaxTestCase(parameterized.TestCase):
       self.assertDtypesMatch(x, y)
 
   def assertDtypesMatch(self, x, y):
+    # special rule for complex128, which XLA doesn't support
+    def c128_to_c64(dtype):
+      if dtype == onp.complex128:
+        return onp.complex64
+      else:
+        return dtype
+
     if FLAGS.jax_enable_x64:
-      self.assertEqual(onp.asarray(x).dtype, onp.asarray(y).dtype)
+      x_dtype = c128_to_c64(onp.asarray(x).dtype)
+      y_dtype = c128_to_c64(onp.asarray(y).dtype)
+      self.assertEqual(x_dtype, y_dtype)
 
   def assertAllClose(self, x, y, check_dtypes, atol=None, rtol=None):
     """"""Assert that x and y, either arrays or nested tuples/lists, are close.""""""",No
jax/experimental/stax.py,jax/experimental/stax.py,ecaae6bdd001cab5e7e9ce7e395fd7536a7ad79f,8ada14e96b8c5ec7c0c60099460003be02aa2541,add Softmax layer to stax (closes #182),"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index e9988f128..ffc2dec4d 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -146,6 +146,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
+Exp = _elemwise_no_params(np.exp)
 LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
@@ -314,3 +315,8 @@ def shape_dependent(make_layer):
   def apply_fun(params, inputs, rng=None):
     return make_layer(inputs.shape)[1](params, inputs, rng)
   return init_fun, apply_fun
+
+
+# Simple compositions
+
+Softmax = serial(LogSoftmax, Exp)","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index e9988f128..ffc2dec4d 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -146,6 +146,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
+Exp = _elemwise_no_params(np.exp)
 LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
@@ -314,3 +315,8 @@ def shape_dependent(make_layer):
   def apply_fun(params, inputs, rng=None):
     return make_layer(inputs.shape)[1](params, inputs, rng)
   return init_fun, apply_fun
+
+
+# Simple compositions
+
+Softmax = serial(LogSoftmax, Exp)",No
tests/stax_test.py,tests/stax_test.py,ecaae6bdd001cab5e7e9ce7e395fd7536a7ad79f,8ada14e96b8c5ec7c0c60099460003be02aa2541,add Softmax layer to stax (closes #182),"diff --git a/tests/stax_test.py b/tests/stax_test.py
index cb3502502..245c9b24e 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -159,6 +159,17 @@ class StaxTest(jtu.JaxTestCase):
     init_fun, apply_fun = stax.FanInConcat(axis)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shapes)
 
+  def testIsuse182(self):
+    init_fun, apply_fun = stax.Softmax
+    input_shape = (10, 3)
+    inputs = onp.arange(30.).astype(""float32"").reshape(input_shape)
+
+    out_shape, params = init_fun(input_shape)
+    out = apply_fun(params, inputs)
+
+    assert out_shape == out.shape
+    assert onp.allclose(onp.sum(onp.asarray(out), -1), 1.)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/stax_test.py b/tests/stax_test.py
index cb3502502..245c9b24e 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -159,6 +159,17 @@ class StaxTest(jtu.JaxTestCase):
     init_fun, apply_fun = stax.FanInConcat(axis)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shapes)
 
+  def testIsuse182(self):
+    init_fun, apply_fun = stax.Softmax
+    input_shape = (10, 3)
+    inputs = onp.arange(30.).astype(""float32"").reshape(input_shape)
+
+    out_shape, params = init_fun(input_shape)
+    out = apply_fun(params, inputs)
+
+    assert out_shape == out.shape
+    assert onp.allclose(onp.sum(onp.asarray(out), -1), 1.)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,555bed74c6bbadd1df017836f1f97cc9bd155073,77f0fca7fa3ec21ef6a6986ae6427955ae303866,revert lax_numpy dtype promotion logic,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7fc3aaaf7..69dbe2fca 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -149,7 +149,8 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    to_dtype = xla_bridge.canonicalize_dtype(result_type(*args))
+    from_dtypes = (_dtype(x) for x in args)
+    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 7fc3aaaf7..69dbe2fca 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -149,7 +149,8 @@ def _promote_dtypes(*args):
   if len(args) < 2:
     return args
   else:
-    to_dtype = xla_bridge.canonicalize_dtype(result_type(*args))
+    from_dtypes = (_dtype(x) for x in args)
+    to_dtype = xla_bridge.canonicalize_dtype(result_type(*from_dtypes))
     return [lax.convert_element_type(x, to_dtype)
             if _dtype(x) != to_dtype else x for x in args]
 ",No
jax/experimental/stax.py,jax/experimental/stax.py,3174b37b487b129b6cc6762a1d271de6f1c5b031,ecaae6bdd001cab5e7e9ce7e395fd7536a7ad79f,implement softmax directly,"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index ffc2dec4d..4a2e4abb7 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -47,6 +47,11 @@ def logsoftmax(x, axis=-1):
   """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
+def softmax(x, axis=-1):
+  """"""Apply softmax to an array of logits, exponentiating and normalizing along an axis.""""""
+  unnormalized = np.exp(x - x.max(axis, keepdims=True))
+  return unnormalized / unnormalized.sum(axis, keepdims=True)
+
 def fastvar(x, axis, keepdims):
   """"""A fast but less numerically-stable variance calculation than np.var.""""""
   return np.mean(x**2, axis, keepdims=keepdims) - np.mean(x, axis, keepdims=keepdims)**2
@@ -148,6 +153,7 @@ Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
 Exp = _elemwise_no_params(np.exp)
 LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
+Softmax = _elemwise_no_params(softmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 
@@ -315,8 +321,3 @@ def shape_dependent(make_layer):
   def apply_fun(params, inputs, rng=None):
     return make_layer(inputs.shape)[1](params, inputs, rng)
   return init_fun, apply_fun
-
-
-# Simple compositions
-
-Softmax = serial(LogSoftmax, Exp)","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index ffc2dec4d..4a2e4abb7 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -47,6 +47,11 @@ def logsoftmax(x, axis=-1):
   """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
+def softmax(x, axis=-1):
+  """"""Apply softmax to an array of logits, exponentiating and normalizing along an axis.""""""
+  unnormalized = np.exp(x - x.max(axis, keepdims=True))
+  return unnormalized / unnormalized.sum(axis, keepdims=True)
+
 def fastvar(x, axis, keepdims):
   """"""A fast but less numerically-stable variance calculation than np.var.""""""
   return np.mean(x**2, axis, keepdims=keepdims) - np.mean(x, axis, keepdims=keepdims)**2
@@ -148,6 +153,7 @@ Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
 Exp = _elemwise_no_params(np.exp)
 LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
+Softmax = _elemwise_no_params(softmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 
@@ -315,8 +321,3 @@ def shape_dependent(make_layer):
   def apply_fun(params, inputs, rng=None):
     return make_layer(inputs.shape)[1](params, inputs, rng)
   return init_fun, apply_fun
-
-
-# Simple compositions
-
-Softmax = serial(LogSoftmax, Exp)",No
examples/advi.py,examples/advi.py,cb7d3550ff4092652258b000a8a624f4884304a9,d85c0dbecbadf8d1aceb1f98e5497f5c130538fd,Updated advi example to map over rngs.,"diff --git a/examples/advi.py b/examples/advi.py
index aee1a7f55..834c54f08 100644
--- a/examples/advi.py
+++ b/examples/advi.py
@@ -19,86 +19,84 @@ import jax.scipy.stats.norm as norm
 
 # ========= Functions to define the evidence lower bound. =========
 
-def diag_gaussian_sample(mean, log_std, rng):
+def diag_gaussian_sample(rng, mean, log_std):
+    # Take a single sample from a diagonal multivariate Gaussian.
     return mean + np.exp(log_std) * random.normal(rng, mean.shape)
 
 def diag_gaussian_logpdf(x, mean, log_std):
+    # Evaluate a single point on a diagonal multivariate Gaussian.
     return np.sum(vmap(norm.logpdf)(x, mean, np.exp(log_std)))
 
-def elbo((mean, log_std), logprob, rng):
-    # Simple Monte Carlo estimate of the variational lower bound.
-    sample = diag_gaussian_sample(mean, log_std, rng)
+def elbo(logprob, rng, (mean, log_std)):
+    # Single-sample Monte Carlo estimate of the variational lower bound.
+    sample = diag_gaussian_sample(rng, mean, log_std)
     return logprob(sample) - diag_gaussian_logpdf(sample, mean, log_std)
 
+def batch_elbo(logprob, rng, params, num_samples):
+    # Average over a batch of random samples.
+    rngs = random.split(rng, num_samples)
+    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None))
+    return np.mean(vectorized_elbo(rngs, params))
 
-# ========= Helper functions for batching. =========
 
-def rng_map(f, rng, num_samples, *args, **kwargs):
-    # Calls f with a batch of different seeds.
-    # f must have signature f(*args, rng, **kwargs).
-    rngs = np.array(random.split(rng, num_samples))
-    return vmap(partial(f, *args, **kwargs))(rngs)
+# ========= Helper function for plotting. =========
 
-def batch_elbo(params, logprob, num_samples, rng):
-    return np.mean(rng_map(elbo, rng, num_samples, params, logprob))
-
-
-# ========= Helper functions for plotting. =========
-
-def eval_zip(func, X, Y):
-    params_vec = np.stack([X.ravel(), Y.ravel()]).T
-    zs = vmap(func)(params_vec)
-    return zs.reshape(X.shape)
-eval_zip = jit(eval_zip, static_argnums=(0,))
-
-def plot_isocontours(ax, func, xlimits, ylimits, numticks=101, **kwargs):
-    x = np.linspace(*xlimits, num=numticks)
-    y = np.linspace(*ylimits, num=numticks)
+@partial(jit, static_argnums=(0, 1, 2, 4))
+def mesh_eval(func, x_limits, y_limits, params, num_ticks=101):
+    # Evaluate func on a 2D grid defined by x_limits and y_limits.
+    x = np.linspace(*x_limits, num=num_ticks)
+    y = np.linspace(*y_limits, num=num_ticks)
     X, Y = np.meshgrid(x, y)
-    Z = eval_zip(func, X, Y)
-    ax.contour(X, Y, Z, **kwargs)
-    ax.set_xlim(xlimits)
-    ax.set_ylim(ylimits)
-    ax.set_yticks([])
-    ax.set_xticks([])
+    xy_vec = np.stack([X.ravel(), Y.ravel()]).T
+    zs = vmap(func, in_axes=(0, None))(xy_vec, params)
+    return X, Y, zs.reshape(X.shape)
 
 
 # ========= Define an intractable unnormalized density =========
 
 def funnel_log_density(params):
-    mean, log_stddev = params[0], params[1]
-    return norm.logpdf(mean,        0, np.exp(log_stddev)) + \
-           norm.logpdf(log_stddev,  0, 1.35)
+    return norm.logpdf(params[0], 0, np.exp(params[1])) + \
+           norm.logpdf(params[1], 0, 1.35)
 
 
 if __name__ == ""__main__"":
-    step_size = 0.1
-    momentum = 0.9
-    num_steps = 100
-    num_samples = 20
+    num_samples = 40
 
-    def objective(params, rng):
-        return -batch_elbo(params, funnel_log_density, num_samples, rng)
+    @jit
+    def objective(params, t):
+        rng = random.PRNGKey(t)
+        return -batch_elbo(funnel_log_density, rng, params, num_samples)
 
     # Set up figure.
     fig = plt.figure(figsize=(8,8), facecolor='white')
     ax = fig.add_subplot(111, frameon=False)
     plt.ion()
     plt.show(block=False)
-    xlimits = [-2, 2]
-    ylimits = [-4, 2]
+    x_limits = [-2, 2]
+    y_limits = [-4, 2]
+    target_dist = lambda x, _: np.exp(funnel_log_density(x))
+    approx_dist = lambda x, params: np.exp(diag_gaussian_logpdf(x, *params))
 
-    def callback(params, t, rng):
-        print(""Iteration {} lower bound {}"".format(t, objective(params, rng)))
+    def callback(params, t):
+        print(""Iteration {} lower bound {}"".format(t, objective(params, t)))
 
         plt.cla()
-        target_dist = lambda x: np.exp(funnel_log_density(x))
-        approx_dist = lambda x: np.exp(diag_gaussian_logpdf(x, *params))
-        plot_isocontours(ax, target_dist, xlimits, ylimits, cmap='summer')
-        plot_isocontours(ax, approx_dist, xlimits, ylimits, cmap='winter')
+        X, Y, Z = mesh_eval(target_dist, x_limits, y_limits, 1)
+        ax.contour(X, Y, Z, cmap='summer')
+        X, Y, Z = mesh_eval(approx_dist, x_limits, y_limits, params)
+        ax.contour(X, Y, Z, cmap='winter')
+        ax.set_xlim(x_limits)
+        ax.set_ylim(y_limits)
+        ax.set_yticks([])
+        ax.set_xticks([])
+
+        # Plot random samples from variational distribution.
+        # Here we clone the rng used in computing the objective
+        # so that we can show exactly the same samples.
+        rngs = random.split(random.PRNGKey(t), num_samples)
+        samples = vmap(diag_gaussian_sample, in_axes=(0, None, None))(rngs, *params)
+        ax.plot(samples[:, 0], samples[:, 1], 'b.')
 
-        samples = rng_map(diag_gaussian_sample, rng, num_samples, *params)
-        plt.plot(samples[:, 0], samples[:, 1], 'b.')
         plt.draw()
         plt.pause(1.0/60.0)
 
@@ -108,19 +106,19 @@ if __name__ == ""__main__"":
     init_mean = np.zeros(D)
     init_std  = np.zeros(D)
     init_params = (init_mean, init_std)
-    opt_init, opt_update = minmax.momentum(step_size, mass=momentum)
+    opt_init, opt_update = minmax.momentum(step_size=0.1, mass=0.9)
     opt_state = opt_init(init_params)
 
     @jit
-    def update(i, opt_state, rng):
+    def update(i, opt_state):
         params = minmax.get_params(opt_state)
-        return opt_update(i, grad(objective)(params, rng), opt_state)
+        gradient = grad(objective)(params, i)
+        return opt_update(i, gradient, opt_state)
 
 
-    # Main fitting loop.
+    # Main loop.
     print(""Optimizing variational parameters..."")
-    rng = random.PRNGKey(0)
-    for t in range(num_steps):
-      opt_state = update(t, opt_state, rng)
-      callback(minmax.get_params(opt_state), t, rng)
-      old, rng = random.split(rng)
+    for t in range(100):
+      opt_state = update(t, opt_state)
+      params = minmax.get_params(opt_state)
+      callback(params, t)","diff --git a/examples/advi.py b/examples/advi.py
index aee1a7f55..834c54f08 100644
--- a/examples/advi.py
+++ b/examples/advi.py
@@ -19,86 +19,84 @@ import jax.scipy.stats.norm as norm
 
 # ========= Functions to define the evidence lower bound. =========
 
-def diag_gaussian_sample(mean, log_std, rng):
+def diag_gaussian_sample(rng, mean, log_std):
+    # Take a single sample from a diagonal multivariate Gaussian.
     return mean + np.exp(log_std) * random.normal(rng, mean.shape)
 
 def diag_gaussian_logpdf(x, mean, log_std):
+    # Evaluate a single point on a diagonal multivariate Gaussian.
     return np.sum(vmap(norm.logpdf)(x, mean, np.exp(log_std)))
 
-def elbo((mean, log_std), logprob, rng):
-    # Simple Monte Carlo estimate of the variational lower bound.
-    sample = diag_gaussian_sample(mean, log_std, rng)
+def elbo(logprob, rng, (mean, log_std)):
+    # Single-sample Monte Carlo estimate of the variational lower bound.
+    sample = diag_gaussian_sample(rng, mean, log_std)
     return logprob(sample) - diag_gaussian_logpdf(sample, mean, log_std)
 
-
-# ========= Helper functions for batching. =========
-
-def rng_map(f, rng, num_samples, *args, **kwargs):
-    # Calls f with a batch of different seeds.
-    # f must have signature f(*args, rng, **kwargs).
-    rngs = np.array(random.split(rng, num_samples))
-    return vmap(partial(f, *args, **kwargs))(rngs)
-
-def batch_elbo(params, logprob, num_samples, rng):
-    return np.mean(rng_map(elbo, rng, num_samples, params, logprob))
+def batch_elbo(logprob, rng, params, num_samples):
+    # Average over a batch of random samples.
+    rngs = random.split(rng, num_samples)
+    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None))
+    return np.mean(vectorized_elbo(rngs, params))
 
 
-# ========= Helper functions for plotting. =========
+# ========= Helper function for plotting. =========
 
-def eval_zip(func, X, Y):
-    params_vec = np.stack([X.ravel(), Y.ravel()]).T
-    zs = vmap(func)(params_vec)
-    return zs.reshape(X.shape)
-eval_zip = jit(eval_zip, static_argnums=(0,))
-
-def plot_isocontours(ax, func, xlimits, ylimits, numticks=101, **kwargs):
-    x = np.linspace(*xlimits, num=numticks)
-    y = np.linspace(*ylimits, num=numticks)
+@partial(jit, static_argnums=(0, 1, 2, 4))
+def mesh_eval(func, x_limits, y_limits, params, num_ticks=101):
+    # Evaluate func on a 2D grid defined by x_limits and y_limits.
+    x = np.linspace(*x_limits, num=num_ticks)
+    y = np.linspace(*y_limits, num=num_ticks)
     X, Y = np.meshgrid(x, y)
-    Z = eval_zip(func, X, Y)
-    ax.contour(X, Y, Z, **kwargs)
-    ax.set_xlim(xlimits)
-    ax.set_ylim(ylimits)
-    ax.set_yticks([])
-    ax.set_xticks([])
+    xy_vec = np.stack([X.ravel(), Y.ravel()]).T
+    zs = vmap(func, in_axes=(0, None))(xy_vec, params)
+    return X, Y, zs.reshape(X.shape)
 
 
 # ========= Define an intractable unnormalized density =========
 
 def funnel_log_density(params):
-    mean, log_stddev = params[0], params[1]
-    return norm.logpdf(mean,        0, np.exp(log_stddev)) + \
-           norm.logpdf(log_stddev,  0, 1.35)
+    return norm.logpdf(params[0], 0, np.exp(params[1])) + \
+           norm.logpdf(params[1], 0, 1.35)
 
 
 if __name__ == ""__main__"":
-    step_size = 0.1
-    momentum = 0.9
-    num_steps = 100
-    num_samples = 20
+    num_samples = 40
 
-    def objective(params, rng):
-        return -batch_elbo(params, funnel_log_density, num_samples, rng)
+    @jit
+    def objective(params, t):
+        rng = random.PRNGKey(t)
+        return -batch_elbo(funnel_log_density, rng, params, num_samples)
 
     # Set up figure.
     fig = plt.figure(figsize=(8,8), facecolor='white')
     ax = fig.add_subplot(111, frameon=False)
     plt.ion()
     plt.show(block=False)
-    xlimits = [-2, 2]
-    ylimits = [-4, 2]
+    x_limits = [-2, 2]
+    y_limits = [-4, 2]
+    target_dist = lambda x, _: np.exp(funnel_log_density(x))
+    approx_dist = lambda x, params: np.exp(diag_gaussian_logpdf(x, *params))
 
-    def callback(params, t, rng):
-        print(""Iteration {} lower bound {}"".format(t, objective(params, rng)))
+    def callback(params, t):
+        print(""Iteration {} lower bound {}"".format(t, objective(params, t)))
 
         plt.cla()
-        target_dist = lambda x: np.exp(funnel_log_density(x))
-        approx_dist = lambda x: np.exp(diag_gaussian_logpdf(x, *params))
-        plot_isocontours(ax, target_dist, xlimits, ylimits, cmap='summer')
-        plot_isocontours(ax, approx_dist, xlimits, ylimits, cmap='winter')
+        X, Y, Z = mesh_eval(target_dist, x_limits, y_limits, 1)
+        ax.contour(X, Y, Z, cmap='summer')
+        X, Y, Z = mesh_eval(approx_dist, x_limits, y_limits, params)
+        ax.contour(X, Y, Z, cmap='winter')
+        ax.set_xlim(x_limits)
+        ax.set_ylim(y_limits)
+        ax.set_yticks([])
+        ax.set_xticks([])
+
+        # Plot random samples from variational distribution.
+        # Here we clone the rng used in computing the objective
+        # so that we can show exactly the same samples.
+        rngs = random.split(random.PRNGKey(t), num_samples)
+        samples = vmap(diag_gaussian_sample, in_axes=(0, None, None))(rngs, *params)
+        ax.plot(samples[:, 0], samples[:, 1], 'b.')
 
-        samples = rng_map(diag_gaussian_sample, rng, num_samples, *params)
-        plt.plot(samples[:, 0], samples[:, 1], 'b.')
         plt.draw()
         plt.pause(1.0/60.0)
 
@@ -108,19 +106,19 @@ if __name__ == ""__main__"":
     init_mean = np.zeros(D)
     init_std  = np.zeros(D)
     init_params = (init_mean, init_std)
-    opt_init, opt_update = minmax.momentum(step_size, mass=momentum)
+    opt_init, opt_update = minmax.momentum(step_size=0.1, mass=0.9)
     opt_state = opt_init(init_params)
 
     @jit
-    def update(i, opt_state, rng):
+    def update(i, opt_state):
         params = minmax.get_params(opt_state)
-        return opt_update(i, grad(objective)(params, rng), opt_state)
+        gradient = grad(objective)(params, i)
+        return opt_update(i, gradient, opt_state)
 
 
-    # Main fitting loop.
+    # Main loop.
     print(""Optimizing variational parameters..."")
-    rng = random.PRNGKey(0)
-    for t in range(num_steps):
-      opt_state = update(t, opt_state, rng)
-      callback(minmax.get_params(opt_state), t, rng)
-      old, rng = random.split(rng)
+    for t in range(100):
+      opt_state = update(t, opt_state)
+      params = minmax.get_params(opt_state)
+      callback(params, t)",Yes
jax/api.py,jax/api.py,ad4322c5daea26fb685ec990746a0dc9252f21ee,014e19d90f4817d6cbcfe228434fdd70ac56280b,playing around with flattening functions,"diff --git a/jax/api.py b/jax/api.py
index 79eebb01c..f3887176b 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -31,7 +31,9 @@ import numpy as onp
 from . import core
 from . import linear_util as lu
 from .core import pack, eval_jaxpr
-from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
+from .api_util import (pytree_fun_to_jaxtupletree_fun, apply_jaxtree_fun,
+                       pytree_to_jaxtupletree, wraps)
+from .flatten_util import ravel_fun, ravel_pytree
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                         tree_map)
 from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
@@ -43,15 +45,6 @@ from .interpreters import batching
 
 map = safe_map
 
-def _wraps(wrapped):
-  def decorator(wrapper):
-    wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
-    wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
-    if hasattr(wrapped, ""__doc__""):
-      wrapper.__doc__ = getattr(wrapped, ""__doc__"")
-    return wrapper
-  return decorator
-
 
 def jit(fun, static_argnums=()):
   """"""Sets up `fun` for just-in-time compilation with XLA.
@@ -70,15 +63,15 @@ def jit(fun, static_argnums=()):
   Returns:
     A wrapped version of `fun`, set up for just-in-time compilation.
   """"""
-  @_wraps(fun)
+  @wraps(fun)
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
     f, dyn_args = argnums_partial(f, dyn_argnums, args)
-    args_flat, in_trees = unzip2(map(tree_to_jaxtuples, dyn_args))
+    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
     check_args(args_flat)
-    flat_fun, out_tree = flatten_fun(f, in_trees)
-    out_flat = xla.xla_call(flat_fun, *args_flat)
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = xla.xla_call(jaxtree_fun, *args_flat)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
@@ -93,8 +86,8 @@ def grad(fun, argnums=0):
       `argnums` should be arrays, scalars, or standard Python containers. It
       should return a scalar (which includes arrays with shape `()` but not
       arrays with shape `(1,)` etc.)
-    argnums: Integer or tuple of integers. Specifies which positional
-    argument(s) to differentiate with respect to.
+    argnums: Optional, integer or tuple of integers. Specifies which positional
+      argument(s) to differentiate with respect to (default 0).
 
   Returns:
     A function with the same arguments as `fun`, that evaluates the gradient of
@@ -119,8 +112,8 @@ def value_and_grad(fun, argnums=0):
       `argnums` should be arrays, scalars, or standard Python containers. It
       should return a scalar (which includes arrays with shape `()` but not
       arrays with shape `(1,)` etc.)
-    argnums: Integer or tuple of integers. Specifies which positional
-      argument(s) to differentiate with respect to.
+    argnums: Optional, integer or tuple of integers. Specifies which positional
+      argument(s) to differentiate with respect to (default 0).
 
   Returns:
     A function with the same arguments as `fun` that evaluates both `fun` and
@@ -144,7 +137,8 @@ def value_and_grad(fun, argnums=0):
 @curry
 def jacfwd(fun, x):
   """"""Jacobian of `fun`, evaluated column-by-column using forward-mode AD""""""
-  fun = lu.wrap_init(fun)
+  if not isinstance(fun, lu.WrappedFun):
+    fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
   y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)
@@ -153,7 +147,8 @@ def jacfwd(fun, x):
 @curry
 def jacrev(fun, x):
   """"""Jacobian of `fun`, evaluated row-by-row using reverse-mode AD""""""
-  fun = lu.wrap_init(fun)
+  if not isinstance(fun, lu.WrappedFun):
+    fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
   jac_flat, = vmap(pullback, out_axes=0)(std_basis)
@@ -162,6 +157,20 @@ def jacrev(fun, x):
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
+
+def general_jacobian(jacfun, fun):
+  def jac_f(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    raveled_input, unravel_inputs = ravel_pytree(args)
+    raveled_fun, unravel_outputs = ravel_fun(f, unravel_inputs)
+    jacmat = jacfun(raveled_fun)(raveled_input)
+    return tree_map(unravel_inputs, vmap(unravel_outputs(), in_axes=1)(jacmat))
+  return jac_f
+jacfwd2 = partial(general_jacobian, jacfwd)
+jacrev2 = partial(general_jacobian, jacrev)
+hessian2 = partial(general_jacobian, hessian)  # TODO(mattjj): doesn't work yet
+
+
 def vmap(fun, in_axes=0, out_axes=0):
   """"""Vectorizing map. Creates a function which maps `fun` over additional axes.
 
@@ -189,32 +198,32 @@ def vmap(fun, in_axes=0, out_axes=0):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun)
     in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
-    in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
-    flat_fun, out_tree = flatten_fun(f, in_trees)
-    out_flat = batching.batch(flat_fun, in_flat, in_axes_, out_axes)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = batching.batch(jaxtree_fun, in_flat, in_axes_, out_axes)
     return build_tree(out_tree(), out_flat)
 
   return batched_fun
 
 def jvp(fun, primals, tangents):
-  def flatten_arg(primal, tangent):
-    primal_jtuple, tree_def = tree_to_jaxtuples(primal)
-    tangent_jtuple, tree_def_2 = tree_to_jaxtuples(tangent)
+  def trim_arg(primal, tangent):
+    primal_jtuple, tree_def = pytree_to_jaxtupletree(primal)
+    tangent_jtuple, tree_def_2 = pytree_to_jaxtupletree(tangent)
     assert tree_def == tree_def_2, (tree_def, tree_def_2)
     return primal_jtuple, tangent_jtuple, tree_def
 
   if not isinstance(fun, lu.WrappedFun):
     fun = lu.wrap_init(fun)
-  ps_flat, ts_flat, in_trees = unzip3(map(flatten_arg, primals, tangents))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_primal, out_tangent = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
+  ps_flat, ts_flat, in_trees = unzip3(map(trim_arg, primals, tangents))
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  out_primal, out_tangent = ad.jvp(jaxtree_fun).call_wrapped(ps_flat, ts_flat)
   return (build_tree(out_tree(), out_primal), build_tree(out_tree(), out_tangent))
 
 def linearize(traceable, *primals):
   fun = lu.wrap_init(traceable)
-  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_primal, out_pval, jaxpr, consts = ad.linearize(flat_fun, *primals_flat)
+  primals_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, primals))
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  out_primal, out_pval, jaxpr, consts = ad.linearize(jaxtree_fun, *primals_flat)
   out_tree = out_tree()
   out_primal_py = build_tree(out_tree, out_primal)
   lifted_jvp = partial(lift_linearized, jaxpr, consts, (in_trees, out_tree), out_pval)
@@ -227,37 +236,37 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
     _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
     return pe.merge_pvals(ans, out_pval)
 
-  return unflatten_fun(fun, io_tree, *py_args)
+  return apply_jaxtree_fun(fun, io_tree, *py_args)
 
 def vjp(fun, *primals):
   if not isinstance(fun, lu.WrappedFun):
     fun = lu.wrap_init(fun)
-  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
+  primals_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, primals))
   check_args(primals_flat)
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  out_primal, out_vjp = ad.vjp(jaxtree_fun, primals_flat)
   out_tree = out_tree()
   out_primal_py = build_tree(out_tree, out_primal)
   ct_in_trees = [out_tree]
   ct_out_tree = PyTreeDef(node_types[tuple], None, in_trees)
   def out_vjp_packed(cotangent_in):
     return out_vjp(cotangent_in)
-  vjp_py = partial(unflatten_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
+  vjp_py = partial(apply_jaxtree_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
   return out_primal_py, vjp_py
 
 
 def trace_to_jaxpr(traceable, py_pvals, **kwargs):
   fun = lu.wrap_init(traceable)
   pvals, in_trees = unzip2(map(tree_to_pval_tuples, py_pvals))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  jaxpr, out_pval, consts = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  jaxpr, out_pval, consts = pe.trace_to_jaxpr(jaxtree_fun, pvals, **kwargs)
   return jaxpr, consts, out_pval, (in_trees, out_tree())
 
 def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   def fun(*args):
     ans = eval_jaxpr(jaxpr, consts, (), *args)
     return pe.merge_pvals(ans, pvals)
-  return unflatten_fun(fun, io_tree, *py_args)
+  return apply_jaxtree_fun(fun, io_tree, *py_args)
 
 def make_jaxpr(f):
   def pv_like(x):
@@ -265,45 +274,26 @@ def make_jaxpr(f):
     return pe.PartialVal((aval, core.unit))
 
   fun = lu.wrap_init(f)
-  @_wraps(f)
+
+  @wraps(f)
   def jaxpr_maker(*args, **kwargs):
-    jax_args, in_trees = unzip2(map(tree_to_jaxtuples, args))
-    flat_fun, out_tree = flatten_fun(fun, in_trees)
+    jax_args, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
     pvals = map(pv_like, jax_args)
-    jaxpr, _, _ = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
+    jaxpr, _, _ = pe.trace_to_jaxpr(jaxtree_fun, pvals, **kwargs)
     return jaxpr
 
   jaxpr_maker.__name__ = ""make_jaxpr({})"".format(jaxpr_maker.__name__)
   return jaxpr_maker
 
+tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
+
 
 device_put = jit(lambda x: x)
 device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
 device_get = partial(tree_map, device_get_array)
 
 
-@lu.transformation_with_aux
-def flatten_fun(in_trees, *args, **kwargs):
-  py_args = map(build_tree, in_trees, args)
-  ans = yield py_args
-  yield process_pytree(pack, ans)
-
-
-def unflatten_fun(fun, io_tree, *py_args):
-  in_trees_expected, out_tree = io_tree
-  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
-  for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
-    if in_tree != expected:
-      raise TypeError(""Expected {}, got {}"".format(expected, in_tree))
-
-  ans = fun(*args)
-  return build_tree(out_tree, ans)
-
-
-tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
-tree_to_jaxtuples = partial(process_pytree, pack)
-
-
 def argnums_partial(f, dyn_argnums, args):
   if isinstance(dyn_argnums, int):
     dyn_argnums = (dyn_argnums,)","diff --git a/jax/api.py b/jax/api.py
index 79eebb01c..f3887176b 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -31,7 +31,9 @@ import numpy as onp
 from . import core
 from . import linear_util as lu
 from .core import pack, eval_jaxpr
-from .api_util import flatten_fun, unflatten_fun, tree_to_jaxtuples
+from .api_util import (pytree_fun_to_jaxtupletree_fun, apply_jaxtree_fun,
+                       pytree_to_jaxtupletree, wraps)
+from .flatten_util import ravel_fun, ravel_pytree
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
                         tree_map)
 from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
@@ -43,15 +45,6 @@ from .interpreters import batching
 
 map = safe_map
 
-def _wraps(wrapped):
-  def decorator(wrapper):
-    wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
-    wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
-    if hasattr(wrapped, ""__doc__""):
-      wrapper.__doc__ = getattr(wrapped, ""__doc__"")
-    return wrapper
-  return decorator
-
 
 def jit(fun, static_argnums=()):
   """"""Sets up `fun` for just-in-time compilation with XLA.
@@ -70,15 +63,15 @@ def jit(fun, static_argnums=()):
   Returns:
     A wrapped version of `fun`, set up for just-in-time compilation.
   """"""
-  @_wraps(fun)
+  @wraps(fun)
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
     f, dyn_args = argnums_partial(f, dyn_argnums, args)
-    args_flat, in_trees = unzip2(map(tree_to_jaxtuples, dyn_args))
+    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
     check_args(args_flat)
-    flat_fun, out_tree = flatten_fun(f, in_trees)
-    out_flat = xla.xla_call(flat_fun, *args_flat)
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = xla.xla_call(jaxtree_fun, *args_flat)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
@@ -93,8 +86,8 @@ def grad(fun, argnums=0):
       `argnums` should be arrays, scalars, or standard Python containers. It
       should return a scalar (which includes arrays with shape `()` but not
       arrays with shape `(1,)` etc.)
-    argnums: Integer or tuple of integers. Specifies which positional
-    argument(s) to differentiate with respect to.
+    argnums: Optional, integer or tuple of integers. Specifies which positional
+      argument(s) to differentiate with respect to (default 0).
 
   Returns:
     A function with the same arguments as `fun`, that evaluates the gradient of
@@ -119,8 +112,8 @@ def value_and_grad(fun, argnums=0):
       `argnums` should be arrays, scalars, or standard Python containers. It
       should return a scalar (which includes arrays with shape `()` but not
       arrays with shape `(1,)` etc.)
-    argnums: Integer or tuple of integers. Specifies which positional
-      argument(s) to differentiate with respect to.
+    argnums: Optional, integer or tuple of integers. Specifies which positional
+      argument(s) to differentiate with respect to (default 0).
 
   Returns:
     A function with the same arguments as `fun` that evaluates both `fun` and
@@ -144,7 +137,8 @@ def value_and_grad(fun, argnums=0):
 @curry
 def jacfwd(fun, x):
   """"""Jacobian of `fun`, evaluated column-by-column using forward-mode AD""""""
-  fun = lu.wrap_init(fun)
+  if not isinstance(fun, lu.WrappedFun):
+    fun = lu.wrap_init(fun)
   pushfwd = partial(jvp, fun, (x,))
   std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
   y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)
@@ -153,7 +147,8 @@ def jacfwd(fun, x):
 @curry
 def jacrev(fun, x):
   """"""Jacobian of `fun`, evaluated row-by-row using reverse-mode AD""""""
-  fun = lu.wrap_init(fun)
+  if not isinstance(fun, lu.WrappedFun):
+    fun = lu.wrap_init(fun)
   y, pullback = vjp(fun, x)
   std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
   jac_flat, = vmap(pullback, out_axes=0)(std_basis)
@@ -162,6 +157,20 @@ def jacrev(fun, x):
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
+
+def general_jacobian(jacfun, fun):
+  def jac_f(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    raveled_input, unravel_inputs = ravel_pytree(args)
+    raveled_fun, unravel_outputs = ravel_fun(f, unravel_inputs)
+    jacmat = jacfun(raveled_fun)(raveled_input)
+    return tree_map(unravel_inputs, vmap(unravel_outputs(), in_axes=1)(jacmat))
+  return jac_f
+jacfwd2 = partial(general_jacobian, jacfwd)
+jacrev2 = partial(general_jacobian, jacrev)
+hessian2 = partial(general_jacobian, hessian)  # TODO(mattjj): doesn't work yet
+
+
 def vmap(fun, in_axes=0, out_axes=0):
   """"""Vectorizing map. Creates a function which maps `fun` over additional axes.
 
@@ -189,32 +198,32 @@ def vmap(fun, in_axes=0, out_axes=0):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun)
     in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
-    in_flat, in_trees = unzip2(map(tree_to_jaxtuples, args))
-    flat_fun, out_tree = flatten_fun(f, in_trees)
-    out_flat = batching.batch(flat_fun, in_flat, in_axes_, out_axes)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = batching.batch(jaxtree_fun, in_flat, in_axes_, out_axes)
     return build_tree(out_tree(), out_flat)
 
   return batched_fun
 
 def jvp(fun, primals, tangents):
-  def flatten_arg(primal, tangent):
-    primal_jtuple, tree_def = tree_to_jaxtuples(primal)
-    tangent_jtuple, tree_def_2 = tree_to_jaxtuples(tangent)
+  def trim_arg(primal, tangent):
+    primal_jtuple, tree_def = pytree_to_jaxtupletree(primal)
+    tangent_jtuple, tree_def_2 = pytree_to_jaxtupletree(tangent)
     assert tree_def == tree_def_2, (tree_def, tree_def_2)
     return primal_jtuple, tangent_jtuple, tree_def
 
   if not isinstance(fun, lu.WrappedFun):
     fun = lu.wrap_init(fun)
-  ps_flat, ts_flat, in_trees = unzip3(map(flatten_arg, primals, tangents))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_primal, out_tangent = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
+  ps_flat, ts_flat, in_trees = unzip3(map(trim_arg, primals, tangents))
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  out_primal, out_tangent = ad.jvp(jaxtree_fun).call_wrapped(ps_flat, ts_flat)
   return (build_tree(out_tree(), out_primal), build_tree(out_tree(), out_tangent))
 
 def linearize(traceable, *primals):
   fun = lu.wrap_init(traceable)
-  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_primal, out_pval, jaxpr, consts = ad.linearize(flat_fun, *primals_flat)
+  primals_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, primals))
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  out_primal, out_pval, jaxpr, consts = ad.linearize(jaxtree_fun, *primals_flat)
   out_tree = out_tree()
   out_primal_py = build_tree(out_tree, out_primal)
   lifted_jvp = partial(lift_linearized, jaxpr, consts, (in_trees, out_tree), out_pval)
@@ -227,37 +236,37 @@ def lift_linearized(jaxpr, consts, io_tree, out_pval, py_args):
     _, ans = eval_jaxpr(jaxpr, consts, (), primals, tangents)
     return pe.merge_pvals(ans, out_pval)
 
-  return unflatten_fun(fun, io_tree, *py_args)
+  return apply_jaxtree_fun(fun, io_tree, *py_args)
 
 def vjp(fun, *primals):
   if not isinstance(fun, lu.WrappedFun):
     fun = lu.wrap_init(fun)
-  primals_flat, in_trees = unzip2(map(tree_to_jaxtuples, primals))
+  primals_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, primals))
   check_args(primals_flat)
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  out_primal, out_vjp = ad.vjp(jaxtree_fun, primals_flat)
   out_tree = out_tree()
   out_primal_py = build_tree(out_tree, out_primal)
   ct_in_trees = [out_tree]
   ct_out_tree = PyTreeDef(node_types[tuple], None, in_trees)
   def out_vjp_packed(cotangent_in):
     return out_vjp(cotangent_in)
-  vjp_py = partial(unflatten_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
+  vjp_py = partial(apply_jaxtree_fun, out_vjp_packed, (ct_in_trees, ct_out_tree))
   return out_primal_py, vjp_py
 
 
 def trace_to_jaxpr(traceable, py_pvals, **kwargs):
   fun = lu.wrap_init(traceable)
   pvals, in_trees = unzip2(map(tree_to_pval_tuples, py_pvals))
-  flat_fun, out_tree = flatten_fun(fun, in_trees)
-  jaxpr, out_pval, consts = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
+  jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
+  jaxpr, out_pval, consts = pe.trace_to_jaxpr(jaxtree_fun, pvals, **kwargs)
   return jaxpr, consts, out_pval, (in_trees, out_tree())
 
 def lift_jaxpr(jaxpr, consts, io_tree, pvals, py_args):
   def fun(*args):
     ans = eval_jaxpr(jaxpr, consts, (), *args)
     return pe.merge_pvals(ans, pvals)
-  return unflatten_fun(fun, io_tree, *py_args)
+  return apply_jaxtree_fun(fun, io_tree, *py_args)
 
 def make_jaxpr(f):
   def pv_like(x):
@@ -265,45 +274,26 @@ def make_jaxpr(f):
     return pe.PartialVal((aval, core.unit))
 
   fun = lu.wrap_init(f)
-  @_wraps(f)
+
+  @wraps(f)
   def jaxpr_maker(*args, **kwargs):
-    jax_args, in_trees = unzip2(map(tree_to_jaxtuples, args))
-    flat_fun, out_tree = flatten_fun(fun, in_trees)
+    jax_args, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(fun, in_trees)
     pvals = map(pv_like, jax_args)
-    jaxpr, _, _ = pe.trace_to_jaxpr(flat_fun, pvals, **kwargs)
+    jaxpr, _, _ = pe.trace_to_jaxpr(jaxtree_fun, pvals, **kwargs)
     return jaxpr
 
   jaxpr_maker.__name__ = ""make_jaxpr({})"".format(jaxpr_maker.__name__)
   return jaxpr_maker
 
+tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
+
 
 device_put = jit(lambda x: x)
 device_get_array = lambda x: x.copy() if type(x) is xla.DeviceArray else x
 device_get = partial(tree_map, device_get_array)
 
 
-@lu.transformation_with_aux
-def flatten_fun(in_trees, *args, **kwargs):
-  py_args = map(build_tree, in_trees, args)
-  ans = yield py_args
-  yield process_pytree(pack, ans)
-
-
-def unflatten_fun(fun, io_tree, *py_args):
-  in_trees_expected, out_tree = io_tree
-  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
-  for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
-    if in_tree != expected:
-      raise TypeError(""Expected {}, got {}"".format(expected, in_tree))
-
-  ans = fun(*args)
-  return build_tree(out_tree, ans)
-
-
-tree_to_pval_tuples = partial(process_pytree, pe.pack_pvals)
-tree_to_jaxtuples = partial(process_pytree, pack)
-
-
 def argnums_partial(f, dyn_argnums, args):
   if isinstance(dyn_argnums, int):
     dyn_argnums = (dyn_argnums,)",No
jax/api_util.py,jax/api_util.py,ad4322c5daea26fb685ec990746a0dc9252f21ee,014e19d90f4817d6cbcfe228434fdd70ac56280b,playing around with flattening functions,"diff --git a/jax/api_util.py b/jax/api_util.py
index 742910697..cedf167f3 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -19,20 +19,29 @@ from __future__ import print_function
 from .core import pack
 from .tree_util import build_tree, process_pytree
 from .linear_util import transformation_with_aux
-from .util import safe_map, unzip2, partial
+from .util import safe_map, unzip2, partial, curry
 
 map = safe_map
 
+
+@curry
+def wraps(wrapped, wrapper):
+  wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
+  wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
+  if hasattr(wrapped, ""__doc__""):
+    wrapper.__doc__ = getattr(wrapped, ""__doc__"")
+  return wrapper
+
+
 @transformation_with_aux
-def flatten_fun(in_trees, *args, **kwargs):
+def pytree_fun_to_jaxtupletree_fun(in_trees, *args, **kwargs):
   py_args = map(build_tree, in_trees, args)
   ans = yield py_args
   yield process_pytree(pack, ans)
 
-
-def unflatten_fun(fun, io_tree, *py_args):
+def apply_jaxtree_fun(fun, io_tree, *py_args):
   in_trees_expected, out_tree = io_tree
-  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
+  args, in_trees = unzip2(map(pytree_to_jaxtupletree, py_args))
   for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
     if in_tree != expected:
       raise TypeError(""Expected {}, got {}"".format(expected, in_tree))
@@ -40,5 +49,4 @@ def unflatten_fun(fun, io_tree, *py_args):
   ans = fun(*args)
   return build_tree(out_tree, ans)
 
-
-tree_to_jaxtuples = partial(process_pytree, pack)
+pytree_to_jaxtupletree = partial(process_pytree, pack)","diff --git a/jax/api_util.py b/jax/api_util.py
index 742910697..cedf167f3 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -19,20 +19,29 @@ from __future__ import print_function
 from .core import pack
 from .tree_util import build_tree, process_pytree
 from .linear_util import transformation_with_aux
-from .util import safe_map, unzip2, partial
+from .util import safe_map, unzip2, partial, curry
 
 map = safe_map
 
+
+@curry
+def wraps(wrapped, wrapper):
+  wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
+  wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
+  if hasattr(wrapped, ""__doc__""):
+    wrapper.__doc__ = getattr(wrapped, ""__doc__"")
+  return wrapper
+
+
 @transformation_with_aux
-def flatten_fun(in_trees, *args, **kwargs):
+def pytree_fun_to_jaxtupletree_fun(in_trees, *args, **kwargs):
   py_args = map(build_tree, in_trees, args)
   ans = yield py_args
   yield process_pytree(pack, ans)
 
-
-def unflatten_fun(fun, io_tree, *py_args):
+def apply_jaxtree_fun(fun, io_tree, *py_args):
   in_trees_expected, out_tree = io_tree
-  args, in_trees = unzip2(map(tree_to_jaxtuples, py_args))
+  args, in_trees = unzip2(map(pytree_to_jaxtupletree, py_args))
   for i, (in_tree, expected) in enumerate(zip(in_trees, in_trees_expected)):
     if in_tree != expected:
       raise TypeError(""Expected {}, got {}"".format(expected, in_tree))
@@ -40,5 +49,4 @@ def unflatten_fun(fun, io_tree, *py_args):
   ans = fun(*args)
   return build_tree(out_tree, ans)
 
-
-tree_to_jaxtuples = partial(process_pytree, pack)
+pytree_to_jaxtupletree = partial(process_pytree, pack)",No
jax/lax.py,jax/lax.py,ad4322c5daea26fb685ec990746a0dc9252f21ee,014e19d90f4817d6cbcfe228434fdd70ac56280b,playing around with flattening functions,"diff --git a/jax/lax.py b/jax/lax.py
index 5a55d5be5..82962acf7 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -32,7 +32,7 @@ from . import linear_util as lu
 from .core import Primitive
 from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                               array_types, make_shaped_array)
-from .api_util import flatten_fun, tree_to_jaxtuples
+from .api_util import pytree_fun_to_jaxtupletree_fun, pytree_to_jaxtupletree
 from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
@@ -389,9 +389,9 @@ def sort_key_val(keys, values, dimension=-1):
   return sorted_keys, sorted_values
 
 def _while_loop(cond_fun, body_fun, init_val):
-  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
-  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
-  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))
+  init_val_flat, in_tree = pytree_to_jaxtupletree(init_val)
+  flat_body_fun, out_tree = pytree_fun_to_jaxtupletree_fun(lu.wrap_init(body_fun), (in_tree,))
+  flat_cond_fun, _ = pytree_fun_to_jaxtupletree_fun(lu.wrap_init(cond_fun), (in_tree,))
 
   pval_flat = _abstractify(init_val_flat)
   cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))","diff --git a/jax/lax.py b/jax/lax.py
index 5a55d5be5..82962acf7 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -32,7 +32,7 @@ from . import linear_util as lu
 from .core import Primitive
 from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                               array_types, make_shaped_array)
-from .api_util import flatten_fun, tree_to_jaxtuples
+from .api_util import pytree_fun_to_jaxtupletree_fun, pytree_to_jaxtupletree
 from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
@@ -389,9 +389,9 @@ def sort_key_val(keys, values, dimension=-1):
   return sorted_keys, sorted_values
 
 def _while_loop(cond_fun, body_fun, init_val):
-  init_val_flat, in_tree = tree_to_jaxtuples(init_val)
-  flat_body_fun, out_tree = flatten_fun(lu.wrap_init(body_fun), (in_tree,))
-  flat_cond_fun, _ = flatten_fun(lu.wrap_init(cond_fun), (in_tree,))
+  init_val_flat, in_tree = pytree_to_jaxtupletree(init_val)
+  flat_body_fun, out_tree = pytree_fun_to_jaxtupletree_fun(lu.wrap_init(body_fun), (in_tree,))
+  flat_cond_fun, _ = pytree_fun_to_jaxtupletree_fun(lu.wrap_init(cond_fun), (in_tree,))
 
   pval_flat = _abstractify(init_val_flat)
   cond_jaxpr, _, cond_consts = pe.trace_to_jaxpr(flat_cond_fun, (pval_flat,))",No
jax/tree_util.py,jax/tree_util.py,ad4322c5daea26fb685ec990746a0dc9252f21ee,014e19d90f4817d6cbcfe228434fdd70ac56280b,playing around with flattening functions,"diff --git a/jax/tree_util.py b/jax/tree_util.py
index ba4e2e9f4..fc059d4e6 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -90,12 +90,15 @@ def build_tree(treedef, xs):
 
 tree_flatten = partial(walk_pytree, concatenate, lambda x: [x])
 
-def tree_unflatten(xs, treedef):
-  xs = iter(xs)
+def tree_unflatten(treedef, xs):
+  # like build_tree, but handles empty containers in the tree
+  return _tree_unflatten(iter(xs), treedef)
+
+def _tree_unflatten(xs, treedef):
   if treedef is leaf:
     return next(xs)
   else:
-    children = map(partial(tree_unflatten, xs), treedef.children)
+    children = map(partial(_tree_unflatten, xs), treedef.children)
     return treedef.node_type.from_iterable(treedef.node_data, children)
 
 ","diff --git a/jax/tree_util.py b/jax/tree_util.py
index ba4e2e9f4..fc059d4e6 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -90,12 +90,15 @@ def build_tree(treedef, xs):
 
 tree_flatten = partial(walk_pytree, concatenate, lambda x: [x])
 
-def tree_unflatten(xs, treedef):
-  xs = iter(xs)
+def tree_unflatten(treedef, xs):
+  # like build_tree, but handles empty containers in the tree
+  return _tree_unflatten(iter(xs), treedef)
+
+def _tree_unflatten(xs, treedef):
   if treedef is leaf:
     return next(xs)
   else:
-    children = map(partial(tree_unflatten, xs), treedef.children)
+    children = map(partial(_tree_unflatten, xs), treedef.children)
     return treedef.node_type.from_iterable(treedef.node_data, children)
 
 ",No
tests/core_test.py,tests/core_test.py,ad4322c5daea26fb685ec990746a0dc9252f21ee,014e19d90f4817d6cbcfe228434fdd70ac56280b,playing around with flattening functions,"diff --git a/tests/core_test.py b/tests/core_test.py
index 3b706034a..074c7b8f2 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -198,10 +198,10 @@ class CoreTest(jtu.JaxTestCase):
     assert flat == [1, 2, 3, 4]
 
   def test_tree_unflatten(self):
-    tree = [(1, 2), {""roy"": (3, [4, 5])}]
+    tree = [(1, 2), {""roy"": (3, [4, 5, ()])}]
     flat, treedef = tree_flatten(tree)
     assert flat == [1, 2, 3, 4, 5]
-    tree2 = tree_unflatten(flat, treedef)
+    tree2 = tree_unflatten(treedef, flat)
     nodes_equal = tree_multimap(operator.eq, tree, tree2)
     assert tree_reduce(operator.and_, nodes_equal)
 ","diff --git a/tests/core_test.py b/tests/core_test.py
index 3b706034a..074c7b8f2 100644
--- a/tests/core_test.py
+++ b/tests/core_test.py
@@ -198,10 +198,10 @@ class CoreTest(jtu.JaxTestCase):
     assert flat == [1, 2, 3, 4]
 
   def test_tree_unflatten(self):
-    tree = [(1, 2), {""roy"": (3, [4, 5])}]
+    tree = [(1, 2), {""roy"": (3, [4, 5, ()])}]
     flat, treedef = tree_flatten(tree)
     assert flat == [1, 2, 3, 4, 5]
-    tree2 = tree_unflatten(flat, treedef)
+    tree2 = tree_unflatten(treedef, flat)
     nodes_equal = tree_multimap(operator.eq, tree, tree2)
     assert tree_reduce(operator.and_, nodes_equal)
 ",No
jax/abstract_arrays.py,jax/abstract_arrays.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index caeb5d0a7..8638d30e2 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -157,9 +157,14 @@ def make_shaped_array(x):
   dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
   return ShapedArray(onp.shape(x), dtype)
 
+def zeros_like_array(x):
+  dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
+  return onp.broadcast_to(onp.array(0, dtype), onp.shape(x))
+
 array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
                onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32,
                complex, float, int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray
+  ad_util.jaxval_zeros_likers[t] = zeros_like_array","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index caeb5d0a7..8638d30e2 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -157,9 +157,14 @@ def make_shaped_array(x):
   dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
   return ShapedArray(onp.shape(x), dtype)
 
+def zeros_like_array(x):
+  dtype = xla_bridge.canonicalize_dtype(onp.result_type(x))
+  return onp.broadcast_to(onp.array(0, dtype), onp.shape(x))
+
 array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
                onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32,
                complex, float, int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray
+  ad_util.jaxval_zeros_likers[t] = zeros_like_array",No
jax/api.py,jax/api.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/api.py b/jax/api.py
index f3887176b..3b0f9d81b 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -25,6 +25,7 @@ from __future__ import division
 from __future__ import print_function
 
 import itertools
+import operator as op
 
 import numpy as onp
 
@@ -33,10 +34,12 @@ from . import linear_util as lu
 from .core import pack, eval_jaxpr
 from .api_util import (pytree_fun_to_jaxtupletree_fun, apply_jaxtree_fun,
                        pytree_to_jaxtupletree, wraps)
-from .flatten_util import ravel_fun, ravel_pytree
-from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
-                        tree_map)
-from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
+from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef,
+                        tree_map, tree_flatten, tree_unflatten, tree_structure,
+                        tree_transpose)
+from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,
+                   WrapHashably, prod)
+from .lib.xla_bridge import canonicalize_dtype
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
@@ -44,6 +47,7 @@ from .interpreters import ad
 from .interpreters import batching
 
 map = safe_map
+zip = safe_zip
 
 
 def jit(fun, static_argnums=()):
@@ -98,6 +102,12 @@ def grad(fun, argnums=0):
   """"""
   value_and_grad_f = value_and_grad(fun, argnums)
 
+  docstr = (""Gradient of {fun} with respect to positional argument(s) ""
+            ""{argnums}. Takes the same arguments as {fun} but returns the ""
+            ""gradient, which has the same shape as the arguments at ""
+            ""positions {argnums}."")
+
+  @wraps(fun, docstr=docstr, argnums=argnums)
   def grad_f(*args, **kwargs):
     ans, g = value_and_grad_f(*args, **kwargs)
     return g
@@ -123,6 +133,14 @@ def value_and_grad(fun, argnums=0):
     integers, the gradient is a tuple of values with the same shapes and types
     as the corresponding arguments.
   """"""
+
+  docstr = (""Value and gradient of {fun} with respect to positional ""
+            ""argument(s) {argnums}. Takes the same arguments as {fun} but ""
+            ""returns a two-element tuple where the first element is the value ""
+            ""of {fun} and the second element is the gradient, which has the ""
+            ""same shape as the arguments at positions {argnums}."")
+
+  @wraps(fun, docstr=docstr, argnums=argnums)
   def value_and_grad_f(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     f_partial, dyn_args = argnums_partial(f, argnums, args)
@@ -134,41 +152,61 @@ def value_and_grad(fun, argnums=0):
 
   return value_and_grad_f
 
-@curry
-def jacfwd(fun, x):
-  """"""Jacobian of `fun`, evaluated column-by-column using forward-mode AD""""""
-  if not isinstance(fun, lu.WrappedFun):
-    fun = lu.wrap_init(fun)
-  pushfwd = partial(jvp, fun, (x,))
-  std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
-  y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)
-  return jac_flat.reshape(onp.shape(y) + onp.shape(x))
-
-@curry
-def jacrev(fun, x):
-  """"""Jacobian of `fun`, evaluated row-by-row using reverse-mode AD""""""
-  if not isinstance(fun, lu.WrappedFun):
-    fun = lu.wrap_init(fun)
-  y, pullback = vjp(fun, x)
-  std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-  jac_flat, = vmap(pullback, out_axes=0)(std_basis)
-  return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+def jacfwd(fun, argnums=0):
+  """"""Jacobian of `fun` evaluated column-by-column using forward-mode AD.""""""
+
+  def jacfun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    f_partial, dyn_args = argnums_partial(f, argnums, args)
+    pushfwd = partial(jvp, f_partial, dyn_args)
+    y, jac = vmap(pushfwd, out_axes=(None, -1))(_std_basis(dyn_args))
+    example_args = dyn_args[0] if isinstance(argnums, int) else dyn_args
+    return tree_map(partial(_unravel_array_into_pytree, example_args, -1), jac)
+
+  return jacfun
+
+def jacrev(fun, argnums=0):
+  """"""Jacobian of `fun` evaluated row-by-row using reverse-mode AD.""""""
+
+  def jacfun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    f_partial, dyn_args = argnums_partial(f, argnums, args)
+    y, pullback = vjp(f_partial, *dyn_args)
+    jac = vmap(pullback)(_std_basis(y))
+    jac = jac[0] if isinstance(argnums, int) else jac
+    example_args = dyn_args[0] if isinstance(argnums, int) else dyn_args
+    jac = tree_map(partial(_unravel_array_into_pytree, y, 0), jac)
+    return tree_transpose(tree_structure(example_args), tree_structure(y), jac)
+
+  return jacfun
 
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
+def _std_basis(pytree):
+  leaves, _ = tree_flatten(pytree)
+  ndim = sum(map(onp.size, leaves))
+  return _unravel_array_into_pytree(pytree, 1, onp.eye(ndim))
+
+def _unravel_array_into_pytree(pytree, axis, arr):
+  leaves, treedef = tree_flatten(pytree)
+  axis = axis % arr.ndim
+  dtypes = map(_dtype, leaves)
+  shapes = [arr.shape[:axis] + onp.shape(l) + arr.shape[axis+1:] for l in leaves]
+  parts = _split(arr, onp.cumsum(map(onp.size, leaves[:-1])), axis)
+  reshaped_parts = [onp.reshape(part.astype(dtype), shape)
+                    for part, dtype, shape in zip(parts, dtypes, shapes)]
+  return tree_unflatten(treedef, reshaped_parts)
+
+def _split(x, indices, axis):
+  if isinstance(x, onp.ndarray):
+    return onp.split(x, indices, axis)
+  else:
+    return x.split(indices, axis)
 
-def general_jacobian(jacfun, fun):
-  def jac_f(*args, **kwargs):
-    f = lu.wrap_init(fun, kwargs)
-    raveled_input, unravel_inputs = ravel_pytree(args)
-    raveled_fun, unravel_outputs = ravel_fun(f, unravel_inputs)
-    jacmat = jacfun(raveled_fun)(raveled_input)
-    return tree_map(unravel_inputs, vmap(unravel_outputs(), in_axes=1)(jacmat))
-  return jac_f
-jacfwd2 = partial(general_jacobian, jacfwd)
-jacrev2 = partial(general_jacobian, jacrev)
-hessian2 = partial(general_jacobian, hessian)  # TODO(mattjj): doesn't work yet
+def _dtype(x):
+  return canonicalize_dtype(onp.result_type(x))
 
 
 def vmap(fun, in_axes=0, out_axes=0):
@@ -194,6 +232,11 @@ def vmap(fun, in_axes=0, out_axes=0):
 
   (`[a,b]` indicates an array with shape (a,b))
   """"""
+
+  docstr = (""Vectorized version of {fun}. Takes similar arguments as {fun} ""
+            ""but with additional array axes over which {fun} is mapped."")
+
+  @wraps(fun, docstr=docstr)
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun)
@@ -301,7 +344,7 @@ def argnums_partial(f, dyn_argnums, args):
     dyn_argnums = tuple(dyn_argnums)
   fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
                       for i, arg in enumerate(args)])
-  dyn_args = [args[i] for i in dyn_argnums]
+  dyn_args = tuple(args[i] for i in dyn_argnums)
   return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args
 
 @lu.transformation","diff --git a/jax/api.py b/jax/api.py
index f3887176b..3b0f9d81b 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -25,6 +25,7 @@ from __future__ import division
 from __future__ import print_function
 
 import itertools
+import operator as op
 
 import numpy as onp
 
@@ -33,10 +34,12 @@ from . import linear_util as lu
 from .core import pack, eval_jaxpr
 from .api_util import (pytree_fun_to_jaxtupletree_fun, apply_jaxtree_fun,
                        pytree_to_jaxtupletree, wraps)
-from .flatten_util import ravel_fun, ravel_pytree
-from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef, leaf,
-                        tree_map)
-from .util import unzip2, unzip3, curry, partial, safe_map, WrapHashably
+from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef,
+                        tree_map, tree_flatten, tree_unflatten, tree_structure,
+                        tree_transpose)
+from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,
+                   WrapHashably, prod)
+from .lib.xla_bridge import canonicalize_dtype
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
@@ -44,6 +47,7 @@ from .interpreters import ad
 from .interpreters import batching
 
 map = safe_map
+zip = safe_zip
 
 
 def jit(fun, static_argnums=()):
@@ -98,6 +102,12 @@ def grad(fun, argnums=0):
   """"""
   value_and_grad_f = value_and_grad(fun, argnums)
 
+  docstr = (""Gradient of {fun} with respect to positional argument(s) ""
+            ""{argnums}. Takes the same arguments as {fun} but returns the ""
+            ""gradient, which has the same shape as the arguments at ""
+            ""positions {argnums}."")
+
+  @wraps(fun, docstr=docstr, argnums=argnums)
   def grad_f(*args, **kwargs):
     ans, g = value_and_grad_f(*args, **kwargs)
     return g
@@ -123,6 +133,14 @@ def value_and_grad(fun, argnums=0):
     integers, the gradient is a tuple of values with the same shapes and types
     as the corresponding arguments.
   """"""
+
+  docstr = (""Value and gradient of {fun} with respect to positional ""
+            ""argument(s) {argnums}. Takes the same arguments as {fun} but ""
+            ""returns a two-element tuple where the first element is the value ""
+            ""of {fun} and the second element is the gradient, which has the ""
+            ""same shape as the arguments at positions {argnums}."")
+
+  @wraps(fun, docstr=docstr, argnums=argnums)
   def value_and_grad_f(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     f_partial, dyn_args = argnums_partial(f, argnums, args)
@@ -134,41 +152,61 @@ def value_and_grad(fun, argnums=0):
 
   return value_and_grad_f
 
-@curry
-def jacfwd(fun, x):
-  """"""Jacobian of `fun`, evaluated column-by-column using forward-mode AD""""""
-  if not isinstance(fun, lu.WrappedFun):
-    fun = lu.wrap_init(fun)
-  pushfwd = partial(jvp, fun, (x,))
-  std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x)),
-  y, jac_flat = vmap(pushfwd, out_axes=(None, -1))(std_basis)
-  return jac_flat.reshape(onp.shape(y) + onp.shape(x))
 
-@curry
-def jacrev(fun, x):
-  """"""Jacobian of `fun`, evaluated row-by-row using reverse-mode AD""""""
-  if not isinstance(fun, lu.WrappedFun):
-    fun = lu.wrap_init(fun)
-  y, pullback = vjp(fun, x)
-  std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
-  jac_flat, = vmap(pullback, out_axes=0)(std_basis)
-  return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+def jacfwd(fun, argnums=0):
+  """"""Jacobian of `fun` evaluated column-by-column using forward-mode AD.""""""
+
+  def jacfun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    f_partial, dyn_args = argnums_partial(f, argnums, args)
+    pushfwd = partial(jvp, f_partial, dyn_args)
+    y, jac = vmap(pushfwd, out_axes=(None, -1))(_std_basis(dyn_args))
+    example_args = dyn_args[0] if isinstance(argnums, int) else dyn_args
+    return tree_map(partial(_unravel_array_into_pytree, example_args, -1), jac)
+
+  return jacfun
+
+def jacrev(fun, argnums=0):
+  """"""Jacobian of `fun` evaluated row-by-row using reverse-mode AD.""""""
+
+  def jacfun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    f_partial, dyn_args = argnums_partial(f, argnums, args)
+    y, pullback = vjp(f_partial, *dyn_args)
+    jac = vmap(pullback)(_std_basis(y))
+    jac = jac[0] if isinstance(argnums, int) else jac
+    example_args = dyn_args[0] if isinstance(argnums, int) else dyn_args
+    jac = tree_map(partial(_unravel_array_into_pytree, y, 0), jac)
+    return tree_transpose(tree_structure(example_args), tree_structure(y), jac)
+
+  return jacfun
 
 def hessian(fun):
   return jacfwd(jacrev(fun))
 
+def _std_basis(pytree):
+  leaves, _ = tree_flatten(pytree)
+  ndim = sum(map(onp.size, leaves))
+  return _unravel_array_into_pytree(pytree, 1, onp.eye(ndim))
 
-def general_jacobian(jacfun, fun):
-  def jac_f(*args, **kwargs):
-    f = lu.wrap_init(fun, kwargs)
-    raveled_input, unravel_inputs = ravel_pytree(args)
-    raveled_fun, unravel_outputs = ravel_fun(f, unravel_inputs)
-    jacmat = jacfun(raveled_fun)(raveled_input)
-    return tree_map(unravel_inputs, vmap(unravel_outputs(), in_axes=1)(jacmat))
-  return jac_f
-jacfwd2 = partial(general_jacobian, jacfwd)
-jacrev2 = partial(general_jacobian, jacrev)
-hessian2 = partial(general_jacobian, hessian)  # TODO(mattjj): doesn't work yet
+def _unravel_array_into_pytree(pytree, axis, arr):
+  leaves, treedef = tree_flatten(pytree)
+  axis = axis % arr.ndim
+  dtypes = map(_dtype, leaves)
+  shapes = [arr.shape[:axis] + onp.shape(l) + arr.shape[axis+1:] for l in leaves]
+  parts = _split(arr, onp.cumsum(map(onp.size, leaves[:-1])), axis)
+  reshaped_parts = [onp.reshape(part.astype(dtype), shape)
+                    for part, dtype, shape in zip(parts, dtypes, shapes)]
+  return tree_unflatten(treedef, reshaped_parts)
+
+def _split(x, indices, axis):
+  if isinstance(x, onp.ndarray):
+    return onp.split(x, indices, axis)
+  else:
+    return x.split(indices, axis)
+
+def _dtype(x):
+  return canonicalize_dtype(onp.result_type(x))
 
 
 def vmap(fun, in_axes=0, out_axes=0):
@@ -194,6 +232,11 @@ def vmap(fun, in_axes=0, out_axes=0):
 
   (`[a,b]` indicates an array with shape (a,b))
   """"""
+
+  docstr = (""Vectorized version of {fun}. Takes similar arguments as {fun} ""
+            ""but with additional array axes over which {fun} is mapped."")
+
+  @wraps(fun, docstr=docstr)
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun)
@@ -301,7 +344,7 @@ def argnums_partial(f, dyn_argnums, args):
     dyn_argnums = tuple(dyn_argnums)
   fixed_args = tuple([None if i in dyn_argnums else WrapHashably(arg)
                       for i, arg in enumerate(args)])
-  dyn_args = [args[i] for i in dyn_argnums]
+  dyn_args = tuple(args[i] for i in dyn_argnums)
   return argnums_partial_(f, dyn_argnums, fixed_args), dyn_args
 
 @lu.transformation",Yes
jax/api_util.py,jax/api_util.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/api_util.py b/jax/api_util.py
index cedf167f3..f2d7dc99d 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -25,12 +25,17 @@ map = safe_map
 
 
 @curry
-def wraps(wrapped, wrapper):
-  wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
-  wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
-  if hasattr(wrapped, ""__doc__""):
-    wrapper.__doc__ = getattr(wrapped, ""__doc__"")
-  return wrapper
+def wraps(wrapped, fun, namestr=""{fun}"", docstr=""{doc}"", **kwargs):
+  try:
+    fun.__name__ = namestr.format(fun=get_name(wrapped))
+    fun.__module__ = get_module(wrapped)
+    fun.__doc__ = docstr.format(fun=get_name(wrapped), doc=get_doc(wrapped), **kwargs)
+  finally:
+    return fun
+
+def get_name(fun): return getattr(fun, ""__name__"", ""<unnamed function>"")
+def get_module(fun): return getattr(fun, ""__module__"", ""<unknown module>"")
+def get_doc(fun): return getattr(fun, ""__doc__"", """")
 
 
 @transformation_with_aux","diff --git a/jax/api_util.py b/jax/api_util.py
index cedf167f3..f2d7dc99d 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -25,12 +25,17 @@ map = safe_map
 
 
 @curry
-def wraps(wrapped, wrapper):
-  wrapper.__name__ = getattr(wrapped, ""__name__"", ""<unnamed function>"")
-  wrapper.__module__ = getattr(wrapped, ""__module__"", ""<unknown module>"")
-  if hasattr(wrapped, ""__doc__""):
-    wrapper.__doc__ = getattr(wrapped, ""__doc__"")
-  return wrapper
+def wraps(wrapped, fun, namestr=""{fun}"", docstr=""{doc}"", **kwargs):
+  try:
+    fun.__name__ = namestr.format(fun=get_name(wrapped))
+    fun.__module__ = get_module(wrapped)
+    fun.__doc__ = docstr.format(fun=get_name(wrapped), doc=get_doc(wrapped), **kwargs)
+  finally:
+    return fun
+
+def get_name(fun): return getattr(fun, ""__name__"", ""<unnamed function>"")
+def get_module(fun): return getattr(fun, ""__module__"", ""<unknown module>"")
+def get_doc(fun): return getattr(fun, ""__doc__"", """")
 
 
 @transformation_with_aux",No
jax/flatten_util.py,jax/flatten_util.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/flatten_util.py b/jax/flatten_util.py
index 7cc52db49..67f3232a0 100644
--- a/jax/flatten_util.py
+++ b/jax/flatten_util.py
@@ -18,17 +18,21 @@ from __future__ import print_function
 
 from .tree_util import tree_flatten, tree_unflatten
 from .linear_util import transformation_with_aux
+from .util import safe_zip
+
+import jax.numpy as np
+from jax.api import vjp
+
+zip = safe_zip
 
 
 def ravel_pytree(pytree):
-  from jax.api import vjp  # TODO(mattjj): fix circular imports
   leaves, treedef = tree_flatten(pytree)
-  flat, unravel_list = vjp(_ravel_list, *leaves)
+  flat, unravel_list = vjp(ravel_list, *leaves)
   unravel_pytree = lambda flat: tree_unflatten(treedef, unravel_list(flat))
   return flat, unravel_pytree
 
-def _ravel_list(*lst):
-  import jax.numpy as np  # TODO(mattjj): fix circular imports
+def ravel_list(*lst):
   return np.concatenate([np.ravel(elt) for elt in lst]) if lst else np.array([])
 
 ","diff --git a/jax/flatten_util.py b/jax/flatten_util.py
index 7cc52db49..67f3232a0 100644
--- a/jax/flatten_util.py
+++ b/jax/flatten_util.py
@@ -18,17 +18,21 @@ from __future__ import print_function
 
 from .tree_util import tree_flatten, tree_unflatten
 from .linear_util import transformation_with_aux
+from .util import safe_zip
+
+import jax.numpy as np
+from jax.api import vjp
+
+zip = safe_zip
 
 
 def ravel_pytree(pytree):
-  from jax.api import vjp  # TODO(mattjj): fix circular imports
   leaves, treedef = tree_flatten(pytree)
-  flat, unravel_list = vjp(_ravel_list, *leaves)
+  flat, unravel_list = vjp(ravel_list, *leaves)
   unravel_pytree = lambda flat: tree_unflatten(treedef, unravel_list(flat))
   return flat, unravel_pytree
 
-def _ravel_list(*lst):
-  import jax.numpy as np  # TODO(mattjj): fix circular imports
+def ravel_list(*lst):
   return np.concatenate([np.ravel(elt) for elt in lst]) if lst else np.array([])
 
 ",No
jax/interpreters/batching.py,jax/interpreters/batching.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index c7f5ffe13..45961cb71 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -295,7 +295,7 @@ def moveaxis(sz, dst, src, x):
       return x
     else:
       if src is None:
-        x = broadcast(x, sz)
+        x = broadcast(x, sz, force_broadcast=True)
         src = 0
       if src == dst:
         return x
@@ -306,21 +306,20 @@ def moveaxis(sz, dst, src, x):
   else:
     raise TypeError(type(aval))
 
-def broadcast(x, sz):
+def broadcast(x, sz, force_broadcast=False):
   aval = get_aval(x)
   if type(aval) is AbstractTuple:
     return pack(map(partial(broadcast, sz=sz), x))
   elif isinstance(aval, ShapedArray):
-    # for scalars, don't actually broadcast
-    if not onp.ndim(x):
+    # for scalars, maybe don't actually broadcast
+    if not onp.ndim(x) and not force_broadcast:
       return x
 
-    # See comment at the top of this section about this try/except.
-    try:
-      return x.broadcast((sz,))
-    except AttributeError:
-      assert not isinstance(x, Tracer)
+    # see comment at the top of this section
+    if isinstance(x, onp.ndarray) or onp.isscalar(x):
       return onp.broadcast_to(x, (sz,) + onp.shape(x))
+    else:
+      return x.broadcast((sz,))  # should be a JAX arraylike
   else:
     raise TypeError(type(x))
 ","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index c7f5ffe13..45961cb71 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -295,7 +295,7 @@ def moveaxis(sz, dst, src, x):
       return x
     else:
       if src is None:
-        x = broadcast(x, sz)
+        x = broadcast(x, sz, force_broadcast=True)
         src = 0
       if src == dst:
         return x
@@ -306,21 +306,20 @@ def moveaxis(sz, dst, src, x):
   else:
     raise TypeError(type(aval))
 
-def broadcast(x, sz):
+def broadcast(x, sz, force_broadcast=False):
   aval = get_aval(x)
   if type(aval) is AbstractTuple:
     return pack(map(partial(broadcast, sz=sz), x))
   elif isinstance(aval, ShapedArray):
-    # for scalars, don't actually broadcast
-    if not onp.ndim(x):
+    # for scalars, maybe don't actually broadcast
+    if not onp.ndim(x) and not force_broadcast:
       return x
 
-    # See comment at the top of this section about this try/except.
-    try:
-      return x.broadcast((sz,))
-    except AttributeError:
-      assert not isinstance(x, Tracer)
+    # see comment at the top of this section
+    if isinstance(x, onp.ndarray) or onp.isscalar(x):
       return onp.broadcast_to(x, (sz,) + onp.shape(x))
+    else:
+      return x.broadcast((sz,))  # should be a JAX arraylike
   else:
     raise TypeError(type(x))
 ",No
jax/interpreters/xla.py,jax/interpreters/xla.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index a0cf754c2..97b6c736d 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -17,9 +17,12 @@ from __future__ import division
 from __future__ import print_function
 
 from collections import namedtuple, defaultdict
+from distutils.util import strtobool
 import itertools as it
-import numpy as onp
 import operator as op
+import os
+
+import numpy as onp
 import six
 from six.moves import xrange
 
@@ -34,7 +37,9 @@ from ..lib import xla_bridge as xb
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 
 FLAGS = flags.FLAGS
-flags.DEFINE_bool('jax_device_values', True, 'Enable device-persistent values.')
+flags.DEFINE_bool('jax_device_values',
+                  strtobool(os.getenv('JAX_DEVICE_VALUES', ""True"")),
+                  'Enable device-persistent values.')
 
 map = safe_map
 ","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index a0cf754c2..97b6c736d 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -17,9 +17,12 @@ from __future__ import division
 from __future__ import print_function
 
 from collections import namedtuple, defaultdict
+from distutils.util import strtobool
 import itertools as it
-import numpy as onp
 import operator as op
+import os
+
+import numpy as onp
 import six
 from six.moves import xrange
 
@@ -34,7 +37,9 @@ from ..lib import xla_bridge as xb
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 
 FLAGS = flags.FLAGS
-flags.DEFINE_bool('jax_device_values', True, 'Enable device-persistent values.')
+flags.DEFINE_bool('jax_device_values',
+                  strtobool(os.getenv('JAX_DEVICE_VALUES', ""True"")),
+                  'Enable device-persistent values.')
 
 map = safe_map
 ",No
jax/lax.py,jax/lax.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/lax.py b/jax/lax.py
index 82962acf7..4a0ab6db3 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -679,7 +679,7 @@ def zeros_like_array(x):
 
 for t in itertools.chain(array_types, [xla.DeviceArray]):
   ad_util.jaxval_adders[t] = add
-  ad_util.jaxval_zeros_likers[t] = zeros_like_array
+ad_util.jaxval_zeros_likers[xla.DeviceArray] = zeros_like_array
 
 batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array
 ","diff --git a/jax/lax.py b/jax/lax.py
index 82962acf7..4a0ab6db3 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -679,7 +679,7 @@ def zeros_like_array(x):
 
 for t in itertools.chain(array_types, [xla.DeviceArray]):
   ad_util.jaxval_adders[t] = add
-  ad_util.jaxval_zeros_likers[t] = zeros_like_array
+ad_util.jaxval_zeros_likers[xla.DeviceArray] = zeros_like_array
 
 batching.pytype_aval_mappings[xla.DeviceArray] = make_shaped_array
 ",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 69dbe2fca..0301408a2 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1724,3 +1724,4 @@ setattr(DeviceArray, ""astype"", lax.convert_element_type)
 
 # Extra methods that are handy
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
+setattr(DeviceArray, ""split"", split)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 69dbe2fca..0301408a2 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1724,3 +1724,4 @@ setattr(DeviceArray, ""astype"", lax.convert_element_type)
 
 # Extra methods that are handy
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
+setattr(DeviceArray, ""split"", split)",No
jax/tree_util.py,jax/tree_util.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/jax/tree_util.py b/jax/tree_util.py
index fc059d4e6..9434d05ba 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -102,8 +102,34 @@ def _tree_unflatten(xs, treedef):
     return treedef.node_type.from_iterable(treedef.node_data, children)
 
 
+def tree_transpose(outer_treedef, inner_treedef, pytree_to_transpose):
+  flat, treedef = tree_flatten(pytree_to_transpose)
+  expected_treedef = _nested_treedef(inner_treedef, outer_treedef)
+  if treedef != expected_treedef:
+    raise TypeError(""Mismatch\n{}\n != \n{}"".format(treedef, expected_treedef))
+
+  inner_size = _num_leaves(inner_treedef)
+  outer_size = _num_leaves(outer_treedef)
+  flat = iter(flat)
+  lol = [[next(flat) for _ in range(inner_size)] for __ in range(outer_size)]
+  transposed_lol = zip(*lol)
+  subtrees = map(partial(tree_unflatten, outer_treedef), transposed_lol)
+  return tree_unflatten(inner_treedef, subtrees)
+
+def _num_leaves(treedef):
+  return 1 if treedef is leaf else sum(map(_num_leaves, treedef.children))
+
+def _nested_treedef(inner, outer):
+  # just used in tree_transpose error checking
+  if outer is leaf:
+    return inner
+  else:
+    children = map(partial(_nested_treedef, inner), outer.children)
+    return PyTreeDef(outer.node_type, outer.node_data, tuple(children))
+
+
 def tree_structure(tree):
-  spec, _ = process_pytree(tree, lambda _: None)
+  _, spec = process_pytree(lambda _: None, tree)
   return spec
 
 
@@ -126,9 +152,12 @@ class PyTreeDef(object):
     return hash((self.node_type, self.node_data, tuple(self.children)))
 
   def __eq__(self, other):
-    return (self.node_type == other.node_type and
-            self.node_data == other.node_data and
-            self.children == other.children)
+    if other is leaf:
+      return False
+    else:
+      return (self.node_type == other.node_type and
+              self.node_data == other.node_data and
+              self.children == other.children)
 
   def __ne__(self, other):
     return not self == other","diff --git a/jax/tree_util.py b/jax/tree_util.py
index fc059d4e6..9434d05ba 100644
--- a/jax/tree_util.py
+++ b/jax/tree_util.py
@@ -102,8 +102,34 @@ def _tree_unflatten(xs, treedef):
     return treedef.node_type.from_iterable(treedef.node_data, children)
 
 
+def tree_transpose(outer_treedef, inner_treedef, pytree_to_transpose):
+  flat, treedef = tree_flatten(pytree_to_transpose)
+  expected_treedef = _nested_treedef(inner_treedef, outer_treedef)
+  if treedef != expected_treedef:
+    raise TypeError(""Mismatch\n{}\n != \n{}"".format(treedef, expected_treedef))
+
+  inner_size = _num_leaves(inner_treedef)
+  outer_size = _num_leaves(outer_treedef)
+  flat = iter(flat)
+  lol = [[next(flat) for _ in range(inner_size)] for __ in range(outer_size)]
+  transposed_lol = zip(*lol)
+  subtrees = map(partial(tree_unflatten, outer_treedef), transposed_lol)
+  return tree_unflatten(inner_treedef, subtrees)
+
+def _num_leaves(treedef):
+  return 1 if treedef is leaf else sum(map(_num_leaves, treedef.children))
+
+def _nested_treedef(inner, outer):
+  # just used in tree_transpose error checking
+  if outer is leaf:
+    return inner
+  else:
+    children = map(partial(_nested_treedef, inner), outer.children)
+    return PyTreeDef(outer.node_type, outer.node_data, tuple(children))
+
+
 def tree_structure(tree):
-  spec, _ = process_pytree(tree, lambda _: None)
+  _, spec = process_pytree(lambda _: None, tree)
   return spec
 
 
@@ -126,9 +152,12 @@ class PyTreeDef(object):
     return hash((self.node_type, self.node_data, tuple(self.children)))
 
   def __eq__(self, other):
-    return (self.node_type == other.node_type and
-            self.node_data == other.node_data and
-            self.children == other.children)
+    if other is leaf:
+      return False
+    else:
+      return (self.node_type == other.node_type and
+              self.node_data == other.node_data and
+              self.children == other.children)
 
   def __ne__(self, other):
     return not self == other",No
tests/api_test.py,tests/api_test.py,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,ad4322c5daea26fb685ec990746a0dc9252f21ee,generalize jacfwd and jacrev to handle pytrees,"diff --git a/tests/api_test.py b/tests/api_test.py
index 786391573..b59496534 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -24,6 +24,7 @@ from jax import test_util as jtu
 
 import jax.numpy as np
 from jax import jit, grad, device_get, device_put, jacfwd, jacrev
+from jax import api
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
 from jax.interpreters.ad import defjvp
@@ -259,6 +260,22 @@ class APITest(jtu.JaxTestCase):
     f = lambda x: np.tanh(np.dot(A, x))
     assert onp.allclose(jacfwd(f)(x), jacrev(f)(x))
 
+  def test_std_basis(self):
+    basis = api._std_basis(np.zeros(3))
+    assert getattr(basis, ""shape"", None) == (3, 3)
+    assert onp.allclose(basis, onp.eye(3))
+
+    basis = api._std_basis(np.zeros((3, 3)))
+    assert getattr(basis, ""shape"", None) == (9, 3, 3)
+    assert onp.allclose(basis, onp.eye(9).reshape(9, 3, 3))
+
+    basis = api._std_basis([0., (np.zeros(3), np.zeros((3, 4)))])
+    assert isinstance(basis, list) and len(basis) == 2
+    assert getattr(basis[0], ""shape"", None) == (16,)
+    assert isinstance(basis[1], tuple) and len(basis[1]) == 2
+    assert getattr(basis[1][0], ""shape"", None) == (16, 3)
+    assert getattr(basis[1][1], ""shape"", None) == (16, 3, 4)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
index 786391573..b59496534 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -24,6 +24,7 @@ from jax import test_util as jtu
 
 import jax.numpy as np
 from jax import jit, grad, device_get, device_put, jacfwd, jacrev
+from jax import api
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
 from jax.interpreters.ad import defjvp
@@ -259,6 +260,22 @@ class APITest(jtu.JaxTestCase):
     f = lambda x: np.tanh(np.dot(A, x))
     assert onp.allclose(jacfwd(f)(x), jacrev(f)(x))
 
+  def test_std_basis(self):
+    basis = api._std_basis(np.zeros(3))
+    assert getattr(basis, ""shape"", None) == (3, 3)
+    assert onp.allclose(basis, onp.eye(3))
+
+    basis = api._std_basis(np.zeros((3, 3)))
+    assert getattr(basis, ""shape"", None) == (9, 3, 3)
+    assert onp.allclose(basis, onp.eye(9).reshape(9, 3, 3))
+
+    basis = api._std_basis([0., (np.zeros(3), np.zeros((3, 4)))])
+    assert isinstance(basis, list) and len(basis) == 2
+    assert getattr(basis[0], ""shape"", None) == (16,)
+    assert isinstance(basis[1], tuple) and len(basis[1]) == 2
+    assert getattr(basis[1][0], ""shape"", None) == (16, 3)
+    assert getattr(basis[1][1], ""shape"", None) == (16, 3, 4)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,81788865458c34f07c536b1733c7ede602c00c1e,0f7c7c4eabc7be7e35f6ed9f86e8c59a71035109,use distutils.util.str2bool on JAX_ENABLE_X64,"diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 363d3ae11..89551ea1d 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -25,6 +25,7 @@ from __future__ import print_function
 
 import os
 import warnings
+from distutils.util import strtobool
 
 from ..config import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
@@ -35,7 +36,7 @@ from jaxlib import xla_client
 
 FLAGS = flags.FLAGS
 flags.DEFINE_bool('jax_enable_x64',
-                  os.getenv('JAX_ENABLE_X64', False),
+                  strtobool(os.getenv('JAX_ENABLE_X64', ""False"")),
                   'Enable 64-bit types to be used.')
 flags.DEFINE_string('jax_dump_hlo_graph', None, 'Regexp of HLO graphs to dump.')
 flags.DEFINE_bool('jax_hlo_profile', False, 'Enables HLO profiling mode.')","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 363d3ae11..89551ea1d 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -25,6 +25,7 @@ from __future__ import print_function
 
 import os
 import warnings
+from distutils.util import strtobool
 
 from ..config import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
@@ -35,7 +36,7 @@ from jaxlib import xla_client
 
 FLAGS = flags.FLAGS
 flags.DEFINE_bool('jax_enable_x64',
-                  os.getenv('JAX_ENABLE_X64', False),
+                  strtobool(os.getenv('JAX_ENABLE_X64', ""False"")),
                   'Enable 64-bit types to be used.')
 flags.DEFINE_string('jax_dump_hlo_graph', None, 'Regexp of HLO graphs to dump.')
 flags.DEFINE_bool('jax_hlo_profile', False, 'Enables HLO profiling mode.')",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,e177205e98ecdff69b96f5542cfbd5f14a65634a,81788865458c34f07c536b1733c7ede602c00c1e,add split and broadcast methods to ShapedArrays,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 0301408a2..56af1b5f0 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1723,5 +1723,7 @@ setattr(DeviceArray, ""astype"", lax.convert_element_type)
 
 
 # Extra methods that are handy
+setattr(ShapedArray, ""broadcast"", core.aval_method(lax.broadcast))
+setattr(ShapedArray, ""split"", core.aval_method(split))
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
 setattr(DeviceArray, ""split"", split)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 0301408a2..56af1b5f0 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1723,5 +1723,7 @@ setattr(DeviceArray, ""astype"", lax.convert_element_type)
 
 
 # Extra methods that are handy
+setattr(ShapedArray, ""broadcast"", core.aval_method(lax.broadcast))
+setattr(ShapedArray, ""split"", core.aval_method(split))
 setattr(DeviceArray, ""broadcast"", lax.broadcast)
 setattr(DeviceArray, ""split"", split)",No
WORKSPACE,WORKSPACE,0f40c744cd57a7d27f3bd49dfcba954be5040ae8,c92ce029ac0983ef89707564de576266caf3d4bd,"Update XLA release to include fix for wrong output bug.

Tensorflow commit https://github.com/tensorflow/tensorflow/commit/38c91321421694432117b077294df43aa31d1193 includes a bugfix to XLA/GPU code generation for transposes.

Fixes #129","diff --git a/WORKSPACE b/WORKSPACE
index 9b21f2d70..bd6fa74e2 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""f10fbc64cc4cd5ada4ebddcdd26f8f76cd97e6aff8e25956172ee18f98a705ea"",
-   strip_prefix = ""tensorflow-787c2a684e39d9c21525c07da3e56bc68621e9b7"",
+   sha256 = ""2434ae2aebbf681534fca88f3ec547f49bab3968fb838b6589c3f1ccbde617ee"",
+   strip_prefix = ""tensorflow-38c91321421694432117b077294df43aa31d1193"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/787c2a684e39d9c21525c07da3e56bc68621e9b7.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/38c91321421694432117b077294df43aa31d1193.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 9b21f2d70..bd6fa74e2 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""f10fbc64cc4cd5ada4ebddcdd26f8f76cd97e6aff8e25956172ee18f98a705ea"",
-   strip_prefix = ""tensorflow-787c2a684e39d9c21525c07da3e56bc68621e9b7"",
+   sha256 = ""2434ae2aebbf681534fca88f3ec547f49bab3968fb838b6589c3f1ccbde617ee"",
+   strip_prefix = ""tensorflow-38c91321421694432117b077294df43aa31d1193"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/787c2a684e39d9c21525c07da3e56bc68621e9b7.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/38c91321421694432117b077294df43aa31d1193.tar.gz"",
    ],
 )
 ",No
build/build.py,build/build.py,5031016465d00ce45591f5957fd32d367baa299c,02c1e77af9100a3212593dd2e096d2508c625352,Disable a number of TF features in build.py that we don't use and don't need to build.,"diff --git a/build/build.py b/build/build.py
index 91664a224..e7328a3e0 100755
--- a/build/build.py
+++ b/build/build.py
@@ -169,6 +169,13 @@ build -c opt
 build:opt --copt=-march=native
 build:opt --host_copt=-march=native
 build:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=1
+
+# Disable enabled-by-default TensorFlow features that we don't care about.
+build --define=no_aws_support=true
+build --define=no_gcp_support=true
+build --define=no_hdfs_support=true
+build --define=no_kafka_support=true
+build --define=no_ignite_support=true
 """"""
 
 ","diff --git a/build/build.py b/build/build.py
index 91664a224..e7328a3e0 100755
--- a/build/build.py
+++ b/build/build.py
@@ -169,6 +169,13 @@ build -c opt
 build:opt --copt=-march=native
 build:opt --host_copt=-march=native
 build:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=1
+
+# Disable enabled-by-default TensorFlow features that we don't care about.
+build --define=no_aws_support=true
+build --define=no_gcp_support=true
+build --define=no_hdfs_support=true
+build --define=no_kafka_support=true
+build --define=no_ignite_support=true
 """"""
 
 ",No
jax/interpreters/batching.py,jax/interpreters/batching.py,e4f56d79110dc1c614871da1ff0067060d8c0d82,5031016465d00ce45591f5957fd32d367baa299c,minor batching bugfixes exposed by general hessian,"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 45961cb71..9fc3deeb4 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -27,7 +27,7 @@ from six.moves import reduce
 from .. import core
 from ..core import Trace, Tracer, new_master, pack, AbstractTuple, JaxTuple
 from ..abstract_arrays import ShapedArray, make_shaped_array, array_types
-from ..ad_util import add_jaxvals_p
+from ..ad_util import add_jaxvals_p, zeros_like_p, zeros_like_jaxval
 from ..linear_util import transformation, transformation_with_aux, wrap_init
 from ..tree_util import register_pytree_node
 from ..util import unzip2, partial, safe_map
@@ -228,6 +228,8 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
     kwargs['input_shape'] = operand.shape
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
+# set up primitive batches for ad_util primitives
+
 def add_batched(batched_args, batch_dims):
   bdx, bdy = batch_dims
   if bdx == bdy:
@@ -238,6 +240,12 @@ def add_batched(batched_args, batch_dims):
     return add_jaxvals_p.bind(xs, ys), 0
 primitive_batchers[add_jaxvals_p] = add_batched
 
+def zeros_like_batched(batched_args, batch_dims):
+  val, = batched_args
+  bdim, = batch_dims
+  return zeros_like_jaxval(val), bdim
+primitive_batchers[zeros_like_p] = zeros_like_batched
+
 
 ### util
 
@@ -287,6 +295,9 @@ def moveaxis(sz, dst, src, x):
       return pack(map(partial(moveaxis, sz), dst, src, x))
     elif type(src) is tuple:
       return pack(map(partial(moveaxis, sz, dst), src, x))
+    elif type(dst) is tuple:
+      srcs = (src,) * len(dst)
+      return pack(map(partial(moveaxis, sz), dst, srcs, x))
     else:
       return pack(map(partial(moveaxis, sz, dst, src), x))
   elif isinstance(aval, ShapedArray):","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 45961cb71..9fc3deeb4 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -27,7 +27,7 @@ from six.moves import reduce
 from .. import core
 from ..core import Trace, Tracer, new_master, pack, AbstractTuple, JaxTuple
 from ..abstract_arrays import ShapedArray, make_shaped_array, array_types
-from ..ad_util import add_jaxvals_p
+from ..ad_util import add_jaxvals_p, zeros_like_p, zeros_like_jaxval
 from ..linear_util import transformation, transformation_with_aux, wrap_init
 from ..tree_util import register_pytree_node
 from ..util import unzip2, partial, safe_map
@@ -228,6 +228,8 @@ def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
     kwargs['input_shape'] = operand.shape
   return prim.bind(operand, axes=axes, **kwargs), bdim_out
 
+# set up primitive batches for ad_util primitives
+
 def add_batched(batched_args, batch_dims):
   bdx, bdy = batch_dims
   if bdx == bdy:
@@ -238,6 +240,12 @@ def add_batched(batched_args, batch_dims):
     return add_jaxvals_p.bind(xs, ys), 0
 primitive_batchers[add_jaxvals_p] = add_batched
 
+def zeros_like_batched(batched_args, batch_dims):
+  val, = batched_args
+  bdim, = batch_dims
+  return zeros_like_jaxval(val), bdim
+primitive_batchers[zeros_like_p] = zeros_like_batched
+
 
 ### util
 
@@ -287,6 +295,9 @@ def moveaxis(sz, dst, src, x):
       return pack(map(partial(moveaxis, sz), dst, src, x))
     elif type(src) is tuple:
       return pack(map(partial(moveaxis, sz, dst), src, x))
+    elif type(dst) is tuple:
+      srcs = (src,) * len(dst)
+      return pack(map(partial(moveaxis, sz), dst, srcs, x))
     else:
       return pack(map(partial(moveaxis, sz, dst, src), x))
   elif isinstance(aval, ShapedArray):",No
jax/test_util.py,jax/test_util.py,ca27f0a2b29bdf5499c4b0ae2a5e812989cbd3e5,e4f56d79110dc1c614871da1ff0067060d8c0d82,add jacobian / hessian pytree tests (fixes #173),"diff --git a/jax/test_util.py b/jax/test_util.py
index 34f633d2a..0aa414f1e 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -427,6 +427,11 @@ class JaxTestCase(parameterized.TestCase):
       self.assertEqual(len(x), len(y))
       for x_elt, y_elt in zip(x, y):
         self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
+    elif isinstance(x, dict):
+      self.assertIsInstance(y, dict)
+      self.assertEqual(set(x.keys()), set(y.keys()))
+      for k in x.keys():
+        self.assertAllClose(x[k], y[k], check_dtypes, atol=atol, rtol=rtol)
     else:
       is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
       self.assertTrue(is_array(x))","diff --git a/jax/test_util.py b/jax/test_util.py
index 34f633d2a..0aa414f1e 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -427,6 +427,11 @@ class JaxTestCase(parameterized.TestCase):
       self.assertEqual(len(x), len(y))
       for x_elt, y_elt in zip(x, y):
         self.assertAllClose(x_elt, y_elt, check_dtypes, atol=atol, rtol=rtol)
+    elif isinstance(x, dict):
+      self.assertIsInstance(y, dict)
+      self.assertEqual(set(x.keys()), set(y.keys()))
+      for k in x.keys():
+        self.assertAllClose(x[k], y[k], check_dtypes, atol=atol, rtol=rtol)
     else:
       is_array = lambda x: hasattr(x, '__array__') or onp.isscalar(x)
       self.assertTrue(is_array(x))",No
tests/api_test.py,tests/api_test.py,ca27f0a2b29bdf5499c4b0ae2a5e812989cbd3e5,e4f56d79110dc1c614871da1ff0067060d8c0d82,add jacobian / hessian pytree tests (fixes #173),"diff --git a/tests/api_test.py b/tests/api_test.py
index b59496534..6f51788e6 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,7 +23,7 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
-from jax import jit, grad, device_get, device_put, jacfwd, jacrev
+from jax import jit, grad, device_get, device_put, jacfwd, jacrev, hessian
 from jax import api
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
@@ -260,6 +260,15 @@ class APITest(jtu.JaxTestCase):
     f = lambda x: np.tanh(np.dot(A, x))
     assert onp.allclose(jacfwd(f)(x), jacrev(f)(x))
 
+  @jtu.skip_on_devices(""tpu"")
+  def test_hessian(self):
+    R = onp.random.RandomState(0).randn
+    A = R(4, 4)
+    x = R(4)
+
+    f = lambda x: np.dot(x, np.dot(A, x))
+    assert onp.allclose(hessian(f)(x), A + A.T)
+
   def test_std_basis(self):
     basis = api._std_basis(np.zeros(3))
     assert getattr(basis, ""shape"", None) == (3, 3)
@@ -276,6 +285,42 @@ class APITest(jtu.JaxTestCase):
     assert getattr(basis[1][0], ""shape"", None) == (16, 3)
     assert getattr(basis[1][1], ""shape"", None) == (16, 3, 4)
 
+  @jtu.skip_on_devices(""tpu"")
+  def test_jacobian_on_pytrees(self):
+    for jacfun in [jacfwd, jacrev]:
+      ans = jacfun(lambda x, y: (x, y))(0., 1.)
+      expected = (1., 0.)
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      ans = jacfun(lambda x, y: (x, y), 1)(0., 1.)
+      expected = (0., 1.)
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      ans = jacfun(lambda x, y: (x, y), (0, 1))(0., 1.)
+      expected = ((1., 0.),
+                  (0., 1.),)
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      ans = jacfun(lambda x: x[:2])((1., 2., 3.))
+      expected = ((1., 0., 0.),
+                  (0., 1., 0.))
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      R = onp.random.RandomState(0).randn
+      x = R(2)
+      y = R(3)
+      ans = jacfun(lambda x, y: {'x': x, 'xy': np.outer(x, y)})(x, y)
+      expected = {'x': onp.eye(2),
+                  'xy': onp.kron(onp.eye(2), y[:, None]).reshape(2, 3, 2)}
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+  @jtu.skip_on_devices(""tpu"")
+  def test_hessian_on_pytrees(self):
+    ans = hessian(lambda x: np.array(x)**2)((1., 2.))
+    expected = ((onp.array([2., 0.]), onp.array([0., 0.])),
+                (onp.array([0., 0.]), onp.array([0., 2.])))
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
index b59496534..6f51788e6 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -23,7 +23,7 @@ from absl.testing import absltest
 from jax import test_util as jtu
 
 import jax.numpy as np
-from jax import jit, grad, device_get, device_put, jacfwd, jacrev
+from jax import jit, grad, device_get, device_put, jacfwd, jacrev, hessian
 from jax import api
 from jax.core import Primitive
 from jax.interpreters.partial_eval import def_abstract_eval
@@ -260,6 +260,15 @@ class APITest(jtu.JaxTestCase):
     f = lambda x: np.tanh(np.dot(A, x))
     assert onp.allclose(jacfwd(f)(x), jacrev(f)(x))
 
+  @jtu.skip_on_devices(""tpu"")
+  def test_hessian(self):
+    R = onp.random.RandomState(0).randn
+    A = R(4, 4)
+    x = R(4)
+
+    f = lambda x: np.dot(x, np.dot(A, x))
+    assert onp.allclose(hessian(f)(x), A + A.T)
+
   def test_std_basis(self):
     basis = api._std_basis(np.zeros(3))
     assert getattr(basis, ""shape"", None) == (3, 3)
@@ -276,6 +285,42 @@ class APITest(jtu.JaxTestCase):
     assert getattr(basis[1][0], ""shape"", None) == (16, 3)
     assert getattr(basis[1][1], ""shape"", None) == (16, 3, 4)
 
+  @jtu.skip_on_devices(""tpu"")
+  def test_jacobian_on_pytrees(self):
+    for jacfun in [jacfwd, jacrev]:
+      ans = jacfun(lambda x, y: (x, y))(0., 1.)
+      expected = (1., 0.)
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      ans = jacfun(lambda x, y: (x, y), 1)(0., 1.)
+      expected = (0., 1.)
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      ans = jacfun(lambda x, y: (x, y), (0, 1))(0., 1.)
+      expected = ((1., 0.),
+                  (0., 1.),)
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      ans = jacfun(lambda x: x[:2])((1., 2., 3.))
+      expected = ((1., 0., 0.),
+                  (0., 1., 0.))
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+      R = onp.random.RandomState(0).randn
+      x = R(2)
+      y = R(3)
+      ans = jacfun(lambda x, y: {'x': x, 'xy': np.outer(x, y)})(x, y)
+      expected = {'x': onp.eye(2),
+                  'xy': onp.kron(onp.eye(2), y[:, None]).reshape(2, 3, 2)}
+      self.assertAllClose(ans, expected, check_dtypes=False)
+
+  @jtu.skip_on_devices(""tpu"")
+  def test_hessian_on_pytrees(self):
+    ans = hessian(lambda x: np.array(x)**2)((1., 2.))
+    expected = ((onp.array([2., 0.]), onp.array([0., 0.])),
+                (onp.array([0., 0.]), onp.array([0., 2.])))
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/api.py,jax/api.py,c3664e95e97045554951c9bccc47517c603520e1,ca27f0a2b29bdf5499c4b0ae2a5e812989cbd3e5,set 'jacobian' to point to jacrev,"diff --git a/jax/api.py b/jax/api.py
index 3b0f9d81b..b27ce1452 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -180,6 +180,7 @@ def jacrev(fun, argnums=0):
     return tree_transpose(tree_structure(example_args), tree_structure(y), jac)
 
   return jacfun
+jacobian = jacrev
 
 def hessian(fun):
   return jacfwd(jacrev(fun))","diff --git a/jax/api.py b/jax/api.py
index 3b0f9d81b..b27ce1452 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -180,6 +180,7 @@ def jacrev(fun, argnums=0):
     return tree_transpose(tree_structure(example_args), tree_structure(y), jac)
 
   return jacfun
+jacobian = jacrev
 
 def hessian(fun):
   return jacfwd(jacrev(fun))",No
examples/advi.py,examples/advi.py,75958898f975088d7f80e96b6cd2ec7c3d9a1347,c3664e95e97045554951c9bccc47517c603520e1,add copyright header to advi.py,"diff --git a/examples/advi.py b/examples/advi.py
index 834c54f08..01983bbd9 100644
--- a/examples/advi.py
+++ b/examples/advi.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 """"""Automatic differentiation variational inference in Numpy and JAX.
 
 This demo fits a Gaussian approximation to an intractable, unnormalized","diff --git a/examples/advi.py b/examples/advi.py
index 834c54f08..01983bbd9 100644
--- a/examples/advi.py
+++ b/examples/advi.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 """"""Automatic differentiation variational inference in Numpy and JAX.
 
 This demo fits a Gaussian approximation to an intractable, unnormalized",No
jax/lax.py,jax/lax.py,df87d5ce43ea07f96b6a573b3c3e6aa3902455cd,5031016465d00ce45591f5957fd32d367baa299c,"make lax.full require concrete shapes

improves error message for #204","diff --git a/jax/lax.py b/jax/lax.py
index 4a0ab6db3..7c838456e 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -418,6 +418,13 @@ def tie_in(x, y):
   return tie_in_p.bind(x, y)
 
 def full(shape, fill_value, dtype):
+  try:
+    shape = tuple(map(int, shape))
+  except TypeError:
+    msg = (""`full` requires shapes to be concrete. If using `jit`, try using ""
+           ""`static_argnums` or applying `jit` to smaller subfunctions instead."")
+    raise TypeError(msg)
+
   if onp.shape(fill_value):
     msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
     raise TypeError(msg.format(onp.shape(fill_value)))
@@ -2532,7 +2539,9 @@ def _check_shapelike(fun_name, arg_name, obj):
 def _dynamic_slice_indices(operand, start_indices):
   if isinstance(start_indices, (tuple, list)):
     start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
-  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))
+  # map int over operand.shape to raise any dynamic-shape errors
+  shape = onp.asarray(map(int, operand.shape), start_indices.dtype)
+  return rem(start_indices, shape)
 
 
 def _const(example, val):","diff --git a/jax/lax.py b/jax/lax.py
index 4a0ab6db3..7c838456e 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -418,6 +418,13 @@ def tie_in(x, y):
   return tie_in_p.bind(x, y)
 
 def full(shape, fill_value, dtype):
+  try:
+    shape = tuple(map(int, shape))
+  except TypeError:
+    msg = (""`full` requires shapes to be concrete. If using `jit`, try using ""
+           ""`static_argnums` or applying `jit` to smaller subfunctions instead."")
+    raise TypeError(msg)
+
   if onp.shape(fill_value):
     msg = ""full must be called with scalar fill_value, got fill_value.shape {}.""
     raise TypeError(msg.format(onp.shape(fill_value)))
@@ -2532,7 +2539,9 @@ def _check_shapelike(fun_name, arg_name, obj):
 def _dynamic_slice_indices(operand, start_indices):
   if isinstance(start_indices, (tuple, list)):
     start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
-  return rem(start_indices, onp.array(operand.shape, start_indices.dtype))
+  # map int over operand.shape to raise any dynamic-shape errors
+  shape = onp.asarray(map(int, operand.shape), start_indices.dtype)
+  return rem(start_indices, shape)
 
 
 def _const(example, val):",No
jax/lax.py,jax/lax.py,47eb8fa17ca2ebc4b91a0e326da2c99e23d82661,df87d5ce43ea07f96b6a573b3c3e6aa3902455cd,add another concreteness check to lax.iota,"diff --git a/jax/lax.py b/jax/lax.py
index 7c838456e..d87c70936 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -440,7 +440,7 @@ def full(shape, fill_value, dtype):
     return broadcast(convert_element_type(fill_value, dtype), shape)
 
 def iota(dtype, size):
-  return broadcasted_iota(dtype, (size,), 0)
+  return broadcasted_iota(dtype, (int(size),), 0)
 
 def broadcasted_iota(dtype, shape, dimension):
   dtype = xla_bridge.canonicalize_dtype(dtype)","diff --git a/jax/lax.py b/jax/lax.py
index 7c838456e..d87c70936 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -440,7 +440,7 @@ def full(shape, fill_value, dtype):
     return broadcast(convert_element_type(fill_value, dtype), shape)
 
 def iota(dtype, size):
-  return broadcasted_iota(dtype, (size,), 0)
+  return broadcasted_iota(dtype, (int(size),), 0)
 
 def broadcasted_iota(dtype, shape, dimension):
   dtype = xla_bridge.canonicalize_dtype(dtype)",No
jax/lax_linalg.py,jax/lax_linalg.py,d3348b9cdd077184685258ed855a48f6269ffcc4,ad6859ebaa3e3ff24a68c7898e61cf67dbadb163,Start work on an implementation of np.linalg.eigh.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index f50933452..e3e4c2c91 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -34,6 +34,8 @@ from jaxlib import lapack
 
 def cholesky(x): return cholesky_p.bind(x)
 
+def eigh(x): return eigh_p.bind(x)
+
 def lu(x): return lu_p.bind(x)
 
 def qr(x, full_matrices=True):
@@ -57,6 +59,7 @@ def _T(x):
 
 _cpu_lapack_types = {np.float32, np.float64, np.complex64}
 
+# Cholesky decomposition
 
 def cholesky_jvp_rule(primals, tangents):
   x, = primals
@@ -90,6 +93,48 @@ def cholesky_cpu_translation_rule(c, operand):
 xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
+# Symmetric/Hermitian eigendecomposition
+
+def eigh_impl(operand):
+  w, v = xla.apply_primitive(eigh_p, operand)
+  return core.pack((w, v))
+
+def eigh_translation_rule(c, operand):
+  raise NotImplementedError(
+    ""Symmetric eigendecomposition is only implemented on the CPU backend"")
+
+def eigh_abstract_eval(operand):
+  if isinstance(operand, ShapedArray):
+    if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:
+      raise ValueError(
+        ""Argument to symmetric eigendecomposition must have shape [..., n, n]"")
+
+    batch_dims = operand.shape[:-2]
+    n = operand.shape[-1]
+    w = ShapedArray(batch_dims + (n,), operand.dtype)
+    v = ShapedArray(batch_dims + (n, n), operand.dtype)
+  else:
+    w, v = operand, operand
+  return core.AbstractTuple((w, v))
+
+def eigh_cpu_translation_rule(c, operand):
+  shape = c.GetShape(operand)
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
+    out = lapack.jax_syevd(c, operand, lower=True)
+    return c.Tuple(c.GetTupleElement(out, 0), c.GetTupleElement(out, 1))
+  else:
+    raise NotImplementedError(
+        ""Only unbatched eigendecomposition is implemented on CPU"")
+
+eigh_p = Primitive('eigh')
+eigh_p.def_impl(eigh_impl)
+eigh_p.def_abstract_eval(eigh_abstract_eval)
+xla.translations[eigh_p] = eigh_translation_rule
+xla.backend_specific_translations['Host'][eigh_p] = eigh_cpu_translation_rule
+
+
+
 triangular_solve_dtype_rule = partial(
     binop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex),
     'triangular_solve')","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index f50933452..e3e4c2c91 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -34,6 +34,8 @@ from jaxlib import lapack
 
 def cholesky(x): return cholesky_p.bind(x)
 
+def eigh(x): return eigh_p.bind(x)
+
 def lu(x): return lu_p.bind(x)
 
 def qr(x, full_matrices=True):
@@ -57,6 +59,7 @@ def _T(x):
 
 _cpu_lapack_types = {np.float32, np.float64, np.complex64}
 
+# Cholesky decomposition
 
 def cholesky_jvp_rule(primals, tangents):
   x, = primals
@@ -90,6 +93,48 @@ def cholesky_cpu_translation_rule(c, operand):
 xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation_rule
 
 
+# Symmetric/Hermitian eigendecomposition
+
+def eigh_impl(operand):
+  w, v = xla.apply_primitive(eigh_p, operand)
+  return core.pack((w, v))
+
+def eigh_translation_rule(c, operand):
+  raise NotImplementedError(
+    ""Symmetric eigendecomposition is only implemented on the CPU backend"")
+
+def eigh_abstract_eval(operand):
+  if isinstance(operand, ShapedArray):
+    if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:
+      raise ValueError(
+        ""Argument to symmetric eigendecomposition must have shape [..., n, n]"")
+
+    batch_dims = operand.shape[:-2]
+    n = operand.shape[-1]
+    w = ShapedArray(batch_dims + (n,), operand.dtype)
+    v = ShapedArray(batch_dims + (n, n), operand.dtype)
+  else:
+    w, v = operand, operand
+  return core.AbstractTuple((w, v))
+
+def eigh_cpu_translation_rule(c, operand):
+  shape = c.GetShape(operand)
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
+    out = lapack.jax_syevd(c, operand, lower=True)
+    return c.Tuple(c.GetTupleElement(out, 0), c.GetTupleElement(out, 1))
+  else:
+    raise NotImplementedError(
+        ""Only unbatched eigendecomposition is implemented on CPU"")
+
+eigh_p = Primitive('eigh')
+eigh_p.def_impl(eigh_impl)
+eigh_p.def_abstract_eval(eigh_abstract_eval)
+xla.translations[eigh_p] = eigh_translation_rule
+xla.backend_specific_translations['Host'][eigh_p] = eigh_cpu_translation_rule
+
+
+
 triangular_solve_dtype_rule = partial(
     binop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex),
     'triangular_solve')",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,d3348b9cdd077184685258ed855a48f6269ffcc4,ad6859ebaa3e3ff24a68c7898e61cf67dbadb163,Start work on an implementation of np.linalg.eigh.,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9f71fbd2b..c870e9959 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,6 +19,7 @@
 
 from __future__ import print_function
 
+from libc.stdlib cimport malloc, free
 from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string
@@ -27,6 +28,8 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
+from scipy.linalg.cython_lapack cimport ssyevd
+
 
 import numpy as np
 from jaxlib import xla_client
@@ -383,3 +386,59 @@ def jax_potrf(c, a, lower=False):
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
       ))
+
+
+# syevd: Symmetric eigendecomposition
+
+cdef void lapack_ssyevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float* a_in = <float*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float* a_out = <float*>(out[0])
+  cdef float* w_out = <float*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(float))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  # Formulas from lapack documentation
+  cdef int lwork = 1 + 6 * n + 2 * n * n
+  cdef int liwork = 3 + 6 * n
+  cdef float* work = <float*> malloc(lwork * sizeof(float))
+  cdef int* iwork = <int*> malloc(liwork * sizeof(int))
+  ssyevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, iwork, &liwork,
+         info_out)
+  free(work)
+  free(iwork)
+
+register_cpu_custom_call_target(b""lapack_syevd"", <void*>(lapack_ssyevd))
+
+def jax_syevd(c, a, lower=False):
+  assert sizeof(int32_t) == sizeof(int)
+
+  a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
+  m, n = a_shape.dimensions()
+  if dtype == np.float32:
+    fn = b""lapack_syevd""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(c.ConstantS32Scalar(1 if lower else 0), c.ConstantS32Scalar(n), a),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+          Shape.array_shape(dtype, (n,), (0,)),
+          Shape.array_shape(np.int32, (), ()),
+      )),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+      ))
+","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9f71fbd2b..c870e9959 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,6 +19,7 @@
 
 from __future__ import print_function
 
+from libc.stdlib cimport malloc, free
 from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string
@@ -27,6 +28,8 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
+from scipy.linalg.cython_lapack cimport ssyevd
+
 
 import numpy as np
 from jaxlib import xla_client
@@ -383,3 +386,59 @@ def jax_potrf(c, a, lower=False):
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
       ))
+
+
+# syevd: Symmetric eigendecomposition
+
+cdef void lapack_ssyevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float* a_in = <float*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float* a_out = <float*>(out[0])
+  cdef float* w_out = <float*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(float))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  # Formulas from lapack documentation
+  cdef int lwork = 1 + 6 * n + 2 * n * n
+  cdef int liwork = 3 + 6 * n
+  cdef float* work = <float*> malloc(lwork * sizeof(float))
+  cdef int* iwork = <int*> malloc(liwork * sizeof(int))
+  ssyevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, iwork, &liwork,
+         info_out)
+  free(work)
+  free(iwork)
+
+register_cpu_custom_call_target(b""lapack_syevd"", <void*>(lapack_ssyevd))
+
+def jax_syevd(c, a, lower=False):
+  assert sizeof(int32_t) == sizeof(int)
+
+  a_shape = c.GetShape(a)
+  dtype = a_shape.element_type()
+  m, n = a_shape.dimensions()
+  if dtype == np.float32:
+    fn = b""lapack_syevd""
+  else:
+    raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
+
+  return c.CustomCall(
+      fn,
+      operands=(c.ConstantS32Scalar(1 if lower else 0), c.ConstantS32Scalar(n), a),
+      shape_with_layout=Shape.tuple_shape((
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+          Shape.array_shape(dtype, (n,), (0,)),
+          Shape.array_shape(np.int32, (), ()),
+      )),
+      operand_shapes_with_layout=(
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(np.int32, (), ()),
+          Shape.array_shape(dtype, (n, n), (0, 1)),
+      ))
+",No
jax/lax_linalg.py,jax/lax_linalg.py,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,d3348b9cdd077184685258ed855a48f6269ffcc4,"Finish implementation of symmetric eigendecomposition on CPU:
* add test case.
* add double and complex64 implementations.

Also add logic to all linalg methods to both coerce arguments to arrays, and to promote to an inexact (float or complex) type if the argument is not inexact.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index e3e4c2c91..5d4ad2075 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -34,7 +34,7 @@ from jaxlib import lapack
 
 def cholesky(x): return cholesky_p.bind(x)
 
-def eigh(x): return eigh_p.bind(x)
+def eigh(x, lower=True): return eigh_p.bind(x, lower=lower)
 
 def lu(x): return lu_p.bind(x)
 
@@ -95,15 +95,15 @@ xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation
 
 # Symmetric/Hermitian eigendecomposition
 
-def eigh_impl(operand):
-  w, v = xla.apply_primitive(eigh_p, operand)
+def eigh_impl(operand, lower):
+  w, v = xla.apply_primitive(eigh_p, operand, lower=lower)
   return core.pack((w, v))
 
-def eigh_translation_rule(c, operand):
+def eigh_translation_rule(c, operand, lower):
   raise NotImplementedError(
     ""Symmetric eigendecomposition is only implemented on the CPU backend"")
 
-def eigh_abstract_eval(operand):
+def eigh_abstract_eval(operand, lower):
   if isinstance(operand, ShapedArray):
     if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:
       raise ValueError(
@@ -117,11 +117,11 @@ def eigh_abstract_eval(operand):
     w, v = operand, operand
   return core.AbstractTuple((w, v))
 
-def eigh_cpu_translation_rule(c, operand):
+def eigh_cpu_translation_rule(c, operand, lower):
   shape = c.GetShape(operand)
   dtype = shape.element_type().type
   if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
-    out = lapack.jax_syevd(c, operand, lower=True)
+    out = lapack.jax_syevd(c, operand, lower=lower)
     return c.Tuple(c.GetTupleElement(out, 0), c.GetTupleElement(out, 1))
   else:
     raise NotImplementedError(","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index e3e4c2c91..5d4ad2075 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -34,7 +34,7 @@ from jaxlib import lapack
 
 def cholesky(x): return cholesky_p.bind(x)
 
-def eigh(x): return eigh_p.bind(x)
+def eigh(x, lower=True): return eigh_p.bind(x, lower=lower)
 
 def lu(x): return lu_p.bind(x)
 
@@ -95,15 +95,15 @@ xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation
 
 # Symmetric/Hermitian eigendecomposition
 
-def eigh_impl(operand):
-  w, v = xla.apply_primitive(eigh_p, operand)
+def eigh_impl(operand, lower):
+  w, v = xla.apply_primitive(eigh_p, operand, lower=lower)
   return core.pack((w, v))
 
-def eigh_translation_rule(c, operand):
+def eigh_translation_rule(c, operand, lower):
   raise NotImplementedError(
     ""Symmetric eigendecomposition is only implemented on the CPU backend"")
 
-def eigh_abstract_eval(operand):
+def eigh_abstract_eval(operand, lower):
   if isinstance(operand, ShapedArray):
     if operand.ndim < 2 or operand.shape[-2] != operand.shape[-1]:
       raise ValueError(
@@ -117,11 +117,11 @@ def eigh_abstract_eval(operand):
     w, v = operand, operand
   return core.AbstractTuple((w, v))
 
-def eigh_cpu_translation_rule(c, operand):
+def eigh_cpu_translation_rule(c, operand, lower):
   shape = c.GetShape(operand)
   dtype = shape.element_type().type
   if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
-    out = lapack.jax_syevd(c, operand, lower=True)
+    out = lapack.jax_syevd(c, operand, lower=lower)
     return c.Tuple(c.GetTupleElement(out, 0), c.GetTupleElement(out, 1))
   else:
     raise NotImplementedError(",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,d3348b9cdd077184685258ed855a48f6269ffcc4,"Finish implementation of symmetric eigendecomposition on CPU:
* add test case.
* add double and complex64 implementations.

Also add logic to all linalg methods to both coerce arguments to arrays, and to promote to an inexact (float or complex) type if the argument is not inexact.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 56af1b5f0..4e81f5c02 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -104,6 +104,11 @@ float64 = onp.float64
 complex64 = onp.complex64
 complex128 = onp.complex128
 
+flexible = onp.flexible
+character = onp.character
+object_ = onp.object_
+number = onp.number
+inexact = onp.inexact
 complexfloating = onp.complexfloating
 floating = onp.floating
 integer = onp.integer","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 56af1b5f0..4e81f5c02 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -104,6 +104,11 @@ float64 = onp.float64
 complex64 = onp.complex64
 complex128 = onp.complex128
 
+flexible = onp.flexible
+character = onp.character
+object_ = onp.object_
+number = onp.number
+inexact = onp.inexact
 complexfloating = onp.complexfloating
 floating = onp.floating
 integer = onp.integer",No
jax/numpy/linalg.py,jax/numpy/linalg.py,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,d3348b9cdd077184685258ed855a48f6269ffcc4,"Finish implementation of symmetric eigendecomposition on CPU:
* add test case.
* add double and complex64 implementations.

Also add logic to all linalg methods to both coerce arguments to arrays, and to promote to an inexact (float or complex) type if the argument is not inexact.","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 6f87e196e..0bcdb41d8 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -26,19 +26,38 @@ from .lax_numpy import _not_implemented
 from .lax_numpy import _wraps
 from . import lax_numpy as np
 from ..util import get_module_functions
+from ..lib import xla_bridge
 
 _EXPERIMENTAL_WARNING = ""numpy.linalg support is experimental and may cause silent failures or wrong outputs""
 
 _T = lambda x: np.swapaxes(x, -1, -2)
 
+
+
+def _promote_arg_dtypes(*args):
+  """"""Promotes `args` to a common inexact type.""""""
+  def _to_inexact_type(type):
+    return type if np.issubdtype(type, np.inexact) else np.float64
+  inexact_types = [_to_inexact_type(np._dtype(arg)) for arg in args]
+  dtype = xla_bridge.canonicalize_dtype(np.result_type(*inexact_types))
+  args = [lax.convert_element_type(arg, dtype) for arg in args]
+  if len(args) == 1:
+    return args[0]
+  else:
+    return args
+
+
+
 @_wraps(onp.linalg.cholesky)
 def cholesky(a):
   warnings.warn(_EXPERIMENTAL_WARNING)
+  a = _promote_arg_dtypes(np.asarray(a))
   return lax_linalg.cholesky(a)
 
 
 @_wraps(onp.linalg.slogdet)
 def slogdet(a):
+  a = _promote_arg_dtypes(np.asarray(a))
   dtype = lax._dtype(a)
   a_shape = np.shape(a)
   if len(a_shape) < 2 or a_shape[-1] != a_shape[-2]:
@@ -68,6 +87,21 @@ def det(a):
   return sign * np.exp(logdet)
 
 
+@_wraps(onp.linalg.eigh)
+def eigh(a, UPLO=None):
+  if UPLO is None or UPLO == ""L"":
+    lower = True
+  elif UPLO == ""U"":
+    lower = False
+  else:
+    msg = ""UPLO must be one of None, 'L', or 'U', got {}"".format(UPLO)
+    raise ValueError(msg)
+
+  a = _promote_arg_dtypes(np.asarray(a))
+  v, w = lax_linalg.eigh(a, lower=lower)
+  return w, v
+
+
 @_wraps(onp.linalg.inv)
 def inv(a):
   warnings.warn(_EXPERIMENTAL_WARNING)
@@ -87,6 +121,7 @@ def qr(a, mode=""reduced""):
     full_matrices = True
   else:
     raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
+  a = _promote_arg_dtypes(np.asarray(a))
   q, r = lax_linalg.qr(a, full_matrices)
   if mode == ""r"":
     return r
@@ -95,6 +130,7 @@ def qr(a, mode=""reduced""):
 
 @_wraps(onp.linalg.solve)
 def solve(a, b):
+  a, b = _promote_arg_dtypes(np.asarray(a), np.asarray(b))
   a_shape = np.shape(a)
   b_shape = np.shape(b)
   a_ndims = len(a_shape)","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 6f87e196e..0bcdb41d8 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -26,19 +26,38 @@ from .lax_numpy import _not_implemented
 from .lax_numpy import _wraps
 from . import lax_numpy as np
 from ..util import get_module_functions
+from ..lib import xla_bridge
 
 _EXPERIMENTAL_WARNING = ""numpy.linalg support is experimental and may cause silent failures or wrong outputs""
 
 _T = lambda x: np.swapaxes(x, -1, -2)
 
+
+
+def _promote_arg_dtypes(*args):
+  """"""Promotes `args` to a common inexact type.""""""
+  def _to_inexact_type(type):
+    return type if np.issubdtype(type, np.inexact) else np.float64
+  inexact_types = [_to_inexact_type(np._dtype(arg)) for arg in args]
+  dtype = xla_bridge.canonicalize_dtype(np.result_type(*inexact_types))
+  args = [lax.convert_element_type(arg, dtype) for arg in args]
+  if len(args) == 1:
+    return args[0]
+  else:
+    return args
+
+
+
 @_wraps(onp.linalg.cholesky)
 def cholesky(a):
   warnings.warn(_EXPERIMENTAL_WARNING)
+  a = _promote_arg_dtypes(np.asarray(a))
   return lax_linalg.cholesky(a)
 
 
 @_wraps(onp.linalg.slogdet)
 def slogdet(a):
+  a = _promote_arg_dtypes(np.asarray(a))
   dtype = lax._dtype(a)
   a_shape = np.shape(a)
   if len(a_shape) < 2 or a_shape[-1] != a_shape[-2]:
@@ -68,6 +87,21 @@ def det(a):
   return sign * np.exp(logdet)
 
 
+@_wraps(onp.linalg.eigh)
+def eigh(a, UPLO=None):
+  if UPLO is None or UPLO == ""L"":
+    lower = True
+  elif UPLO == ""U"":
+    lower = False
+  else:
+    msg = ""UPLO must be one of None, 'L', or 'U', got {}"".format(UPLO)
+    raise ValueError(msg)
+
+  a = _promote_arg_dtypes(np.asarray(a))
+  v, w = lax_linalg.eigh(a, lower=lower)
+  return w, v
+
+
 @_wraps(onp.linalg.inv)
 def inv(a):
   warnings.warn(_EXPERIMENTAL_WARNING)
@@ -87,6 +121,7 @@ def qr(a, mode=""reduced""):
     full_matrices = True
   else:
     raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
+  a = _promote_arg_dtypes(np.asarray(a))
   q, r = lax_linalg.qr(a, full_matrices)
   if mode == ""r"":
     return r
@@ -95,6 +130,7 @@ def qr(a, mode=""reduced""):
 
 @_wraps(onp.linalg.solve)
 def solve(a, b):
+  a, b = _promote_arg_dtypes(np.asarray(a), np.asarray(b))
   a_shape = np.shape(a)
   b_shape = np.shape(b)
   a_ndims = len(a_shape)",No
jax/scipy/linalg.py,jax/scipy/linalg.py,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,d3348b9cdd077184685258ed855a48f6269ffcc4,"Finish implementation of symmetric eigendecomposition on CPU:
* add test case.
* add double and complex64 implementations.

Also add logic to all linalg methods to both coerce arguments to arrays, and to promote to an inexact (float or complex) type if the argument is not inexact.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 088d10751..7d1cf7bce 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -35,6 +35,7 @@ _T = lambda x: np.swapaxes(x, -1, -2)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   l = lax_linalg.cholesky(a if lower else np.conj(a.T))
   return l if lower else np.conj(l.T)
 
@@ -46,6 +47,29 @@ def det(a, overwrite_a=False, check_finite=True):
   return np_linalg.det(a)
 
 
+@_wraps(scipy.linalg.eigh)
+def eigh(a, b=None, lower=True, eigvals_only=False, overwrite_a=False,
+         overwrite_b=False, turbo=True, eigvals=None, type=1,
+         check_finite=True):
+  del overwrite_a, overwrite_b, turbo, check_finite
+  if b is not None:
+    raise NotImplemented(""Only the b=None case of eigh is implemented"")
+  if type != 1:
+    raise NotImplementedError(""Only the type=1 case of eigh is implemented."")
+  if eigvals is not None:
+    raise NotImplementedError(
+        ""Only the eigvals=None case of eigh is implemented."")
+
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
+  v, w = lax_linalg.eigh(a, lower=lower)
+
+  if eigvals_only:
+    return w
+  else:
+    return w, v
+
+
+
 @_wraps(scipy.linalg.inv)
 def inv(a, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
@@ -56,12 +80,14 @@ def inv(a, overwrite_a=False, check_finite=True):
 @_wraps(scipy.linalg.lu_factor)
 def lu_factor(a, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   return lax_linalg.lu(a)
 
 
 @_wraps(scipy.linalg.lu)
 def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   lu, pivots = lax_linalg.lu(a)
   dtype = lax._dtype(a)
   m, n = np.shape(a)
@@ -90,6 +116,7 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
     full_matrices = False
   else:
     raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   q, r = lax_linalg.qr(a, full_matrices)
   if mode == ""r"":
     return r
@@ -102,6 +129,7 @@ def solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False
   if not sym_pos:
     return np_linalg.solve(a, b)
 
+  a, b = np_linalg._promote_arg_dtypes(np.asarray(a), np.asarray(b))
   a_shape = np.shape(a)
   b_shape = np.shape(b)
   a_ndims = len(a_shape)
@@ -141,6 +169,8 @@ def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,
   else:
     raise ValueError(""Invalid 'trans' value {}"".format(trans))
 
+  a, b = np_linalg._promote_arg_dtypes(np.asarray(a), np.asarray(b))
+
   a = np.tril(a) if lower else np.triu(a)
 
   # lax_linalg.triangular_solve only supports matrix 'b's at the moment.","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 088d10751..7d1cf7bce 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -35,6 +35,7 @@ _T = lambda x: np.swapaxes(x, -1, -2)
 def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   l = lax_linalg.cholesky(a if lower else np.conj(a.T))
   return l if lower else np.conj(l.T)
 
@@ -46,6 +47,29 @@ def det(a, overwrite_a=False, check_finite=True):
   return np_linalg.det(a)
 
 
+@_wraps(scipy.linalg.eigh)
+def eigh(a, b=None, lower=True, eigvals_only=False, overwrite_a=False,
+         overwrite_b=False, turbo=True, eigvals=None, type=1,
+         check_finite=True):
+  del overwrite_a, overwrite_b, turbo, check_finite
+  if b is not None:
+    raise NotImplemented(""Only the b=None case of eigh is implemented"")
+  if type != 1:
+    raise NotImplementedError(""Only the type=1 case of eigh is implemented."")
+  if eigvals is not None:
+    raise NotImplementedError(
+        ""Only the eigvals=None case of eigh is implemented."")
+
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
+  v, w = lax_linalg.eigh(a, lower=lower)
+
+  if eigvals_only:
+    return w
+  else:
+    return w, v
+
+
+
 @_wraps(scipy.linalg.inv)
 def inv(a, overwrite_a=False, check_finite=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
@@ -56,12 +80,14 @@ def inv(a, overwrite_a=False, check_finite=True):
 @_wraps(scipy.linalg.lu_factor)
 def lu_factor(a, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   return lax_linalg.lu(a)
 
 
 @_wraps(scipy.linalg.lu)
 def lu(a, permute_l=False, overwrite_a=False, check_finite=True):
   del overwrite_a, check_finite
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   lu, pivots = lax_linalg.lu(a)
   dtype = lax._dtype(a)
   m, n = np.shape(a)
@@ -90,6 +116,7 @@ def qr(a, overwrite_a=False, lwork=None, mode=""full"", pivoting=False,
     full_matrices = False
   else:
     raise ValueError(""Unsupported QR decomposition mode '{}'"".format(mode))
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
   q, r = lax_linalg.qr(a, full_matrices)
   if mode == ""r"":
     return r
@@ -102,6 +129,7 @@ def solve(a, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False
   if not sym_pos:
     return np_linalg.solve(a, b)
 
+  a, b = np_linalg._promote_arg_dtypes(np.asarray(a), np.asarray(b))
   a_shape = np.shape(a)
   b_shape = np.shape(b)
   a_ndims = len(a_shape)
@@ -141,6 +169,8 @@ def solve_triangular(a, b, trans=0, lower=False, unit_diagonal=False,
   else:
     raise ValueError(""Invalid 'trans' value {}"".format(trans))
 
+  a, b = np_linalg._promote_arg_dtypes(np.asarray(a), np.asarray(b))
+
   a = np.tril(a) if lower else np.triu(a)
 
   # lax_linalg.triangular_solve only supports matrix 'b's at the moment.",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,d3348b9cdd077184685258ed855a48f6269ffcc4,"Finish implementation of symmetric eigendecomposition on CPU:
* add test case.
* add double and complex64 implementations.

Also add logic to all linalg methods to both coerce arguments to arrays, and to promote to an inexact (float or complex) type if the argument is not inexact.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index c870e9959..f3152a19f 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -28,7 +28,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport ssyevd
+from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
 
 
 import numpy as np
@@ -390,6 +390,13 @@ def jax_potrf(c, a, lower=False):
 
 # syevd: Symmetric eigendecomposition
 
+# Workspace sizes, taken from the LAPACK documentation.
+cdef int syevd_work_size(int n) nogil:
+  return 1 + 6 * n + 2 * n * n
+
+cdef int syevd_iwork_size(int n) nogil:
+  return 3 + 5 * n
+
 cdef void lapack_ssyevd(void* out_tuple, void** data) nogil:
   cdef int32_t lower = (<int32_t*>(data[0]))[0]
   cdef int n = (<int32_t*>(data[1]))[0]
@@ -399,23 +406,78 @@ cdef void lapack_ssyevd(void* out_tuple, void** data) nogil:
   cdef float* a_out = <float*>(out[0])
   cdef float* w_out = <float*>(out[1])
   cdef int* info_out = <int*>(out[2])
+  cdef float* work = <float*>(out[3])
+  cdef int* iwork = <int*>(out[4])
   if a_out != a_in:
     memcpy(a_out, a_in, n * n * sizeof(float))
 
   cdef char jobz = 'V'
   cdef char uplo = 'L' if lower else 'U'
 
-  # Formulas from lapack documentation
-  cdef int lwork = 1 + 6 * n + 2 * n * n
-  cdef int liwork = 3 + 6 * n
-  cdef float* work = <float*> malloc(lwork * sizeof(float))
-  cdef int* iwork = <int*> malloc(liwork * sizeof(int))
+  cdef int lwork = syevd_work_size(n)
+  cdef int liwork = syevd_iwork_size(n)
   ssyevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, iwork, &liwork,
          info_out)
-  free(work)
-  free(iwork)
 
-register_cpu_custom_call_target(b""lapack_syevd"", <void*>(lapack_ssyevd))
+register_cpu_custom_call_target(b""lapack_ssyevd"", <void*>(lapack_ssyevd))
+
+cdef void lapack_dsyevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double* a_in = <double*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double* a_out = <double*>(out[0])
+  cdef double* w_out = <double*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  cdef double* work = <double*>(out[3])
+  cdef int* iwork = <int*>(out[4])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(double))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef int lwork = syevd_work_size(n)
+  cdef int liwork = syevd_iwork_size(n)
+  dsyevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, iwork, &liwork,
+         info_out)
+
+register_cpu_custom_call_target(b""lapack_dsyevd"", <void*>(lapack_dsyevd))
+
+# Workspace sizes, taken from the LAPACK documentation.
+cdef int heevd_work_size(int n) nogil:
+  return 1 + 2 * n + n * n
+
+cdef int heevd_rwork_size(int n) nogil:
+  return 1 + 5 * n + 2 * n * n
+
+
+cdef void lapack_cheevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float complex* a_in = <float complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef float* w_out = <float*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  cdef float complex* work = <float complex*>(out[3])
+  cdef float* rwork = <float*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(float complex))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef int lwork = heevd_work_size(n)
+  cdef int lrwork = heevd_rwork_size(n)
+  cdef int liwork = syevd_iwork_size(n)
+  cheevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, rwork, &lrwork,
+         iwork, &liwork, info_out)
+
+register_cpu_custom_call_target(b""lapack_cheevd"", <void*>(lapack_cheevd))
 
 def jax_syevd(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
@@ -424,21 +486,39 @@ def jax_syevd(c, a, lower=False):
   dtype = a_shape.element_type()
   m, n = a_shape.dimensions()
   if dtype == np.float32:
-    fn = b""lapack_syevd""
+    fn = b""lapack_ssyevd""
+    eigvals_type = np.float32
+    workspace = (Shape.array_shape(dtype, (syevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
+  elif dtype == np.float64:
+    fn = b""lapack_dsyevd""
+    eigvals_type = np.float64
+    workspace = (Shape.array_shape(dtype, (syevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
+  elif dtype == np.complex64:
+    fn = b""lapack_cheevd""
+    eigvals_type = np.float32
+    workspace = (Shape.array_shape(dtype, (heevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.float32, (heevd_rwork_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
-  return c.CustomCall(
+  out = c.CustomCall(
       fn,
-      operands=(c.ConstantS32Scalar(1 if lower else 0), c.ConstantS32Scalar(n), a),
+      operands=(c.ConstantS32Scalar(1 if lower else 0),
+                c.ConstantS32Scalar(n),
+                a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (n, n), (0, 1)),
-          Shape.array_shape(dtype, (n,), (0,)),
-          Shape.array_shape(np.int32, (), ()),
-      )),
+          Shape.array_shape(eigvals_type, (n,), (0,)),
+          Shape.array_shape(np.int32, (), ())) + workspace
+      ),
       operand_shapes_with_layout=(
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
       ))
+  return c.Tuple(c.GetTupleElement(out, 0), c.GetTupleElement(out, 1),
+                 c.GetTupleElement(out, 2))
 ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index c870e9959..f3152a19f 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -28,7 +28,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport ssyevd
+from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
 
 
 import numpy as np
@@ -390,6 +390,13 @@ def jax_potrf(c, a, lower=False):
 
 # syevd: Symmetric eigendecomposition
 
+# Workspace sizes, taken from the LAPACK documentation.
+cdef int syevd_work_size(int n) nogil:
+  return 1 + 6 * n + 2 * n * n
+
+cdef int syevd_iwork_size(int n) nogil:
+  return 3 + 5 * n
+
 cdef void lapack_ssyevd(void* out_tuple, void** data) nogil:
   cdef int32_t lower = (<int32_t*>(data[0]))[0]
   cdef int n = (<int32_t*>(data[1]))[0]
@@ -399,23 +406,78 @@ cdef void lapack_ssyevd(void* out_tuple, void** data) nogil:
   cdef float* a_out = <float*>(out[0])
   cdef float* w_out = <float*>(out[1])
   cdef int* info_out = <int*>(out[2])
+  cdef float* work = <float*>(out[3])
+  cdef int* iwork = <int*>(out[4])
   if a_out != a_in:
     memcpy(a_out, a_in, n * n * sizeof(float))
 
   cdef char jobz = 'V'
   cdef char uplo = 'L' if lower else 'U'
 
-  # Formulas from lapack documentation
-  cdef int lwork = 1 + 6 * n + 2 * n * n
-  cdef int liwork = 3 + 6 * n
-  cdef float* work = <float*> malloc(lwork * sizeof(float))
-  cdef int* iwork = <int*> malloc(liwork * sizeof(int))
+  cdef int lwork = syevd_work_size(n)
+  cdef int liwork = syevd_iwork_size(n)
   ssyevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, iwork, &liwork,
          info_out)
-  free(work)
-  free(iwork)
 
-register_cpu_custom_call_target(b""lapack_syevd"", <void*>(lapack_ssyevd))
+register_cpu_custom_call_target(b""lapack_ssyevd"", <void*>(lapack_ssyevd))
+
+cdef void lapack_dsyevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double* a_in = <double*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double* a_out = <double*>(out[0])
+  cdef double* w_out = <double*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  cdef double* work = <double*>(out[3])
+  cdef int* iwork = <int*>(out[4])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(double))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef int lwork = syevd_work_size(n)
+  cdef int liwork = syevd_iwork_size(n)
+  dsyevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, iwork, &liwork,
+         info_out)
+
+register_cpu_custom_call_target(b""lapack_dsyevd"", <void*>(lapack_dsyevd))
+
+# Workspace sizes, taken from the LAPACK documentation.
+cdef int heevd_work_size(int n) nogil:
+  return 1 + 2 * n + n * n
+
+cdef int heevd_rwork_size(int n) nogil:
+  return 1 + 5 * n + 2 * n * n
+
+
+cdef void lapack_cheevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const float complex* a_in = <float complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef float* w_out = <float*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  cdef float complex* work = <float complex*>(out[3])
+  cdef float* rwork = <float*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(float complex))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef int lwork = heevd_work_size(n)
+  cdef int lrwork = heevd_rwork_size(n)
+  cdef int liwork = syevd_iwork_size(n)
+  cheevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, rwork, &lrwork,
+         iwork, &liwork, info_out)
+
+register_cpu_custom_call_target(b""lapack_cheevd"", <void*>(lapack_cheevd))
 
 def jax_syevd(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
@@ -424,21 +486,39 @@ def jax_syevd(c, a, lower=False):
   dtype = a_shape.element_type()
   m, n = a_shape.dimensions()
   if dtype == np.float32:
-    fn = b""lapack_syevd""
+    fn = b""lapack_ssyevd""
+    eigvals_type = np.float32
+    workspace = (Shape.array_shape(dtype, (syevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
+  elif dtype == np.float64:
+    fn = b""lapack_dsyevd""
+    eigvals_type = np.float64
+    workspace = (Shape.array_shape(dtype, (syevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
+  elif dtype == np.complex64:
+    fn = b""lapack_cheevd""
+    eigvals_type = np.float32
+    workspace = (Shape.array_shape(dtype, (heevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.float32, (heevd_rwork_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
-  return c.CustomCall(
+  out = c.CustomCall(
       fn,
-      operands=(c.ConstantS32Scalar(1 if lower else 0), c.ConstantS32Scalar(n), a),
+      operands=(c.ConstantS32Scalar(1 if lower else 0),
+                c.ConstantS32Scalar(n),
+                a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (n, n), (0, 1)),
-          Shape.array_shape(dtype, (n,), (0,)),
-          Shape.array_shape(np.int32, (), ()),
-      )),
+          Shape.array_shape(eigvals_type, (n,), (0,)),
+          Shape.array_shape(np.int32, (), ())) + workspace
+      ),
       operand_shapes_with_layout=(
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (n, n), (0, 1)),
       ))
+  return c.Tuple(c.GetTupleElement(out, 0), c.GetTupleElement(out, 1),
+                 c.GetTupleElement(out, 2))
 ",No
tests/linalg_test.py,tests/linalg_test.py,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,d3348b9cdd077184685258ed855a48f6269ffcc4,"Finish implementation of symmetric eigendecomposition on CPU:
* add test case.
* add double and complex64 implementations.

Also add logic to all linalg methods to both coerce arguments to arrays, and to promote to an inexact (float or complex) type if the argument is not inexact.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 4ff55d271..e5425dbbd 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -101,6 +101,40 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.slogdet, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_n={}_lower={}"".format(
+           jtu.format_shape_dtype_string((n,n), dtype), lower),
+       ""n"": n, ""dtype"": dtype, ""lower"": lower, ""rng"": rng}
+      for n in [0, 4, 5, 50]
+      for dtype in float_types() | complex_types()
+      for lower in [False, True]
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an eigendecomposition implementation
+  # for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testEigh(self, n, dtype, lower, rng):
+    if not hasattr(lapack, ""jax_syevd""):
+      self.skipTest(""No symmetric eigendecomposition implementation available"")
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    uplo = ""L"" if lower else ""U""
+
+    # Norm, adjusted for dimension and type.
+    def norm(x):
+      norm = onp.linalg.norm(x, axis=(-2, -1))
+      return norm / ((n + 1) * onp.finfo(dtype).eps)
+
+    a, = args_maker()
+    a = (a + onp.conj(a.T)) / 2
+    w, v = np.linalg.eigh(onp.tril(a) if lower else onp.triu(a), UPLO=uplo)
+
+    self.assertTrue(norm(onp.eye(n) - onp.matmul(onp.conj(T(v)), v)) < 5)
+    self.assertTrue(norm(onp.matmul(a, v) - w * v) < 20)
+
+    self._CompileAndCheck(partial(np.linalg.eigh, UPLO=uplo), args_maker,
+                          check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), full_matrices),","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 4ff55d271..e5425dbbd 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -101,6 +101,40 @@ class NumpyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(np.linalg.slogdet, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_n={}_lower={}"".format(
+           jtu.format_shape_dtype_string((n,n), dtype), lower),
+       ""n"": n, ""dtype"": dtype, ""lower"": lower, ""rng"": rng}
+      for n in [0, 4, 5, 50]
+      for dtype in float_types() | complex_types()
+      for lower in [False, True]
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an eigendecomposition implementation
+  # for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testEigh(self, n, dtype, lower, rng):
+    if not hasattr(lapack, ""jax_syevd""):
+      self.skipTest(""No symmetric eigendecomposition implementation available"")
+    args_maker = lambda: [rng((n, n), dtype)]
+
+    uplo = ""L"" if lower else ""U""
+
+    # Norm, adjusted for dimension and type.
+    def norm(x):
+      norm = onp.linalg.norm(x, axis=(-2, -1))
+      return norm / ((n + 1) * onp.finfo(dtype).eps)
+
+    a, = args_maker()
+    a = (a + onp.conj(a.T)) / 2
+    w, v = np.linalg.eigh(onp.tril(a) if lower else onp.triu(a), UPLO=uplo)
+
+    self.assertTrue(norm(onp.eye(n) - onp.matmul(onp.conj(T(v)), v)) < 5)
+    self.assertTrue(norm(onp.matmul(a, v) - w * v) < 20)
+
+    self._CompileAndCheck(partial(np.linalg.eigh, UPLO=uplo), args_maker,
+                          check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), full_matrices),",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,7a114c8f6a488d51c2236fe0faeed1ee97a68328,1c2cff15d254f0d5e035c64d4bac3a8804cbfaf8,Remove unused malloc/free imports.,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index f3152a19f..5c060f4f3 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,7 +19,6 @@
 
 from __future__ import print_function
 
-from libc.stdlib cimport malloc, free
 from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index f3152a19f..5c060f4f3 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,7 +19,6 @@
 
 from __future__ import print_function
 
-from libc.stdlib cimport malloc, free
 from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string",No
jax/lax_linalg.py,jax/lax_linalg.py,e149ae5d09a22d15c4010ca6abfb03740efb5aa3,7a114c8f6a488d51c2236fe0faeed1ee97a68328,Fix reversed shapes in abstract shape rule for eigh.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 5d4ad2075..cc7f7aaef 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -96,8 +96,8 @@ xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation
 # Symmetric/Hermitian eigendecomposition
 
 def eigh_impl(operand, lower):
-  w, v = xla.apply_primitive(eigh_p, operand, lower=lower)
-  return core.pack((w, v))
+  v, w = xla.apply_primitive(eigh_p, operand, lower=lower)
+  return core.pack((v, w))
 
 def eigh_translation_rule(c, operand, lower):
   raise NotImplementedError(
@@ -111,11 +111,11 @@ def eigh_abstract_eval(operand, lower):
 
     batch_dims = operand.shape[:-2]
     n = operand.shape[-1]
-    w = ShapedArray(batch_dims + (n,), operand.dtype)
     v = ShapedArray(batch_dims + (n, n), operand.dtype)
+    w = ShapedArray(batch_dims + (n,), operand.dtype)
   else:
-    w, v = operand, operand
-  return core.AbstractTuple((w, v))
+    v, w = operand, operand
+  return core.AbstractTuple((v, w))
 
 def eigh_cpu_translation_rule(c, operand, lower):
   shape = c.GetShape(operand)","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 5d4ad2075..cc7f7aaef 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -96,8 +96,8 @@ xla.backend_specific_translations['Host'][cholesky_p] = cholesky_cpu_translation
 # Symmetric/Hermitian eigendecomposition
 
 def eigh_impl(operand, lower):
-  w, v = xla.apply_primitive(eigh_p, operand, lower=lower)
-  return core.pack((w, v))
+  v, w = xla.apply_primitive(eigh_p, operand, lower=lower)
+  return core.pack((v, w))
 
 def eigh_translation_rule(c, operand, lower):
   raise NotImplementedError(
@@ -111,11 +111,11 @@ def eigh_abstract_eval(operand, lower):
 
     batch_dims = operand.shape[:-2]
     n = operand.shape[-1]
-    w = ShapedArray(batch_dims + (n,), operand.dtype)
     v = ShapedArray(batch_dims + (n, n), operand.dtype)
+    w = ShapedArray(batch_dims + (n,), operand.dtype)
   else:
-    w, v = operand, operand
-  return core.AbstractTuple((w, v))
+    v, w = operand, operand
+  return core.AbstractTuple((v, w))
 
 def eigh_cpu_translation_rule(c, operand, lower):
   shape = c.GetShape(operand)",No
jax/core.py,jax/core.py,1e84a3a0fb774955689c72e2b0c1408223d7a680,5031016465d00ce45591f5957fd32d367baa299c,make tuple unpacking cause a full_lower,"diff --git a/jax/core.py b/jax/core.py
index eb26cebc0..ac2faa825 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -449,7 +449,7 @@ class JaxTuple(tuple):
 class AbstractTuple(AbstractValue, tuple):
   @staticmethod
   def _iter(tracer):
-    return tracer.unpack()
+    return map(full_lower, tracer.unpack())
 
   def _len(self, ignored_tracer):
     return len(self)  # tuples have a known length","diff --git a/jax/core.py b/jax/core.py
index eb26cebc0..ac2faa825 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -449,7 +449,7 @@ class JaxTuple(tuple):
 class AbstractTuple(AbstractValue, tuple):
   @staticmethod
   def _iter(tracer):
-    return tracer.unpack()
+    return map(full_lower, tracer.unpack())
 
   def _len(self, ignored_tracer):
     return len(self)  # tuples have a known length",No
jax/interpreters/ad.py,jax/interpreters/ad.py,1e84a3a0fb774955689c72e2b0c1408223d7a680,5031016465d00ce45591f5957fd32d367baa299c,make tuple unpacking cause a full_lower,"diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 072b3a5d2..34f16529e 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -247,10 +247,9 @@ class JVPTracer(Tracer):
 
   def unpack(self):
     if self.tangent is zero:
-      tangent = [zero] * len(self.primal)
+      return self.full_lower()
     else:
-      tangent = self.tangent
-    return map(partial(JVPTracer, self.trace), self.primal, tangent)
+      return map(partial(JVPTracer, self.trace), self.primal, self.tangent)
 
   def full_lower(self):
     if self.tangent is zero:","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 072b3a5d2..34f16529e 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -247,10 +247,9 @@ class JVPTracer(Tracer):
 
   def unpack(self):
     if self.tangent is zero:
-      tangent = [zero] * len(self.primal)
+      return self.full_lower()
     else:
-      tangent = self.tangent
-    return map(partial(JVPTracer, self.trace), self.primal, tangent)
+      return map(partial(JVPTracer, self.trace), self.primal, self.tangent)
 
   def full_lower(self):
     if self.tangent is zero:",No
jax/lax.py,jax/lax.py,280b3fe2fc2b03b1e9d0fb22c4631fee00666f3c,47eb8fa17ca2ebc4b91a0e326da2c99e23d82661,python3 likes list(map(...)),"diff --git a/jax/lax.py b/jax/lax.py
index d87c70936..85c432542 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2540,7 +2540,7 @@ def _dynamic_slice_indices(operand, start_indices):
   if isinstance(start_indices, (tuple, list)):
     start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
   # map int over operand.shape to raise any dynamic-shape errors
-  shape = onp.asarray(map(int, operand.shape), start_indices.dtype)
+  shape = onp.asarray(list(map(int, operand.shape)), start_indices.dtype)
   return rem(start_indices, shape)
 
 ","diff --git a/jax/lax.py b/jax/lax.py
index d87c70936..85c432542 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2540,7 +2540,7 @@ def _dynamic_slice_indices(operand, start_indices):
   if isinstance(start_indices, (tuple, list)):
     start_indices = concatenate([reshape(i, [1]) for i in start_indices], 0)
   # map int over operand.shape to raise any dynamic-shape errors
-  shape = onp.asarray(map(int, operand.shape), start_indices.dtype)
+  shape = onp.asarray(list(map(int, operand.shape)), start_indices.dtype)
   return rem(start_indices, shape)
 
 ",No
tests/linalg_test.py,tests/linalg_test.py,5d15cfbd28b531182affa3370e697e5fd63db970,e149ae5d09a22d15c4010ca6abfb03740efb5aa3,Relax test tolerance for eigh test.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index e5425dbbd..21c00c192 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -129,7 +129,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     w, v = np.linalg.eigh(onp.tril(a) if lower else onp.triu(a), UPLO=uplo)
 
     self.assertTrue(norm(onp.eye(n) - onp.matmul(onp.conj(T(v)), v)) < 5)
-    self.assertTrue(norm(onp.matmul(a, v) - w * v) < 20)
+    self.assertTrue(norm(onp.matmul(a, v) - w * v) < 30)
 
     self._CompileAndCheck(partial(np.linalg.eigh, UPLO=uplo), args_maker,
                           check_dtypes=True)","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index e5425dbbd..21c00c192 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -129,7 +129,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     w, v = np.linalg.eigh(onp.tril(a) if lower else onp.triu(a), UPLO=uplo)
 
     self.assertTrue(norm(onp.eye(n) - onp.matmul(onp.conj(T(v)), v)) < 5)
-    self.assertTrue(norm(onp.matmul(a, v) - w * v) < 20)
+    self.assertTrue(norm(onp.matmul(a, v) - w * v) < 30)
 
     self._CompileAndCheck(partial(np.linalg.eigh, UPLO=uplo), args_maker,
                           check_dtypes=True)",No
jax/lax_linalg.py,jax/lax_linalg.py,954b047bea0b95ad1df178a75bc136c1b45888c6,484db1e15fd188ef977e6c15bed12fa8f611bfe7,Address review comments,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 0e3efe71c..c27530fe0 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -41,7 +41,7 @@ def qr(x, full_matrices=True):
   return q, r
 
 def svd(x, full_matrices=True, compute_uv=True):
-  s, u, v = qr_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
+  s, u, v = svd_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
   if compute_uv:
     return s, u, v
   else:
@@ -286,14 +286,14 @@ def svd_impl(operand, full_matrices, compute_uv):
   s, u, vt = xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
   return core.pack((s, u, vt))
 
-def svd_translation_rule(c, operand):
+def svd_translation_rule(c, operand, full_matrices, compute_uv):
   raise NotImplementedError(
-    ""SVD is only implemented on the CPU backend"")
+    ""Singular value decomposition is only implemented on the CPU backend"")
 
 def svd_abstract_eval(operand, full_matrices, compute_uv):
   if isinstance(operand, ShapedArray):
     if operand.ndim < 2:
-      raise ValueError(""Argument to SVD must have ndims >= 2"")
+      raise ValueError(""Argument to singular value decomposition must have ndims >= 2"")
 
     batch_dims = operand.shape[:-2]
     m = operand.shape[-2]
@@ -307,7 +307,20 @@ def svd_abstract_eval(operand, full_matrices, compute_uv):
     vt = operand
   return core.AbstractTuple((s, u, vt))
 
+def svd_cpu_translation_rule(c, operand, full_matrices, compute_uv):
+  shape = c.GetShape(operand)
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in {np.float32, np.float64}:
+    out = lapack.jax_gesdd(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
+    return c.Tuple(c.GetTupleElement(out, 0),
+                   c.GetTupleElement(out, 1),
+                   c.GetTupleElement(out, 2))
+  else:
+    raise NotImplementedError(
+        ""Only unbatched singular value decomposition for real matrices is implemented on CPU"")
+
 svd_p = Primitive('svd')
 svd_p.def_impl(svd_impl)
 svd_p.def_abstract_eval(svd_abstract_eval)
 xla.translations[svd_p] = svd_translation_rule
+xla.backend_specific_translations['Host'][svd_p] = svd_cpu_translation_rule","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 0e3efe71c..c27530fe0 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -41,7 +41,7 @@ def qr(x, full_matrices=True):
   return q, r
 
 def svd(x, full_matrices=True, compute_uv=True):
-  s, u, v = qr_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
+  s, u, v = svd_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
   if compute_uv:
     return s, u, v
   else:
@@ -286,14 +286,14 @@ def svd_impl(operand, full_matrices, compute_uv):
   s, u, vt = xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
   return core.pack((s, u, vt))
 
-def svd_translation_rule(c, operand):
+def svd_translation_rule(c, operand, full_matrices, compute_uv):
   raise NotImplementedError(
-    ""SVD is only implemented on the CPU backend"")
+    ""Singular value decomposition is only implemented on the CPU backend"")
 
 def svd_abstract_eval(operand, full_matrices, compute_uv):
   if isinstance(operand, ShapedArray):
     if operand.ndim < 2:
-      raise ValueError(""Argument to SVD must have ndims >= 2"")
+      raise ValueError(""Argument to singular value decomposition must have ndims >= 2"")
 
     batch_dims = operand.shape[:-2]
     m = operand.shape[-2]
@@ -307,7 +307,20 @@ def svd_abstract_eval(operand, full_matrices, compute_uv):
     vt = operand
   return core.AbstractTuple((s, u, vt))
 
+def svd_cpu_translation_rule(c, operand, full_matrices, compute_uv):
+  shape = c.GetShape(operand)
+  dtype = shape.element_type().type
+  if len(shape.dimensions()) == 2 and dtype in {np.float32, np.float64}:
+    out = lapack.jax_gesdd(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
+    return c.Tuple(c.GetTupleElement(out, 0),
+                   c.GetTupleElement(out, 1),
+                   c.GetTupleElement(out, 2))
+  else:
+    raise NotImplementedError(
+        ""Only unbatched singular value decomposition for real matrices is implemented on CPU"")
+
 svd_p = Primitive('svd')
 svd_p.def_impl(svd_impl)
 svd_p.def_abstract_eval(svd_abstract_eval)
 xla.translations[svd_p] = svd_translation_rule
+xla.backend_specific_translations['Host'][svd_p] = svd_cpu_translation_rule",No
jax/numpy/linalg.py,jax/numpy/linalg.py,954b047bea0b95ad1df178a75bc136c1b45888c6,484db1e15fd188ef977e6c15bed12fa8f611bfe7,Address review comments,"diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 2b05ce82d..5660a7462 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -40,6 +40,7 @@ def cholesky(a):
 @_wraps(onp.linalg.svd)
 def svd(a, full_matrices=True, compute_uv=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
+  a = _promote_arg_dtypes(np.asarray(a))
   return lax_linalg.svd(a, full_matrices, compute_uv)
 
 ","diff --git a/jax/numpy/linalg.py b/jax/numpy/linalg.py
index 2b05ce82d..5660a7462 100644
--- a/jax/numpy/linalg.py
+++ b/jax/numpy/linalg.py
@@ -40,6 +40,7 @@ def cholesky(a):
 @_wraps(onp.linalg.svd)
 def svd(a, full_matrices=True, compute_uv=True):
   warnings.warn(_EXPERIMENTAL_WARNING)
+  a = _promote_arg_dtypes(np.asarray(a))
   return lax_linalg.svd(a, full_matrices, compute_uv)
 
 ",No
jax/scipy/linalg.py,jax/scipy/linalg.py,954b047bea0b95ad1df178a75bc136c1b45888c6,484db1e15fd188ef977e6c15bed12fa8f611bfe7,Address review comments,"diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 388d030e8..caa448e46 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -43,7 +43,8 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
 def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd'):
   warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite, lapack_driver
-  return lax_linalg.svd(a)
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
+  return lax_linalg.svd(a, full_matrices, compute_uv)
 
 
 @_wraps(scipy.linalg.det)","diff --git a/jax/scipy/linalg.py b/jax/scipy/linalg.py
index 388d030e8..caa448e46 100644
--- a/jax/scipy/linalg.py
+++ b/jax/scipy/linalg.py
@@ -43,7 +43,8 @@ def cholesky(a, lower=False, overwrite_a=False, check_finite=True):
 def svd(a, full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd'):
   warnings.warn(_EXPERIMENTAL_WARNING)
   del overwrite_a, check_finite, lapack_driver
-  return lax_linalg.svd(a)
+  a = np_linalg._promote_arg_dtypes(np.asarray(a))
+  return lax_linalg.svd(a, full_matrices, compute_uv)
 
 
 @_wraps(scipy.linalg.det)",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,954b047bea0b95ad1df178a75bc136c1b45888c6,484db1e15fd188ef977e6c15bed12fa8f611bfe7,Address review comments,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 3b689020d..d9bd59ba7 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,6 +19,7 @@
 
 from __future__ import print_function
 
+from libc.stdlib cimport malloc, free
 from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string
@@ -390,10 +391,11 @@ def jax_potrf(c, a, lower=False):
 # ?gesdd: SVD decomposition
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
-  cdef int32_t job_opt = (<int32_t*>(data[0]))[0]
-  cdef int m = (<int32_t*>(data[1]))[0]
-  cdef int n = (<int32_t*>(data[2]))[0]
-  cdef float* a_in = <float*>(data[3])
+  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef float* a_in = <float*>(data[4])
 
   cdef void** out = <void**>(out_tuple)
   cdef float* a_out = <float*>(out[0])
@@ -490,7 +492,7 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
 
   return c.CustomCall(
       fn,
-      operands=(c.ConstantS32Scalar(int(full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
+      operands=(c.ConstantS32Scalar(int(not full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
                 c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (m, n), (0, 1)),","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 3b689020d..d9bd59ba7 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -19,6 +19,7 @@
 
 from __future__ import print_function
 
+from libc.stdlib cimport malloc, free
 from libc.stdint cimport int32_t
 from libc.string cimport memcpy
 from libcpp.string cimport string
@@ -390,10 +391,11 @@ def jax_potrf(c, a, lower=False):
 # ?gesdd: SVD decomposition
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
-  cdef int32_t job_opt = (<int32_t*>(data[0]))[0]
-  cdef int m = (<int32_t*>(data[1]))[0]
-  cdef int n = (<int32_t*>(data[2]))[0]
-  cdef float* a_in = <float*>(data[3])
+  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef float* a_in = <float*>(data[4])
 
   cdef void** out = <void**>(out_tuple)
   cdef float* a_out = <float*>(out[0])
@@ -490,7 +492,7 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
 
   return c.CustomCall(
       fn,
-      operands=(c.ConstantS32Scalar(int(full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
+      operands=(c.ConstantS32Scalar(int(not full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
                 c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (m, n), (0, 1)),",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,ff6d110975b284417ebbeb9caf6a3b797b8be08c,e16f31e94f4c82d6aa3157fe18747fe102544e15,"Added tests for SVD, and fixed some errors in computation","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index ff3161a40..f217b486a 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -391,7 +391,7 @@ def jax_potrf(c, a, lower=False):
 # ?gesdd: SVD decomposition
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
-  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
   cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
   cdef int m = (<int32_t*>(data[2]))[0]
   cdef int n = (<int32_t*>(data[3]))[0]
@@ -412,30 +412,36 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   if job_opt_compute_uv == 0:
     jobz = 'N'
   else:
-    if job_opt_some == 1:
+    if job_opt_full_matrices == 0:
       jobz = 'S'
 
   cdef int lda = m
   cdef int ldu = m
   cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
 
   cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array, 
+  # because it is officially recommended.
   cdef float wkopt = 0
   cdef int lwork = -1
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
   lwork = <int> wkopt
 
   # Now get the actual SVD
-  cdef work = <float *> malloc(lwork * sizeof(float))
+  cdef float* work = <float *> malloc(lwork * sizeof(float))
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+  free(iwork)
+  free(work)
 
 register_cpu_custom_call_target(b""lapack_sgesdd"", <void*>(lapack_sgesdd))
 
 
 cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
-  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
   cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
   cdef int m = (<int32_t*>(data[2]))[0]
   cdef int n = (<int32_t*>(data[3]))[0]
@@ -456,24 +462,30 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   if job_opt_compute_uv == 0:
     jobz = 'N'
   else:
-    if job_opt_some == 1:
+    if job_opt_full_matrices == 0:
       jobz = 'S'
 
   cdef int lda = m
   cdef int ldu = m
   cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
 
   cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array, 
+  # because it is officially recommended.
   cdef double wkopt = 0
   cdef int lwork = -1
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
   lwork = <int> wkopt
 
   # Now get the actual SVD
-  cdef work = <double *> malloc(lwork * sizeof(double))
+  cdef double* work = <double *> malloc(lwork * sizeof(double))
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+  free(iwork)
+  free(work)
 
 register_cpu_custom_call_target(b""lapack_dgesdd"", <void*>(lapack_dgesdd))
 
@@ -490,9 +502,9 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
-  return c.CustomCall(
+  out = c.CustomCall(
       fn,
-      operands=(c.ConstantS32Scalar(int(not full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
+      operands=(c.ConstantS32Scalar(int(full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
                 c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (m, n), (0, 1)),
@@ -508,6 +520,8 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (m, n), (0, 1)),
       ))
+  return c.Tuple(c.GetTupleElement(out, 1), c.GetTupleElement(out, 2),
+                 c.GetTupleElement(out, 3), c.GetTupleElement(out, 4))
 
 
 # syevd: Symmetric eigendecomposition","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index ff3161a40..f217b486a 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -391,7 +391,7 @@ def jax_potrf(c, a, lower=False):
 # ?gesdd: SVD decomposition
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
-  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
   cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
   cdef int m = (<int32_t*>(data[2]))[0]
   cdef int n = (<int32_t*>(data[3]))[0]
@@ -412,30 +412,36 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   if job_opt_compute_uv == 0:
     jobz = 'N'
   else:
-    if job_opt_some == 1:
+    if job_opt_full_matrices == 0:
       jobz = 'S'
 
   cdef int lda = m
   cdef int ldu = m
   cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
 
   cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array, 
+  # because it is officially recommended.
   cdef float wkopt = 0
   cdef int lwork = -1
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
   lwork = <int> wkopt
 
   # Now get the actual SVD
-  cdef work = <float *> malloc(lwork * sizeof(float))
+  cdef float* work = <float *> malloc(lwork * sizeof(float))
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+  free(iwork)
+  free(work)
 
 register_cpu_custom_call_target(b""lapack_sgesdd"", <void*>(lapack_sgesdd))
 
 
 cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
-  cdef int32_t job_opt_some = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
   cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
   cdef int m = (<int32_t*>(data[2]))[0]
   cdef int n = (<int32_t*>(data[3]))[0]
@@ -456,24 +462,30 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   if job_opt_compute_uv == 0:
     jobz = 'N'
   else:
-    if job_opt_some == 1:
+    if job_opt_full_matrices == 0:
       jobz = 'S'
 
   cdef int lda = m
   cdef int ldu = m
   cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
 
   cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array, 
+  # because it is officially recommended.
   cdef double wkopt = 0
   cdef int lwork = -1
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
   lwork = <int> wkopt
 
   # Now get the actual SVD
-  cdef work = <double *> malloc(lwork * sizeof(double))
+  cdef double* work = <double *> malloc(lwork * sizeof(double))
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
+  free(iwork)
+  free(work)
 
 register_cpu_custom_call_target(b""lapack_dgesdd"", <void*>(lapack_dgesdd))
 
@@ -490,9 +502,9 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
-  return c.CustomCall(
+  out = c.CustomCall(
       fn,
-      operands=(c.ConstantS32Scalar(int(not full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
+      operands=(c.ConstantS32Scalar(int(full_matrices)), c.ConstantS32Scalar(int(compute_uv)),
                 c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (m, n), (0, 1)),
@@ -508,6 +520,8 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(dtype, (m, n), (0, 1)),
       ))
+  return c.Tuple(c.GetTupleElement(out, 1), c.GetTupleElement(out, 2),
+                 c.GetTupleElement(out, 3), c.GetTupleElement(out, 4))
 
 
 # syevd: Symmetric eigendecomposition",No
tests/linalg_test.py,tests/linalg_test.py,ff6d110975b284417ebbeb9caf6a3b797b8be08c,e16f31e94f4c82d6aa3157fe18747fe102544e15,"Added tests for SVD, and fixed some errors in computation","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 21c00c192..7916329b3 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -134,6 +134,51 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     self._CompileAndCheck(partial(np.linalg.eigh, UPLO=uplo), args_maker,
                           check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_n={}_full_matrices={}_compute_uv={}"".format(
+          jtu.format_shape_dtype_string((m, n), dtype), full_matrices, compute_uv),
+       ""m"": m, ""n"": n, ""dtype"": dtype, ""full_matrices"": full_matrices,
+       ""compute_uv"": compute_uv, ""rng"": rng}
+      for m in [2, 7, 29, 53]
+      for n in [2, 7, 29, 53]
+      for dtype in float_types()
+      for full_matrices in [False, True]
+      for compute_uv in [False, True]
+      for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSVD(self, m, n, dtype, full_matrices, compute_uv, rng):
+    if not hasattr(lapack, ""jax_gesdd""):
+      self.skipTest(""No singular value decomposition implementation available"")
+    args_maker = lambda: [rng((m, n), dtype)]
+
+    # Norm, adjusted for dimension and type.
+    def norm(x):
+      n = onp.linalg.norm(x, axis=(-2, -1))
+      return n / (max(m, n) * onp.finfo(dtype).eps)
+
+    a, = args_maker()
+    out = np.linalg.svd(a, full_matrices=full_matrices, compute_uv=compute_uv)
+
+    if compute_uv:
+      # Check the reconstructed matrices
+      if full_matrices:
+        k = min(m, n)
+        if m < n:
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2][:k, :])) < 30))
+        else:
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1][:, :k], out[2])) < 30))
+      else:
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2])) < 30))
+
+      # Check the unitary properties of the singular vector matrices.
+      self.assertTrue(onp.all(norm(onp.eye(out[1].shape[1]) - onp.matmul(T(out[1]), out[1])) < 5))
+      if m >= n:
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(T(out[2]), out[2])) < 5))
+      else:
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], T(out[2]))) < 5))
+
+    self._CompileAndCheck(partial(np.linalg.svd, full_matrices=full_matrices, compute_uv=compute_uv),
+                          args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 21c00c192..7916329b3 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -134,6 +134,51 @@ class NumpyLinalgTest(jtu.JaxTestCase):
     self._CompileAndCheck(partial(np.linalg.eigh, UPLO=uplo), args_maker,
                           check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_n={}_full_matrices={}_compute_uv={}"".format(
+          jtu.format_shape_dtype_string((m, n), dtype), full_matrices, compute_uv),
+       ""m"": m, ""n"": n, ""dtype"": dtype, ""full_matrices"": full_matrices,
+       ""compute_uv"": compute_uv, ""rng"": rng}
+      for m in [2, 7, 29, 53]
+      for n in [2, 7, 29, 53]
+      for dtype in float_types()
+      for full_matrices in [False, True]
+      for compute_uv in [False, True]
+      for rng in [jtu.rand_default()]))
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testSVD(self, m, n, dtype, full_matrices, compute_uv, rng):
+    if not hasattr(lapack, ""jax_gesdd""):
+      self.skipTest(""No singular value decomposition implementation available"")
+    args_maker = lambda: [rng((m, n), dtype)]
+
+    # Norm, adjusted for dimension and type.
+    def norm(x):
+      n = onp.linalg.norm(x, axis=(-2, -1))
+      return n / (max(m, n) * onp.finfo(dtype).eps)
+
+    a, = args_maker()
+    out = np.linalg.svd(a, full_matrices=full_matrices, compute_uv=compute_uv)
+
+    if compute_uv:
+      # Check the reconstructed matrices
+      if full_matrices:
+        k = min(m, n)
+        if m < n:
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2][:k, :])) < 30))
+        else:
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1][:, :k], out[2])) < 30))
+      else:
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2])) < 30))
+
+      # Check the unitary properties of the singular vector matrices.
+      self.assertTrue(onp.all(norm(onp.eye(out[1].shape[1]) - onp.matmul(T(out[1]), out[1])) < 5))
+      if m >= n:
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(T(out[2]), out[2])) < 5))
+      else:
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], T(out[2]))) < 5))
+
+    self._CompileAndCheck(partial(np.linalg.svd, full_matrices=full_matrices, compute_uv=compute_uv),
+                          args_maker, check_dtypes=True)
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_fullmatrices={}"".format(",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,1643b1ecc83b83beb459d6fd323afaa2a8eff8cd,6687734bb839392cc8778095285fb22fafc265a2,Implement `np.square`.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 4e81f5c02..46dbe100e 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -389,6 +389,11 @@ def sqrt(x):
   x, = _promote_to_result_dtype(onp.sqrt, x)
   return power(x, _constant_like(x, 0.5))
 
+@_wraps(onp.square)
+def square(x):
+  x, = _promote_to_result_dtype(onp.square, x)
+  return x * x
+
 
 @_wraps(onp.transpose)
 def transpose(x, axis=None):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 4e81f5c02..46dbe100e 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -389,6 +389,11 @@ def sqrt(x):
   x, = _promote_to_result_dtype(onp.sqrt, x)
   return power(x, _constant_like(x, 0.5))
 
+@_wraps(onp.square)
+def square(x):
+  x, = _promote_to_result_dtype(onp.square, x)
+  return x * x
+
 
 @_wraps(onp.transpose)
 def transpose(x, axis=None):",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,1643b1ecc83b83beb459d6fd323afaa2a8eff8cd,6687734bb839392cc8778095285fb22fafc265a2,Implement `np.square`.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index dda5dc26a..9057abd24 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -123,6 +123,7 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""polyval"", 2, numeric_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
+    op_record(""square"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index dda5dc26a..9057abd24 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -123,6 +123,7 @@ JAX_COMPOUND_OP_RECORDS = [
     op_record(""polyval"", 2, numeric_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
     op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
+    op_record(""square"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
     op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),",No
jax/lax_linalg.py,jax/lax_linalg.py,c509d48b086623038741b993a006b6f232a431cc,ff6d110975b284417ebbeb9caf6a3b797b8be08c,"Address comments

- Fix acronym
- Include test for compute_uv=False
- Remove unnecessary imports","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 7874c7654..fb6015cf0 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -45,7 +45,7 @@ def qr(x, full_matrices=True):
 def svd(x, full_matrices=True, compute_uv=True):
   s, u, v = svd_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
   if compute_uv:
-    return s, u, v
+    return u, s, v
   else:
     return s
 
@@ -325,7 +325,7 @@ xla.translations[qr_p] = qr_translation_rule
 ad.primitive_jvps[qr_p] = qr_jvp_rule
 
 
-# SVD decomposition
+# Singular value decomposition
 
 def svd_impl(operand, full_matrices, compute_uv):
   s, u, vt = xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 7874c7654..fb6015cf0 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -45,7 +45,7 @@ def qr(x, full_matrices=True):
 def svd(x, full_matrices=True, compute_uv=True):
   s, u, v = svd_p.bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
   if compute_uv:
-    return s, u, v
+    return u, s, v
   else:
     return s
 
@@ -325,7 +325,7 @@ xla.translations[qr_p] = qr_translation_rule
 ad.primitive_jvps[qr_p] = qr_jvp_rule
 
 
-# SVD decomposition
+# Singular value decomposition
 
 def svd_impl(operand, full_matrices, compute_uv):
   s, u, vt = xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,c509d48b086623038741b993a006b6f232a431cc,ff6d110975b284417ebbeb9caf6a3b797b8be08c,"Address comments

- Fix acronym
- Include test for compute_uv=False
- Remove unnecessary imports","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index f217b486a..df1c2c320 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -28,7 +28,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd
 from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
 
 import numpy as np
@@ -388,7 +388,7 @@ def jax_potrf(c, a, lower=False):
       ))
 
 
-# ?gesdd: SVD decomposition
+# ?gesdd: Singular value decomposition
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index f217b486a..df1c2c320 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -28,7 +28,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd
 from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
 
 import numpy as np
@@ -388,7 +388,7 @@ def jax_potrf(c, a, lower=False):
       ))
 
 
-# ?gesdd: SVD decomposition
+# ?gesdd: Singular value decomposition
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]",No
tests/linalg_test.py,tests/linalg_test.py,c509d48b086623038741b993a006b6f232a431cc,ff6d110975b284417ebbeb9caf6a3b797b8be08c,"Address comments

- Fix acronym
- Include test for compute_uv=False
- Remove unnecessary imports","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 7916329b3..79eec179b 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -164,19 +164,22 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2][:k, :])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 30))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1][:, :k], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 30))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 30))
 
       # Check the unitary properties of the singular vector matrices.
-      self.assertTrue(onp.all(norm(onp.eye(out[1].shape[1]) - onp.matmul(T(out[1]), out[1])) < 5))
+      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(T(out[0]), out[0])) < 5))
       if m >= n:
         self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(T(out[2]), out[2])) < 5))
       else:
         self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], T(out[2]))) < 5))
 
+    else:
+      self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))
+
     self._CompileAndCheck(partial(np.linalg.svd, full_matrices=full_matrices, compute_uv=compute_uv),
                           args_maker, check_dtypes=True)
 ","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 7916329b3..79eec179b 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -164,19 +164,22 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2][:k, :])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 30))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1][:, :k], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 30))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[0] * out[1], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 30))
 
       # Check the unitary properties of the singular vector matrices.
-      self.assertTrue(onp.all(norm(onp.eye(out[1].shape[1]) - onp.matmul(T(out[1]), out[1])) < 5))
+      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(T(out[0]), out[0])) < 5))
       if m >= n:
         self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(T(out[2]), out[2])) < 5))
       else:
         self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], T(out[2]))) < 5))
 
+    else:
+      self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))
+
     self._CompileAndCheck(partial(np.linalg.svd, full_matrices=full_matrices, compute_uv=compute_uv),
                           args_maker, check_dtypes=True)
 ",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,170e5ab0a383f3ac1deadd67626625ce9046152f,c509d48b086623038741b993a006b6f232a431cc,Minor nits,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index df1c2c320..892d0e403 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -421,7 +421,7 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
@@ -471,7 +471,7 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index df1c2c320..892d0e403 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -421,7 +421,7 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
@@ -471,7 +471,7 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(min(m, n) * sizeof(int))
+  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, ",No
tests/linalg_test.py,tests/linalg_test.py,170e5ab0a383f3ac1deadd67626625ce9046152f,c509d48b086623038741b993a006b6f232a431cc,Minor nits,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 79eec179b..67d400b8a 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -164,11 +164,11 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 50))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 50))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
 
       # Check the unitary properties of the singular vector matrices.
       self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(T(out[0]), out[0])) < 5))","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 79eec179b..67d400b8a 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -164,11 +164,11 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 50))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 50))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 30))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
 
       # Check the unitary properties of the singular vector matrices.
       self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(T(out[0]), out[0])) < 5))",No
WORKSPACE,WORKSPACE,44d952f74cfec9c691fff05d84b5a007d5eb033b,04bfd30fd107859641894a69307d72fd66310046,"Update XLA release to include XLA Gather and Scatter Python bindings..

https://github.com/tensorflow/tensorflow/commit/0ce305b6ce8560945be3a92eae1bcddd60af5e10","diff --git a/WORKSPACE b/WORKSPACE
index bd6fa74e2..5f19906d2 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""2434ae2aebbf681534fca88f3ec547f49bab3968fb838b6589c3f1ccbde617ee"",
-   strip_prefix = ""tensorflow-38c91321421694432117b077294df43aa31d1193"",
+   sha256 = ""20f26f90359f7dad7472ebcb3142c5b6166e057ed89442614f9077c9b8d56278"",
+   strip_prefix = ""tensorflow-0ce305b6ce8560945be3a92eae1bcddd60af5e10"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/38c91321421694432117b077294df43aa31d1193.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/0ce305b6ce8560945be3a92eae1bcddd60af5e10.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index bd6fa74e2..5f19906d2 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""2434ae2aebbf681534fca88f3ec547f49bab3968fb838b6589c3f1ccbde617ee"",
-   strip_prefix = ""tensorflow-38c91321421694432117b077294df43aa31d1193"",
+   sha256 = ""20f26f90359f7dad7472ebcb3142c5b6166e057ed89442614f9077c9b8d56278"",
+   strip_prefix = ""tensorflow-0ce305b6ce8560945be3a92eae1bcddd60af5e10"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/38c91321421694432117b077294df43aa31d1193.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/0ce305b6ce8560945be3a92eae1bcddd60af5e10.tar.gz"",
    ],
 )
 ",No
jax/lax.py,jax/lax.py,5e48420f12e7b8d7aef388862f618597ae38a02c,a5c737a08571026c563f647453e0766edc8ac184,First attempt at lax.gather and lax.scatter.,"diff --git a/jax/lax.py b/jax/lax.py
index 85c432542..b272543a2 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -203,6 +203,19 @@ def dynamic_update_slice(operand, update, start_indices):
   return dynamic_update_slice_p.bind(operand, update, start_indices,
                                      update_shape=update.shape)
 
+def gather(operand, start_indices, dimension_numbers=None, slice_sizes=None):
+  return gather_p.bind(
+      operand, start_indices, dimension_numbers=dimension_numbers,
+      slice_sizes=tuple(slice_sizes), operand_shape=operand.shape)
+
+def scatter(operand, scatter_indices, updates, update_computation,
+            dimension_numbers=None):
+  jaxpr, consts = _reduction_jaxpr(update_computation, _const(operand, 0))
+  return scatter_p.bind(
+      operand, scatter_indices, updates, update_jaxpr=jaxpr,
+      update_consts=consts, dimension_numbers=dimension_numbers)
+
+
 def index_take(src, idxs, axes):
   pvals = [_abstractify(arg) for arg in (src,) + idxs]
   jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
@@ -1848,6 +1861,118 @@ ad.primitive_transposes[dynamic_update_slice_p] = \
     dynamic_update_slice_transpose_rule
 
 
+
+GatherDimensionNumbers = collections.namedtuple(
+    ""GatherDimensionNumbers"",
+    [""offset_dims"", ""collapsed_slice_dims"", ""start_index_map"",
+     ""index_vector_dim""])
+
+def _gather_dimensions_proto(dimension_numbers):
+  assert type(dimension_numbers) is GatherDimensionNumbers
+  proto = xla_bridge.xla_data_pb2.GatherDimensionNumbers()
+  proto.offset_dims.extend(dimension_numbers.offset_dims)
+  proto.collapsed_slice_dims.extend(dimension_numbers.collapsed_slice_dims)
+  proto.start_index_map.extend(dimension_numbers.start_index_map)
+  proto.index_vector_dim = dimension_numbers.index_vector_dim
+  return proto
+
+def gather_dtype_rule(operand, start_indices, **kwargs):
+  if not onp.issubdtype(start_indices.dtype, onp.integer):
+    raise ValueError(""start_indices must have an integer type"")
+  return xla_bridge.canonicalize_dtype(operand.dtype)
+
+def gather_shape_rule(operand, start_indices, dimension_numbers, slice_sizes,
+                      operand_shape):
+  assert operand.shape == operand_shape
+  expanded_start_indices_shape = start_indices
+  if len(expanded_start_indices_shape) == dimension_numbers.index_vector_dim:
+    expanded_start_indices_shape.append(1)
+  result_rank = len(dimension_numbers.offset_dims)
+  result_rank += len(expanded_start_indices_shape) - 1
+  output_shape = []
+  offset_dims_seen = 0
+  gather_dims_seen = 0
+  for i in xrange(result_rank):
+    if i in dimension_numbers.offset_dims:
+      while offset_dims_seen in dimension_numbers.collapsed_slice_dims:
+        offset_dims_seen += 1
+      output_shape.append(slice_sizes[offset_dims_seen])
+      offset_dims_seen += 1
+    else:
+      if gather_dims_seen == dimension_numbers.index_vector_dim:
+        gather_dims_seen += 1
+      output_shape.append(expanded_start_indices_shape[gather_dims_seen])
+      gather_dims_seen += 1
+  return tuple(output_shape)
+
+def gather_translation_rule(c, operand, start_indices, dimension_numbers,
+                            slice_sizes, operand_shape):
+  return c.Gather(operand, start_indices,
+                  _gather_dimensions_proto(dimension_numbers), slice_sizes)
+
+def gather_jvp_rule(g, operand, start_indices, dimension_numbers, slice_sizes,
+                    operand_shape):
+  return gather(g, start_indices, dimension_numbers, slice_sizes)
+
+def gather_transpose_rule(t, operand, start_indices, dimension_numbers,
+                          slice_sizes, operand_shape):
+  # TODO(phawkins): this is completely untested and probably wrong.
+  assert operand is None
+  zeros = broadcast(_const(t, 0), operand_shape)
+  scatter_dnums = ScatterDimensionNumbers(
+    update_window_dims=dimension_numbers.update_window_dims,
+    inserted_window_dims=dimension_numbers.collapsed_slice_dims,
+    scatter_dims_to_operand_dims=dimension_numbers.start_index_map,
+    index_vector_dim=dimension_numbers.index_vector_dim)
+  return [scatter(zeros, start_indices, t, add, scatter_dnums), ad_util.zero]
+
+
+gather_p = standard_primitive(
+    gather_shape_rule, gather_dtype_rule, 'gather',
+    gather_translation_rule)
+ad.defjvp(gather_p, gather_jvp_rule, None)
+ad.primitive_transposes[gather_p] = gather_transpose_rule
+
+
+ScatterDimensionNumbers = collections.namedtuple(
+    ""ScatterDimensionNumbers"",
+    [""update_window_dims"", ""inserted_window_dims"",
+     ""scatter_dims_to_operand_dims"", ""index_vector_dim""])
+
+def _scatter_dimensions_proto(dimension_numbers):
+  assert type(dimension_numbers) is ScatterDimensionNumbers
+  proto = xla_bridge.xla_data_pb2.ScatterDimensionNumbers()
+  proto.update_window_dims.extend(dimension_numbers.update_window_dims)
+  proto.inserted_window_dims.extend(dimension_numbers.inserted_window_dims)
+  proto.scatter_dims_to_operand_dims.extend(
+      dimension_numbers.scatter_dims_to_operand_dims)
+  proto.index_vector_dim = dimension_numbers.index_vector_dim
+  return proto
+
+def scatter_dtype_rule(operand, scatter_indices, updates, **kwargs):
+  if not onp.issubdtype(scatter_indices.dtype, onp.integer):
+    raise ValueError(""start_indices must have an integer type"")
+  _check_same_dtypes(""scatter"", False, operand.dtype, updates.dtype)
+  return xla_bridge.canonicalize_dtype(operand.dtype)
+
+def scatter_shape_rule(operand, scatter_indices, updates, **kwargs):
+  return operand.shape
+
+def scatter_translation_rule(c, operand, scatter_indices, updates,
+                             update_jaxpr, update_consts, dimension_numbers):
+  dtype = c.GetShape(operand).numpy_dtype()
+  init_value = c.Constant(onp.array(0, dtype))
+  update_computation = _reduction_computation(
+      c, update_jaxpr, update_consts, init_value)
+  return c.Scatter(operand, scatter_indices, updates, update_computation,
+                  _scatter_dimensions_proto(dimension_numbers))
+
+scatter_p = standard_primitive(
+    scatter_shape_rule, scatter_dtype_rule, 'scatter',
+    scatter_translation_rule)
+
+# TODO(phawkins): define JVP and transpose rules for scatter.
+
 def index_take_shape_rule(src, *idxs, **kwargs):
   axes = kwargs['axes']
   return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))","diff --git a/jax/lax.py b/jax/lax.py
index 85c432542..b272543a2 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -203,6 +203,19 @@ def dynamic_update_slice(operand, update, start_indices):
   return dynamic_update_slice_p.bind(operand, update, start_indices,
                                      update_shape=update.shape)
 
+def gather(operand, start_indices, dimension_numbers=None, slice_sizes=None):
+  return gather_p.bind(
+      operand, start_indices, dimension_numbers=dimension_numbers,
+      slice_sizes=tuple(slice_sizes), operand_shape=operand.shape)
+
+def scatter(operand, scatter_indices, updates, update_computation,
+            dimension_numbers=None):
+  jaxpr, consts = _reduction_jaxpr(update_computation, _const(operand, 0))
+  return scatter_p.bind(
+      operand, scatter_indices, updates, update_jaxpr=jaxpr,
+      update_consts=consts, dimension_numbers=dimension_numbers)
+
+
 def index_take(src, idxs, axes):
   pvals = [_abstractify(arg) for arg in (src,) + idxs]
   jaxpr, _, consts = pe.trace_unwrapped_to_jaxpr(partial(_index_take, axes), pvals)
@@ -1848,6 +1861,118 @@ ad.primitive_transposes[dynamic_update_slice_p] = \
     dynamic_update_slice_transpose_rule
 
 
+
+GatherDimensionNumbers = collections.namedtuple(
+    ""GatherDimensionNumbers"",
+    [""offset_dims"", ""collapsed_slice_dims"", ""start_index_map"",
+     ""index_vector_dim""])
+
+def _gather_dimensions_proto(dimension_numbers):
+  assert type(dimension_numbers) is GatherDimensionNumbers
+  proto = xla_bridge.xla_data_pb2.GatherDimensionNumbers()
+  proto.offset_dims.extend(dimension_numbers.offset_dims)
+  proto.collapsed_slice_dims.extend(dimension_numbers.collapsed_slice_dims)
+  proto.start_index_map.extend(dimension_numbers.start_index_map)
+  proto.index_vector_dim = dimension_numbers.index_vector_dim
+  return proto
+
+def gather_dtype_rule(operand, start_indices, **kwargs):
+  if not onp.issubdtype(start_indices.dtype, onp.integer):
+    raise ValueError(""start_indices must have an integer type"")
+  return xla_bridge.canonicalize_dtype(operand.dtype)
+
+def gather_shape_rule(operand, start_indices, dimension_numbers, slice_sizes,
+                      operand_shape):
+  assert operand.shape == operand_shape
+  expanded_start_indices_shape = start_indices
+  if len(expanded_start_indices_shape) == dimension_numbers.index_vector_dim:
+    expanded_start_indices_shape.append(1)
+  result_rank = len(dimension_numbers.offset_dims)
+  result_rank += len(expanded_start_indices_shape) - 1
+  output_shape = []
+  offset_dims_seen = 0
+  gather_dims_seen = 0
+  for i in xrange(result_rank):
+    if i in dimension_numbers.offset_dims:
+      while offset_dims_seen in dimension_numbers.collapsed_slice_dims:
+        offset_dims_seen += 1
+      output_shape.append(slice_sizes[offset_dims_seen])
+      offset_dims_seen += 1
+    else:
+      if gather_dims_seen == dimension_numbers.index_vector_dim:
+        gather_dims_seen += 1
+      output_shape.append(expanded_start_indices_shape[gather_dims_seen])
+      gather_dims_seen += 1
+  return tuple(output_shape)
+
+def gather_translation_rule(c, operand, start_indices, dimension_numbers,
+                            slice_sizes, operand_shape):
+  return c.Gather(operand, start_indices,
+                  _gather_dimensions_proto(dimension_numbers), slice_sizes)
+
+def gather_jvp_rule(g, operand, start_indices, dimension_numbers, slice_sizes,
+                    operand_shape):
+  return gather(g, start_indices, dimension_numbers, slice_sizes)
+
+def gather_transpose_rule(t, operand, start_indices, dimension_numbers,
+                          slice_sizes, operand_shape):
+  # TODO(phawkins): this is completely untested and probably wrong.
+  assert operand is None
+  zeros = broadcast(_const(t, 0), operand_shape)
+  scatter_dnums = ScatterDimensionNumbers(
+    update_window_dims=dimension_numbers.update_window_dims,
+    inserted_window_dims=dimension_numbers.collapsed_slice_dims,
+    scatter_dims_to_operand_dims=dimension_numbers.start_index_map,
+    index_vector_dim=dimension_numbers.index_vector_dim)
+  return [scatter(zeros, start_indices, t, add, scatter_dnums), ad_util.zero]
+
+
+gather_p = standard_primitive(
+    gather_shape_rule, gather_dtype_rule, 'gather',
+    gather_translation_rule)
+ad.defjvp(gather_p, gather_jvp_rule, None)
+ad.primitive_transposes[gather_p] = gather_transpose_rule
+
+
+ScatterDimensionNumbers = collections.namedtuple(
+    ""ScatterDimensionNumbers"",
+    [""update_window_dims"", ""inserted_window_dims"",
+     ""scatter_dims_to_operand_dims"", ""index_vector_dim""])
+
+def _scatter_dimensions_proto(dimension_numbers):
+  assert type(dimension_numbers) is ScatterDimensionNumbers
+  proto = xla_bridge.xla_data_pb2.ScatterDimensionNumbers()
+  proto.update_window_dims.extend(dimension_numbers.update_window_dims)
+  proto.inserted_window_dims.extend(dimension_numbers.inserted_window_dims)
+  proto.scatter_dims_to_operand_dims.extend(
+      dimension_numbers.scatter_dims_to_operand_dims)
+  proto.index_vector_dim = dimension_numbers.index_vector_dim
+  return proto
+
+def scatter_dtype_rule(operand, scatter_indices, updates, **kwargs):
+  if not onp.issubdtype(scatter_indices.dtype, onp.integer):
+    raise ValueError(""start_indices must have an integer type"")
+  _check_same_dtypes(""scatter"", False, operand.dtype, updates.dtype)
+  return xla_bridge.canonicalize_dtype(operand.dtype)
+
+def scatter_shape_rule(operand, scatter_indices, updates, **kwargs):
+  return operand.shape
+
+def scatter_translation_rule(c, operand, scatter_indices, updates,
+                             update_jaxpr, update_consts, dimension_numbers):
+  dtype = c.GetShape(operand).numpy_dtype()
+  init_value = c.Constant(onp.array(0, dtype))
+  update_computation = _reduction_computation(
+      c, update_jaxpr, update_consts, init_value)
+  return c.Scatter(operand, scatter_indices, updates, update_computation,
+                  _scatter_dimensions_proto(dimension_numbers))
+
+scatter_p = standard_primitive(
+    scatter_shape_rule, scatter_dtype_rule, 'scatter',
+    scatter_translation_rule)
+
+# TODO(phawkins): define JVP and transpose rules for scatter.
+
 def index_take_shape_rule(src, *idxs, **kwargs):
   axes = kwargs['axes']
   return (idxs[0].shape[0],) + tuple(onp.delete(src.shape, axes))",No
jax/lax.py,jax/lax.py,adaa3444003e951047656f00a882bc1e2038b11f,5e48420f12e7b8d7aef388862f618597ae38a02c,More progress on scatter/gather.,"diff --git a/jax/lax.py b/jax/lax.py
index b272543a2..62e6f26ef 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -213,7 +213,8 @@ def scatter(operand, scatter_indices, updates, update_computation,
   jaxpr, consts = _reduction_jaxpr(update_computation, _const(operand, 0))
   return scatter_p.bind(
       operand, scatter_indices, updates, update_jaxpr=jaxpr,
-      update_consts=consts, dimension_numbers=dimension_numbers)
+      update_consts=consts, dimension_numbers=dimension_numbers,
+      updates_shape=updates.shape)
 
 
 def index_take(src, idxs, axes):
@@ -1884,7 +1885,7 @@ def gather_dtype_rule(operand, start_indices, **kwargs):
 def gather_shape_rule(operand, start_indices, dimension_numbers, slice_sizes,
                       operand_shape):
   assert operand.shape == operand_shape
-  expanded_start_indices_shape = start_indices
+  expanded_start_indices_shape = start_indices.shape
   if len(expanded_start_indices_shape) == dimension_numbers.index_vector_dim:
     expanded_start_indices_shape.append(1)
   result_rank = len(dimension_numbers.offset_dims)
@@ -1916,11 +1917,12 @@ def gather_jvp_rule(g, operand, start_indices, dimension_numbers, slice_sizes,
 
 def gather_transpose_rule(t, operand, start_indices, dimension_numbers,
                           slice_sizes, operand_shape):
-  # TODO(phawkins): this is completely untested and probably wrong.
   assert operand is None
+  if t is ad_util.zero:
+    return [ad_util.zero, ad_util.zero]
   zeros = broadcast(_const(t, 0), operand_shape)
   scatter_dnums = ScatterDimensionNumbers(
-    update_window_dims=dimension_numbers.update_window_dims,
+    update_window_dims=dimension_numbers.offset_dims,
     inserted_window_dims=dimension_numbers.collapsed_slice_dims,
     scatter_dims_to_operand_dims=dimension_numbers.start_index_map,
     index_vector_dim=dimension_numbers.index_vector_dim)
@@ -1959,7 +1961,8 @@ def scatter_shape_rule(operand, scatter_indices, updates, **kwargs):
   return operand.shape
 
 def scatter_translation_rule(c, operand, scatter_indices, updates,
-                             update_jaxpr, update_consts, dimension_numbers):
+                             update_jaxpr, update_consts, dimension_numbers,
+                             updates_shape):
   dtype = c.GetShape(operand).numpy_dtype()
   init_value = c.Constant(onp.array(0, dtype))
   update_computation = _reduction_computation(
@@ -1967,11 +1970,63 @@ def scatter_translation_rule(c, operand, scatter_indices, updates,
   return c.Scatter(operand, scatter_indices, updates, update_computation,
                   _scatter_dimensions_proto(dimension_numbers))
 
+def scatter_jvp(primals, tangents, update_jaxpr, update_consts,
+                dimension_numbers, updates_shape):
+  operand, scatter_indices, updates = primals
+  g_operand, g_scatter_indices, g_updates = tangents
+  assert g_scatter_indices is ad_util.zero
+  val_out = scatter_p.bind(
+      operand, scatter_indices, updates, update_jaxpr=update_jaxpr,
+      update_consts=update_consts, dimension_numbers=dimension_numbers,
+      updates_shape=updates_shape)
+  if g_operand is ad_util.zero and g_updates is ad_util.zero:
+    tangent_out = ad_util.zero
+  else:
+    print(""scatter jvp "", g_operand, g_updates)
+    g_operand = ad.instantiate_zeros(operand, g_operand)
+    g_updates = ad.instantiate_zeros(updates, g_updates)
+    tangent_out = scatter_p.bind(
+        g_operand, scatter_indices, g_updates, update_jaxpr=update_jaxpr,
+        update_consts=update_consts, dimension_numbers=dimension_numbers,
+        updates_shape=updates_shape)
+  return val_out, tangent_out
+
+
+def scatter_transpose_rule(t, operand, scatter_indices, updates,
+                           update_jaxpr, update_consts, dimension_numbers,
+                           updates_shape):
+  assert scatter_indices is not None
+  assert operand is None and updates is None
+  operand_t = update_t = None
+  if operand is None:
+    # TODO(phawkins): only correct for scatter-add.
+    operand_t = t
+    #zeros = _zeros(t, shape=updates_shape)
+    #operand_t = scatter_p.bind(
+    #  t, scatter_indices, zeros, update_jaxpr=update_jaxpr,
+    #  update_consts=update_consts, dimension_numbers=dimension_numbers,
+    #  updates_shape=updates_shape)
+
+  if updates is None:
+    gather_dnums = GatherDimensionNumbers(
+      offset_dims=dimension_numbers.update_window_dims,
+      collapsed_slice_dims=dimension_numbers.inserted_window_dims,
+      start_index_map=dimension_numbers.scatter_dims_to_operand_dims,
+      index_vector_dim=dimension_numbers.index_vector_dim)
+    slice_sizes = onp.array(updates_shape)[
+      list(dimension_numbers.update_window_dims)]
+    update_t = gather(t, scatter_indices, dimension_numbers=gather_dnums,
+                      slice_sizes=slice_sizes)
+  return [operand_t, update_t, None]
+
+
 scatter_p = standard_primitive(
     scatter_shape_rule, scatter_dtype_rule, 'scatter',
     scatter_translation_rule)
+ad.primitive_jvps[scatter_p] = scatter_jvp
+ad.primitive_transposes[scatter_p] = scatter_transpose_rule
+
 
-# TODO(phawkins): define JVP and transpose rules for scatter.
 
 def index_take_shape_rule(src, *idxs, **kwargs):
   axes = kwargs['axes']","diff --git a/jax/lax.py b/jax/lax.py
index b272543a2..62e6f26ef 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -213,7 +213,8 @@ def scatter(operand, scatter_indices, updates, update_computation,
   jaxpr, consts = _reduction_jaxpr(update_computation, _const(operand, 0))
   return scatter_p.bind(
       operand, scatter_indices, updates, update_jaxpr=jaxpr,
-      update_consts=consts, dimension_numbers=dimension_numbers)
+      update_consts=consts, dimension_numbers=dimension_numbers,
+      updates_shape=updates.shape)
 
 
 def index_take(src, idxs, axes):
@@ -1884,7 +1885,7 @@ def gather_dtype_rule(operand, start_indices, **kwargs):
 def gather_shape_rule(operand, start_indices, dimension_numbers, slice_sizes,
                       operand_shape):
   assert operand.shape == operand_shape
-  expanded_start_indices_shape = start_indices
+  expanded_start_indices_shape = start_indices.shape
   if len(expanded_start_indices_shape) == dimension_numbers.index_vector_dim:
     expanded_start_indices_shape.append(1)
   result_rank = len(dimension_numbers.offset_dims)
@@ -1916,11 +1917,12 @@ def gather_jvp_rule(g, operand, start_indices, dimension_numbers, slice_sizes,
 
 def gather_transpose_rule(t, operand, start_indices, dimension_numbers,
                           slice_sizes, operand_shape):
-  # TODO(phawkins): this is completely untested and probably wrong.
   assert operand is None
+  if t is ad_util.zero:
+    return [ad_util.zero, ad_util.zero]
   zeros = broadcast(_const(t, 0), operand_shape)
   scatter_dnums = ScatterDimensionNumbers(
-    update_window_dims=dimension_numbers.update_window_dims,
+    update_window_dims=dimension_numbers.offset_dims,
     inserted_window_dims=dimension_numbers.collapsed_slice_dims,
     scatter_dims_to_operand_dims=dimension_numbers.start_index_map,
     index_vector_dim=dimension_numbers.index_vector_dim)
@@ -1959,7 +1961,8 @@ def scatter_shape_rule(operand, scatter_indices, updates, **kwargs):
   return operand.shape
 
 def scatter_translation_rule(c, operand, scatter_indices, updates,
-                             update_jaxpr, update_consts, dimension_numbers):
+                             update_jaxpr, update_consts, dimension_numbers,
+                             updates_shape):
   dtype = c.GetShape(operand).numpy_dtype()
   init_value = c.Constant(onp.array(0, dtype))
   update_computation = _reduction_computation(
@@ -1967,11 +1970,63 @@ def scatter_translation_rule(c, operand, scatter_indices, updates,
   return c.Scatter(operand, scatter_indices, updates, update_computation,
                   _scatter_dimensions_proto(dimension_numbers))
 
+def scatter_jvp(primals, tangents, update_jaxpr, update_consts,
+                dimension_numbers, updates_shape):
+  operand, scatter_indices, updates = primals
+  g_operand, g_scatter_indices, g_updates = tangents
+  assert g_scatter_indices is ad_util.zero
+  val_out = scatter_p.bind(
+      operand, scatter_indices, updates, update_jaxpr=update_jaxpr,
+      update_consts=update_consts, dimension_numbers=dimension_numbers,
+      updates_shape=updates_shape)
+  if g_operand is ad_util.zero and g_updates is ad_util.zero:
+    tangent_out = ad_util.zero
+  else:
+    print(""scatter jvp "", g_operand, g_updates)
+    g_operand = ad.instantiate_zeros(operand, g_operand)
+    g_updates = ad.instantiate_zeros(updates, g_updates)
+    tangent_out = scatter_p.bind(
+        g_operand, scatter_indices, g_updates, update_jaxpr=update_jaxpr,
+        update_consts=update_consts, dimension_numbers=dimension_numbers,
+        updates_shape=updates_shape)
+  return val_out, tangent_out
+
+
+def scatter_transpose_rule(t, operand, scatter_indices, updates,
+                           update_jaxpr, update_consts, dimension_numbers,
+                           updates_shape):
+  assert scatter_indices is not None
+  assert operand is None and updates is None
+  operand_t = update_t = None
+  if operand is None:
+    # TODO(phawkins): only correct for scatter-add.
+    operand_t = t
+    #zeros = _zeros(t, shape=updates_shape)
+    #operand_t = scatter_p.bind(
+    #  t, scatter_indices, zeros, update_jaxpr=update_jaxpr,
+    #  update_consts=update_consts, dimension_numbers=dimension_numbers,
+    #  updates_shape=updates_shape)
+
+  if updates is None:
+    gather_dnums = GatherDimensionNumbers(
+      offset_dims=dimension_numbers.update_window_dims,
+      collapsed_slice_dims=dimension_numbers.inserted_window_dims,
+      start_index_map=dimension_numbers.scatter_dims_to_operand_dims,
+      index_vector_dim=dimension_numbers.index_vector_dim)
+    slice_sizes = onp.array(updates_shape)[
+      list(dimension_numbers.update_window_dims)]
+    update_t = gather(t, scatter_indices, dimension_numbers=gather_dnums,
+                      slice_sizes=slice_sizes)
+  return [operand_t, update_t, None]
+
+
 scatter_p = standard_primitive(
     scatter_shape_rule, scatter_dtype_rule, 'scatter',
     scatter_translation_rule)
+ad.primitive_jvps[scatter_p] = scatter_jvp
+ad.primitive_transposes[scatter_p] = scatter_transpose_rule
+
 
-# TODO(phawkins): define JVP and transpose rules for scatter.
 
 def index_take_shape_rule(src, *idxs, **kwargs):
   axes = kwargs['axes']",No
jax/lax_linalg.py,jax/lax_linalg.py,525a492d5d74ca2e8f68433f748039d0dfb7c049,a5c737a08571026c563f647453e0766edc8ac184,Start work on a JVP rule for LU decomposition.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index fb6015cf0..d877d2b1d 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -236,10 +236,51 @@ def lu_abstract_eval(operand):
     pivot = operand
   return core.AbstractTuple((operand, pivot))
 
+def lu_jvp_rule(primals, tangents):
+  a, = primals
+  a_dot, = tangents
+  lu, pivots = lu_p.bind(a)
+
+  m, n = np.shape(a)[-2:]
+  dtype = lax._dtype(a)
+  k = min(m, n)
+
+  # TODO(phawkins): use a gather rather than a matrix multiplication here.
+  permutation = lu_pivots_to_permutation(pivots, m)
+  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  x = np.matmul(p, a_dot)
+
+  # Derivation of the JVP is from:
+  # Differentiation of Matrix Functionals Using Triangular Factorization
+  # F. R. De Hoog, R. S. Anderssen, and M. A. Lukas
+
+  # TODO(phawkins): add unit_diagonal support to solve_triangular, use it here
+  # instead of explicit masking of l.
+  l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
+  u = np.triu(lu)
+
+  la = triangular_solve(l, x, left_side=True, transpose_a=False, lower=True)
+
+  lau = triangular_solve(u, la, left_side=False, transpose_a=False,
+                         lower=False)
+
+  l_dot = np.matmul(l, np.tril(lau)[:, :k])
+  u_dot = np.matmul(np.triu(lau)[:k, :], u)
+
+  # The LU decomposition output must have a unit diagonal, so rescale L and U
+  # so L's diagonal is a unit.
+  l_diag = np.diagonal(l_dot, axis1=-2, axis2=-1)
+  l_dot = l_dot / l_diag[..., None, :]
+  u_dot = u_dot * l_diag[..., None]
+  lu_dot = np.tril(l_dot, -1) + u_dot
+  return core.pack((lu, pivots)), core.pack((lu_dot, ad.zeros_like_jaxval(pivots)))
+
+
 lu_p = Primitive('lu')
 lu_p.def_impl(lu_impl)
 lu_p.def_abstract_eval(lu_abstract_eval)
 xla.translations[lu_p] = lu_translation_rule
+ad.primitive_jvps[lu_p] = lu_jvp_rule
 
 def lu_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index fb6015cf0..d877d2b1d 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -236,10 +236,51 @@ def lu_abstract_eval(operand):
     pivot = operand
   return core.AbstractTuple((operand, pivot))
 
+def lu_jvp_rule(primals, tangents):
+  a, = primals
+  a_dot, = tangents
+  lu, pivots = lu_p.bind(a)
+
+  m, n = np.shape(a)[-2:]
+  dtype = lax._dtype(a)
+  k = min(m, n)
+
+  # TODO(phawkins): use a gather rather than a matrix multiplication here.
+  permutation = lu_pivots_to_permutation(pivots, m)
+  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  x = np.matmul(p, a_dot)
+
+  # Derivation of the JVP is from:
+  # Differentiation of Matrix Functionals Using Triangular Factorization
+  # F. R. De Hoog, R. S. Anderssen, and M. A. Lukas
+
+  # TODO(phawkins): add unit_diagonal support to solve_triangular, use it here
+  # instead of explicit masking of l.
+  l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
+  u = np.triu(lu)
+
+  la = triangular_solve(l, x, left_side=True, transpose_a=False, lower=True)
+
+  lau = triangular_solve(u, la, left_side=False, transpose_a=False,
+                         lower=False)
+
+  l_dot = np.matmul(l, np.tril(lau)[:, :k])
+  u_dot = np.matmul(np.triu(lau)[:k, :], u)
+
+  # The LU decomposition output must have a unit diagonal, so rescale L and U
+  # so L's diagonal is a unit.
+  l_diag = np.diagonal(l_dot, axis1=-2, axis2=-1)
+  l_dot = l_dot / l_diag[..., None, :]
+  u_dot = u_dot * l_diag[..., None]
+  lu_dot = np.tril(l_dot, -1) + u_dot
+  return core.pack((lu, pivots)), core.pack((lu_dot, ad.zeros_like_jaxval(pivots)))
+
+
 lu_p = Primitive('lu')
 lu_p.def_impl(lu_impl)
 lu_p.def_abstract_eval(lu_abstract_eval)
 xla.translations[lu_p] = lu_translation_rule
+ad.primitive_jvps[lu_p] = lu_jvp_rule
 
 def lu_cpu_translation_rule(c, operand):
   shape = c.GetShape(operand)",No
tests/linalg_test.py,tests/linalg_test.py,525a492d5d74ca2e8f68433f748039d0dfb7c049,a5c737a08571026c563f647453e0766edc8ac184,Start work on a JVP rule for LU decomposition.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 67d400b8a..b61e93571 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -309,6 +309,24 @@ class ScipyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 5), (10, 5), (10, 10)]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testLuGrad(self, shape, dtype, rng):
+    # TODO(phawkins): remove this after a jaxlib release.
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    a = rng(shape, dtype)
+
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-3)
+
+
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 67d400b8a..b61e93571 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -309,6 +309,24 @@ class ScipyLinalgTest(jtu.JaxTestCase):
                             check_dtypes=True, tol=1e-3)
     self._CompileAndCheck(jsp.linalg.lu, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"":
+       ""_shape={}"".format(jtu.format_shape_dtype_string(shape, dtype)),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng}
+      for shape in [(1, 1), (4, 5), (10, 5), (10, 10)]
+      for dtype in float_types() | complex_types()
+      for rng in [jtu.rand_default()]))
+  # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
+  @jtu.skip_on_devices(""gpu"", ""tpu"")
+  def testLuGrad(self, shape, dtype, rng):
+    # TODO(phawkins): remove this after a jaxlib release.
+    if not hasattr(lapack, ""jax_getrf""):
+      self.skipTest(""No LU implementation available"")
+    a = rng(shape, dtype)
+
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-3)
+
+
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"":",No
jax/lax_linalg.py,jax/lax_linalg.py,bf4c60635b547f4dc945e2ab69e84c214e91aa2e,525a492d5d74ca2e8f68433f748039d0dfb7c049,Make JVP work for square matrices.,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index d877d2b1d..7a6a4ffc2 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -21,6 +21,7 @@ import numpy as onp
 from jax.numpy import lax_numpy as np
 from jax import core
 from jax import lax
+from jax import ad_util
 from jax.interpreters import xla
 from jax.interpreters import ad
 from jax.util import partial
@@ -247,33 +248,29 @@ def lu_jvp_rule(primals, tangents):
 
   # TODO(phawkins): use a gather rather than a matrix multiplication here.
   permutation = lu_pivots_to_permutation(pivots, m)
-  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  p = np.array(permutation[:, None] == np.arange(m), dtype=dtype)
   x = np.matmul(p, a_dot)
 
-  # Derivation of the JVP is from:
   # Differentiation of Matrix Functionals Using Triangular Factorization
   # F. R. De Hoog, R. S. Anderssen, and M. A. Lukas
+  #
+  #     LU = A
+  # ==> L'U + LU' = A'
+  # ==> inv(L) . L' + U' . inv(U) = inv(L) A' inv(U)
+  # ==> L' = L . tril(inv(L) . A' . inv(U), -1)
+  #     U' = triu(inv(L) . A' . inv(U)) . U
 
-  # TODO(phawkins): add unit_diagonal support to solve_triangular, use it here
-  # instead of explicit masking of l.
   l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
   u = np.triu(lu)
 
   la = triangular_solve(l, x, left_side=True, transpose_a=False, lower=True)
-
   lau = triangular_solve(u, la, left_side=False, transpose_a=False,
                          lower=False)
 
-  l_dot = np.matmul(l, np.tril(lau)[:, :k])
+  l_dot = np.matmul(l, np.tril(lau, -1)[:, :k])
   u_dot = np.matmul(np.triu(lau)[:k, :], u)
-
-  # The LU decomposition output must have a unit diagonal, so rescale L and U
-  # so L's diagonal is a unit.
-  l_diag = np.diagonal(l_dot, axis1=-2, axis2=-1)
-  l_dot = l_dot / l_diag[..., None, :]
-  u_dot = u_dot * l_diag[..., None]
-  lu_dot = np.tril(l_dot, -1) + u_dot
-  return core.pack((lu, pivots)), core.pack((lu_dot, ad.zeros_like_jaxval(pivots)))
+  lu_dot = l_dot + u_dot
+  return core.pack((lu, pivots)), ad.TangentTuple((lu_dot, ad_util.zero))
 
 
 lu_p = Primitive('lu')","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index d877d2b1d..7a6a4ffc2 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -21,6 +21,7 @@ import numpy as onp
 from jax.numpy import lax_numpy as np
 from jax import core
 from jax import lax
+from jax import ad_util
 from jax.interpreters import xla
 from jax.interpreters import ad
 from jax.util import partial
@@ -247,33 +248,29 @@ def lu_jvp_rule(primals, tangents):
 
   # TODO(phawkins): use a gather rather than a matrix multiplication here.
   permutation = lu_pivots_to_permutation(pivots, m)
-  p = np.array(permutation == np.arange(m)[:, None], dtype=dtype)
+  p = np.array(permutation[:, None] == np.arange(m), dtype=dtype)
   x = np.matmul(p, a_dot)
 
-  # Derivation of the JVP is from:
   # Differentiation of Matrix Functionals Using Triangular Factorization
   # F. R. De Hoog, R. S. Anderssen, and M. A. Lukas
+  #
+  #     LU = A
+  # ==> L'U + LU' = A'
+  # ==> inv(L) . L' + U' . inv(U) = inv(L) A' inv(U)
+  # ==> L' = L . tril(inv(L) . A' . inv(U), -1)
+  #     U' = triu(inv(L) . A' . inv(U)) . U
 
-  # TODO(phawkins): add unit_diagonal support to solve_triangular, use it here
-  # instead of explicit masking of l.
   l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
   u = np.triu(lu)
 
   la = triangular_solve(l, x, left_side=True, transpose_a=False, lower=True)
-
   lau = triangular_solve(u, la, left_side=False, transpose_a=False,
                          lower=False)
 
-  l_dot = np.matmul(l, np.tril(lau)[:, :k])
+  l_dot = np.matmul(l, np.tril(lau, -1)[:, :k])
   u_dot = np.matmul(np.triu(lau)[:k, :], u)
-
-  # The LU decomposition output must have a unit diagonal, so rescale L and U
-  # so L's diagonal is a unit.
-  l_diag = np.diagonal(l_dot, axis1=-2, axis2=-1)
-  l_dot = l_dot / l_diag[..., None, :]
-  u_dot = u_dot * l_diag[..., None]
-  lu_dot = np.tril(l_dot, -1) + u_dot
-  return core.pack((lu, pivots)), core.pack((lu_dot, ad.zeros_like_jaxval(pivots)))
+  lu_dot = l_dot + u_dot
+  return core.pack((lu, pivots)), ad.TangentTuple((lu_dot, ad_util.zero))
 
 
 lu_p = Primitive('lu')",No
jax/lax_linalg.py,jax/lax_linalg.py,9e91fb42cb17a16203469228a19d108b89e0d128,bf4c60635b547f4dc945e2ab69e84c214e91aa2e,"Make LU JVP work for rectangular matrices.

Fix bug in BLAS TRSM kernel if left_side=False.","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 7a6a4ffc2..24bb9de83 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -242,7 +242,8 @@ def lu_jvp_rule(primals, tangents):
   a_dot, = tangents
   lu, pivots = lu_p.bind(a)
 
-  m, n = np.shape(a)[-2:]
+  a_shape = np.shape(a)
+  m, n = a_shape[-2:]
   dtype = lax._dtype(a)
   k = min(m, n)
 
@@ -260,15 +261,26 @@ def lu_jvp_rule(primals, tangents):
   # ==> L' = L . tril(inv(L) . A' . inv(U), -1)
   #     U' = triu(inv(L) . A' . inv(U)) . U
 
-  l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
-  u = np.triu(lu)
+  ndims = len(a_shape)
+  l_padding = [(0, 0, 0)] * ndims
+  l_padding[-1] = (0, m - k, 0)
+  zero = np._constant_like(lu, 0)
+  l = lax.pad(np.tril(lu[..., :, :k], -1), zero, l_padding)
+  l = l + np.eye(m, m, dtype=dtype)
+
+  u_eye = lax.pad(np.eye(n - k, n - k, dtype=dtype), zero,
+                  ((k, 0, 0), (k, 0, 0)))
+  u_padding = [(0, 0, 0)] * ndims
+  u_padding[-2] = (0, n - k, 0)
+  u = lax.pad(np.triu(lu[..., :k, :]), zero, u_padding) + u_eye
+
 
   la = triangular_solve(l, x, left_side=True, transpose_a=False, lower=True)
   lau = triangular_solve(u, la, left_side=False, transpose_a=False,
                          lower=False)
 
-  l_dot = np.matmul(l, np.tril(lau, -1)[:, :k])
-  u_dot = np.matmul(np.triu(lau)[:k, :], u)
+  l_dot = np.matmul(l, np.tril(lau, -1))
+  u_dot = np.matmul(np.triu(lau), u)
   lu_dot = l_dot + u_dot
   return core.pack((lu, pivots)), ad.TangentTuple((lu_dot, ad_util.zero))
 ","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 7a6a4ffc2..24bb9de83 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -242,7 +242,8 @@ def lu_jvp_rule(primals, tangents):
   a_dot, = tangents
   lu, pivots = lu_p.bind(a)
 
-  m, n = np.shape(a)[-2:]
+  a_shape = np.shape(a)
+  m, n = a_shape[-2:]
   dtype = lax._dtype(a)
   k = min(m, n)
 
@@ -260,15 +261,26 @@ def lu_jvp_rule(primals, tangents):
   # ==> L' = L . tril(inv(L) . A' . inv(U), -1)
   #     U' = triu(inv(L) . A' . inv(U)) . U
 
-  l = np.tril(lu, -1)[:, :k] + np.eye(m, k, dtype=dtype)
-  u = np.triu(lu)
+  ndims = len(a_shape)
+  l_padding = [(0, 0, 0)] * ndims
+  l_padding[-1] = (0, m - k, 0)
+  zero = np._constant_like(lu, 0)
+  l = lax.pad(np.tril(lu[..., :, :k], -1), zero, l_padding)
+  l = l + np.eye(m, m, dtype=dtype)
+
+  u_eye = lax.pad(np.eye(n - k, n - k, dtype=dtype), zero,
+                  ((k, 0, 0), (k, 0, 0)))
+  u_padding = [(0, 0, 0)] * ndims
+  u_padding[-2] = (0, n - k, 0)
+  u = lax.pad(np.triu(lu[..., :k, :]), zero, u_padding) + u_eye
+
 
   la = triangular_solve(l, x, left_side=True, transpose_a=False, lower=True)
   lau = triangular_solve(u, la, left_side=False, transpose_a=False,
                          lower=False)
 
-  l_dot = np.matmul(l, np.tril(lau, -1)[:, :k])
-  u_dot = np.matmul(np.triu(lau)[:k, :], u)
+  l_dot = np.matmul(l, np.tril(lau, -1))
+  u_dot = np.matmul(np.triu(lau), u)
   lu_dot = l_dot + u_dot
   return core.pack((lu, pivots)), ad.TangentTuple((lu_dot, ad_util.zero))
 ",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,9e91fb42cb17a16203469228a19d108b89e0d128,bf4c60635b547f4dc945e2ab69e84c214e91aa2e,"Make LU JVP work for rectangular matrices.

Fix bug in BLAS TRSM kernel if left_side=False.","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 892d0e403..9ce0aacfb 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -70,8 +70,8 @@ cdef void blas_strsm(void* out, void** data) nogil:
   elif trans_a == 2:
     ctransa = 'C'
   cdef char cdiag = 'U' if diag else 'N'
-  cdef int lda = m
-  cdef int ldb = m if left_side else n
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
   strsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
 
 register_cpu_custom_call_target(b""blas_strsm"", <void*>(blas_strsm))
@@ -99,8 +99,8 @@ cdef void blas_dtrsm(void* out, void** data) nogil:
   elif trans_a == 2:
     ctransa = 'C'
   cdef char cdiag = 'U' if diag else 'N'
-  cdef int lda = m
-  cdef int ldb = m if left_side else n
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
   dtrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
 
 register_cpu_custom_call_target(b""blas_dtrsm"", <void*>(blas_dtrsm))
@@ -129,8 +129,8 @@ cdef void blas_ctrsm(void* out, void** data) nogil:
   elif trans_a == 2:
     ctransa = 'C'
   cdef char cdiag = 'U' if diag else 'N'
-  cdef int lda = m
-  cdef int ldb = m if left_side else n
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
   ctrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
 
 register_cpu_custom_call_target(b""blas_ctrsm"", <void*>(blas_ctrsm))
@@ -139,13 +139,11 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
              conj_a=False, diag=False):
   b_shape = c.GetShape(b)
   dtype = b_shape.element_type()
-  #if left_side:
   m, n = b_shape.dimensions()
-  #else:
-  #  n, m = b_shape.dimensions()
+  k = m if left_side else n
 
   a_shape = c.GetShape(a)
-  if (m, m) != a_shape.dimensions() or a_shape.element_type() != dtype:
+  if (k, k) != a_shape.dimensions() or a_shape.element_type() != dtype:
     raise ValueError(""Argument mismatch for trsm, got {} and {}"".format(
       a_shape, b_shape))
 ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 892d0e403..9ce0aacfb 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -70,8 +70,8 @@ cdef void blas_strsm(void* out, void** data) nogil:
   elif trans_a == 2:
     ctransa = 'C'
   cdef char cdiag = 'U' if diag else 'N'
-  cdef int lda = m
-  cdef int ldb = m if left_side else n
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
   strsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
 
 register_cpu_custom_call_target(b""blas_strsm"", <void*>(blas_strsm))
@@ -99,8 +99,8 @@ cdef void blas_dtrsm(void* out, void** data) nogil:
   elif trans_a == 2:
     ctransa = 'C'
   cdef char cdiag = 'U' if diag else 'N'
-  cdef int lda = m
-  cdef int ldb = m if left_side else n
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
   dtrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
 
 register_cpu_custom_call_target(b""blas_dtrsm"", <void*>(blas_dtrsm))
@@ -129,8 +129,8 @@ cdef void blas_ctrsm(void* out, void** data) nogil:
   elif trans_a == 2:
     ctransa = 'C'
   cdef char cdiag = 'U' if diag else 'N'
-  cdef int lda = m
-  cdef int ldb = m if left_side else n
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
   ctrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
 
 register_cpu_custom_call_target(b""blas_ctrsm"", <void*>(blas_ctrsm))
@@ -139,13 +139,11 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
              conj_a=False, diag=False):
   b_shape = c.GetShape(b)
   dtype = b_shape.element_type()
-  #if left_side:
   m, n = b_shape.dimensions()
-  #else:
-  #  n, m = b_shape.dimensions()
+  k = m if left_side else n
 
   a_shape = c.GetShape(a)
-  if (m, m) != a_shape.dimensions() or a_shape.element_type() != dtype:
+  if (k, k) != a_shape.dimensions() or a_shape.element_type() != dtype:
     raise ValueError(""Argument mismatch for trsm, got {} and {}"".format(
       a_shape, b_shape))
 ",No
tests/linalg_test.py,tests/linalg_test.py,5bacc59e23ec472c0eca1875a7c36928ea541a4b,9e91fb42cb17a16203469228a19d108b89e0d128,Relax tolerance of LuGrad test to make it pass in jax_enable_x64 mode.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index b61e93571..66d877d9b 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -324,7 +324,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
-    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-3)
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-2)
 
 
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index b61e93571..66d877d9b 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -324,7 +324,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
-    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-3)
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-2)
 
 
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.",No
tests/linalg_test.py,tests/linalg_test.py,12ef2b7f7401e3a48b0cc8fd7fd5a2af9e70c964,5bacc59e23ec472c0eca1875a7c36928ea541a4b,Relax test tolerance even more.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 66d877d9b..ded8d7259 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -324,7 +324,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
-    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-2)
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=2e-2)
 
 
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 66d877d9b..ded8d7259 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -324,7 +324,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
-    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-2)
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=2e-2)
 
 
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,0fa5af9dbb380ad61fe5c8f1d2dbbdfcea5bff92,a5c737a08571026c563f647453e0766edc8ac184,Implement the `mode='constant'` case of `np.pad`.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 46dbe100e..a7c2c63d1 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -23,7 +23,7 @@ import string
 import numpy as onp
 import opt_einsum
 import six
-from six.moves import builtins
+from six.moves import builtins, xrange
 
 from jax import jit
 from .. import core
@@ -795,6 +795,23 @@ nanprod = _make_nan_reduction(onp.nanprod, prod, 1, nan_if_all_nan=False)
 
 ### Array-creation functions
 
+@_wraps(onp.pad)
+def pad(array, pad_width, mode, constant_values=0):
+  if mode != ""constant"":
+    msg = ""Only the 'constant' case of np.pad is implemented, got mode={}.""
+    raise NotImplementedError(msg.format(mode))
+
+  array = asarray(array)
+  pad_width = onp.broadcast_to(onp.asarray(pad_width), (array.ndim, 2))
+  constant_values = broadcast_to(asarray(constant_values), (array.ndim, 2))
+  for i in xrange(array.ndim):
+    widths = [(0, 0, 0)] * array.ndim
+    widths[i] = (pad_width[i, 0], 0, 0)
+    array = lax.pad(array, constant_values[i, 0], widths)
+    widths[i] = (0, pad_width[i, 1], 0)
+    array = lax.pad(array, constant_values[i, 1], widths)
+  return array
+
 
 @_wraps(onp.stack)
 def stack(arrays):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 46dbe100e..a7c2c63d1 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -23,7 +23,7 @@ import string
 import numpy as onp
 import opt_einsum
 import six
-from six.moves import builtins
+from six.moves import builtins, xrange
 
 from jax import jit
 from .. import core
@@ -795,6 +795,23 @@ nanprod = _make_nan_reduction(onp.nanprod, prod, 1, nan_if_all_nan=False)
 
 ### Array-creation functions
 
+@_wraps(onp.pad)
+def pad(array, pad_width, mode, constant_values=0):
+  if mode != ""constant"":
+    msg = ""Only the 'constant' case of np.pad is implemented, got mode={}.""
+    raise NotImplementedError(msg.format(mode))
+
+  array = asarray(array)
+  pad_width = onp.broadcast_to(onp.asarray(pad_width), (array.ndim, 2))
+  constant_values = broadcast_to(asarray(constant_values), (array.ndim, 2))
+  for i in xrange(array.ndim):
+    widths = [(0, 0, 0)] * array.ndim
+    widths[i] = (pad_width[i, 0], 0, 0)
+    array = lax.pad(array, constant_values[i, 0], widths)
+    widths[i] = (0, pad_width[i, 1], 0)
+    array = lax.pad(array, constant_values[i, 1], widths)
+  return array
+
 
 @_wraps(onp.stack)
 def stack(arrays):",No
jax/test_util.py,jax/test_util.py,0fa5af9dbb380ad61fe5c8f1d2dbbdfcea5bff92,a5c737a08571026c563f647453e0766edc8ac184,Implement the `mode='constant'` case of `np.pad`.,"diff --git a/jax/test_util.py b/jax/test_util.py
index 0aa414f1e..c243cf7c2 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -349,6 +349,12 @@ def rand_some_zero():
   return rand
 
 
+def rand_int(low, high=None):
+  randint = npr.RandomState(0).randint
+  def fn(shape, dtype):
+    return randint(low, high=high, size=shape, dtype=dtype)
+  return fn
+
 def rand_bool():
   rng = npr.RandomState(0)
   def generator(shape, dtype):","diff --git a/jax/test_util.py b/jax/test_util.py
index 0aa414f1e..c243cf7c2 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -349,6 +349,12 @@ def rand_some_zero():
   return rand
 
 
+def rand_int(low, high=None):
+  randint = npr.RandomState(0).randint
+  def fn(shape, dtype):
+    return randint(low, high=high, size=shape, dtype=dtype)
+  return fn
+
 def rand_bool():
   rng = npr.RandomState(0)
   def generator(shape, dtype):",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,0fa5af9dbb380ad61fe5c8f1d2dbbdfcea5bff92,a5c737a08571026c563f647453e0766edc8ac184,Implement the `mode='constant'` case of `np.pad`.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 9057abd24..7f04c5add 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -431,6 +431,34 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_rpadwidth={}_rconstantvalues={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), pad_width_rank,
+          constant_values_rank),
+       ""shape"": shape, ""dtype"": dtype, ""pad_width_rank"": pad_width_rank,
+       ""constant_values_rank"": constant_values_rank, ""rng"": jtu.rand_default(),
+       ""irng"": jtu.rand_int(3)}
+      for shape in all_shapes for dtype in all_dtypes
+      for pad_width_rank in range(3)
+      for constant_values_rank in range(3)))
+  def testPad(self, shape, dtype, pad_width_rank, constant_values_rank, rng,
+              irng):
+    pad_width = irng([len(shape), 2][2 - pad_width_rank:], onp.int32)
+    def onp_fun(x, constant_vals):
+      return onp.pad(x, pad_width, mode='constant',
+                     constant_values=constant_vals)
+    def lnp_fun(x, constant_vals):
+      return lnp.pad(x, pad_width, mode='constant',
+                     constant_values=constant_vals)
+
+    def args_maker():
+      constant_vals = rng([len(shape), 2][2 - constant_values_rank:], dtype)
+      return rng(shape, dtype), constant_vals
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
           axis, "","".join(str(d) for d in base_shape),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 9057abd24..7f04c5add 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -431,6 +431,34 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_rpadwidth={}_rconstantvalues={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), pad_width_rank,
+          constant_values_rank),
+       ""shape"": shape, ""dtype"": dtype, ""pad_width_rank"": pad_width_rank,
+       ""constant_values_rank"": constant_values_rank, ""rng"": jtu.rand_default(),
+       ""irng"": jtu.rand_int(3)}
+      for shape in all_shapes for dtype in all_dtypes
+      for pad_width_rank in range(3)
+      for constant_values_rank in range(3)))
+  def testPad(self, shape, dtype, pad_width_rank, constant_values_rank, rng,
+              irng):
+    pad_width = irng([len(shape), 2][2 - pad_width_rank:], onp.int32)
+    def onp_fun(x, constant_vals):
+      return onp.pad(x, pad_width, mode='constant',
+                     constant_values=constant_vals)
+    def lnp_fun(x, constant_vals):
+      return lnp.pad(x, pad_width, mode='constant',
+                     constant_values=constant_vals)
+
+    def args_maker():
+      constant_vals = rng([len(shape), 2][2 - constant_values_rank:], dtype)
+      return rng(shape, dtype), constant_vals
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
           axis, "","".join(str(d) for d in base_shape),",No
WORKSPACE,WORKSPACE,31a72d5d9ea2b717ea82575b7a3f72676b71daff,087d0730c5d37fd87b3654a39f3e2bf77147ab4e,"Update XLA release to fix build problem.

Fixes #217.","diff --git a/WORKSPACE b/WORKSPACE
index 5f19906d2..ed24f56c2 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""20f26f90359f7dad7472ebcb3142c5b6166e057ed89442614f9077c9b8d56278"",
-   strip_prefix = ""tensorflow-0ce305b6ce8560945be3a92eae1bcddd60af5e10"",
+   sha256 = ""37138f72ba6db3fcb9b805a8b3432831771be9c0a70f6fded4c12c392c6205e7"",
+   strip_prefix = ""tensorflow-9ad0810fd55096fc86a58300c5a2710b2f3b5175"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/0ce305b6ce8560945be3a92eae1bcddd60af5e10.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/9ad0810fd55096fc86a58300c5a2710b2f3b5175.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 5f19906d2..ed24f56c2 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""20f26f90359f7dad7472ebcb3142c5b6166e057ed89442614f9077c9b8d56278"",
-   strip_prefix = ""tensorflow-0ce305b6ce8560945be3a92eae1bcddd60af5e10"",
+   sha256 = ""37138f72ba6db3fcb9b805a8b3432831771be9c0a70f6fded4c12c392c6205e7"",
+   strip_prefix = ""tensorflow-9ad0810fd55096fc86a58300c5a2710b2f3b5175"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/0ce305b6ce8560945be3a92eae1bcddd60af5e10.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/9ad0810fd55096fc86a58300c5a2710b2f3b5175.tar.gz"",
    ],
 )
 ",No
jax/lax_linalg.py,jax/lax_linalg.py,e21cb1a99faaf47a5f2c85d6cd5184b52973a248,240a775f1db64643848611b38b74ecb1c9bea508,Add complex support for SVD,"diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 24bb9de83..a0fadfc77 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -405,14 +405,14 @@ def svd_abstract_eval(operand, full_matrices, compute_uv):
 def svd_cpu_translation_rule(c, operand, full_matrices, compute_uv):
   shape = c.GetShape(operand)
   dtype = shape.element_type().type
-  if len(shape.dimensions()) == 2 and dtype in {np.float32, np.float64}:
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     out = lapack.jax_gesdd(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
     return c.Tuple(c.GetTupleElement(out, 0),
                    c.GetTupleElement(out, 1),
                    c.GetTupleElement(out, 2))
   else:
     raise NotImplementedError(
-        ""Only unbatched singular value decomposition for real matrices is implemented on CPU"")
+        ""Only unbatched singular value decomposition is implemented on CPU"")
 
 svd_p = Primitive('svd')
 svd_p.def_impl(svd_impl)","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index 24bb9de83..a0fadfc77 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -405,14 +405,14 @@ def svd_abstract_eval(operand, full_matrices, compute_uv):
 def svd_cpu_translation_rule(c, operand, full_matrices, compute_uv):
   shape = c.GetShape(operand)
   dtype = shape.element_type().type
-  if len(shape.dimensions()) == 2 and dtype in {np.float32, np.float64}:
+  if len(shape.dimensions()) == 2 and dtype in _cpu_lapack_types:
     out = lapack.jax_gesdd(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
     return c.Tuple(c.GetTupleElement(out, 0),
                    c.GetTupleElement(out, 1),
                    c.GetTupleElement(out, 2))
   else:
     raise NotImplementedError(
-        ""Only unbatched singular value decomposition for real matrices is implemented on CPU"")
+        ""Only unbatched singular value decomposition is implemented on CPU"")
 
 svd_p = Primitive('svd')
 svd_p.def_impl(svd_impl)",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,e21cb1a99faaf47a5f2c85d6cd5184b52973a248,240a775f1db64643848611b38b74ecb1c9bea508,Add complex support for SVD,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9ce0aacfb..9bcb9be47 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -28,7 +28,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport sgesdd, dgesdd
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
 from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
 
 import numpy as np
@@ -388,6 +388,14 @@ def jax_potrf(c, a, lower=False):
 
 # ?gesdd: Singular value decomposition
 
+cdef int gesdd_iwork_size(int m, int n) nogil:
+  return 8 * min(m, n)
+
+cdef int cgesdd_rwork_size(int m, int n) nogil:
+  cdef int mx = max(m, n)
+  cdef int mn = min(m, n)
+  return max(5 * mn * mn + 5 * mn, 2 * mx * mn + 2 * mn * mn + mn)
+
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
   cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
@@ -401,6 +409,7 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   cdef float* u = <float*>(out[2])
   cdef float* vt = <float*>(out[3])
   cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
 
   if a_out != a_in:
     memcpy(a_out, a_in, m * n * sizeof(float))
@@ -419,8 +428,6 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
-
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
   # because it is officially recommended.
@@ -432,7 +439,6 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   # Now get the actual SVD
   cdef float* work = <float *> malloc(lwork * sizeof(float))
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
-  free(iwork)
   free(work)
 
 register_cpu_custom_call_target(b""lapack_sgesdd"", <void*>(lapack_sgesdd))
@@ -451,6 +457,7 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   cdef double* u = <double*>(out[2])
   cdef double* vt = <double*>(out[3])
   cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
 
   if a_out != a_in:
     memcpy(a_out, a_in, m * n * sizeof(double))
@@ -469,8 +476,6 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
-
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
   # because it is officially recommended.
@@ -482,11 +487,60 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   # Now get the actual SVD
   cdef double* work = <double *> malloc(lwork * sizeof(double))
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
-  free(iwork)
   free(work)
 
 register_cpu_custom_call_target(b""lapack_dgesdd"", <void*>(lapack_dgesdd))
 
+
+cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef float complex* a_in = <float complex*>(data[4])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef float* s = <float*>(out[1])
+  cdef float complex* u = <float complex*>(out[2])
+  cdef float complex* vt = <float complex*>(out[3])
+  cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  cdef float* rwork = <float*>(out[6])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float complex))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_full_matrices == 0:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
+
+  # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array, 
+  # because it is officially recommended.
+  cdef float complex wkopt = 0
+  cdef int lwork = -1
+  cgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, rwork, iwork, info)
+  lwork = <int>(wkopt.real)
+
+  # Now get the actual SVD
+  cdef float complex* work = <float complex*> malloc(lwork * sizeof(float complex))
+  cgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, rwork, iwork, info)
+  free(work)
+
+register_cpu_custom_call_target(b""lapack_cgesdd"", <void*>(lapack_cgesdd))
+
+
 def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -495,8 +549,17 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   m, n = a_shape.dimensions()
   if dtype == np.float32:
     fn = b""lapack_sgesdd""
+    singular_vals_dtype = np.float32
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),)
   elif dtype == np.float64:
     fn = b""lapack_dgesdd""
+    singular_vals_dtype = np.float64
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),)
+  elif dtype == np.complex64:
+    fn = b""lapack_cgesdd""
+    singular_vals_dtype = np.float32
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
+                 Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -506,11 +569,11 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
                 c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (m, n), (0, 1)),
-          Shape.array_shape(dtype, (min(m, n),), (0,)),
+          Shape.array_shape(singular_vals_dtype, (min(m, n),), (0,)),
           Shape.array_shape(dtype, (m, m if full_matrices else min(m, n)), (0, 1)),
           Shape.array_shape(dtype, (n if full_matrices else min(m, n), n), (0, 1)),
-          Shape.array_shape(np.int32, (), ()),
-      )),
+          Shape.array_shape(np.int32, (), ())) + workspace
+      ),
       operand_shapes_with_layout=(
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9ce0aacfb..9bcb9be47 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -28,7 +28,7 @@ from cpython.pycapsule cimport PyCapsule_New
 from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
 from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
 from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport sgesdd, dgesdd
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
 from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
 
 import numpy as np
@@ -388,6 +388,14 @@ def jax_potrf(c, a, lower=False):
 
 # ?gesdd: Singular value decomposition
 
+cdef int gesdd_iwork_size(int m, int n) nogil:
+  return 8 * min(m, n)
+
+cdef int cgesdd_rwork_size(int m, int n) nogil:
+  cdef int mx = max(m, n)
+  cdef int mn = min(m, n)
+  return max(5 * mn * mn + 5 * mn, 2 * mx * mn + 2 * mn * mn + mn)
+
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
   cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
@@ -401,6 +409,7 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   cdef float* u = <float*>(out[2])
   cdef float* vt = <float*>(out[3])
   cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
 
   if a_out != a_in:
     memcpy(a_out, a_in, m * n * sizeof(float))
@@ -419,8 +428,6 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
-
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
   # because it is officially recommended.
@@ -432,7 +439,6 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
   # Now get the actual SVD
   cdef float* work = <float *> malloc(lwork * sizeof(float))
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
-  free(iwork)
   free(work)
 
 register_cpu_custom_call_target(b""lapack_sgesdd"", <void*>(lapack_sgesdd))
@@ -451,6 +457,7 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   cdef double* u = <double*>(out[2])
   cdef double* vt = <double*>(out[3])
   cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
 
   if a_out != a_in:
     memcpy(a_out, a_in, m * n * sizeof(double))
@@ -469,8 +476,6 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   if job_opt_full_matrices == 0:
     ldvt = min(m, n)
 
-  cdef int* iwork = <int *> malloc(8 * min(m, n) * sizeof(int))
-
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
   # because it is officially recommended.
@@ -482,11 +487,60 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
   # Now get the actual SVD
   cdef double* work = <double *> malloc(lwork * sizeof(double))
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, iwork, info)
-  free(iwork)
   free(work)
 
 register_cpu_custom_call_target(b""lapack_dgesdd"", <void*>(lapack_dgesdd))
 
+
+cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef float complex* a_in = <float complex*>(data[4])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef float complex* a_out = <float complex*>(out[0])
+  cdef float* s = <float*>(out[1])
+  cdef float complex* u = <float complex*>(out[2])
+  cdef float complex* vt = <float complex*>(out[3])
+  cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  cdef float* rwork = <float*>(out[6])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(float complex))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_full_matrices == 0:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
+
+  # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array, 
+  # because it is officially recommended.
+  cdef float complex wkopt = 0
+  cdef int lwork = -1
+  cgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, rwork, iwork, info)
+  lwork = <int>(wkopt.real)
+
+  # Now get the actual SVD
+  cdef float complex* work = <float complex*> malloc(lwork * sizeof(float complex))
+  cgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, rwork, iwork, info)
+  free(work)
+
+register_cpu_custom_call_target(b""lapack_cgesdd"", <void*>(lapack_cgesdd))
+
+
 def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -495,8 +549,17 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   m, n = a_shape.dimensions()
   if dtype == np.float32:
     fn = b""lapack_sgesdd""
+    singular_vals_dtype = np.float32
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),)
   elif dtype == np.float64:
     fn = b""lapack_dgesdd""
+    singular_vals_dtype = np.float64
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),)
+  elif dtype == np.complex64:
+    fn = b""lapack_cgesdd""
+    singular_vals_dtype = np.float32
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
+                 Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -506,11 +569,11 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
                 c.ConstantS32Scalar(m), c.ConstantS32Scalar(n), a),
       shape_with_layout=Shape.tuple_shape((
           Shape.array_shape(dtype, (m, n), (0, 1)),
-          Shape.array_shape(dtype, (min(m, n),), (0,)),
+          Shape.array_shape(singular_vals_dtype, (min(m, n),), (0,)),
           Shape.array_shape(dtype, (m, m if full_matrices else min(m, n)), (0, 1)),
           Shape.array_shape(dtype, (n if full_matrices else min(m, n), n), (0, 1)),
-          Shape.array_shape(np.int32, (), ()),
-      )),
+          Shape.array_shape(np.int32, (), ())) + workspace
+      ),
       operand_shapes_with_layout=(
           Shape.array_shape(np.int32, (), ()),
           Shape.array_shape(np.int32, (), ()),",No
tests/linalg_test.py,tests/linalg_test.py,e21cb1a99faaf47a5f2c85d6cd5184b52973a248,240a775f1db64643848611b38b74ecb1c9bea508,Add complex support for SVD,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index ded8d7259..75e76c658 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -141,7 +141,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
        ""compute_uv"": compute_uv, ""rng"": rng}
       for m in [2, 7, 29, 53]
       for n in [2, 7, 29, 53]
-      for dtype in float_types()
+      for dtype in float_types() | complex_types()
       for full_matrices in [False, True]
       for compute_uv in [False, True]
       for rng in [jtu.rand_default()]))
@@ -171,11 +171,11 @@ class NumpyLinalgTest(jtu.JaxTestCase):
           self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
 
       # Check the unitary properties of the singular vector matrices.
-      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(T(out[0]), out[0])) < 5))
+      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 5))
       if m >= n:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(T(out[2]), out[2])) < 5))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(onp.conj(T(out[2])), out[2])) < 5))
       else:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], T(out[2]))) < 5))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 10))
 
     else:
       self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index ded8d7259..75e76c658 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -141,7 +141,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
        ""compute_uv"": compute_uv, ""rng"": rng}
       for m in [2, 7, 29, 53]
       for n in [2, 7, 29, 53]
-      for dtype in float_types()
+      for dtype in float_types() | complex_types()
       for full_matrices in [False, True]
       for compute_uv in [False, True]
       for rng in [jtu.rand_default()]))
@@ -171,11 +171,11 @@ class NumpyLinalgTest(jtu.JaxTestCase):
           self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
 
       # Check the unitary properties of the singular vector matrices.
-      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(T(out[0]), out[0])) < 5))
+      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 5))
       if m >= n:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(T(out[2]), out[2])) < 5))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(onp.conj(T(out[2])), out[2])) < 5))
       else:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], T(out[2]))) < 5))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 10))
 
     else:
       self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,20332b260402ad8ff3f69e2c18c9288e03b6eb70,e21cb1a99faaf47a5f2c85d6cd5184b52973a248,LAPACK 3.6 and below requires more workspace that the newer versions,"diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9bcb9be47..4e627682f 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -391,9 +391,11 @@ def jax_potrf(c, a, lower=False):
 cdef int gesdd_iwork_size(int m, int n) nogil:
   return 8 * min(m, n)
 
-cdef int cgesdd_rwork_size(int m, int n) nogil:
-  cdef int mx = max(m, n)
+cdef int cgesdd_rwork_size(int m, int n, int compute_uv) nogil:
   cdef int mn = min(m, n)
+  if compute_uv == 0:
+    return 7 * mn
+  cdef int mx = max(m, n)
   return max(5 * mn * mn + 5 * mn, 2 * mx * mn + 2 * mn * mn + mn)
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
@@ -430,7 +432,7 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
-  # because it is officially recommended.
+  # because it is officially recommended in the LAPACK documentation
   cdef float wkopt = 0
   cdef int lwork = -1
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
@@ -478,7 +480,7 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
-  # because it is officially recommended.
+  # because it is officially recommended in the LAPACK documentation
   cdef double wkopt = 0
   cdef int lwork = -1
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
@@ -527,7 +529,7 @@ cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
-  # because it is officially recommended.
+  # because it is officially recommended in the LAPACK documentation
   cdef float complex wkopt = 0
   cdef int lwork = -1
   cgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, rwork, iwork, info)
@@ -559,7 +561,7 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
     fn = b""lapack_cgesdd""
     singular_vals_dtype = np.float32
     workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
-                 Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n),), (0,)))
+                 Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n, int(compute_uv)),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 9bcb9be47..4e627682f 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -391,9 +391,11 @@ def jax_potrf(c, a, lower=False):
 cdef int gesdd_iwork_size(int m, int n) nogil:
   return 8 * min(m, n)
 
-cdef int cgesdd_rwork_size(int m, int n) nogil:
-  cdef int mx = max(m, n)
+cdef int cgesdd_rwork_size(int m, int n, int compute_uv) nogil:
   cdef int mn = min(m, n)
+  if compute_uv == 0:
+    return 7 * mn
+  cdef int mx = max(m, n)
   return max(5 * mn * mn + 5 * mn, 2 * mx * mn + 2 * mn * mn + mn)
 
 cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
@@ -430,7 +432,7 @@ cdef void lapack_sgesdd(void* out_tuple, void** data) nogil:
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
-  # because it is officially recommended.
+  # because it is officially recommended in the LAPACK documentation
   cdef float wkopt = 0
   cdef int lwork = -1
   sgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
@@ -478,7 +480,7 @@ cdef void lapack_dgesdd(void* out_tuple, void** data) nogil:
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
-  # because it is officially recommended.
+  # because it is officially recommended in the LAPACK documentation
   cdef double wkopt = 0
   cdef int lwork = -1
   dgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, iwork, info)
@@ -527,7 +529,7 @@ cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
 
   # First perform a workspace query to get the optimal lwork
   # NB: We perform a workspace query with malloc and free for the work array, 
-  # because it is officially recommended.
+  # because it is officially recommended in the LAPACK documentation
   cdef float complex wkopt = 0
   cdef int lwork = -1
   cgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, rwork, iwork, info)
@@ -559,7 +561,7 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
     fn = b""lapack_cgesdd""
     singular_vals_dtype = np.float32
     workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
-                 Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n),), (0,)))
+                 Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n, int(compute_uv)),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ",No
jax/lax.py,jax/lax.py,65efd45fc9c24362b466a1952eeb0aa16a7b07d0,240a775f1db64643848611b38b74ecb1c9bea508,"Test more Numpy ops for complex types.

Fix a number of ops that did not handle complex numbers the same way as regular numpy.","diff --git a/jax/lax.py b/jax/lax.py
index 85c432542..228079de8 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -17,15 +17,18 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-from .util import partial, prod
 import itertools
 import operator
+import string
+import warnings
+
 import six
 from six.moves import builtins, xrange
-import string
 
 import numpy as onp
 
+from .util import partial, prod
+
 from . import core
 from . import ad_util
 from . import linear_util as lu
@@ -108,6 +111,11 @@ def convert_element_type(operand, new_dtype):
   new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
   old_dtype = _dtype(operand)
   if old_dtype != new_dtype:
+    if (onp.issubdtype(old_dtype, onp.complexfloating) and
+        not onp.issubdtype(new_dtype, onp.complexfloating)):
+      msg = ""Casting complex values to real discards the imaginary part""
+      warnings.warn(msg, onp.ComplexWarning)
+      operand = real(operand)
     return convert_element_type_p.bind(
         operand, new_dtype=new_dtype, old_dtype=old_dtype)
   else:","diff --git a/jax/lax.py b/jax/lax.py
index 85c432542..228079de8 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -17,15 +17,18 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-from .util import partial, prod
 import itertools
 import operator
+import string
+import warnings
+
 import six
 from six.moves import builtins, xrange
-import string
 
 import numpy as onp
 
+from .util import partial, prod
+
 from . import core
 from . import ad_util
 from . import linear_util as lu
@@ -108,6 +111,11 @@ def convert_element_type(operand, new_dtype):
   new_dtype = xla_bridge.canonicalize_dtype(new_dtype)
   old_dtype = _dtype(operand)
   if old_dtype != new_dtype:
+    if (onp.issubdtype(old_dtype, onp.complexfloating) and
+        not onp.issubdtype(new_dtype, onp.complexfloating)):
+      msg = ""Casting complex values to real discards the imaginary part""
+      warnings.warn(msg, onp.ComplexWarning)
+      operand = real(operand)
     return convert_element_type_p.bind(
         operand, new_dtype=new_dtype, old_dtype=old_dtype)
   else:",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,65efd45fc9c24362b466a1952eeb0aa16a7b07d0,240a775f1db64643848611b38b74ecb1c9bea508,"Test more Numpy ops for complex types.

Fix a number of ops that did not handle complex numbers the same way as regular numpy.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a7c2c63d1..a778b72ed 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -118,6 +118,8 @@ finfo = onp.finfo
 
 issubdtype = onp.issubdtype
 
+ComplexWarning = onp.ComplexWarning
+
 ### utility functions
 
 
@@ -229,8 +231,8 @@ def _one_to_one_binop(numpy_fn, lax_fn, promote_like=False):
     fn = lambda x, y: lax_fn(*_promote_args(numpy_fn.__name__, x, y))
   return _wraps(numpy_fn)(fn)
 
-
 absolute = abs = _one_to_one_unop(onp.absolute, lax.abs)
+fabs = _one_to_one_unop(onp.fabs, lax.abs, True)
 bitwise_not = _one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
 negative = _one_to_one_unop(onp.negative, lax.neg)
 sort = _one_to_one_unop(onp.sort, lax.sort)
@@ -262,12 +264,6 @@ bitwise_xor = _one_to_one_binop(onp.bitwise_xor, lax.bitwise_xor)
 right_shift = _one_to_one_binop(onp.right_shift, lax.shift_right_arithmetic)
 left_shift = _one_to_one_binop(onp.left_shift, lax.shift_left)
 equal = _one_to_one_binop(onp.equal, lax.eq)
-greater_equal = _one_to_one_binop(onp.greater_equal, lax.ge)
-greater = _one_to_one_binop(onp.greater, lax.gt)
-less_equal = _one_to_one_binop(onp.less_equal, lax.le)
-less = _one_to_one_binop(onp.less, lax.lt)
-maximum = _one_to_one_binop(onp.maximum, lax.max)
-minimum = _one_to_one_binop(onp.minimum, lax.min)
 multiply = _one_to_one_binop(onp.multiply, lax.mul)
 not_equal = _one_to_one_binop(onp.not_equal, lax.ne)
 subtract = _one_to_one_binop(onp.subtract, lax.sub)
@@ -275,6 +271,43 @@ power = _one_to_one_binop(onp.power, lax.pow, True)
 arctan2 = _one_to_one_binop(onp.arctan2, lax.atan2, True)
 
 
+def _comparison_op(numpy_fn, lax_fn):
+  def fn(x, y):
+    x, y =  _promote_args(numpy_fn.__name__, x, y)
+    # Comparison on complex types are defined as a lexicographic ordering on
+    # the (real, imag) pair.
+    if issubdtype(_dtype(x), complexfloating):
+      rx = lax.real(x)
+      ry = lax.real(y)
+      return lax.select(lax.eq(rx, ry), lax_fn(lax.imag(x), lax.imag(y)),
+                        lax_fn(rx, ry))
+    return lax_fn(x, y)
+  return _wraps(numpy_fn)(fn)
+
+greater_equal = _comparison_op(onp.greater_equal, lax.ge)
+greater = _comparison_op(onp.greater, lax.gt)
+less_equal = _comparison_op(onp.less_equal, lax.le)
+less = _comparison_op(onp.less, lax.lt)
+
+def _minmax_op(numpy_fn, lax_fn, lax_cmp_fn):
+  def fn(x, y):
+    x, y =  _promote_args(numpy_fn.__name__, x, y)
+    # Comparison on complex types are defined as a lexicographic ordering on
+    # the (real, imag) pair.
+    if issubdtype(_dtype(x), complexfloating):
+      rx = lax.real(x)
+      ry = lax.real(y)
+      return where(
+          lax.select(lax.eq(rx, ry), lax_cmp_fn(lax.imag(x), lax.imag(y)),
+                     lax_cmp_fn(rx, ry)),
+          x, y)
+    return lax_fn(x, y)
+  return _wraps(numpy_fn)(fn)
+
+maximum = _minmax_op(onp.maximum, lax.max, lax.gt)
+minimum = _minmax_op(onp.minimum, lax.min, lax.lt)
+
+
 def _logical_op(np_op, bitwise_op):
   @_wraps(np_op)
   def op(*args):
@@ -312,11 +345,23 @@ def divide(x1, x2):
 @_wraps(onp.floor_divide)
 def floor_divide(x1, x2):
   x1, x2 = _promote_args(""floor_divide"", x1, x2)
-  if onp.issubdtype(_dtype(x1), onp.integer):
+  dtype = _dtype(x1)
+  if issubdtype(dtype, integer):
     quotient = lax.div(x1, x2)
     select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
     # TODO(mattjj): investigate why subtracting a scalar was causing promotion
     return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
+  elif issubdtype(dtype, complexfloating):
+    x1r = lax.real(x1)
+    x1i = lax.imag(x1)
+    x2r = lax.real(x2)
+    x2i = lax.imag(x2)
+    which = lax.ge(lax.abs(x2r), lax.abs(x2i))
+    rat1 = where(which, lax._const(x2i, 1), lax.div(x2r, x2i))
+    rat2 = where(which, lax.div(x2i, x2r), lax._const(x2i, 1))
+    out = lax.floor(lax.div(lax.add(lax.mul(x1r, rat1), lax.mul(x1i, rat2)),
+                            lax.add(lax.mul(x2r, rat1), lax.mul(x2i, rat2))))
+    return lax.convert_element_type(out, dtype)
   else:
     return _float_divmod(x1, x2)[0]
 
@@ -514,11 +559,18 @@ def moveaxis(a, source, destination):
 
 @_wraps(onp.isclose)
 def isclose(a, b, rtol=1e-05, atol=1e-08):
-  a, b = _promote_args(""isclose"", a, b)
-  rtol = lax.convert_element_type(rtol, _dtype(a))
-  atol = lax.convert_element_type(atol, _dtype(a))
-  return lax.le(lax.abs(lax.sub(a, b)),
-                lax.add(atol, lax.mul(rtol, lax.abs(b))))
+  a, b = _promote_args(""isclose"", asarray(a), asarray(b))
+  dtype = _dtype(a)
+  if issubdtype(dtype, inexact):
+    if issubdtype(dtype, complexfloating):
+      dtype = _result_dtype(real, a)
+    rtol = lax.convert_element_type(rtol, dtype)
+    atol = lax.convert_element_type(atol, dtype)
+    return lax.le(
+      lax.abs(lax.sub(a, b)),
+      lax.add(atol, lax.mul(rtol, lax.abs(b))))
+  else:
+    return lax.eq(a, b)
 
 
 @_wraps(onp.where)
@@ -585,7 +637,10 @@ def clip(a, a_min=None, a_max=None):
     a_min = lax.convert_element_type(a_min, _dtype(a))
   if _dtype(a_max) != _dtype(a):
     a_max = lax.convert_element_type(a_max, _dtype(a))
-  return lax.clamp(a_min, a, a_max)
+  if issubdtype(_dtype(a), complexfloating):
+    return minimum(maximum(a_min, a), a_max)
+  else:
+    return lax.clamp(a_min, a, a_max)
 
 
 def _dtype_info(dtype):
@@ -597,14 +652,24 @@ def _dtype_info(dtype):
 
 @_wraps(onp.round)
 def round(a, decimals=0):
-  if onp.issubdtype(_dtype(a), onp.integer):
+  dtype = _dtype(a)
+  if issubdtype(dtype, integer):
+    if decimals < 0:
+      raise NotImplementedError(
+        ""integer np.round not implemented for decimals < 0"")
     return a  # no-op on integer types
 
-  if decimals == 0:
-    return lax.round(a)
+  def _round_float(x):
+    if decimals == 0:
+      return lax.round(x)
+
+    factor = _constant_like(x, 10 ** decimals)
+    return lax.div(lax.round(lax.mul(x, factor)), factor)
 
-  factor = _constant_like(a, 10 ** decimals)
-  return lax.div(lax.round(lax.mul(a, factor)), factor)
+  if issubdtype(dtype, complexfloating):
+    return lax.complex(_round_float(lax.real(a)), _round_float(lax.imag(a)))
+  else:
+    return _round_float(a)
 around = round
 
 
@@ -712,8 +777,8 @@ _cast_to_bool = partial(lax.convert_element_type, new_dtype=onp.bool_)
 
 sum = _make_reduction(onp.sum, lax.add, 0)
 prod = _make_reduction(onp.prod, lax.mul, 1)
-max = _make_reduction(onp.max, lax.max, -onp.inf)
-min = _make_reduction(onp.min, lax.min, onp.inf)
+amax = max = _make_reduction(onp.max, maximum, -onp.inf)
+amin = min = _make_reduction(onp.min, minimum, onp.inf)
 all = alltrue = _make_reduction(onp.all, lax.bitwise_and, True, _cast_to_bool)
 any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a7c2c63d1..a778b72ed 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -118,6 +118,8 @@ finfo = onp.finfo
 
 issubdtype = onp.issubdtype
 
+ComplexWarning = onp.ComplexWarning
+
 ### utility functions
 
 
@@ -229,8 +231,8 @@ def _one_to_one_binop(numpy_fn, lax_fn, promote_like=False):
     fn = lambda x, y: lax_fn(*_promote_args(numpy_fn.__name__, x, y))
   return _wraps(numpy_fn)(fn)
 
-
 absolute = abs = _one_to_one_unop(onp.absolute, lax.abs)
+fabs = _one_to_one_unop(onp.fabs, lax.abs, True)
 bitwise_not = _one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
 negative = _one_to_one_unop(onp.negative, lax.neg)
 sort = _one_to_one_unop(onp.sort, lax.sort)
@@ -262,12 +264,6 @@ bitwise_xor = _one_to_one_binop(onp.bitwise_xor, lax.bitwise_xor)
 right_shift = _one_to_one_binop(onp.right_shift, lax.shift_right_arithmetic)
 left_shift = _one_to_one_binop(onp.left_shift, lax.shift_left)
 equal = _one_to_one_binop(onp.equal, lax.eq)
-greater_equal = _one_to_one_binop(onp.greater_equal, lax.ge)
-greater = _one_to_one_binop(onp.greater, lax.gt)
-less_equal = _one_to_one_binop(onp.less_equal, lax.le)
-less = _one_to_one_binop(onp.less, lax.lt)
-maximum = _one_to_one_binop(onp.maximum, lax.max)
-minimum = _one_to_one_binop(onp.minimum, lax.min)
 multiply = _one_to_one_binop(onp.multiply, lax.mul)
 not_equal = _one_to_one_binop(onp.not_equal, lax.ne)
 subtract = _one_to_one_binop(onp.subtract, lax.sub)
@@ -275,6 +271,43 @@ power = _one_to_one_binop(onp.power, lax.pow, True)
 arctan2 = _one_to_one_binop(onp.arctan2, lax.atan2, True)
 
 
+def _comparison_op(numpy_fn, lax_fn):
+  def fn(x, y):
+    x, y =  _promote_args(numpy_fn.__name__, x, y)
+    # Comparison on complex types are defined as a lexicographic ordering on
+    # the (real, imag) pair.
+    if issubdtype(_dtype(x), complexfloating):
+      rx = lax.real(x)
+      ry = lax.real(y)
+      return lax.select(lax.eq(rx, ry), lax_fn(lax.imag(x), lax.imag(y)),
+                        lax_fn(rx, ry))
+    return lax_fn(x, y)
+  return _wraps(numpy_fn)(fn)
+
+greater_equal = _comparison_op(onp.greater_equal, lax.ge)
+greater = _comparison_op(onp.greater, lax.gt)
+less_equal = _comparison_op(onp.less_equal, lax.le)
+less = _comparison_op(onp.less, lax.lt)
+
+def _minmax_op(numpy_fn, lax_fn, lax_cmp_fn):
+  def fn(x, y):
+    x, y =  _promote_args(numpy_fn.__name__, x, y)
+    # Comparison on complex types are defined as a lexicographic ordering on
+    # the (real, imag) pair.
+    if issubdtype(_dtype(x), complexfloating):
+      rx = lax.real(x)
+      ry = lax.real(y)
+      return where(
+          lax.select(lax.eq(rx, ry), lax_cmp_fn(lax.imag(x), lax.imag(y)),
+                     lax_cmp_fn(rx, ry)),
+          x, y)
+    return lax_fn(x, y)
+  return _wraps(numpy_fn)(fn)
+
+maximum = _minmax_op(onp.maximum, lax.max, lax.gt)
+minimum = _minmax_op(onp.minimum, lax.min, lax.lt)
+
+
 def _logical_op(np_op, bitwise_op):
   @_wraps(np_op)
   def op(*args):
@@ -312,11 +345,23 @@ def divide(x1, x2):
 @_wraps(onp.floor_divide)
 def floor_divide(x1, x2):
   x1, x2 = _promote_args(""floor_divide"", x1, x2)
-  if onp.issubdtype(_dtype(x1), onp.integer):
+  dtype = _dtype(x1)
+  if issubdtype(dtype, integer):
     quotient = lax.div(x1, x2)
     select = logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
     # TODO(mattjj): investigate why subtracting a scalar was causing promotion
     return where(select, quotient - onp.array(1, _dtype(quotient)), quotient)
+  elif issubdtype(dtype, complexfloating):
+    x1r = lax.real(x1)
+    x1i = lax.imag(x1)
+    x2r = lax.real(x2)
+    x2i = lax.imag(x2)
+    which = lax.ge(lax.abs(x2r), lax.abs(x2i))
+    rat1 = where(which, lax._const(x2i, 1), lax.div(x2r, x2i))
+    rat2 = where(which, lax.div(x2i, x2r), lax._const(x2i, 1))
+    out = lax.floor(lax.div(lax.add(lax.mul(x1r, rat1), lax.mul(x1i, rat2)),
+                            lax.add(lax.mul(x2r, rat1), lax.mul(x2i, rat2))))
+    return lax.convert_element_type(out, dtype)
   else:
     return _float_divmod(x1, x2)[0]
 
@@ -514,11 +559,18 @@ def moveaxis(a, source, destination):
 
 @_wraps(onp.isclose)
 def isclose(a, b, rtol=1e-05, atol=1e-08):
-  a, b = _promote_args(""isclose"", a, b)
-  rtol = lax.convert_element_type(rtol, _dtype(a))
-  atol = lax.convert_element_type(atol, _dtype(a))
-  return lax.le(lax.abs(lax.sub(a, b)),
-                lax.add(atol, lax.mul(rtol, lax.abs(b))))
+  a, b = _promote_args(""isclose"", asarray(a), asarray(b))
+  dtype = _dtype(a)
+  if issubdtype(dtype, inexact):
+    if issubdtype(dtype, complexfloating):
+      dtype = _result_dtype(real, a)
+    rtol = lax.convert_element_type(rtol, dtype)
+    atol = lax.convert_element_type(atol, dtype)
+    return lax.le(
+      lax.abs(lax.sub(a, b)),
+      lax.add(atol, lax.mul(rtol, lax.abs(b))))
+  else:
+    return lax.eq(a, b)
 
 
 @_wraps(onp.where)
@@ -585,7 +637,10 @@ def clip(a, a_min=None, a_max=None):
     a_min = lax.convert_element_type(a_min, _dtype(a))
   if _dtype(a_max) != _dtype(a):
     a_max = lax.convert_element_type(a_max, _dtype(a))
-  return lax.clamp(a_min, a, a_max)
+  if issubdtype(_dtype(a), complexfloating):
+    return minimum(maximum(a_min, a), a_max)
+  else:
+    return lax.clamp(a_min, a, a_max)
 
 
 def _dtype_info(dtype):
@@ -597,14 +652,24 @@ def _dtype_info(dtype):
 
 @_wraps(onp.round)
 def round(a, decimals=0):
-  if onp.issubdtype(_dtype(a), onp.integer):
+  dtype = _dtype(a)
+  if issubdtype(dtype, integer):
+    if decimals < 0:
+      raise NotImplementedError(
+        ""integer np.round not implemented for decimals < 0"")
     return a  # no-op on integer types
 
-  if decimals == 0:
-    return lax.round(a)
+  def _round_float(x):
+    if decimals == 0:
+      return lax.round(x)
 
-  factor = _constant_like(a, 10 ** decimals)
-  return lax.div(lax.round(lax.mul(a, factor)), factor)
+    factor = _constant_like(x, 10 ** decimals)
+    return lax.div(lax.round(lax.mul(x, factor)), factor)
+
+  if issubdtype(dtype, complexfloating):
+    return lax.complex(_round_float(lax.real(a)), _round_float(lax.imag(a)))
+  else:
+    return _round_float(a)
 around = round
 
 
@@ -712,8 +777,8 @@ _cast_to_bool = partial(lax.convert_element_type, new_dtype=onp.bool_)
 
 sum = _make_reduction(onp.sum, lax.add, 0)
 prod = _make_reduction(onp.prod, lax.mul, 1)
-max = _make_reduction(onp.max, lax.max, -onp.inf)
-min = _make_reduction(onp.min, lax.min, onp.inf)
+amax = max = _make_reduction(onp.max, maximum, -onp.inf)
+amin = min = _make_reduction(onp.min, minimum, onp.inf)
 all = alltrue = _make_reduction(onp.all, lax.bitwise_and, True, _cast_to_bool)
 any = sometrue = _make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
 ",Yes
tests/lax_numpy_test.py,tests/lax_numpy_test.py,65efd45fc9c24362b466a1952eeb0aa16a7b07d0,240a775f1db64643848611b38b74ecb1c9bea508,"Test more Numpy ops for complex types.

Fix a number of ops that did not handle complex numbers the same way as regular numpy.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 7f04c5add..0200985ab 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -50,8 +50,9 @@ int_dtypes = [onp.int32, onp.int64]
 unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
-numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
-all_dtypes = numeric_dtypes + bool_dtypes
+inexact_dtypes = float_dtypes + complex_dtypes
+number_dtypes = float_dtypes + complex_dtypes + int_dtypes
+all_dtypes = number_dtypes + bool_dtypes
 
 OpRecord = collections.namedtuple(
   ""OpRecord"",
@@ -63,70 +64,71 @@ def op_record(name, nargs, dtypes, shapes, rng, diff_modes, test_name=None):
   return OpRecord(name, nargs, dtypes, shapes, rng, diff_modes, test_name)
 
 JAX_ONE_TO_ONE_OP_RECORDS = [
-    op_record(""abs"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""add"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""abs"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""ceil"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conj"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, all_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""exp"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""fabs"", 1, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""isfinite"", 1, numeric_dtypes, all_shapes, jtu.rand_some_inf(), []),
-    op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""logical_and"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""logical_not"", 1, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""logical_or"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""logical_xor"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""maximum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
-    op_record(""minimum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
-    op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
-    op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tan"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""sinh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""cosh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""arcsin"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arccos"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arctan"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arctan2"", 2, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arcsinh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arccosh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arctanh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""greater"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""isfinite"", 1, number_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""less"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, number_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, number_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
+    op_record(""power"", 2, inexact_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tan"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sinh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cosh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""arcsin"", 1, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccos"", 1, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan"", 1, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan2"", 2, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arcsinh"", 1, number_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccosh"", 1, number_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctanh"", 1, number_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""exp2"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
+    op_record(""divide"", 2, number_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""exp2"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""expm1"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
-    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""kron"", 2, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""outer"", 2, default_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""log10"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
+    op_record(""expm1"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, number_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""kron"", 2, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""outer"", 2, number_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""isclose"", 2, all_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log2"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""log10"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""log1p"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
-    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""polyval"", 2, numeric_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
-    op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""polyval"", 2, number_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
+    op_record(""ravel"", 1, all_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
-    op_record(""square"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""square"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, all_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, all_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
     op_record(""where"", 3, (onp.float32, onp.int64), all_shapes, jtu.rand_some_zero(), []),
 ]
 
@@ -142,23 +144,24 @@ JAX_BITWISE_OP_RECORDS = [
 ]
 
 JAX_REDUCER_RECORDS = [
-    op_record(""mean"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""mean"", 1, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""prod"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, number_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""var"", 1, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    # TODO(phawkins): test inexact_dtypes for std after a jaxlib release.
     op_record(""std"", 1, float_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [
-    op_record(""all"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
-    op_record(""any"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
-    op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""all"", 1, all_dtypes, all_shapes, jtu.rand_some_zero(), []),
+    op_record(""any"", 1, all_dtypes, all_shapes, jtu.rand_some_zero(), []),
+    op_record(""max"", 1, all_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""min"", 1, all_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
-    op_record(""argmin"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
-    op_record(""argmax"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmin"", 1, all_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, all_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
 ]
 
 CombosWithReplacement = itertools.combinations_with_replacement
@@ -326,7 +329,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (4, 3, 2), (2, 5)),
           (""matrix-tensor"", (5, 2), (3, 2, 4)),
           (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)))
   def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
@@ -352,7 +355,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (5, 2, 3), (3, 2)),
           (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
           (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)))
   def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
@@ -374,7 +377,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           [(2, 3, 4), (5, 4, 3, 6), [[1, 2], [2, 1]]],
           [(1, 2, 3, 4), (4, 5, 3, 6), [[2, 3], [2, 0]]],
       ]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)))
   def testTensordot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, axes, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     lnp_fun = lambda a, b: lnp.tensordot(a, b, axes)
@@ -390,7 +393,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
        ""rng"": jtu.rand_default()}
       # TODO(phawkins): support integer dtypes too.
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)
       for lhs_shape, rhs_shape in [
         (l, r) for l, r in CombosWithReplacement(all_shapes, 2)
         if len(jtu._dims_of_shape(l)) == 0
@@ -408,7 +411,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
        ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
        ""rng"": jtu.rand_default()}
-      for shape in all_shapes for dtype in float_dtypes
+      for shape in all_shapes for dtype in number_dtypes
       for a_min, a_max in [(-1, None), (None, 1), (-1, 1)]))
   def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
     onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
@@ -422,9 +425,11 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, dtype), decimals),
        ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
        ""rng"": jtu.rand_default()}
-      for shape in all_shapes for dtype in float_dtypes
+      for shape in all_shapes for dtype in number_dtypes
       for decimals in [0, 1, -2]))
   def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
+    if onp.issubdtype(dtype, onp.integer) and decimals < 0:
+      self.skipTest(""Integer rounding with decimals < 0 not implemented"")
     onp_fun = lambda x: onp.round(x, decimals=decimals)
     lnp_fun = lambda x: lnp.round(x, decimals=decimals)
     args_maker = lambda: [rng(shape, dtype)]
@@ -604,7 +609,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""dtype"": dtype, ""out_dtype"": out_dtype, ""shape"": shape, ""offset"": offset,
        ""axis1"": axis1, ""axis2"": axis2, ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
-      for out_dtype in [None] + default_dtypes
+      for out_dtype in [None] + number_dtypes
       for shape in [shape for shape in all_shapes if len(shape) >= 2]
       for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
       for offset in list(range(-4, 4))))","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 7f04c5add..0200985ab 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -50,8 +50,9 @@ int_dtypes = [onp.int32, onp.int64]
 unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes
-numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
-all_dtypes = numeric_dtypes + bool_dtypes
+inexact_dtypes = float_dtypes + complex_dtypes
+number_dtypes = float_dtypes + complex_dtypes + int_dtypes
+all_dtypes = number_dtypes + bool_dtypes
 
 OpRecord = collections.namedtuple(
   ""OpRecord"",
@@ -63,70 +64,71 @@ def op_record(name, nargs, dtypes, shapes, rng, diff_modes, test_name=None):
   return OpRecord(name, nargs, dtypes, shapes, rng, diff_modes, test_name)
 
 JAX_ONE_TO_ONE_OP_RECORDS = [
-    op_record(""abs"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""add"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""abs"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""ceil"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""conj"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""conjugate"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""exp"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conj"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, all_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""exp"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""fabs"", 1, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""floor"", 1, float_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""greater"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""greater_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""isfinite"", 1, numeric_dtypes, all_shapes, jtu.rand_some_inf(), []),
-    op_record(""less"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""less_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), []),
-    op_record(""log"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""logical_and"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""logical_not"", 1, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""logical_or"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""logical_xor"", 2, default_dtypes, all_shapes, jtu.rand_bool(), []),
-    op_record(""maximum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
-    op_record(""minimum"", 2, default_dtypes, all_shapes, jtu.rand_some_inf(), []),
-    op_record(""multiply"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""negative"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""not_equal"", 2, default_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
-    op_record(""power"", 2, float_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""subtract"", 2, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""sin"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""cos"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tan"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""sinh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""cosh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tanh"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""arcsin"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arccos"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arctan"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arctan2"", 2, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arcsinh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arccosh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
-    op_record(""arctanh"", 1, default_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""greater"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""isfinite"", 1, number_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""less"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, all_dtypes, all_shapes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, number_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, number_dtypes, all_shapes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, number_dtypes, all_shapes, jtu.rand_some_equal(), [""rev""]),
+    op_record(""power"", 2, inexact_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tan"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sinh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""cosh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""arcsin"", 1, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccos"", 1, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan"", 1, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctan2"", 2, float_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arcsinh"", 1, number_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arccosh"", 1, number_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
+    op_record(""arctanh"", 1, number_dtypes, all_shapes, jtu.rand_small(), [""rev""]),
 ]
 
 JAX_COMPOUND_OP_RECORDS = [
-    op_record(""divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""exp2"", 1, numeric_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
+    op_record(""divide"", 2, number_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""exp2"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""expm1"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""expm1_large""),
-    op_record(""expm1"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""floor_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
-    op_record(""kron"", 2, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""outer"", 2, default_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""isclose"", 2, float_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""log2"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""log10"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_positive(), [],
+    op_record(""expm1"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, number_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""kron"", 2, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""outer"", 2, number_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""isclose"", 2, all_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log2"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""log10"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""log1p"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [],
               test_name=""log1p_large""),
-    op_record(""log1p"", 1, numeric_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""logaddexp"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""logaddexp2"", 2, float_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""polyval"", 2, numeric_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
-    op_record(""ravel"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""polyval"", 2, number_dtypes, nonempty_nonscalar_array_shapes, jtu.rand_default(), []),
+    op_record(""ravel"", 1, all_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""remainder"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), []),
-    op_record(""square"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""sqrt"", 1, default_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
-    op_record(""transpose"", 1, default_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""true_divide"", 2, default_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""square"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, number_dtypes, all_shapes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, all_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, all_dtypes, all_shapes, jtu.rand_nonzero(), [""rev""]),
     op_record(""where"", 3, (onp.float32, onp.int64), all_shapes, jtu.rand_some_zero(), []),
 ]
 
@@ -142,23 +144,24 @@ JAX_BITWISE_OP_RECORDS = [
 ]
 
 JAX_REDUCER_RECORDS = [
-    op_record(""mean"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""prod"", 1, default_dtypes, all_shapes, jtu.rand_small_positive(), []),
-    op_record(""sum"", 1, default_dtypes, all_shapes, jtu.rand_default(), []),
-    op_record(""var"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""mean"", 1, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""prod"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, number_dtypes, all_shapes, jtu.rand_default(), []),
+    op_record(""var"", 1, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    # TODO(phawkins): test inexact_dtypes for std after a jaxlib release.
     op_record(""std"", 1, float_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [
-    op_record(""all"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
-    op_record(""any"", 1, default_dtypes + bool_dtypes, all_shapes, jtu.rand_some_zero(), []),
-    op_record(""max"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    op_record(""min"", 1, default_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""all"", 1, all_dtypes, all_shapes, jtu.rand_some_zero(), []),
+    op_record(""any"", 1, all_dtypes, all_shapes, jtu.rand_some_zero(), []),
+    op_record(""max"", 1, all_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""min"", 1, all_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_ARGMINMAX_RECORDS = [
-    op_record(""argmin"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
-    op_record(""argmax"", 1, default_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmin"", 1, all_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, all_dtypes, nonempty_shapes, jtu.rand_some_equal(), []),
 ]
 
 CombosWithReplacement = itertools.combinations_with_replacement
@@ -326,7 +329,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (4, 3, 2), (2, 5)),
           (""matrix-tensor"", (5, 2), (3, 2, 4)),
           (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)))
   def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
@@ -352,7 +355,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           (""tensor-matrix"", (5, 2, 3), (3, 2)),
           (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
           (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)))
   def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
@@ -374,7 +377,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           [(2, 3, 4), (5, 4, 3, 6), [[1, 2], [2, 1]]],
           [(1, 2, 3, 4), (4, 5, 3, 6), [[2, 3], [2, 0]]],
       ]
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)))
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)))
   def testTensordot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, axes, rng):
     args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
     lnp_fun = lambda a, b: lnp.tensordot(a, b, axes)
@@ -390,7 +393,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
        ""rng"": jtu.rand_default()}
       # TODO(phawkins): support integer dtypes too.
-      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2)
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(inexact_dtypes, 2)
       for lhs_shape, rhs_shape in [
         (l, r) for l, r in CombosWithReplacement(all_shapes, 2)
         if len(jtu._dims_of_shape(l)) == 0
@@ -408,7 +411,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
        ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
        ""rng"": jtu.rand_default()}
-      for shape in all_shapes for dtype in float_dtypes
+      for shape in all_shapes for dtype in number_dtypes
       for a_min, a_max in [(-1, None), (None, 1), (-1, 1)]))
   def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
     onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
@@ -422,9 +425,11 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
           jtu.format_shape_dtype_string(shape, dtype), decimals),
        ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
        ""rng"": jtu.rand_default()}
-      for shape in all_shapes for dtype in float_dtypes
+      for shape in all_shapes for dtype in number_dtypes
       for decimals in [0, 1, -2]))
   def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
+    if onp.issubdtype(dtype, onp.integer) and decimals < 0:
+      self.skipTest(""Integer rounding with decimals < 0 not implemented"")
     onp_fun = lambda x: onp.round(x, decimals=decimals)
     lnp_fun = lambda x: lnp.round(x, decimals=decimals)
     args_maker = lambda: [rng(shape, dtype)]
@@ -604,7 +609,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
        ""dtype"": dtype, ""out_dtype"": out_dtype, ""shape"": shape, ""offset"": offset,
        ""axis1"": axis1, ""axis2"": axis2, ""rng"": jtu.rand_default()}
       for dtype in default_dtypes
-      for out_dtype in [None] + default_dtypes
+      for out_dtype in [None] + number_dtypes
       for shape in [shape for shape in all_shapes if len(shape) >= 2]
       for (axis1, axis2) in itertools.combinations(range(len(shape)), 2)
       for offset in list(range(-4, 4))))",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,4a8f088ac929d487473e117ad3eb1749beed17eb,65efd45fc9c24362b466a1952eeb0aa16a7b07d0,Simplify and fix test failures of `np.clip` for cases where one of the inputs was None.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a778b72ed..ef34e02f4 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -631,16 +631,17 @@ def split(ary, indices_or_sections, axis=0):
 
 @_wraps(onp.clip)
 def clip(a, a_min=None, a_max=None):
-  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
-  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
-  if _dtype(a_min) != _dtype(a):
-    a_min = lax.convert_element_type(a_min, _dtype(a))
-  if _dtype(a_max) != _dtype(a):
-    a_max = lax.convert_element_type(a_max, _dtype(a))
-  if issubdtype(_dtype(a), complexfloating):
-    return minimum(maximum(a_min, a), a_max)
-  else:
-    return lax.clamp(a_min, a, a_max)
+  if a_min is None and a_max is None:
+    raise ""At most one of a_min and a_max may be None""
+  if a_min is not None:
+    if _dtype(a_min) != _dtype(a):
+      a_min = lax.convert_element_type(a_min, _dtype(a))
+    a = maximum(a_min, a)
+  if a_max is not None:
+    if _dtype(a_max) != _dtype(a):
+      a_max = lax.convert_element_type(a_max, _dtype(a))
+    a = minimum(a_max, a)
+  return a
 
 
 def _dtype_info(dtype):","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index a778b72ed..ef34e02f4 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -631,16 +631,17 @@ def split(ary, indices_or_sections, axis=0):
 
 @_wraps(onp.clip)
 def clip(a, a_min=None, a_max=None):
-  a_min = _dtype_info(_dtype(a)).min if a_min is None else a_min
-  a_max = _dtype_info(_dtype(a)).max if a_max is None else a_max
-  if _dtype(a_min) != _dtype(a):
-    a_min = lax.convert_element_type(a_min, _dtype(a))
-  if _dtype(a_max) != _dtype(a):
-    a_max = lax.convert_element_type(a_max, _dtype(a))
-  if issubdtype(_dtype(a), complexfloating):
-    return minimum(maximum(a_min, a), a_max)
-  else:
-    return lax.clamp(a_min, a, a_max)
+  if a_min is None and a_max is None:
+    raise ""At most one of a_min and a_max may be None""
+  if a_min is not None:
+    if _dtype(a_min) != _dtype(a):
+      a_min = lax.convert_element_type(a_min, _dtype(a))
+    a = maximum(a_min, a)
+  if a_max is not None:
+    if _dtype(a_max) != _dtype(a):
+      a_max = lax.convert_element_type(a_max, _dtype(a))
+    a = minimum(a_max, a)
+  return a
 
 
 def _dtype_info(dtype):",No
WORKSPACE,WORKSPACE,385ab96206a204f65e3438c6e04e815a9fccfd7f,8cb73c1c2e6487d52c4d2b369b3fdc0142decf19,Update XLA revision to https://github.com/tensorflow/tensorflow/commit/937ff1b4cf1d954806e075d66198429a6d2312be,"diff --git a/WORKSPACE b/WORKSPACE
index ed24f56c2..20d1dd24f 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""37138f72ba6db3fcb9b805a8b3432831771be9c0a70f6fded4c12c392c6205e7"",
-   strip_prefix = ""tensorflow-9ad0810fd55096fc86a58300c5a2710b2f3b5175"",
+   sha256 = ""8899ef47ab4e43de47397b0e95a5a5b0fcf8bd25ca8f86ca96547064d4384ab3"",
+   strip_prefix = ""tensorflow-937ff1b4cf1d954806e075d66198429a6d2312be"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/9ad0810fd55096fc86a58300c5a2710b2f3b5175.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/937ff1b4cf1d954806e075d66198429a6d2312be.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index ed24f56c2..20d1dd24f 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""37138f72ba6db3fcb9b805a8b3432831771be9c0a70f6fded4c12c392c6205e7"",
-   strip_prefix = ""tensorflow-9ad0810fd55096fc86a58300c5a2710b2f3b5175"",
+   sha256 = ""8899ef47ab4e43de47397b0e95a5a5b0fcf8bd25ca8f86ca96547064d4384ab3"",
+   strip_prefix = ""tensorflow-937ff1b4cf1d954806e075d66198429a6d2312be"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/9ad0810fd55096fc86a58300c5a2710b2f3b5175.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/937ff1b4cf1d954806e075d66198429a6d2312be.tar.gz"",
    ],
 )
 ",No
jax/abstract_arrays.py,jax/abstract_arrays.py,d43c65dcd8201556ec77c11b9520dee426095537,385ab96206a204f65e3438c6e04e815a9fccfd7f,"Add preliminary support for np.complex128.

Only lightly tested.","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 8638d30e2..92966a453 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -162,8 +162,8 @@ def zeros_like_array(x):
   return onp.broadcast_to(onp.array(0, dtype), onp.shape(x))
 
 array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
-               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32,
-               complex, float, int, bool]
+               onp.complex128, onp.int64, onp.int32, onp.bool_, onp.uint64,
+               onp.uint32, complex, float, int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray","diff --git a/jax/abstract_arrays.py b/jax/abstract_arrays.py
index 8638d30e2..92966a453 100644
--- a/jax/abstract_arrays.py
+++ b/jax/abstract_arrays.py
@@ -162,8 +162,8 @@ def zeros_like_array(x):
   return onp.broadcast_to(onp.array(0, dtype), onp.shape(x))
 
 array_types = [onp.ndarray, onp.float64, onp.float32, onp.complex64,
-               onp.int64, onp.int32, onp.bool_, onp.uint64, onp.uint32,
-               complex, float, int, bool]
+               onp.complex128, onp.int64, onp.int32, onp.bool_, onp.uint64,
+               onp.uint32, complex, float, int, bool]
 
 for t in array_types:
   core.pytype_aval_mappings[t] = ConcreteArray",No
jax/lax.py,jax/lax.py,d43c65dcd8201556ec77c11b9520dee426095537,385ab96206a204f65e3438c6e04e815a9fccfd7f,"Add preliminary support for np.complex128.

Only lightly tested.","diff --git a/jax/lax.py b/jax/lax.py
index 228079de8..98a4a64c4 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -706,7 +706,6 @@ _input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
 _fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
 _complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype
 
-
 def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
   prim = Primitive(name)
   prim.def_impl(partial(xla.apply_primitive, prim))
@@ -819,7 +818,8 @@ def _brcast_to(x, shape):
 
 _f32 = {onp.float32}
 _float = {onp.floating}
-_complex = {onp.complex64}
+_complex = {onp.complex}
+_complex_elem_types = {onp.float32, onp.float64}
 _int = {onp.integer}
 _bool = {onp.bool_}
 
@@ -885,16 +885,18 @@ erf_inv_p = standard_unop(_float, 'erf_inv')
 ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                             mul(g, exp(square(ans)))))
 
-real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
-ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])
+real_p = unop(_complex_basetype, _complex, 'real')
+ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), _dtype(t)))])
 
-imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
-ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), t)])
+imag_p = unop(_complex_basetype, _complex, 'imag')
+ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), _dtype(t)), t)])
 
-complex_p = binop(_fixed_dtype(onp.complex64), [_f32, _f32], 'complex')
+_complex_dtype = lambda dtype, *args: (onp.zeros((), dtype) + onp.zeros((), onp.complex64)).dtype
+complex_p = binop(_complex_dtype, [_complex_elem_types, _complex_elem_types],
+                  'complex')
 ad.deflinear(complex_p, lambda t: [real(t), imag(t)])
 
-conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
+conj_p = unop(_complex_dtype, _float | _complex, 'conj')
 
 def conj_transpose_rule(t, x, input_dtype):
   assert x is None","diff --git a/jax/lax.py b/jax/lax.py
index 228079de8..98a4a64c4 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -706,7 +706,6 @@ _input_dtype = lambda *args, **_: xla_bridge.canonicalize_dtype(args[0].dtype)
 _fixed_dtype = lambda dtype: lambda *args, **kwargs: xla_bridge.canonicalize_dtype(dtype)
 _complex_basetype = lambda dtype: onp.abs(onp.zeros((), dtype)).dtype
 
-
 def standard_primitive(shape_rule, dtype_rule, name, translation_rule=None):
   prim = Primitive(name)
   prim.def_impl(partial(xla.apply_primitive, prim))
@@ -819,7 +818,8 @@ def _brcast_to(x, shape):
 
 _f32 = {onp.float32}
 _float = {onp.floating}
-_complex = {onp.complex64}
+_complex = {onp.complex}
+_complex_elem_types = {onp.float32, onp.float64}
 _int = {onp.integer}
 _bool = {onp.bool_}
 
@@ -885,16 +885,18 @@ erf_inv_p = standard_unop(_float, 'erf_inv')
 ad.defjvp2(erf_inv_p, lambda g, ans, x: mul(_const(x, onp.sqrt(onp.pi) / 2.),
                                             mul(g, exp(square(ans)))))
 
-real_p = unop(_fixed_dtype(onp.float32), _complex, 'real')
-ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), onp.float32))])
+real_p = unop(_complex_basetype, _complex, 'real')
+ad.deflinear(real_p, lambda t: [complex(t, onp.zeros((), _dtype(t)))])
 
-imag_p = unop(_fixed_dtype(onp.float32), _complex, 'imag')
-ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), onp.float32), t)])
+imag_p = unop(_complex_basetype, _complex, 'imag')
+ad.deflinear(imag_p, lambda t: [complex(onp.zeros((), _dtype(t)), t)])
 
-complex_p = binop(_fixed_dtype(onp.complex64), [_f32, _f32], 'complex')
+_complex_dtype = lambda dtype, *args: (onp.zeros((), dtype) + onp.zeros((), onp.complex64)).dtype
+complex_p = binop(_complex_dtype, [_complex_elem_types, _complex_elem_types],
+                  'complex')
 ad.deflinear(complex_p, lambda t: [real(t), imag(t)])
 
-conj_p = unop(_fixed_dtype(onp.complex64), _float | _complex, 'conj')
+conj_p = unop(_complex_dtype, _float | _complex, 'conj')
 
 def conj_transpose_rule(t, x, input_dtype):
   assert x is None",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,d43c65dcd8201556ec77c11b9520dee426095537,385ab96206a204f65e3438c6e04e815a9fccfd7f,"Add preliminary support for np.complex128.

Only lightly tested.","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 89551ea1d..c38b8226e 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -193,6 +193,7 @@ _etype_to_dtype = {
     xla_data_pb2.F32: onp.dtype('float32'),
     xla_data_pb2.F64: onp.dtype('float64'),
     xla_data_pb2.C64: onp.dtype('complex64'),
+    xla_data_pb2.C128: onp.dtype('complex128'),
 }
 
 # Note the conversion on the key. Numpy has a known issue wherein dtype hashing
@@ -220,10 +221,6 @@ def canonicalize_dtype(dtype):
   """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
   dtype = onp.dtype(dtype)
 
-  # special rule for complex128, which XLA doesn't support
-  if dtype == onp.complex128:
-    dtype = onp.dtype('complex64')
-
   if FLAGS.jax_enable_x64:
     return str(dtype)
   else:","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 89551ea1d..c38b8226e 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -193,6 +193,7 @@ _etype_to_dtype = {
     xla_data_pb2.F32: onp.dtype('float32'),
     xla_data_pb2.F64: onp.dtype('float64'),
     xla_data_pb2.C64: onp.dtype('complex64'),
+    xla_data_pb2.C128: onp.dtype('complex128'),
 }
 
 # Note the conversion on the key. Numpy has a known issue wherein dtype hashing
@@ -220,10 +221,6 @@ def canonicalize_dtype(dtype):
   """"""Convert from a dtype to a canonical dtype based on FLAGS.jax_enable_x64.""""""
   dtype = onp.dtype(dtype)
 
-  # special rule for complex128, which XLA doesn't support
-  if dtype == onp.complex128:
-    dtype = onp.dtype('complex64')
-
   if FLAGS.jax_enable_x64:
     return str(dtype)
   else:",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,d43c65dcd8201556ec77c11b9520dee426095537,385ab96206a204f65e3438c6e04e815a9fccfd7f,"Add preliminary support for np.complex128.

Only lightly tested.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef34e02f4..e46cd1c8c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -99,10 +99,10 @@ int16 = onp.int16
 int32 = onp.int32
 int64 = onp.int64
 float16 = onp.float16
-float32 = onp.float32
-float64 = onp.float64
-complex64 = onp.complex64
-complex128 = onp.complex128
+float32 = single = onp.float32
+float64 = double = onp.float64
+complex64 = csingle = onp.complex64
+complex128 = cdouble = onp.complex128
 
 flexible = onp.flexible
 character = onp.character","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef34e02f4..e46cd1c8c 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -99,10 +99,10 @@ int16 = onp.int16
 int32 = onp.int32
 int64 = onp.int64
 float16 = onp.float16
-float32 = onp.float32
-float64 = onp.float64
-complex64 = onp.complex64
-complex128 = onp.complex128
+float32 = single = onp.float32
+float64 = double = onp.float64
+complex64 = csingle = onp.complex64
+complex128 = cdouble = onp.complex128
 
 flexible = onp.flexible
 character = onp.character",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,d43c65dcd8201556ec77c11b9520dee426095537,385ab96206a204f65e3438c6e04e815a9fccfd7f,"Add preliminary support for np.complex128.

Only lightly tested.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 0200985ab..b4f92d90b 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -45,7 +45,7 @@ nonempty_shapes = scalar_shapes + nonempty_array_shapes
 all_shapes =  scalar_shapes + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
-complex_dtypes = [onp.complex64]
+complex_dtypes = [onp.complex64, onp.complex128]
 int_dtypes = [onp.int32, onp.int64]
 unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 0200985ab..b4f92d90b 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -45,7 +45,7 @@ nonempty_shapes = scalar_shapes + nonempty_array_shapes
 all_shapes =  scalar_shapes + array_shapes
 
 float_dtypes = [onp.float32, onp.float64]
-complex_dtypes = [onp.complex64]
+complex_dtypes = [onp.complex64, onp.complex128]
 int_dtypes = [onp.int32, onp.int64]
 unsigned_dtypes = [onp.uint32, onp.uint64]
 bool_dtypes = [onp.bool_]",No
tests/lax_test.py,tests/lax_test.py,d43c65dcd8201556ec77c11b9520dee426095537,385ab96206a204f65e3438c6e04e815a9fccfd7f,"Add preliminary support for np.complex128.

Only lightly tested.","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 867577100..e1d3a3367 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -52,7 +52,7 @@ def num_float_bits(dtype):
 # arguments of appropriate shapes and dtypes using the following table.
 
 float_dtypes = [onp.float32, onp.float64]
-complex_dtypes = [onp.complex64]
+complex_dtypes = [onp.complex64, onp.complex128]
 int_dtypes = [onp.int32, onp.int64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 867577100..e1d3a3367 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -52,7 +52,7 @@ def num_float_bits(dtype):
 # arguments of appropriate shapes and dtypes using the following table.
 
 float_dtypes = [onp.float32, onp.float64]
-complex_dtypes = [onp.complex64]
+complex_dtypes = [onp.complex64, onp.complex128]
 int_dtypes = [onp.int32, onp.int64]
 bool_dtypes = [onp.bool_]
 default_dtypes = float_dtypes + int_dtypes",No
build/setup.py,build/setup.py,aaf0e3cfaf88bb71540cf623850c537b505af958,44c24e5abfc7fa550dc1dac71c79e96fc94ba262,bump jaxlib version for new wheels,"diff --git a/build/setup.py b/build/setup.py
index b0f12773e..7968d30b3 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.3',
+    version='0.1.4',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index b0f12773e..7968d30b3 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.3',
+    version='0.1.4',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
setup.py,setup.py,2166d3f2692a96178a7392c0f2c39b0361514edf,aaf0e3cfaf88bb71540cf623850c537b505af958,bump version number for pypi,"diff --git a/setup.py b/setup.py
index deb377b46..0f4b2c22c 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.15',
+    version='0.1.16',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/setup.py b/setup.py
index deb377b46..0f4b2c22c 100644
--- a/setup.py
+++ b/setup.py
@@ -16,7 +16,7 @@ from setuptools import setup, find_packages
 
 setup(
     name='jax',
-    version='0.1.15',
+    version='0.1.16',
     description='Differentiate, compile, and transform Numpy code.',
     author='JAX team',
     author_email='jax-dev@google.com',",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,6be53c7a0bd0e89c72e380386bd3b6f0a5579361,2166d3f2692a96178a7392c0f2c39b0361514edf,fix isinstance check in indexing (fixes #227),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef34e02f4..2f676424d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1592,8 +1592,8 @@ def _rewriting_take(arr, idx, axis=0):
       axis += isinstance(elt, slice)   # advance axis index if not eliminated
     unexpanded_shape_itr = iter(result.shape)
     result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
-                         for elt in canonical_idx if not isinstance(elt, int))
-    return lax.reshape(result, result_shape)
+                         for elt in canonical_idx if isinstance(elt, (type(None), slice)))
+    return lax.reshape(result, result_shape) if result_shape else result
 
   # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
   # or a tuple with at least one sequence object).","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index ef34e02f4..2f676424d 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1592,8 +1592,8 @@ def _rewriting_take(arr, idx, axis=0):
       axis += isinstance(elt, slice)   # advance axis index if not eliminated
     unexpanded_shape_itr = iter(result.shape)
     result_shape = tuple(1 if elt is None else next(unexpanded_shape_itr)
-                         for elt in canonical_idx if not isinstance(elt, int))
-    return lax.reshape(result, result_shape)
+                         for elt in canonical_idx if isinstance(elt, (type(None), slice)))
+    return lax.reshape(result, result_shape) if result_shape else result
 
   # Handle advanced indexing (non-tuple sequence, ndarray of dtype int or bool,
   # or a tuple with at least one sequence object).",No
jax/random.py,jax/random.py,70d1a0044326e67ef5a236379f33e16ccf761770,04835b9ca5d032cef785c0f4741d32efbb6e649c,"set behavior when random.randint has invalid range

(closes #222)","diff --git a/jax/random.py b/jax/random.py
index 8b00c9ca7..8b4af32d3 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -245,6 +245,10 @@ def randint(key, shape, minval, maxval, dtype=onp.int32):
   if nbits not in (32, 64):
     raise TypeError(""randint only accepts 32- or 64-bit dtypes."")
 
+  # if we don't have minval < maxval, just always return minval
+  # https://github.com/google/jax/issues/222
+  maxval = lax.max(lax.add(minval, onp.array(1, dtype)), maxval)
+
   # This algorithm is biased whenever (maxval - minval) is not a power of 2.
   # We generate double the number of random bits required by the dtype so as to
   # reduce that bias.","diff --git a/jax/random.py b/jax/random.py
index 8b00c9ca7..8b4af32d3 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -245,6 +245,10 @@ def randint(key, shape, minval, maxval, dtype=onp.int32):
   if nbits not in (32, 64):
     raise TypeError(""randint only accepts 32- or 64-bit dtypes."")
 
+  # if we don't have minval < maxval, just always return minval
+  # https://github.com/google/jax/issues/222
+  maxval = lax.max(lax.add(minval, onp.array(1, dtype)), maxval)
+
   # This algorithm is biased whenever (maxval - minval) is not a power of 2.
   # We generate double the number of random bits required by the dtype so as to
   # reduce that bias.",No
tests/random_test.py,tests/random_test.py,70d1a0044326e67ef5a236379f33e16ccf761770,04835b9ca5d032cef785c0f4741d32efbb6e649c,"set behavior when random.randint has invalid range

(closes #222)","diff --git a/tests/random_test.py b/tests/random_test.py
index 46963f076..9aada5a38 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -149,6 +149,10 @@ class LaxRandomTest(jtu.JaxTestCase):
     self.assertFalse(onp.all(perm1 == x))  # seems unlikely!
     self.assertTrue(onp.all(onp.sort(perm1) == x))
 
+  def testIssue222(self):
+    x = random.randint(random.PRNGKey(10003), (), 0, 0)
+    assert x == 0
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
index 46963f076..9aada5a38 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -149,6 +149,10 @@ class LaxRandomTest(jtu.JaxTestCase):
     self.assertFalse(onp.all(perm1 == x))  # seems unlikely!
     self.assertTrue(onp.all(onp.sort(perm1) == x))
 
+  def testIssue222(self):
+    x = random.randint(random.PRNGKey(10003), (), 0, 0)
+    assert x == 0
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
build/BUILD,build/BUILD.bazel,0316d314797542f524d5ffd7aa6a73b198461f14,cf3ff022f6c66495e9f622a520734cb86bbacc86,"Rename build/BUILD to build/BUILD.bazel.

Avoids name conflict when building wheels on case-insensitive filesystems as on Mac OS X.","diff --git a/build/BUILD.bazel b/build/BUILD.bazel
new file mode 100644
index 000000000..f70cb3975
--- /dev/null
+++ b/build/BUILD.bazel
@@ -0,0 +1,30 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# JAX is Autograd and XLA
+
+licenses([""notice""])  # Apache 2
+
+package(default_visibility = [""//visibility:public""])
+
+sh_binary(
+    name = ""install_xla_in_source_tree"",
+    srcs = [""install_xla_in_source_tree.sh""],
+    data = [
+        ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
+        ""//jaxlib:lapack.so"",
+    ],
+    deps = [""@bazel_tools//tools/bash/runfiles""],
+)
+","diff --git a/build/BUILD.bazel b/build/BUILD.bazel
new file mode 100644
index 000000000..f70cb3975
--- /dev/null
+++ b/build/BUILD.bazel
@@ -0,0 +1,30 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# JAX is Autograd and XLA
+
+licenses([""notice""])  # Apache 2
+
+package(default_visibility = [""//visibility:public""])
+
+sh_binary(
+    name = ""install_xla_in_source_tree"",
+    srcs = [""install_xla_in_source_tree.sh""],
+    data = [
+        ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
+        ""//jaxlib:lapack.so"",
+    ],
+    deps = [""@bazel_tools//tools/bash/runfiles""],
+)
+",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,61504a63d7187226ae3413005235b6557d2644b6,0316d314797542f524d5ffd7aa6a73b198461f14,Update Jaxlib versions in Colab notebooks.,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 3dd1d14fd..cd0fed0a7 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 3dd1d14fd..cd0fed0a7 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,61504a63d7187226ae3413005235b6557d2644b6,0316d314797542f524d5ffd7aa6a73b198461f14,Update Jaxlib versions in Colab notebooks.,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 470770078..141cf0bee 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 470770078..141cf0bee 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,61504a63d7187226ae3413005235b6557d2644b6,0316d314797542f524d5ffd7aa6a73b198461f14,Update Jaxlib versions in Colab notebooks.,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 1f3099209..b86ac9d89 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 1f3099209..b86ac9d89 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.3-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
tests/linalg_test.py,tests/linalg_test.py,78dbc031b87b262ca28701d5f604a528a5aa1866,61504a63d7187226ae3413005235b6557d2644b6,double linalg test tolerances to avoid flakiness,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 75e76c658..b1973efc1 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -149,6 +149,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
   def testSVD(self, m, n, dtype, full_matrices, compute_uv, rng):
     if not hasattr(lapack, ""jax_gesdd""):
       self.skipTest(""No singular value decomposition implementation available"")
+
     args_maker = lambda: [rng((m, n), dtype)]
 
     # Norm, adjusted for dimension and type.
@@ -164,18 +165,18 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 50))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 100))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 50))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 100))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 100))
 
       # Check the unitary properties of the singular vector matrices.
-      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 5))
+      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 10))
       if m >= n:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(onp.conj(T(out[2])), out[2])) < 5))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(onp.conj(T(out[2])), out[2])) < 10))
       else:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 10))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 20))
 
     else:
       self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))
@@ -324,7 +325,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
-    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=2e-2)
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-1)
 
 
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 75e76c658..b1973efc1 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -149,6 +149,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
   def testSVD(self, m, n, dtype, full_matrices, compute_uv, rng):
     if not hasattr(lapack, ""jax_gesdd""):
       self.skipTest(""No singular value decomposition implementation available"")
+
     args_maker = lambda: [rng((m, n), dtype)]
 
     # Norm, adjusted for dimension and type.
@@ -164,18 +165,18 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 50))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 100))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 50))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 100))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 100))
 
       # Check the unitary properties of the singular vector matrices.
-      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 5))
+      self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 10))
       if m >= n:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(onp.conj(T(out[2])), out[2])) < 5))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[1]) - onp.matmul(onp.conj(T(out[2])), out[2])) < 10))
       else:
-        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 10))
+        self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 20))
 
     else:
       self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))
@@ -324,7 +325,7 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
-    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=2e-2)
+    jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-1)
 
 
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.",No
jax/lax.py,jax/lax.py,5a4713f108ae188f72d209bdf611970a2f1fe7d9,dfa2cb821f226df3b3f988d380426f86fdc9ff92,add tests for np.sort (c.f. #221),"diff --git a/jax/lax.py b/jax/lax.py
index 228079de8..a7d451dde 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -391,7 +391,7 @@ def sort(operand, dimension=-1):
   return sort_p.bind(operand, dimension=dimension)
 
 def sort_key_val(keys, values, dimension=-1):
-  # TODO new sort_key_val is variadic
+  # TODO(mattjj): new sort_key_val is variadic
   result = sort_key_val_p.bind(keys, values, dimension=dimension)
   sorted_keys, sorted_values = result
   return sorted_keys, sorted_values","diff --git a/jax/lax.py b/jax/lax.py
index 228079de8..a7d451dde 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -391,7 +391,7 @@ def sort(operand, dimension=-1):
   return sort_p.bind(operand, dimension=dimension)
 
 def sort_key_val(keys, values, dimension=-1):
-  # TODO new sort_key_val is variadic
+  # TODO(mattjj): new sort_key_val is variadic
   result = sort_key_val_p.bind(keys, values, dimension=dimension)
   sorted_keys, sorted_values = result
   return sorted_keys, sorted_values",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,5a4713f108ae188f72d209bdf611970a2f1fe7d9,dfa2cb821f226df3b3f988d380426f86fdc9ff92,add tests for np.sort (c.f. #221),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 2f676424d..5b94f59bc 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -235,7 +235,6 @@ absolute = abs = _one_to_one_unop(onp.absolute, lax.abs)
 fabs = _one_to_one_unop(onp.fabs, lax.abs, True)
 bitwise_not = _one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
 negative = _one_to_one_unop(onp.negative, lax.neg)
-sort = _one_to_one_unop(onp.sort, lax.sort)
 sign = _one_to_one_unop(onp.sign, lax.sign)
 
 floor = _one_to_one_unop(onp.floor, lax.floor, True)
@@ -1523,6 +1522,20 @@ def _argminmax(op, a, axis):
   mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
   return min(mask_idxs, axis)
 
+
+@_wraps(onp.sort)
+def sort(a, axis=-1, kind='quicksort', order=None):
+  if kind != 'quicksort':
+    warnings.warn(""'kind' argument to sort is ignored."")
+  if order is not None:
+    msg = ""'order' argument to sort is not supported.""
+    raise ValueError(msg)
+  if axis is None:
+    return lax.sort(a.ravel(), 0)
+  else:
+    return lax.sort(a, axis % ndim(a))
+
+
 ### Indexing
 
 
@@ -1722,7 +1735,8 @@ def _static_idx(idx, size):
 def _not_implemented(fun):
   @_wraps(fun)
   def wrapped(*args, **kwargs):
-    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+    msg = ""Numpy function {} not yet implemented""
+    raise NotImplementedError(msg.format(fun))
   return wrapped
 
 # Build a set of all unimplemented NumPy functions.","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 2f676424d..5b94f59bc 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -235,7 +235,6 @@ absolute = abs = _one_to_one_unop(onp.absolute, lax.abs)
 fabs = _one_to_one_unop(onp.fabs, lax.abs, True)
 bitwise_not = _one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
 negative = _one_to_one_unop(onp.negative, lax.neg)
-sort = _one_to_one_unop(onp.sort, lax.sort)
 sign = _one_to_one_unop(onp.sign, lax.sign)
 
 floor = _one_to_one_unop(onp.floor, lax.floor, True)
@@ -1523,6 +1522,20 @@ def _argminmax(op, a, axis):
   mask_idxs = where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
   return min(mask_idxs, axis)
 
+
+@_wraps(onp.sort)
+def sort(a, axis=-1, kind='quicksort', order=None):
+  if kind != 'quicksort':
+    warnings.warn(""'kind' argument to sort is ignored."")
+  if order is not None:
+    msg = ""'order' argument to sort is not supported.""
+    raise ValueError(msg)
+  if axis is None:
+    return lax.sort(a.ravel(), 0)
+  else:
+    return lax.sort(a, axis % ndim(a))
+
+
 ### Indexing
 
 
@@ -1722,7 +1735,8 @@ def _static_idx(idx, size):
 def _not_implemented(fun):
   @_wraps(fun)
   def wrapped(*args, **kwargs):
-    raise Exception(""Numpy function {} not yet implemented"".format(fun))
+    msg = ""Numpy function {} not yet implemented""
+    raise NotImplementedError(msg.format(fun))
   return wrapped
 
 # Build a set of all unimplemented NumPy functions.",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,5a4713f108ae188f72d209bdf611970a2f1fe7d9,dfa2cb821f226df3b3f988d380426f86fdc9ff92,add tests for np.sort (c.f. #221),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 0200985ab..934f2fad8 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -932,7 +932,6 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
   # TODO(mattjj): test infix operator overrides
 
   def testRavel(self):
-    # TODO(mattjj): support this method-based syntax?
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
     self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
@@ -959,6 +958,28 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     ans = lnp.arange(0.0, 1.0, 0.1)
     self.assertAllClose(expected, ans, check_dtypes=True)
 
+  def testSortManual(self):
+    # manual tests for sort are nice because we don't have to worry about ties.
+    # lax.sort is tested combinatorially.
+    ans = lnp.sort(onp.array([16, 15, 23, 42, 8, 4]))
+    expected = onp.array([4, 8, 15, 16, 23, 42])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
+    a = onp.array([[1, 4], [3, 1]])
+    ans = lnp.sort(a, axis=None)
+    expected = onp.array([[1, 1, 3, 4]])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
+    a = onp.array([[1, 4], [3, 1]])
+    ans = lnp.sort(a)  # last axis
+    expected = onp.array([[1, 4], [1, 3]])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
+    a = onp.array([[1, 4], [3, 1]])
+    ans = lnp.sort(a, axis=0)
+    expected = onp.array([[1, 1], [3, 4]])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 0200985ab..934f2fad8 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -932,7 +932,6 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
   # TODO(mattjj): test infix operator overrides
 
   def testRavel(self):
-    # TODO(mattjj): support this method-based syntax?
     rng = onp.random.RandomState(0)
     args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
     self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
@@ -959,6 +958,28 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     ans = lnp.arange(0.0, 1.0, 0.1)
     self.assertAllClose(expected, ans, check_dtypes=True)
 
+  def testSortManual(self):
+    # manual tests for sort are nice because we don't have to worry about ties.
+    # lax.sort is tested combinatorially.
+    ans = lnp.sort(onp.array([16, 15, 23, 42, 8, 4]))
+    expected = onp.array([4, 8, 15, 16, 23, 42])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
+    a = onp.array([[1, 4], [3, 1]])
+    ans = lnp.sort(a, axis=None)
+    expected = onp.array([[1, 1, 3, 4]])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
+    a = onp.array([[1, 4], [3, 1]])
+    ans = lnp.sort(a)  # last axis
+    expected = onp.array([[1, 4], [1, 3]])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
+    a = onp.array([[1, 4], [3, 1]])
+    ans = lnp.sort(a, axis=0)
+    expected = onp.array([[1, 1], [3, 4]])
+    self.assertAllClose(expected, ans, check_dtypes=True)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/api.py,jax/api.py,54886bd3109b3b4462e1e72e67cd7ffae69c5399,5a4713f108ae188f72d209bdf611970a2f1fe7d9,add sort_key_val batching rule (fixes #221),"diff --git a/jax/api.py b/jax/api.py
index b27ce1452..b7c551ed0 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -240,7 +240,7 @@ def vmap(fun, in_axes=0, out_axes=0):
   @wraps(fun, docstr=docstr)
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
-      f = lu.wrap_init(fun)
+      f = lu.wrap_init(fun, kwargs)
     in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
     in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)","diff --git a/jax/api.py b/jax/api.py
index b27ce1452..b7c551ed0 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -240,7 +240,7 @@ def vmap(fun, in_axes=0, out_axes=0):
   @wraps(fun, docstr=docstr)
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
-      f = lu.wrap_init(fun)
+      f = lu.wrap_init(fun, kwargs)
     in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
     in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)",No
jax/lax.py,jax/lax.py,54886bd3109b3b4462e1e72e67cd7ffae69c5399,5a4713f108ae188f72d209bdf611970a2f1fe7d9,add sort_key_val batching rule (fixes #221),"diff --git a/jax/lax.py b/jax/lax.py
index a7d451dde..a3bfca7ac 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2275,19 +2275,48 @@ def sort_key_val_jvp(primals, tangents, dimension):
 def sort_key_val_transpose_rule(t, keys, values, dimension):
   t_keys, t_values = t
   assert t_keys is ad_util.zero
-  broadcasted_iota = broadcast_in_dim(
-      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
-  _, perm = sort_key_val(keys, broadcasted_iota)
+  iota = broadcasted_iota(onp.int32, keys.shape, dimension % keys.ndim)
+  _, perm = sort_key_val(keys, iota)
   keys_result = ad_util.zero if keys is None else None
   values_result = sort_key_val(perm, t_values)[1] if values is None else None
   return [keys_result, values_result]
 
+def sort_key_val_batch_rule(batched_args, batch_dims, dimension):
+  keys, values = batched_args
+  keys_bdim, values_bdim = batch_dims
+  assert keys_bdim is not None or values_bdim is not None
+  if keys_bdim == values_bdim:
+    new_dimension = dimension + (keys_bdim <= dimension)
+    out = sort_key_val(keys, values, new_dimension)
+    return core.pack(out), keys_bdim
+  elif keys_bdim is not None and values_bdim is not None:
+    keys_trans = batching.moveaxis(keys.shape[keys_bdim], values_bdim,
+                                   keys_bdim, keys)
+    new_dimension = dimension + (values_bdim <= dimension)
+    out = sort_key_val(keys_trans, values, new_dimension)
+    return core.pack(out), values_bdim
+  elif keys_bdim is None:
+    broadcast_dimensions = onp.delete(onp.arange(values.ndim), values_bdim)
+    new_keys = broadcast_in_dim(keys, values.shape, broadcast_dimensions)
+    new_dimension = dimension + (values_bdim <= dimension)
+    out = sort_key_val(new_keys, values, new_dimension)
+    return core.pack(out), values_bdim
+  elif values_bdim is None:
+    broadcast_dimensions = onp.delete(onp.arange(keys.ndim), keys_bdim)
+    new_values = broadcast_in_dim(values, keys.shape, broadcast_dimensions)
+    new_dimension = dimension + (keys_bdim <= dimension)
+    out = sort_key_val(keys, new_values, new_dimension)
+    return core.pack(out), keys_bdim
+  else:
+    raise Exception  # unreachable
+
 sort_key_val_p = Primitive('sort_key_val')
 sort_key_val_p.def_impl(sort_key_val_impl)
 sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
 xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
 ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
 ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule
+batching.primitive_batchers[sort_key_val_p] = sort_key_val_batch_rule
 
 
 def while_loop_abstract_eval(init_val, opaque_params):","diff --git a/jax/lax.py b/jax/lax.py
index a7d451dde..a3bfca7ac 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -2275,19 +2275,48 @@ def sort_key_val_jvp(primals, tangents, dimension):
 def sort_key_val_transpose_rule(t, keys, values, dimension):
   t_keys, t_values = t
   assert t_keys is ad_util.zero
-  broadcasted_iota = broadcast_in_dim(
-      onp.arange(keys.shape[dimension]), keys.shape, [dimension % keys.ndim])
-  _, perm = sort_key_val(keys, broadcasted_iota)
+  iota = broadcasted_iota(onp.int32, keys.shape, dimension % keys.ndim)
+  _, perm = sort_key_val(keys, iota)
   keys_result = ad_util.zero if keys is None else None
   values_result = sort_key_val(perm, t_values)[1] if values is None else None
   return [keys_result, values_result]
 
+def sort_key_val_batch_rule(batched_args, batch_dims, dimension):
+  keys, values = batched_args
+  keys_bdim, values_bdim = batch_dims
+  assert keys_bdim is not None or values_bdim is not None
+  if keys_bdim == values_bdim:
+    new_dimension = dimension + (keys_bdim <= dimension)
+    out = sort_key_val(keys, values, new_dimension)
+    return core.pack(out), keys_bdim
+  elif keys_bdim is not None and values_bdim is not None:
+    keys_trans = batching.moveaxis(keys.shape[keys_bdim], values_bdim,
+                                   keys_bdim, keys)
+    new_dimension = dimension + (values_bdim <= dimension)
+    out = sort_key_val(keys_trans, values, new_dimension)
+    return core.pack(out), values_bdim
+  elif keys_bdim is None:
+    broadcast_dimensions = onp.delete(onp.arange(values.ndim), values_bdim)
+    new_keys = broadcast_in_dim(keys, values.shape, broadcast_dimensions)
+    new_dimension = dimension + (values_bdim <= dimension)
+    out = sort_key_val(new_keys, values, new_dimension)
+    return core.pack(out), values_bdim
+  elif values_bdim is None:
+    broadcast_dimensions = onp.delete(onp.arange(keys.ndim), keys_bdim)
+    new_values = broadcast_in_dim(values, keys.shape, broadcast_dimensions)
+    new_dimension = dimension + (keys_bdim <= dimension)
+    out = sort_key_val(keys, new_values, new_dimension)
+    return core.pack(out), keys_bdim
+  else:
+    raise Exception  # unreachable
+
 sort_key_val_p = Primitive('sort_key_val')
 sort_key_val_p.def_impl(sort_key_val_impl)
 sort_key_val_p.def_abstract_eval(sort_key_val_abstract_eval)
 xla.translations[sort_key_val_p] = partial(standard_translate, 'sort_key_val')
 ad.primitive_jvps[sort_key_val_p] = sort_key_val_jvp
 ad.primitive_transposes[sort_key_val_p] = sort_key_val_transpose_rule
+batching.primitive_batchers[sort_key_val_p] = sort_key_val_batch_rule
 
 
 def while_loop_abstract_eval(init_val, opaque_params):",No
tests/batching_test.py,tests/batching_test.py,54886bd3109b3b4462e1e72e67cd7ffae69c5399,5a4713f108ae188f72d209bdf611970a2f1fe7d9,add sort_key_val batching rule (fixes #221),"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 07dd22abb..db4874580 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -335,6 +335,36 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(ans, expected, check_dtypes=False)
     assert len(onp.unique(ans)) == 10 * 3 * 2
 
+  def testSortKeyVal(self):
+    k = onp.arange(12)[::-1].reshape(3, 4)
+    v = onp.random.RandomState(0).permutation(12).reshape(3, 4)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (0, 0))(k, v)
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (1, 1), 1)(k, v)
+    self.assertAllClose(sk, k[::-1, :], check_dtypes=True)
+    self.assertAllClose(sv, v[::-1, :], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (0, 1))(k, v.T)
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (1, 0))(k.T, v)
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (None, 0))(k[0], v)
+    self.assertAllClose(sk, onp.broadcast_to(k[0, ::-1], (3, 4)),
+                        check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (1, None))(k.T, v[0])
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, onp.broadcast_to(v[0, ::-1], (3, 4)),
+                        check_dtypes=True)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 07dd22abb..db4874580 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -335,6 +335,36 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(ans, expected, check_dtypes=False)
     assert len(onp.unique(ans)) == 10 * 3 * 2
 
+  def testSortKeyVal(self):
+    k = onp.arange(12)[::-1].reshape(3, 4)
+    v = onp.random.RandomState(0).permutation(12).reshape(3, 4)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (0, 0))(k, v)
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (1, 1), 1)(k, v)
+    self.assertAllClose(sk, k[::-1, :], check_dtypes=True)
+    self.assertAllClose(sv, v[::-1, :], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (0, 1))(k, v.T)
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (1, 0))(k.T, v)
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (None, 0))(k[0], v)
+    self.assertAllClose(sk, onp.broadcast_to(k[0, ::-1], (3, 4)),
+                        check_dtypes=True)
+    self.assertAllClose(sv, v[:, ::-1], check_dtypes=True)
+
+    sk, sv = vmap(partial(lax.sort_key_val, dimension=0), (1, None))(k.T, v[0])
+    self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
+    self.assertAllClose(sv, onp.broadcast_to(v[0, ::-1], (3, 4)),
+                        check_dtypes=True)
+
 
 if __name__ == '__main__':
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,30683f8f4343f81a9b4ef4251710a4668056a50f,54886bd3109b3b4462e1e72e67cd7ffae69c5399,add numpy.argsort (c.f. #221),"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5b94f59bc..22d836f84 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import collections
 import itertools
 import string
+import warnings
 
 import numpy as onp
 import opt_einsum
@@ -1528,14 +1529,30 @@ def sort(a, axis=-1, kind='quicksort', order=None):
   if kind != 'quicksort':
     warnings.warn(""'kind' argument to sort is ignored."")
   if order is not None:
-    msg = ""'order' argument to sort is not supported.""
-    raise ValueError(msg)
+    raise ValueError(""'order' argument to sort is not supported."")
+
   if axis is None:
     return lax.sort(a.ravel(), 0)
   else:
     return lax.sort(a, axis % ndim(a))
 
 
+@_wraps(onp.argsort)
+def argsort(a, axis=-1, kind='quicksort', order=None):
+  if kind != 'quicksort':
+    warnings.warn(""'kind' argument to argsort is ignored."")
+  if order is not None:
+    raise ValueError(""'order' argument to argsort is not supported."")
+
+  if axis is None:
+    return argsort(a.ravel(), 0)
+  else:
+    axis = axis % ndim(a)
+    iota = lax.broadcasted_iota(onp.int64, shape(a), axis)
+    _, perm = lax.sort_key_val(a, iota, dimension=axis)
+    return perm
+
+
 ### Indexing
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 5b94f59bc..22d836f84 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -19,6 +19,7 @@ from __future__ import print_function
 import collections
 import itertools
 import string
+import warnings
 
 import numpy as onp
 import opt_einsum
@@ -1528,14 +1529,30 @@ def sort(a, axis=-1, kind='quicksort', order=None):
   if kind != 'quicksort':
     warnings.warn(""'kind' argument to sort is ignored."")
   if order is not None:
-    msg = ""'order' argument to sort is not supported.""
-    raise ValueError(msg)
+    raise ValueError(""'order' argument to sort is not supported."")
+
   if axis is None:
     return lax.sort(a.ravel(), 0)
   else:
     return lax.sort(a, axis % ndim(a))
 
 
+@_wraps(onp.argsort)
+def argsort(a, axis=-1, kind='quicksort', order=None):
+  if kind != 'quicksort':
+    warnings.warn(""'kind' argument to argsort is ignored."")
+  if order is not None:
+    raise ValueError(""'order' argument to argsort is not supported."")
+
+  if axis is None:
+    return argsort(a.ravel(), 0)
+  else:
+    axis = axis % ndim(a)
+    iota = lax.broadcasted_iota(onp.int64, shape(a), axis)
+    _, perm = lax.sort_key_val(a, iota, dimension=axis)
+    return perm
+
+
 ### Indexing
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,30683f8f4343f81a9b4ef4251710a4668056a50f,54886bd3109b3b4462e1e72e67cd7ffae69c5399,add numpy.argsort (c.f. #221),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 934f2fad8..b9d7b2be1 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -958,7 +958,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     ans = lnp.arange(0.0, 1.0, 0.1)
     self.assertAllClose(expected, ans, check_dtypes=True)
 
-  def testSortManual(self):
+  def testSortManually(self):
     # manual tests for sort are nice because we don't have to worry about ties.
     # lax.sort is tested combinatorially.
     ans = lnp.sort(onp.array([16, 15, 23, 42, 8, 4]))
@@ -980,6 +980,32 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     expected = onp.array([[1, 1], [3, 4]])
     self.assertAllClose(expected, ans, check_dtypes=True)
 
+  def testArgsortManually(self):
+    x = onp.array([16, 15, 23, 42, 8, 4])
+    ans = lnp.argsort(x)
+    expected = onp.argsort(x)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x, axis=0)
+    expected = onp.argsort(x, axis=0)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x, axis=1)
+    expected = onp.argsort(x, axis=1)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x, axis=None)
+    expected = onp.argsort(x, axis=None)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x)
+    expected = onp.argsort(x)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 934f2fad8..b9d7b2be1 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -958,7 +958,7 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     ans = lnp.arange(0.0, 1.0, 0.1)
     self.assertAllClose(expected, ans, check_dtypes=True)
 
-  def testSortManual(self):
+  def testSortManually(self):
     # manual tests for sort are nice because we don't have to worry about ties.
     # lax.sort is tested combinatorially.
     ans = lnp.sort(onp.array([16, 15, 23, 42, 8, 4]))
@@ -980,6 +980,32 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     expected = onp.array([[1, 1], [3, 4]])
     self.assertAllClose(expected, ans, check_dtypes=True)
 
+  def testArgsortManually(self):
+    x = onp.array([16, 15, 23, 42, 8, 4])
+    ans = lnp.argsort(x)
+    expected = onp.argsort(x)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x, axis=0)
+    expected = onp.argsort(x, axis=0)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x, axis=1)
+    expected = onp.argsort(x, axis=1)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x, axis=None)
+    expected = onp.argsort(x, axis=None)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
+    x = onp.array([[16, 15, 23], [42, 8, 4]])
+    ans = lnp.argsort(x)
+    expected = onp.argsort(x)
+    self.assertAllClose(expected, ans, check_dtypes=False)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,f7c6284313f4c95c7c1c3d09e8c83eb4fb8360dd,1ea77af36466ab6137ea6b10b6b486ed67e468ba,"add sort_along_axis, no tests (c.f. #220)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 22d836f84..6c03f7f99 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1553,6 +1553,21 @@ def argsort(a, axis=-1, kind='quicksort', order=None):
     return perm
 
 
+@_wraps(onp.take_along_axis)
+def take_along_axis(arr, indices, axis):
+  if axis is None and ndim(arr) != 1:
+    return take_along_axis(arr.ravel(), indices.ravel(), 0)
+  elif ndim(arr) == 1:
+    return lax.index_take(arr, (indices,), (0,))
+  else:
+    all_indices = [lax.broadcasted_iota(_dtype(indices), shape(indices), i)
+                   for i in range(ndim(arr))]
+    all_indices[axis] = indices
+    all_indices = tuple(map(ravel, all_indices))
+    out_flat = lax.index_take(arr, all_indices, tuple(range(ndim(arr))))
+    return reshape(out_flat, shape(indices))
+
+
 ### Indexing
 
 ","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 22d836f84..6c03f7f99 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -1553,6 +1553,21 @@ def argsort(a, axis=-1, kind='quicksort', order=None):
     return perm
 
 
+@_wraps(onp.take_along_axis)
+def take_along_axis(arr, indices, axis):
+  if axis is None and ndim(arr) != 1:
+    return take_along_axis(arr.ravel(), indices.ravel(), 0)
+  elif ndim(arr) == 1:
+    return lax.index_take(arr, (indices,), (0,))
+  else:
+    all_indices = [lax.broadcasted_iota(_dtype(indices), shape(indices), i)
+                   for i in range(ndim(arr))]
+    all_indices[axis] = indices
+    all_indices = tuple(map(ravel, all_indices))
+    out_flat = lax.index_take(arr, all_indices, tuple(range(ndim(arr))))
+    return reshape(out_flat, shape(indices))
+
+
 ### Indexing
 
 ",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,4a1d189d0553b32960ea2405f4fc9e9bf58aec82,f7c6284313f4c95c7c1c3d09e8c83eb4fb8360dd,add tests for numpy.take_along_axis (fixes #220),"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index b9d7b2be1..9a7f7e7a0 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -1006,6 +1006,25 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     expected = onp.argsort(x)
     self.assertAllClose(expected, ans, check_dtypes=False)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for shape in [(3,), (3, 4), (3, 4, 5)]
+      for axis in itertools.chain(range(len(shape)), [-1], [None])
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()]))
+  def testTakeAlongAxis(self, shape, dtype, axis, rng):
+    def args_maker():
+      x = rng(shape, dtype)
+      i = onp.argsort(x, axis=axis)
+      return x, i
+
+    lnp_op = lambda x, i: lnp.take_along_axis(x, i, axis=axis)
+    onp_op = lambda x, i: onp.take_along_axis(x, i, axis=axis)
+    self._CheckAgainstNumpy(lnp_op, onp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index b9d7b2be1..9a7f7e7a0 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -1006,6 +1006,25 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     expected = onp.argsort(x)
     self.assertAllClose(expected, ans, check_dtypes=False)
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_{}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for shape in [(3,), (3, 4), (3, 4, 5)]
+      for axis in itertools.chain(range(len(shape)), [-1], [None])
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()]))
+  def testTakeAlongAxis(self, shape, dtype, axis, rng):
+    def args_maker():
+      x = rng(shape, dtype)
+      i = onp.argsort(x, axis=axis)
+      return x, i
+
+    lnp_op = lambda x, i: lnp.take_along_axis(x, i, axis=axis)
+    onp_op = lambda x, i: onp.take_along_axis(x, i, axis=axis)
+    self._CheckAgainstNumpy(lnp_op, onp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
jax/lax.py,jax/lax.py,39257b24426eee34dd6c23ff3b6e08ed6c77fef1,9812bea1ef1991ad58d087e3465713dcfb503ea3,Work on scatter JVP/transpose.,"diff --git a/jax/lax.py b/jax/lax.py
index 79897ce17..ce514bb5c 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1893,7 +1893,11 @@ def gather_dtype_rule(operand, start_indices, **kwargs):
 def gather_shape_rule(operand, start_indices, dimension_numbers, slice_sizes,
                       operand_shape):
   assert operand.shape == operand_shape
-  expanded_start_indices_shape = start_indices.shape
+  if len(operand_shape) != len(slice_sizes):
+    msg = (""slice_sizes must have rank equal to the gather operand; ""
+          ""operand.shape={}, slice_sizes={}"".format(operand_shape, slice_sizes))
+    raise ValueError(msg)
+  expanded_start_indices_shape = list(start_indices.shape)
   if len(expanded_start_indices_shape) == dimension_numbers.index_vector_dim:
     expanded_start_indices_shape.append(1)
   result_rank = len(dimension_numbers.offset_dims)
@@ -2004,7 +2008,8 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
                            update_jaxpr, update_consts, dimension_numbers,
                            updates_shape):
   assert scatter_indices is not None
-  assert operand is None and updates is None
+  print(""scatter transpose "", t, operand, updates, dimension_numbers)
+  assert (operand is None) ^ (updates is None)
   operand_t = update_t = None
   if operand is None:
     # TODO(phawkins): only correct for scatter-add.
@@ -2021,10 +2026,17 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
       collapsed_slice_dims=dimension_numbers.inserted_window_dims,
       start_index_map=dimension_numbers.scatter_dims_to_operand_dims,
       index_vector_dim=dimension_numbers.index_vector_dim)
-    slice_sizes = onp.array(updates_shape)[
-      list(dimension_numbers.update_window_dims)]
+    slice_sizes = []
+    pos = 0
+    for i in xrange(len(t.shape)):
+      if i in dimension_numbers.inserted_window_dims:
+        slice_sizes.append(1)
+      else:
+        slice_sizes.append(updates_shape[dimension_numbers.update_window_dims[pos]])
+        pos += 1
     update_t = gather(t, scatter_indices, dimension_numbers=gather_dnums,
                       slice_sizes=slice_sizes)
+    print(""transpose out "", update_t, scatter_indices, gather_dnums, slice_sizes)
   return [operand_t, update_t, None]
 
 ","diff --git a/jax/lax.py b/jax/lax.py
index 79897ce17..ce514bb5c 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1893,7 +1893,11 @@ def gather_dtype_rule(operand, start_indices, **kwargs):
 def gather_shape_rule(operand, start_indices, dimension_numbers, slice_sizes,
                       operand_shape):
   assert operand.shape == operand_shape
-  expanded_start_indices_shape = start_indices.shape
+  if len(operand_shape) != len(slice_sizes):
+    msg = (""slice_sizes must have rank equal to the gather operand; ""
+          ""operand.shape={}, slice_sizes={}"".format(operand_shape, slice_sizes))
+    raise ValueError(msg)
+  expanded_start_indices_shape = list(start_indices.shape)
   if len(expanded_start_indices_shape) == dimension_numbers.index_vector_dim:
     expanded_start_indices_shape.append(1)
   result_rank = len(dimension_numbers.offset_dims)
@@ -2004,7 +2008,8 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
                            update_jaxpr, update_consts, dimension_numbers,
                            updates_shape):
   assert scatter_indices is not None
-  assert operand is None and updates is None
+  print(""scatter transpose "", t, operand, updates, dimension_numbers)
+  assert (operand is None) ^ (updates is None)
   operand_t = update_t = None
   if operand is None:
     # TODO(phawkins): only correct for scatter-add.
@@ -2021,10 +2026,17 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
       collapsed_slice_dims=dimension_numbers.inserted_window_dims,
       start_index_map=dimension_numbers.scatter_dims_to_operand_dims,
       index_vector_dim=dimension_numbers.index_vector_dim)
-    slice_sizes = onp.array(updates_shape)[
-      list(dimension_numbers.update_window_dims)]
+    slice_sizes = []
+    pos = 0
+    for i in xrange(len(t.shape)):
+      if i in dimension_numbers.inserted_window_dims:
+        slice_sizes.append(1)
+      else:
+        slice_sizes.append(updates_shape[dimension_numbers.update_window_dims[pos]])
+        pos += 1
     update_t = gather(t, scatter_indices, dimension_numbers=gather_dnums,
                       slice_sizes=slice_sizes)
+    print(""transpose out "", update_t, scatter_indices, gather_dnums, slice_sizes)
   return [operand_t, update_t, None]
 
 ",No
jax/lax.py,jax/lax.py,5fac477a8a49c7770d555a82f9f0941f1a2ab8e7,39257b24426eee34dd6c23ff3b6e08ed6c77fef1,"Fix bug in scatter transpose rule.

Add some simple gather and scatter tests.","diff --git a/jax/lax.py b/jax/lax.py
index ce514bb5c..eae671100 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -216,9 +216,8 @@ def gather(operand, start_indices, dimension_numbers=None, slice_sizes=None):
       operand, start_indices, dimension_numbers=dimension_numbers,
       slice_sizes=tuple(slice_sizes), operand_shape=operand.shape)
 
-def scatter(operand, scatter_indices, updates, update_computation,
-            dimension_numbers=None):
-  jaxpr, consts = _reduction_jaxpr(update_computation, _const(operand, 0))
+def scatter_add(operand, scatter_indices, updates, dimension_numbers=None):
+  jaxpr, consts = _reduction_jaxpr(add, _const(operand, 0))
   return scatter_p.bind(
       operand, scatter_indices, updates, update_jaxpr=jaxpr,
       update_consts=consts, dimension_numbers=dimension_numbers,
@@ -1938,7 +1937,7 @@ def gather_transpose_rule(t, operand, start_indices, dimension_numbers,
     inserted_window_dims=dimension_numbers.collapsed_slice_dims,
     scatter_dims_to_operand_dims=dimension_numbers.start_index_map,
     index_vector_dim=dimension_numbers.index_vector_dim)
-  return [scatter(zeros, start_indices, t, add, scatter_dnums), ad_util.zero]
+  return [scatter_add(zeros, start_indices, t, scatter_dnums), ad_util.zero]
 
 
 gather_p = standard_primitive(
@@ -1994,7 +1993,6 @@ def scatter_jvp(primals, tangents, update_jaxpr, update_consts,
   if g_operand is ad_util.zero and g_updates is ad_util.zero:
     tangent_out = ad_util.zero
   else:
-    print(""scatter jvp "", g_operand, g_updates)
     g_operand = ad.instantiate_zeros(operand, g_operand)
     g_updates = ad.instantiate_zeros(updates, g_updates)
     tangent_out = scatter_p.bind(
@@ -2008,17 +2006,9 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
                            update_jaxpr, update_consts, dimension_numbers,
                            updates_shape):
   assert scatter_indices is not None
-  print(""scatter transpose "", t, operand, updates, dimension_numbers)
-  assert (operand is None) ^ (updates is None)
   operand_t = update_t = None
   if operand is None:
-    # TODO(phawkins): only correct for scatter-add.
     operand_t = t
-    #zeros = _zeros(t, shape=updates_shape)
-    #operand_t = scatter_p.bind(
-    #  t, scatter_indices, zeros, update_jaxpr=update_jaxpr,
-    #  update_consts=update_consts, dimension_numbers=dimension_numbers,
-    #  updates_shape=updates_shape)
 
   if updates is None:
     gather_dnums = GatherDimensionNumbers(
@@ -2036,12 +2026,11 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
         pos += 1
     update_t = gather(t, scatter_indices, dimension_numbers=gather_dnums,
                       slice_sizes=slice_sizes)
-    print(""transpose out "", update_t, scatter_indices, gather_dnums, slice_sizes)
-  return [operand_t, update_t, None]
+  return [operand_t, None, update_t]
 
 
 scatter_p = standard_primitive(
-    scatter_shape_rule, scatter_dtype_rule, 'scatter',
+    scatter_shape_rule, scatter_dtype_rule, 'scatter-add',
     scatter_translation_rule)
 ad.primitive_jvps[scatter_p] = scatter_jvp
 ad.primitive_transposes[scatter_p] = scatter_transpose_rule","diff --git a/jax/lax.py b/jax/lax.py
index ce514bb5c..eae671100 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -216,9 +216,8 @@ def gather(operand, start_indices, dimension_numbers=None, slice_sizes=None):
       operand, start_indices, dimension_numbers=dimension_numbers,
       slice_sizes=tuple(slice_sizes), operand_shape=operand.shape)
 
-def scatter(operand, scatter_indices, updates, update_computation,
-            dimension_numbers=None):
-  jaxpr, consts = _reduction_jaxpr(update_computation, _const(operand, 0))
+def scatter_add(operand, scatter_indices, updates, dimension_numbers=None):
+  jaxpr, consts = _reduction_jaxpr(add, _const(operand, 0))
   return scatter_p.bind(
       operand, scatter_indices, updates, update_jaxpr=jaxpr,
       update_consts=consts, dimension_numbers=dimension_numbers,
@@ -1938,7 +1937,7 @@ def gather_transpose_rule(t, operand, start_indices, dimension_numbers,
     inserted_window_dims=dimension_numbers.collapsed_slice_dims,
     scatter_dims_to_operand_dims=dimension_numbers.start_index_map,
     index_vector_dim=dimension_numbers.index_vector_dim)
-  return [scatter(zeros, start_indices, t, add, scatter_dnums), ad_util.zero]
+  return [scatter_add(zeros, start_indices, t, scatter_dnums), ad_util.zero]
 
 
 gather_p = standard_primitive(
@@ -1994,7 +1993,6 @@ def scatter_jvp(primals, tangents, update_jaxpr, update_consts,
   if g_operand is ad_util.zero and g_updates is ad_util.zero:
     tangent_out = ad_util.zero
   else:
-    print(""scatter jvp "", g_operand, g_updates)
     g_operand = ad.instantiate_zeros(operand, g_operand)
     g_updates = ad.instantiate_zeros(updates, g_updates)
     tangent_out = scatter_p.bind(
@@ -2008,17 +2006,9 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
                            update_jaxpr, update_consts, dimension_numbers,
                            updates_shape):
   assert scatter_indices is not None
-  print(""scatter transpose "", t, operand, updates, dimension_numbers)
-  assert (operand is None) ^ (updates is None)
   operand_t = update_t = None
   if operand is None:
-    # TODO(phawkins): only correct for scatter-add.
     operand_t = t
-    #zeros = _zeros(t, shape=updates_shape)
-    #operand_t = scatter_p.bind(
-    #  t, scatter_indices, zeros, update_jaxpr=update_jaxpr,
-    #  update_consts=update_consts, dimension_numbers=dimension_numbers,
-    #  updates_shape=updates_shape)
 
   if updates is None:
     gather_dnums = GatherDimensionNumbers(
@@ -2036,12 +2026,11 @@ def scatter_transpose_rule(t, operand, scatter_indices, updates,
         pos += 1
     update_t = gather(t, scatter_indices, dimension_numbers=gather_dnums,
                       slice_sizes=slice_sizes)
-    print(""transpose out "", update_t, scatter_indices, gather_dnums, slice_sizes)
-  return [operand_t, update_t, None]
+  return [operand_t, None, update_t]
 
 
 scatter_p = standard_primitive(
-    scatter_shape_rule, scatter_dtype_rule, 'scatter',
+    scatter_shape_rule, scatter_dtype_rule, 'scatter-add',
     scatter_translation_rule)
 ad.primitive_jvps[scatter_p] = scatter_jvp
 ad.primitive_transposes[scatter_p] = scatter_transpose_rule",No
tests/lax_test.py,tests/lax_test.py,5fac477a8a49c7770d555a82f9f0941f1a2ab8e7,39257b24426eee34dd6c23ff3b6e08ed6c77fef1,"Fix bug in scatter transpose rule.

Add some simple gather and scatter tests.","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 867577100..faf901dad 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1347,6 +1347,62 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_dnums={}_slice_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, dnums,
+          slice_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""dnums"": dnums,
+       ""slice_sizes"": slice_sizes, ""rng"": rng, ""rng_idx"": rng_idx}
+      for dtype in all_dtypes
+      for shape, idxs, dnums, slice_sizes in [
+          ((5,), onp.array([0, 2]), lax.GatherDimensionNumbers(
+            offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1,)),
+          ((10,), onp.array([0, 0, 0]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,),
+            index_vector_dim=1), (2,)),
+          ((10, 5,), onp.array([0, 2, 1]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1, 3)),
+      ]
+      for rng_idx in [jtu.rand_int(max(shape))]
+      for rng in [jtu.rand_default()]))
+  def testGather(self, shape, dtype, idxs, dnums, slice_sizes, rng, rng_idx):
+    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)
+    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
+    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_update={}_dnums={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          idxs, update_shape, dnums),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""idxs"": idxs,
+       ""update_shape"": update_shape, ""dnums"": dnums, ""rng"": rng,
+       ""rng_idx"": rng_idx}
+      for dtype in float_dtypes
+      for arg_shape, idxs, update_shape, dnums in [
+          ((5,), onp.array([0, 2]), (2,), lax.ScatterDimensionNumbers(
+            update_window_dims=(), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10,), onp.array([0, 0, 0]), (3, 2), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10, 5,), onp.array([0, 2, 1]), (3, 3), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+      ]
+      for rng_idx in [jtu.rand_int(max(arg_shape))]
+      for rng in [jtu.rand_default()]))
+  def testScatterAdd(self, arg_shape, dtype, idxs, update_shape, dnums, rng,
+                     rng_idx):
+    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)
+    args_maker = lambda: [rng(arg_shape, dtype), rand_idxs(),
+                          rng(update_shape, dtype)]
+    fun = partial(lax.scatter_add, dimension_numbers=dnums)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+
 class DeviceConstantTest(jtu.JaxTestCase):
   def _CheckDeviceConstant(self, make_const, expected):
     # check casting to ndarray works
@@ -2055,5 +2111,63 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
 
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_dnums={}_slice_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, dnums,
+          slice_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""dnums"": dnums,
+       ""slice_sizes"": slice_sizes, ""rng"": rng, ""rng_idx"": rng_idx}
+      for dtype in float_dtypes
+      for shape, idxs, dnums, slice_sizes in [
+          ((5,), onp.array([0, 2]), lax.GatherDimensionNumbers(
+            offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1,)),
+          ((10,), onp.array([0, 0, 0]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,),
+            index_vector_dim=1), (2,)),
+          ((10, 5,), onp.array([0, 2, 1]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1, 3)),
+      ]
+      for rng_idx in [jtu.rand_int(max(shape))]
+      for rng in [jtu.rand_default()]))
+  def testGatherGrad(self, shape, dtype, idxs, dnums, slice_sizes, rng, rng_idx):
+    idxs = rng_idx(idxs.shape, idxs.dtype)
+    gather = lambda x: lax.gather(x, idxs, dimension_numbers=dnums,
+                                  slice_sizes=slice_sizes)
+    x = rng(shape, dtype)
+    check_grads(gather, (x,), 2)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_update={}_dnums={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          idxs, update_shape, dnums),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""idxs"": idxs,
+       ""update_shape"": update_shape, ""dnums"": dnums, ""rng"": rng,
+       ""rng_idx"": rng_idx}
+      for dtype in float_dtypes
+      for arg_shape, idxs, update_shape, dnums in [
+          ((5,), onp.array([0, 2]), (2,), lax.ScatterDimensionNumbers(
+            update_window_dims=(), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10,), onp.array([0, 0, 0]), (3, 2), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10, 5,), onp.array([0, 2, 1]), (3, 3), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+      ]
+      for rng_idx in [jtu.rand_int(max(arg_shape))]
+      for rng in [jtu.rand_default()]))
+  def testScatterAddGrad(self, arg_shape, dtype, idxs, update_shape, dnums, rng,
+                         rng_idx):
+    idxs = rng_idx(idxs.shape, idxs.dtype)
+    scatter_add = lambda x, y: lax.scatter_add(x, idxs, y,
+                                               dimension_numbers=dnums)
+    x = rng(arg_shape, dtype)
+    y = rng(update_shape, dtype)
+    check_grads(scatter_add, (x, y), 2)
+
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/lax_test.py b/tests/lax_test.py
index 867577100..faf901dad 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1347,6 +1347,62 @@ class LaxTest(jtu.JaxTestCase):
     self._CompileAndCheck(fun, args_maker, check_dtypes=True)
 
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_dnums={}_slice_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, dnums,
+          slice_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""dnums"": dnums,
+       ""slice_sizes"": slice_sizes, ""rng"": rng, ""rng_idx"": rng_idx}
+      for dtype in all_dtypes
+      for shape, idxs, dnums, slice_sizes in [
+          ((5,), onp.array([0, 2]), lax.GatherDimensionNumbers(
+            offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1,)),
+          ((10,), onp.array([0, 0, 0]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,),
+            index_vector_dim=1), (2,)),
+          ((10, 5,), onp.array([0, 2, 1]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1, 3)),
+      ]
+      for rng_idx in [jtu.rand_int(max(shape))]
+      for rng in [jtu.rand_default()]))
+  def testGather(self, shape, dtype, idxs, dnums, slice_sizes, rng, rng_idx):
+    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)
+    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
+    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_update={}_dnums={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          idxs, update_shape, dnums),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""idxs"": idxs,
+       ""update_shape"": update_shape, ""dnums"": dnums, ""rng"": rng,
+       ""rng_idx"": rng_idx}
+      for dtype in float_dtypes
+      for arg_shape, idxs, update_shape, dnums in [
+          ((5,), onp.array([0, 2]), (2,), lax.ScatterDimensionNumbers(
+            update_window_dims=(), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10,), onp.array([0, 0, 0]), (3, 2), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10, 5,), onp.array([0, 2, 1]), (3, 3), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+      ]
+      for rng_idx in [jtu.rand_int(max(arg_shape))]
+      for rng in [jtu.rand_default()]))
+  def testScatterAdd(self, arg_shape, dtype, idxs, update_shape, dnums, rng,
+                     rng_idx):
+    rand_idxs = lambda: rng_idx(idxs.shape, idxs.dtype)
+    args_maker = lambda: [rng(arg_shape, dtype), rand_idxs(),
+                          rng(update_shape, dtype)]
+    fun = partial(lax.scatter_add, dimension_numbers=dnums)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+
 class DeviceConstantTest(jtu.JaxTestCase):
   def _CheckDeviceConstant(self, make_const, expected):
     # check casting to ndarray works
@@ -2055,5 +2111,63 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
 
 
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_dnums={}_slice_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, dnums,
+          slice_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""dnums"": dnums,
+       ""slice_sizes"": slice_sizes, ""rng"": rng, ""rng_idx"": rng_idx}
+      for dtype in float_dtypes
+      for shape, idxs, dnums, slice_sizes in [
+          ((5,), onp.array([0, 2]), lax.GatherDimensionNumbers(
+            offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1,)),
+          ((10,), onp.array([0, 0, 0]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,),
+            index_vector_dim=1), (2,)),
+          ((10, 5,), onp.array([0, 2, 1]), lax.GatherDimensionNumbers(
+            offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,),
+            index_vector_dim=1), (1, 3)),
+      ]
+      for rng_idx in [jtu.rand_int(max(shape))]
+      for rng in [jtu.rand_default()]))
+  def testGatherGrad(self, shape, dtype, idxs, dnums, slice_sizes, rng, rng_idx):
+    idxs = rng_idx(idxs.shape, idxs.dtype)
+    gather = lambda x: lax.gather(x, idxs, dimension_numbers=dnums,
+                                  slice_sizes=slice_sizes)
+    x = rng(shape, dtype)
+    check_grads(gather, (x,), 2)
+
+  @parameterized.named_parameters(jtu.cases_from_list(
+      {""testcase_name"": ""_shape={}_idxs={}_update={}_dnums={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          idxs, update_shape, dnums),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""idxs"": idxs,
+       ""update_shape"": update_shape, ""dnums"": dnums, ""rng"": rng,
+       ""rng_idx"": rng_idx}
+      for dtype in float_dtypes
+      for arg_shape, idxs, update_shape, dnums in [
+          ((5,), onp.array([0, 2]), (2,), lax.ScatterDimensionNumbers(
+            update_window_dims=(), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10,), onp.array([0, 0, 0]), (3, 2), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+          ((10, 5,), onp.array([0, 2, 1]), (3, 3), lax.ScatterDimensionNumbers(
+            update_window_dims=(1,), inserted_window_dims=(0,),
+            scatter_dims_to_operand_dims=(0,), index_vector_dim=1)),
+      ]
+      for rng_idx in [jtu.rand_int(max(arg_shape))]
+      for rng in [jtu.rand_default()]))
+  def testScatterAddGrad(self, arg_shape, dtype, idxs, update_shape, dnums, rng,
+                         rng_idx):
+    idxs = rng_idx(idxs.shape, idxs.dtype)
+    scatter_add = lambda x, y: lax.scatter_add(x, idxs, y,
+                                               dimension_numbers=dnums)
+    x = rng(arg_shape, dtype)
+    y = rng(update_shape, dtype)
+    check_grads(scatter_add, (x, y), 2)
+
+
 if __name__ == '__main__':
   absltest.main()",No
jax/interpreters/xla.py,jax/interpreters/xla.py,33bd020bd5c16ce4ff0b92cb62e61354f720e986,fc170b2b912f08ca4cacb51b19cd3ecab5156f1b,"Use repr of value for DeviceArray.__repr__

Currently, DevieArray.__repr__ returns the class name and the size of
the object. This is inconsistent with np.ndarray.__repr__ which produces
code to reconstruct the array (including the contents, unless the
contents are too long to print cleanly). This provides a strange
reminder that we aren't using ""real"" numpy so it may be nicer to just
use the standard numpy __repr__ instead.","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 97b6c736d..f6a6891da 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -267,10 +267,6 @@ class DeviceArray(DeviceValue):
     """"""Returns an ndarray (backed by host memory, not device memory).""""""
     return onp.asarray(self)
 
-  def __repr__(self):
-    shape_str = "","".join(map(str, self.shape))
-    return ""DeviceArray{{{}[{}]}}"".format(onp.dtype(self.dtype).name, shape_str)
-
   def __len__(self):
     try:
       return self.shape[0]
@@ -298,6 +294,7 @@ class DeviceArray(DeviceValue):
 
   __array__ = partialmethod(forward_to_value, onp.asarray)
   __str__ = partialmethod(forward_to_value, str)
+  __repr__ = partialmethod(forward_to_value, repr)
   __bool__ = __nonzero__ = partialmethod(forward_to_value, bool)
   __float__ = partialmethod(forward_to_value, float)
   __int__ = partialmethod(forward_to_value, int)","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 97b6c736d..f6a6891da 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -267,10 +267,6 @@ class DeviceArray(DeviceValue):
     """"""Returns an ndarray (backed by host memory, not device memory).""""""
     return onp.asarray(self)
 
-  def __repr__(self):
-    shape_str = "","".join(map(str, self.shape))
-    return ""DeviceArray{{{}[{}]}}"".format(onp.dtype(self.dtype).name, shape_str)
-
   def __len__(self):
     try:
       return self.shape[0]
@@ -298,6 +294,7 @@ class DeviceArray(DeviceValue):
 
   __array__ = partialmethod(forward_to_value, onp.asarray)
   __str__ = partialmethod(forward_to_value, str)
+  __repr__ = partialmethod(forward_to_value, repr)
   __bool__ = __nonzero__ = partialmethod(forward_to_value, bool)
   __float__ = partialmethod(forward_to_value, float)
   __int__ = partialmethod(forward_to_value, int)",No
jax/test_util.py,jax/test_util.py,e00dc5d39df49fbfd009563ee3aa9f73b7ce8ed2,d43c65dcd8201556ec77c11b9520dee426095537,"Restrict the range of np.tan() test to [-1.5, 1.5) to avoid numerical problems.","diff --git a/jax/test_util.py b/jax/test_util.py
index c243cf7c2..3c45681f5 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -290,6 +290,12 @@ def rand_small_positive():
   rand = npr.RandomState(0).rand
   return partial(_rand_dtype, rand, scale=2e-5)
 
+def rand_uniform(low=0.0, high=1.0):
+  assert low < high
+  rand = npr.RandomState(0).rand
+  def fn():
+    return partial(_rand_dtype, rand) * (high - low) + low
+
 
 def rand_some_equal():
   randn = npr.RandomState(0).randn","diff --git a/jax/test_util.py b/jax/test_util.py
index c243cf7c2..3c45681f5 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -290,6 +290,12 @@ def rand_small_positive():
   rand = npr.RandomState(0).rand
   return partial(_rand_dtype, rand, scale=2e-5)
 
+def rand_uniform(low=0.0, high=1.0):
+  assert low < high
+  rand = npr.RandomState(0).rand
+  def fn():
+    return partial(_rand_dtype, rand) * (high - low) + low
+
 
 def rand_some_equal():
   randn = npr.RandomState(0).randn",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,e00dc5d39df49fbfd009563ee3aa9f73b7ce8ed2,d43c65dcd8201556ec77c11b9520dee426095537,"Restrict the range of np.tan() test to [-1.5, 1.5) to avoid numerical problems.","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index b4f92d90b..f74c0f6c6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -92,7 +92,8 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""subtract"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sin"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""cos"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tan"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tan"", 1, number_dtypes, all_shapes, jtu.rand_uniform(-1.5, 1.5),
+              [""rev""]),
     op_record(""sinh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""cosh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""tanh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index b4f92d90b..f74c0f6c6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -92,7 +92,8 @@ JAX_ONE_TO_ONE_OP_RECORDS = [
     op_record(""subtract"", 2, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""sin"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""cos"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
-    op_record(""tan"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
+    op_record(""tan"", 1, number_dtypes, all_shapes, jtu.rand_uniform(-1.5, 1.5),
+              [""rev""]),
     op_record(""sinh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""cosh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),
     op_record(""tanh"", 1, number_dtypes, all_shapes, jtu.rand_default(), [""rev""]),",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,6e684fd3abdca8e7e4d52e78d8ddfbf33777c476,fc170b2b912f08ca4cacb51b19cd3ecab5156f1b,Work around breakage due to Numpy 1.16.0 version update.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 9a7f7e7a0..f4d2b4db5 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -450,6 +450,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
               irng):
     pad_width = irng([len(shape), 2][2 - pad_width_rank:], onp.int32)
     def onp_fun(x, constant_vals):
+      if pad_width.size == 0:
+        return x
       return onp.pad(x, pad_width, mode='constant',
                      constant_values=constant_vals)
     def lnp_fun(x, constant_vals):","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 9a7f7e7a0..f4d2b4db5 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -450,6 +450,8 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
               irng):
     pad_width = irng([len(shape), 2][2 - pad_width_rank:], onp.int32)
     def onp_fun(x, constant_vals):
+      if pad_width.size == 0:
+        return x
       return onp.pad(x, pad_width, mode='constant',
                      constant_values=constant_vals)
     def lnp_fun(x, constant_vals):",No
jax/test_util.py,jax/test_util.py,a21c3c45623ca2d42c3dd4f2822d45a6a8d2e1be,e00dc5d39df49fbfd009563ee3aa9f73b7ce8ed2,Fix definition of rand_uniform().,"diff --git a/jax/test_util.py b/jax/test_util.py
index 3c45681f5..c953c8ee4 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -293,8 +293,8 @@ def rand_small_positive():
 def rand_uniform(low=0.0, high=1.0):
   assert low < high
   rand = npr.RandomState(0).rand
-  def fn():
-    return partial(_rand_dtype, rand) * (high - low) + low
+  post = lambda x: x * (high - low) + low
+  return partial(_rand_dtype, rand, post=post)
 
 
 def rand_some_equal():","diff --git a/jax/test_util.py b/jax/test_util.py
index 3c45681f5..c953c8ee4 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -293,8 +293,8 @@ def rand_small_positive():
 def rand_uniform(low=0.0, high=1.0):
   assert low < high
   rand = npr.RandomState(0).rand
-  def fn():
-    return partial(_rand_dtype, rand) * (high - low) + low
+  post = lambda x: x * (high - low) + low
+  return partial(_rand_dtype, rand, post=post)
 
 
 def rand_some_equal():",No
tests/lax_test.py,tests/lax_test.py,f16de58bba833f7c90a0c19b9e4eb0efcc779bc0,5fac477a8a49c7770d555a82f9f0941f1a2ab8e7,Relax tolerance for scatter/gather gradient tests.,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index faf901dad..401d16e40 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -2110,7 +2110,6 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
     check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
 
-
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_dnums={}_slice_sizes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), idxs, dnums,
@@ -2136,7 +2135,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     gather = lambda x: lax.gather(x, idxs, dimension_numbers=dnums,
                                   slice_sizes=slice_sizes)
     x = rng(shape, dtype)
-    check_grads(gather, (x,), 2)
+    check_grads(gather, (x,), 2, 1e-2, 1e-2, 1e-2)
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_update={}_dnums={}"".format(
@@ -2166,7 +2165,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
                                                dimension_numbers=dnums)
     x = rng(arg_shape, dtype)
     y = rng(update_shape, dtype)
-    check_grads(scatter_add, (x, y), 2)
+    check_grads(scatter_add, (x, y), 2, 1e-2, 1e-2, 1e-2)
 
 
 if __name__ == '__main__':","diff --git a/tests/lax_test.py b/tests/lax_test.py
index faf901dad..401d16e40 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -2110,7 +2110,6 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
     check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
 
-
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_dnums={}_slice_sizes={}"".format(
           jtu.format_shape_dtype_string(shape, dtype), idxs, dnums,
@@ -2136,7 +2135,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     gather = lambda x: lax.gather(x, idxs, dimension_numbers=dnums,
                                   slice_sizes=slice_sizes)
     x = rng(shape, dtype)
-    check_grads(gather, (x,), 2)
+    check_grads(gather, (x,), 2, 1e-2, 1e-2, 1e-2)
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_shape={}_idxs={}_update={}_dnums={}"".format(
@@ -2166,7 +2165,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
                                                dimension_numbers=dnums)
     x = rng(arg_shape, dtype)
     y = rng(update_shape, dtype)
-    check_grads(scatter_add, (x, y), 2)
+    check_grads(scatter_add, (x, y), 2, 1e-2, 1e-2, 1e-2)
 
 
 if __name__ == '__main__':",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,ade92703ec100d54c04de83daec8660377db4ac2,7404cb95b4538c9f8686f7a74318d5f096a76005,guard np.take_along_axis test in case of old numpy,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 6c03f7f99..e0fc520c6 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -201,13 +201,13 @@ def _constant_like(x, const):
 
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Original docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
   def wrap(op):
     try:
+      docstr = """"""
+      LAX-backed implementation of {fun}. Original docstring below.
+
+      {np_doc}
+      """""".format(fun=fun.__name__, np_doc=fun.__doc__)
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
@@ -1553,7 +1553,7 @@ def argsort(a, axis=-1, kind='quicksort', order=None):
     return perm
 
 
-@_wraps(onp.take_along_axis)
+@_wraps(getattr(onp, ""take_along_axis"", None))
 def take_along_axis(arr, indices, axis):
   if axis is None and ndim(arr) != 1:
     return take_along_axis(arr.ravel(), indices.ravel(), 0)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 6c03f7f99..e0fc520c6 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -201,13 +201,13 @@ def _constant_like(x, const):
 
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Original docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
   def wrap(op):
     try:
+      docstr = """"""
+      LAX-backed implementation of {fun}. Original docstring below.
+
+      {np_doc}
+      """""".format(fun=fun.__name__, np_doc=fun.__doc__)
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
@@ -1553,7 +1553,7 @@ def argsort(a, axis=-1, kind='quicksort', order=None):
     return perm
 
 
-@_wraps(onp.take_along_axis)
+@_wraps(getattr(onp, ""take_along_axis"", None))
 def take_along_axis(arr, indices, axis):
   if axis is None and ndim(arr) != 1:
     return take_along_axis(arr.ravel(), indices.ravel(), 0)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,ade92703ec100d54c04de83daec8660377db4ac2,7404cb95b4538c9f8686f7a74318d5f096a76005,guard np.take_along_axis test in case of old numpy,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index f4d2b4db5..49702588e 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -1023,8 +1023,10 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       return x, i
 
     lnp_op = lambda x, i: lnp.take_along_axis(x, i, axis=axis)
-    onp_op = lambda x, i: onp.take_along_axis(x, i, axis=axis)
-    self._CheckAgainstNumpy(lnp_op, onp_op, args_maker, check_dtypes=True)
+
+    if hasattr(onp, ""take_along_axis""):
+      onp_op = lambda x, i: onp.take_along_axis(x, i, axis=axis)
+      self._CheckAgainstNumpy(lnp_op, onp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
 ","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index f4d2b4db5..49702588e 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -1023,8 +1023,10 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
       return x, i
 
     lnp_op = lambda x, i: lnp.take_along_axis(x, i, axis=axis)
-    onp_op = lambda x, i: onp.take_along_axis(x, i, axis=axis)
-    self._CheckAgainstNumpy(lnp_op, onp_op, args_maker, check_dtypes=True)
+
+    if hasattr(onp, ""take_along_axis""):
+      onp_op = lambda x, i: onp.take_along_axis(x, i, axis=axis)
+      self._CheckAgainstNumpy(lnp_op, onp_op, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
 
 ",No
tests/linalg_test.py,tests/linalg_test.py,bc7231e6a2203b7b99fdb17889f03aa6e1391367,7404cb95b4538c9f8686f7a74318d5f096a76005,"Fix incorrect norm definition in SVD test case, and adjust test tolerances.","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index b1973efc1..d91bb2e6f 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -154,8 +154,8 @@ class NumpyLinalgTest(jtu.JaxTestCase):
 
     # Norm, adjusted for dimension and type.
     def norm(x):
-      n = onp.linalg.norm(x, axis=(-2, -1))
-      return n / (max(m, n) * onp.finfo(dtype).eps)
+      norm = onp.linalg.norm(x, axis=(-2, -1))
+      return norm / (max(m, n) * onp.finfo(dtype).eps)
 
     a, = args_maker()
     out = np.linalg.svd(a, full_matrices=full_matrices, compute_uv=compute_uv)
@@ -165,11 +165,11 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 100))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 50))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 100))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 50))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 100))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
 
       # Check the unitary properties of the singular vector matrices.
       self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 10))
@@ -179,7 +179,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
         self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 20))
 
     else:
-      self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))
+      self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out), atol=1e-4, rtol=1e-4))
 
     self._CompileAndCheck(partial(np.linalg.svd, full_matrices=full_matrices, compute_uv=compute_uv),
                           args_maker, check_dtypes=True)","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index b1973efc1..d91bb2e6f 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -154,8 +154,8 @@ class NumpyLinalgTest(jtu.JaxTestCase):
 
     # Norm, adjusted for dimension and type.
     def norm(x):
-      n = onp.linalg.norm(x, axis=(-2, -1))
-      return n / (max(m, n) * onp.finfo(dtype).eps)
+      norm = onp.linalg.norm(x, axis=(-2, -1))
+      return norm / (max(m, n) * onp.finfo(dtype).eps)
 
     a, = args_maker()
     out = np.linalg.svd(a, full_matrices=full_matrices, compute_uv=compute_uv)
@@ -165,11 +165,11 @@ class NumpyLinalgTest(jtu.JaxTestCase):
       if full_matrices:
         k = min(m, n)
         if m < n:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 100))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2][:k, :])) < 50))
         else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 100))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0][:, :k], out[2])) < 50))
       else:
-          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 100))
+          self.assertTrue(onp.all(norm(a - onp.matmul(out[1] * out[0], out[2])) < 50))
 
       # Check the unitary properties of the singular vector matrices.
       self.assertTrue(onp.all(norm(onp.eye(out[0].shape[1]) - onp.matmul(onp.conj(T(out[0])), out[0])) < 10))
@@ -179,7 +179,7 @@ class NumpyLinalgTest(jtu.JaxTestCase):
         self.assertTrue(onp.all(norm(onp.eye(out[2].shape[0]) - onp.matmul(out[2], onp.conj(T(out[2])))) < 20))
 
     else:
-      self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out)))
+      self.assertTrue(onp.allclose(onp.linalg.svd(a, compute_uv=False), onp.asarray(out), atol=1e-4, rtol=1e-4))
 
     self._CompileAndCheck(partial(np.linalg.svd, full_matrices=full_matrices, compute_uv=compute_uv),
                           args_maker, check_dtypes=True)",No
jax/util.py,jax/util.py,05b1049e497b241942f3ebc17dd0f295eaae547a,3f63684b842b8fbcb6583633fdad52c95b53d277,"Change util.memoize to be an LRU cache with a default size of 64 entries.

The goal is to limit peak memory when running a large number of computations, e.g., the test suite.","diff --git a/jax/util.py b/jax/util.py
index ed6fd0b12..1855a5cab 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -16,12 +16,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import collections
 import functools
 import itertools as it
 from operator import mul
 import types
 import numpy as onp
 
+import six
+
 allow_memoize_hash_failures = False
 
 
@@ -126,22 +129,31 @@ def split_merge(predicate, xs):
   return lhs, rhs, merge
 
 
-class _MemoizeNoEntry(object):
-  pass
+_NO_MEMO_ENTRY = object()
 
-_NO_MEMO_ENTRY = _MemoizeNoEntry
+def memoize(fun, max_size=64):
+  cache = collections.OrderedDict()
+  if six.PY3:
+    move_to_end = lambda key, _: cache.move_to_end(key)
+  else:
+    def move_to_end(key, value):
+      del cache[key]
+      cache[key] = value
 
-def memoize(fun):
-  cache = {}
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))
     try:
       ans = cache.get(key, _NO_MEMO_ENTRY)
       if ans != _NO_MEMO_ENTRY:
+        move_to_end(key, ans)
         return ans
     except TypeError:
       if not allow_memoize_hash_failures:
         raise
+
+    if len(cache) > max_size:
+      cache.popitem(last=False)
+
     ans = cache[key] = fun(*args, **kwargs)
     return ans
   return memoized_fun","diff --git a/jax/util.py b/jax/util.py
index ed6fd0b12..1855a5cab 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -16,12 +16,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import collections
 import functools
 import itertools as it
 from operator import mul
 import types
 import numpy as onp
 
+import six
+
 allow_memoize_hash_failures = False
 
 
@@ -126,22 +129,31 @@ def split_merge(predicate, xs):
   return lhs, rhs, merge
 
 
-class _MemoizeNoEntry(object):
-  pass
+_NO_MEMO_ENTRY = object()
 
-_NO_MEMO_ENTRY = _MemoizeNoEntry
+def memoize(fun, max_size=64):
+  cache = collections.OrderedDict()
+  if six.PY3:
+    move_to_end = lambda key, _: cache.move_to_end(key)
+  else:
+    def move_to_end(key, value):
+      del cache[key]
+      cache[key] = value
 
-def memoize(fun):
-  cache = {}
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))
     try:
       ans = cache.get(key, _NO_MEMO_ENTRY)
       if ans != _NO_MEMO_ENTRY:
+        move_to_end(key, ans)
         return ans
     except TypeError:
       if not allow_memoize_hash_failures:
         raise
+
+    if len(cache) > max_size:
+      cache.popitem(last=False)
+
     ans = cache[key] = fun(*args, **kwargs)
     return ans
   return memoized_fun",No
jax/linear_util.py,jax/linear_util.py,3266bb31221f6c1955660c951c7550351ad243c1,05b1049e497b241942f3ebc17dd0f295eaae547a,"Change linear_util.memoize to use an LRU cache.

Add util.OrderedDict that retrofits a move_to_end method onto Python 2 OrderedDicts.","diff --git a/jax/linear_util.py b/jax/linear_util.py
index 857f3bce6..4b84b13c7 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -16,7 +16,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from .util import curry, partial
+from .util import curry, partial, OrderedDict
+
 
 def thunk(f):
   store = Store()
@@ -131,14 +132,17 @@ def wrap_init(f, kwargs={}):
   return WrappedFun(f, [], kwargs)
 
 
-def memoize(call):
-  cache = {}
+def memoize(call, max_size=64):
+  cache = OrderedDict()
   def memoized_fun(f, *args):
     key = (f, args)
     if key in cache:
       ans, f_prev = cache[key]
+      cache.move_to_end(key)
       f.populate_stores(f_prev)
     else:
+      if len(cache) > max_size:
+        cache.popitem(last=False)
       ans = call(f, *args)
       cache[key] = (ans, f)
     return ans","diff --git a/jax/linear_util.py b/jax/linear_util.py
index 857f3bce6..4b84b13c7 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -16,7 +16,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from .util import curry, partial
+from .util import curry, partial, OrderedDict
+
 
 def thunk(f):
   store = Store()
@@ -131,14 +132,17 @@ def wrap_init(f, kwargs={}):
   return WrappedFun(f, [], kwargs)
 
 
-def memoize(call):
-  cache = {}
+def memoize(call, max_size=64):
+  cache = OrderedDict()
   def memoized_fun(f, *args):
     key = (f, args)
     if key in cache:
       ans, f_prev = cache[key]
+      cache.move_to_end(key)
       f.populate_stores(f_prev)
     else:
+      if len(cache) > max_size:
+        cache.popitem(last=False)
       ans = call(f, *args)
       cache[key] = (ans, f)
     return ans",No
jax/util.py,jax/util.py,3266bb31221f6c1955660c951c7550351ad243c1,05b1049e497b241942f3ebc17dd0f295eaae547a,"Change linear_util.memoize to use an LRU cache.

Add util.OrderedDict that retrofits a move_to_end method onto Python 2 OrderedDicts.","diff --git a/jax/util.py b/jax/util.py
index 1855a5cab..c084d0eac 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -129,23 +129,27 @@ def split_merge(predicate, xs):
   return lhs, rhs, merge
 
 
+if six.PY3:
+  OrderedDict = collections.OrderedDict
+else:
+  # Retrofits a move_to_end method to OrderedDict in Python 2 mode.
+  class OrderedDict(collections.OrderedDict):
+    def move_to_end(self, key):
+      value = self[key]
+      del self[key]
+      self[key] = value
+
+
 _NO_MEMO_ENTRY = object()
 
 def memoize(fun, max_size=64):
-  cache = collections.OrderedDict()
-  if six.PY3:
-    move_to_end = lambda key, _: cache.move_to_end(key)
-  else:
-    def move_to_end(key, value):
-      del cache[key]
-      cache[key] = value
-
+  cache = OrderedDict()
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))
     try:
       ans = cache.get(key, _NO_MEMO_ENTRY)
       if ans != _NO_MEMO_ENTRY:
-        move_to_end(key, ans)
+        cache.move_to_end(key)
         return ans
     except TypeError:
       if not allow_memoize_hash_failures:","diff --git a/jax/util.py b/jax/util.py
index 1855a5cab..c084d0eac 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -129,23 +129,27 @@ def split_merge(predicate, xs):
   return lhs, rhs, merge
 
 
+if six.PY3:
+  OrderedDict = collections.OrderedDict
+else:
+  # Retrofits a move_to_end method to OrderedDict in Python 2 mode.
+  class OrderedDict(collections.OrderedDict):
+    def move_to_end(self, key):
+      value = self[key]
+      del self[key]
+      self[key] = value
+
+
 _NO_MEMO_ENTRY = object()
 
 def memoize(fun, max_size=64):
-  cache = collections.OrderedDict()
-  if six.PY3:
-    move_to_end = lambda key, _: cache.move_to_end(key)
-  else:
-    def move_to_end(key, value):
-      del cache[key]
-      cache[key] = value
-
+  cache = OrderedDict()
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))
     try:
       ans = cache.get(key, _NO_MEMO_ENTRY)
       if ans != _NO_MEMO_ENTRY:
-        move_to_end(key, ans)
+        cache.move_to_end(key)
         return ans
     except TypeError:
       if not allow_memoize_hash_failures:",No
jax/linear_util.py,jax/linear_util.py,2b383bdbd99ec89a5a006c2a944aa6b5ef4c55af,3266bb31221f6c1955660c951c7550351ad243c1,Increase cache size to 4096.,"diff --git a/jax/linear_util.py b/jax/linear_util.py
index 4b84b13c7..2a4804a57 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -132,7 +132,7 @@ def wrap_init(f, kwargs={}):
   return WrappedFun(f, [], kwargs)
 
 
-def memoize(call, max_size=64):
+def memoize(call, max_size=4096):
   cache = OrderedDict()
   def memoized_fun(f, *args):
     key = (f, args)","diff --git a/jax/linear_util.py b/jax/linear_util.py
index 4b84b13c7..2a4804a57 100644
--- a/jax/linear_util.py
+++ b/jax/linear_util.py
@@ -132,7 +132,7 @@ def wrap_init(f, kwargs={}):
   return WrappedFun(f, [], kwargs)
 
 
-def memoize(call, max_size=64):
+def memoize(call, max_size=4096):
   cache = OrderedDict()
   def memoized_fun(f, *args):
     key = (f, args)",No
jax/util.py,jax/util.py,2b383bdbd99ec89a5a006c2a944aa6b5ef4c55af,3266bb31221f6c1955660c951c7550351ad243c1,Increase cache size to 4096.,"diff --git a/jax/util.py b/jax/util.py
index c084d0eac..e7859479d 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -142,7 +142,7 @@ else:
 
 _NO_MEMO_ENTRY = object()
 
-def memoize(fun, max_size=64):
+def memoize(fun, max_size=4096):
   cache = OrderedDict()
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))","diff --git a/jax/util.py b/jax/util.py
index c084d0eac..e7859479d 100644
--- a/jax/util.py
+++ b/jax/util.py
@@ -142,7 +142,7 @@ else:
 
 _NO_MEMO_ENTRY = object()
 
-def memoize(fun, max_size=64):
+def memoize(fun, max_size=4096):
   cache = OrderedDict()
   def memoized_fun(*args, **kwargs):
     key = (args, tuple(kwargs and sorted(kwargs.items())))",No
jax/api.py,jax/api.py,86d8915c3d88ee2da77fdc6a1e339376c2067731,4792b9bed323fe6f7735f35d863220eb4e29c299,Add Sphinx-generated reference documentation for JAX.,"diff --git a/jax/api.py b/jax/api.py
index b7c551ed0..16f15c618 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -227,9 +227,9 @@ def vmap(fun, in_axes=0, out_axes=0):
   For example, we can implement a matrix-matrix product using a vector dot
   product:
 
-    vv = lambda x, y: np.vdot(x, y)  #  ([a], [a]) -> []
-    mv = vmap(vv, (0, None), 0)      #  ([a,b], [b]) -> [a]
-    mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
+  >>> vv = lambda x, y: np.vdot(x, y)  #  ([a], [a]) -> []
+  >>> mv = vmap(vv, (0, None), 0)      #  ([a,b], [b]) -> [a]
+  >>> mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
 
   (`[a,b]` indicates an array with shape (a,b))
   """"""","diff --git a/jax/api.py b/jax/api.py
index b7c551ed0..16f15c618 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -227,9 +227,9 @@ def vmap(fun, in_axes=0, out_axes=0):
   For example, we can implement a matrix-matrix product using a vector dot
   product:
 
-    vv = lambda x, y: np.vdot(x, y)  #  ([a], [a]) -> []
-    mv = vmap(vv, (0, None), 0)      #  ([a,b], [b]) -> [a]
-    mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
+  >>> vv = lambda x, y: np.vdot(x, y)  #  ([a], [a]) -> []
+  >>> mv = vmap(vv, (0, None), 0)      #  ([a,b], [b]) -> [a]
+  >>> mm = vmap(mv, (None, 1), 1)      #  ([a,b], [b,c]) -> [a,c]
 
   (`[a,b]` indicates an array with shape (a,b))
   """"""",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,86d8915c3d88ee2da77fdc6a1e339376c2067731,4792b9bed323fe6f7735f35d863220eb4e29c299,Add Sphinx-generated reference documentation for JAX.,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 14b69f2f3..1ddc4b18e 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 import collections
 import itertools
+import re
 import string
 import warnings
 
@@ -199,15 +200,36 @@ def _promote_args_like(op, *args):
 def _constant_like(x, const):
   return onp.array(const, dtype=_dtype(x))
 
+_numpy_signature_re = re.compile(r'^([\w., ]+=)?\s*[\w\.]+\(.*\)$')
+
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   def wrap(op):
     try:
-      docstr = """"""
-      LAX-backed implementation of {fun}. Original docstring below.
-
-      {np_doc}
-      """""".format(fun=fun.__name__, np_doc=fun.__doc__)
+      # Numpy doc comments have the form:
+      # fn(x, y, z)          (optional)
+      #
+      # A one-line summary
+      #
+      # ... everything else ...
+      # We (a) move the summary to the top, since it is what the Sphinx
+      # autosummary extension expects, and (b) add a comment below the summary
+      # to the effect that this is a LAX wrapper of a Numpy function.
+      sections = fun.__doc__.split(""\n\n"")
+
+      signatures = []
+      summary = None
+      for i in xrange(len(sections)):
+        if _numpy_signature_re.match(sections[i]):
+          signatures.append(sections[i])
+        else:
+          summary = sections[i].strip()
+          break
+      body = ""\n\n"".join(signatures + sections[i + 1:])
+      docstr = (
+        ""{summary}\n\nLAX-backend implementation of :func:`{fun}`. ""
+        ""Original docstring below.\n\n{body}"".format(
+          summary=summary, fun=fun.__name__, body=body))
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
@@ -426,7 +448,7 @@ def remainder(x1, x2):
   x1, x2 = _promote_args(""remainder"", x1, x2)
   return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
 mod = remainder
-fmod = lax.rem
+fmod = _wraps(onp.fmod)(lambda x, y: lax.rem(x, y))
 
 
 @_wraps(onp.sqrt)
@@ -1268,6 +1290,7 @@ def tensordot(a, b, axes=2):
   raise TypeError(msg)
 
 
+@_wraps(onp.einsum)
 def einsum(*operands):
   # using einsum_call=True here is an internal api for opt_einsum
   operands, contractions = opt_einsum.contract_path(","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 14b69f2f3..1ddc4b18e 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 import collections
 import itertools
+import re
 import string
 import warnings
 
@@ -199,15 +200,36 @@ def _promote_args_like(op, *args):
 def _constant_like(x, const):
   return onp.array(const, dtype=_dtype(x))
 
+_numpy_signature_re = re.compile(r'^([\w., ]+=)?\s*[\w\.]+\(.*\)$')
+
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   def wrap(op):
     try:
-      docstr = """"""
-      LAX-backed implementation of {fun}. Original docstring below.
+      # Numpy doc comments have the form:
+      # fn(x, y, z)          (optional)
+      #
+      # A one-line summary
+      #
+      # ... everything else ...
+      # We (a) move the summary to the top, since it is what the Sphinx
+      # autosummary extension expects, and (b) add a comment below the summary
+      # to the effect that this is a LAX wrapper of a Numpy function.
+      sections = fun.__doc__.split(""\n\n"")
 
-      {np_doc}
-      """""".format(fun=fun.__name__, np_doc=fun.__doc__)
+      signatures = []
+      summary = None
+      for i in xrange(len(sections)):
+        if _numpy_signature_re.match(sections[i]):
+          signatures.append(sections[i])
+        else:
+          summary = sections[i].strip()
+          break
+      body = ""\n\n"".join(signatures + sections[i + 1:])
+      docstr = (
+        ""{summary}\n\nLAX-backend implementation of :func:`{fun}`. ""
+        ""Original docstring below.\n\n{body}"".format(
+          summary=summary, fun=fun.__name__, body=body))
       op.__name__ = fun.__name__
       op.__doc__ = docstr
     finally:
@@ -426,7 +448,7 @@ def remainder(x1, x2):
   x1, x2 = _promote_args(""remainder"", x1, x2)
   return lax.rem(lax.add(lax.rem(x1, x2), x2), x2)
 mod = remainder
-fmod = lax.rem
+fmod = _wraps(onp.fmod)(lambda x, y: lax.rem(x, y))
 
 
 @_wraps(onp.sqrt)
@@ -1268,6 +1290,7 @@ def tensordot(a, b, axes=2):
   raise TypeError(msg)
 
 
+@_wraps(onp.einsum)
 def einsum(*operands):
   # using einsum_call=True here is an internal api for opt_einsum
   operands, contractions = opt_einsum.contract_path(",Yes
setup.py,setup.py,86d8915c3d88ee2da77fdc6a1e339376c2067731,4792b9bed323fe6f7735f35d863220eb4e29c299,Add Sphinx-generated reference documentation for JAX.,"diff --git a/setup.py b/setup.py
index 0f4b2c22c..1aec5283b 100644
--- a/setup.py
+++ b/setup.py
@@ -21,8 +21,10 @@ setup(
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=find_packages(),
-    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
-                      'opt_einsum'],
+    install_requires=[
+        'jaxlib>=0.1.4', 'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
+        'opt_einsum'
+    ],
     url='https://github.com/google/jax',
     license='Apache-2.0',
 )","diff --git a/setup.py b/setup.py
index 0f4b2c22c..1aec5283b 100644
--- a/setup.py
+++ b/setup.py
@@ -21,8 +21,10 @@ setup(
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=find_packages(),
-    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
-                      'opt_einsum'],
+    install_requires=[
+        'jaxlib>=0.1.4', 'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
+        'opt_einsum'
+    ],
     url='https://github.com/google/jax',
     license='Apache-2.0',
 )",No
readthedocs.yml,readthedocs.yml,7d78c3c84cf4e08d3c578f6d4ad9d6a8552ec65d,86d8915c3d88ee2da77fdc6a1e339376c2067731,"Drop jaxlib dependency from setup.py, add a requirements.txt for readthedoc build instead.","diff --git a/readthedocs.yml b/readthedocs.yml
index 1afb6e706..dbdb739f2 100644
--- a/readthedocs.yml
+++ b/readthedocs.yml
@@ -2,4 +2,6 @@ build:
     image: latest
 
 python:
-    version: 3.6
\ No newline at end of file
+    version: 3.6
+
+requirements_file: docs/requirements.txt","diff --git a/readthedocs.yml b/readthedocs.yml
index 1afb6e706..dbdb739f2 100644
--- a/readthedocs.yml
+++ b/readthedocs.yml
@@ -2,4 +2,6 @@ build:
     image: latest
 
 python:
-    version: 3.6
\ No newline at end of file
+    version: 3.6
+
+requirements_file: docs/requirements.txt",No
setup.py,setup.py,7d78c3c84cf4e08d3c578f6d4ad9d6a8552ec65d,86d8915c3d88ee2da77fdc6a1e339376c2067731,"Drop jaxlib dependency from setup.py, add a requirements.txt for readthedoc build instead.","diff --git a/setup.py b/setup.py
index 1aec5283b..d1235b890 100644
--- a/setup.py
+++ b/setup.py
@@ -22,8 +22,7 @@ setup(
     author_email='jax-dev@google.com',
     packages=find_packages(),
     install_requires=[
-        'jaxlib>=0.1.4', 'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
-        'opt_einsum'
+        'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py', 'opt_einsum'
     ],
     url='https://github.com/google/jax',
     license='Apache-2.0',","diff --git a/setup.py b/setup.py
index 1aec5283b..d1235b890 100644
--- a/setup.py
+++ b/setup.py
@@ -22,8 +22,7 @@ setup(
     author_email='jax-dev@google.com',
     packages=find_packages(),
     install_requires=[
-        'jaxlib>=0.1.4', 'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py',
-        'opt_einsum'
+        'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py', 'opt_einsum'
     ],
     url='https://github.com/google/jax',
     license='Apache-2.0',",No
README.md,README.md,63b49e4cbc72af0a7c67b94d7824bfcb38a39348,7d78c3c84cf4e08d3c578f6d4ad9d6a8552ec65d,Add links to reference documentation from README.md.,"diff --git a/README.md b/README.md
index ab95e6e38..905eb5ecf 100644
--- a/README.md
+++ b/README.md
@@ -75,9 +75,10 @@ open](https://github.com/google/jax) by a growing number of
 * [How it works](#how-it-works)
 * [What we're working on](#what-were-working-on)
 * [Current gotchas](#current-gotchas)
+* [Reference documentation](#reference-documentation)
 
 ## Quickstart: Colab in the Cloud
-Jump right in using a notebook in your browser, connected to a Google Cloud GPU: 
+Jump right in using a notebook in your browser, connected to a Google Cloud GPU:
 - [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
 - [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
 
@@ -263,7 +264,10 @@ examples](https://github.com/google/jax/blob/master/examples/).
 
 If youre using JAX just as an accelerator-backed NumPy, without using `grad` or
 `jit` in your code, then in principle there are no constraints, though some
-NumPy functions havent been implemented yet. Generally using `np.dot(A, B)` is
+NumPy functions havent been implemented yet. A list of supported functions can
+be found in the [reference documentation](https://jax.readthedocs.io/).
+
+Generally using `np.dot(A, B)` is
 better than `A.dot(B)` because the former gives us more opportunities to run the
 computation on the device. NumPy also does a lot of work to cast any array-like
 function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
@@ -295,9 +299,9 @@ debugging but they may only be executed once if they're under a `jit` decorator.
 > TLDR **Do use**
 >
 > *   Functional programming
-> *   [Many](https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py) of NumPys
+> *   [Many](https://jax.readthedocs.io/en/latest/jax.numpy.html) of NumPys
 >     functions (help us add more!)
-> *   [Some](https://github.com/google/jax/tree/master/jax/scipy) SciPy functions
+> *   [Some](https://jax.readthedocs.io/en/latest/jax.scipy.html) SciPy functions
 > *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`
 > *   Explicit array creation from lists like `A = np.array([x, y])`
 >
@@ -669,6 +673,11 @@ write unrestricted Python+Numpy and still make use of a hardware accelerator.
 But when you want to maximize performance, you can often use `jit` in your own
 code to compile and end-to-end optimize much bigger functions.
 
+## Reference documentation
+
+For more details about the JAX API, see the
+[reference documentation](https://jax.readthedocs.io/).
+
 ## What we're working on
 1. Documentation!
 2. Cloud TPU support","diff --git a/README.md b/README.md
index ab95e6e38..905eb5ecf 100644
--- a/README.md
+++ b/README.md
@@ -75,9 +75,10 @@ open](https://github.com/google/jax) by a growing number of
 * [How it works](#how-it-works)
 * [What we're working on](#what-were-working-on)
 * [Current gotchas](#current-gotchas)
+* [Reference documentation](#reference-documentation)
 
 ## Quickstart: Colab in the Cloud
-Jump right in using a notebook in your browser, connected to a Google Cloud GPU: 
+Jump right in using a notebook in your browser, connected to a Google Cloud GPU:
 - [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
 - [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
 
@@ -263,7 +264,10 @@ examples](https://github.com/google/jax/blob/master/examples/).
 
 If youre using JAX just as an accelerator-backed NumPy, without using `grad` or
 `jit` in your code, then in principle there are no constraints, though some
-NumPy functions havent been implemented yet. Generally using `np.dot(A, B)` is
+NumPy functions havent been implemented yet. A list of supported functions can
+be found in the [reference documentation](https://jax.readthedocs.io/).
+
+Generally using `np.dot(A, B)` is
 better than `A.dot(B)` because the former gives us more opportunities to run the
 computation on the device. NumPy also does a lot of work to cast any array-like
 function arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`
@@ -295,9 +299,9 @@ debugging but they may only be executed once if they're under a `jit` decorator.
 > TLDR **Do use**
 >
 > *   Functional programming
-> *   [Many](https://github.com/google/jax/blob/master/jax/numpy/lax_numpy.py) of NumPys
+> *   [Many](https://jax.readthedocs.io/en/latest/jax.numpy.html) of NumPys
 >     functions (help us add more!)
-> *   [Some](https://github.com/google/jax/tree/master/jax/scipy) SciPy functions
+> *   [Some](https://jax.readthedocs.io/en/latest/jax.scipy.html) SciPy functions
 > *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`
 > *   Explicit array creation from lists like `A = np.array([x, y])`
 >
@@ -669,6 +673,11 @@ write unrestricted Python+Numpy and still make use of a hardware accelerator.
 But when you want to maximize performance, you can often use `jit` in your own
 code to compile and end-to-end optimize much bigger functions.
 
+## Reference documentation
+
+For more details about the JAX API, see the
+[reference documentation](https://jax.readthedocs.io/).
+
 ## What we're working on
 1. Documentation!
 2. Cloud TPU support",No
WORKSPACE,WORKSPACE,5219bd60cfca8823cd90537077b3375b57fe8fbd,d6184562179548fa5abe04e348ca2bbff62681e7,"Update XLA to https://github.com/tensorflow/tensorflow/commit/bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee .

Fixes #39.","diff --git a/WORKSPACE b/WORKSPACE
index 20d1dd24f..9d1623367 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""8899ef47ab4e43de47397b0e95a5a5b0fcf8bd25ca8f86ca96547064d4384ab3"",
-   strip_prefix = ""tensorflow-937ff1b4cf1d954806e075d66198429a6d2312be"",
+   sha256 = ""35d3fdfee69ad53bb12f67a6d37ebb7492fbe18509c4e4b07d43fa2c2808fb78"",
+   strip_prefix = ""tensorflow-bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/937ff1b4cf1d954806e075d66198429a6d2312be.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 20d1dd24f..9d1623367 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""8899ef47ab4e43de47397b0e95a5a5b0fcf8bd25ca8f86ca96547064d4384ab3"",
-   strip_prefix = ""tensorflow-937ff1b4cf1d954806e075d66198429a6d2312be"",
+   sha256 = ""35d3fdfee69ad53bb12f67a6d37ebb7492fbe18509c4e4b07d43fa2c2808fb78"",
+   strip_prefix = ""tensorflow-bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/937ff1b4cf1d954806e075d66198429a6d2312be.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee.tar.gz"",
    ],
 )
 ",No
jax/interpreters/ad.py,jax/interpreters/ad.py,38b2da82c5a1874ed65bccbc69150679f5ce5866,9820e3885f80ab2c68343d3798e4cec42f09e842,Use slots for tracer classes,"diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 34f16529e..683413e87 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -233,6 +233,8 @@ class JVPTrace(Trace):
 
 
 class JVPTracer(Tracer):
+  __slots__ = ['primal', 'tangent']
+
   def __init__(self, trace, primal, tangent):
     self.trace = trace
     self.primal = primal","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 34f16529e..683413e87 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -233,6 +233,8 @@ class JVPTrace(Trace):
 
 
 class JVPTracer(Tracer):
+  __slots__ = ['primal', 'tangent']
+
   def __init__(self, trace, primal, tangent):
     self.trace = trace
     self.primal = primal",No
jax/interpreters/batching.py,jax/interpreters/batching.py,38b2da82c5a1874ed65bccbc69150679f5ce5866,9820e3885f80ab2c68343d3798e4cec42f09e842,Use slots for tracer classes,"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 9fc3deeb4..37080c60d 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -72,6 +72,8 @@ def batch_subtrace(master, dims, *vals):
 
 
 class BatchTracer(Tracer):
+  __slots__ = ['val', 'batch_dim']
+
   def __init__(self, trace, val, batch_dim):
     self.trace = trace
     self.val = val","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 9fc3deeb4..37080c60d 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -72,6 +72,8 @@ def batch_subtrace(master, dims, *vals):
 
 
 class BatchTracer(Tracer):
+  __slots__ = ['val', 'batch_dim']
+
   def __init__(self, trace, val, batch_dim):
     self.trace = trace
     self.val = val",No
jax/interpreters/partial_eval.py,jax/interpreters/partial_eval.py,38b2da82c5a1874ed65bccbc69150679f5ce5866,9820e3885f80ab2c68343d3798e4cec42f09e842,Use slots for tracer classes,"diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index ad4834f07..610963c88 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -117,6 +117,7 @@ def partial_eval_wrapper(avals, *consts, **kwargs):
 
 
 class JaxprTracer(Tracer):
+  __slots__ = ['pval', 'recipe']
 
   def __init__(self, trace, pval, recipe):
     assert isinstance(pval, PartialVal)","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index ad4834f07..610963c88 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -117,6 +117,7 @@ def partial_eval_wrapper(avals, *consts, **kwargs):
 
 
 class JaxprTracer(Tracer):
+  __slots__ = ['pval', 'recipe']
 
   def __init__(self, trace, pval, recipe):
     assert isinstance(pval, PartialVal)",No
README.md,README.md,bfa35d26dba9d333e73fbf666f921e7454ea4369,63b49e4cbc72af0a7c67b94d7824bfcb38a39348,"Move link to reference documentation earlier.

Add instructions to install scipy when building from source.","diff --git a/README.md b/README.md
index 905eb5ecf..c684b14dd 100644
--- a/README.md
+++ b/README.md
@@ -67,6 +67,7 @@ open](https://github.com/google/jax) by a growing number of
 * [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)
 * [Installation](#installation)
 * [Running the tests](#running-the-tests)
+* [Reference documentation](#reference-documentation)
 * [A brief tour](#a-brief-tour)
 * [What's supported](#whats-supported)
 * [Transformations](#transformations)
@@ -75,25 +76,24 @@ open](https://github.com/google/jax) by a growing number of
 * [How it works](#how-it-works)
 * [What we're working on](#what-were-working-on)
 * [Current gotchas](#current-gotchas)
-* [Reference documentation](#reference-documentation)
 
 ## Quickstart: Colab in the Cloud
 Jump right in using a notebook in your browser, connected to a Google Cloud GPU:
 - [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
 - [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
 
-
 ## Installation
 JAX is written in pure Python, but it depends on XLA, which needs to be
 compiled and installed as the `jaxlib` package. Use the following instructions
 to build JAX from source or install a binary package with pip.
 
 ### Building JAX from source
-First, obtain the JAX source code:
+First, obtain the JAX source code, and make sure `scipy` is installed.
 
 ```bash
 git clone https://github.com/google/jax
 cd jax
+pip install scipy
 ```
 
 To build XLA with CUDA support, you can run
@@ -184,6 +184,11 @@ more detailed information about the cases being run:
 python tests/lax_numpy_test.py --num_generated_cases=5
 ```
 
+## Reference documentation
+
+For details about the JAX API, see the
+[reference documentation](https://jax.readthedocs.io/).
+
 ## A brief tour
 
 ```python
@@ -673,11 +678,6 @@ write unrestricted Python+Numpy and still make use of a hardware accelerator.
 But when you want to maximize performance, you can often use `jit` in your own
 code to compile and end-to-end optimize much bigger functions.
 
-## Reference documentation
-
-For more details about the JAX API, see the
-[reference documentation](https://jax.readthedocs.io/).
-
 ## What we're working on
 1. Documentation!
 2. Cloud TPU support","diff --git a/README.md b/README.md
index 905eb5ecf..c684b14dd 100644
--- a/README.md
+++ b/README.md
@@ -67,6 +67,7 @@ open](https://github.com/google/jax) by a growing number of
 * [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)
 * [Installation](#installation)
 * [Running the tests](#running-the-tests)
+* [Reference documentation](#reference-documentation)
 * [A brief tour](#a-brief-tour)
 * [What's supported](#whats-supported)
 * [Transformations](#transformations)
@@ -75,25 +76,24 @@ open](https://github.com/google/jax) by a growing number of
 * [How it works](#how-it-works)
 * [What we're working on](#what-were-working-on)
 * [Current gotchas](#current-gotchas)
-* [Reference documentation](#reference-documentation)
 
 ## Quickstart: Colab in the Cloud
 Jump right in using a notebook in your browser, connected to a Google Cloud GPU:
 - [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://colab.research.google.com/github/google/jax/blob/master/notebooks/quickstart.ipynb)
 - [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb)
 
-
 ## Installation
 JAX is written in pure Python, but it depends on XLA, which needs to be
 compiled and installed as the `jaxlib` package. Use the following instructions
 to build JAX from source or install a binary package with pip.
 
 ### Building JAX from source
-First, obtain the JAX source code:
+First, obtain the JAX source code, and make sure `scipy` is installed.
 
 ```bash
 git clone https://github.com/google/jax
 cd jax
+pip install scipy
 ```
 
 To build XLA with CUDA support, you can run
@@ -184,6 +184,11 @@ more detailed information about the cases being run:
 python tests/lax_numpy_test.py --num_generated_cases=5
 ```
 
+## Reference documentation
+
+For details about the JAX API, see the
+[reference documentation](https://jax.readthedocs.io/).
+
 ## A brief tour
 
 ```python
@@ -673,11 +678,6 @@ write unrestricted Python+Numpy and still make use of a hardware accelerator.
 But when you want to maximize performance, you can often use `jit` in your own
 code to compile and end-to-end optimize much bigger functions.
 
-## Reference documentation
-
-For more details about the JAX API, see the
-[reference documentation](https://jax.readthedocs.io/).
-
 ## What we're working on
 1. Documentation!
 2. Cloud TPU support",No
build/setup.py,build/setup.py,913d7562a8be59561c2e7309eb51aa63bc013ad3,e44ccd7cf6ed48a21d7b8bd22a6784210880976d,"Add scipy as a dependency of jaxlib.

Jaxlib depends on LAPACK kernels provided by Scipy.","diff --git a/build/setup.py b/build/setup.py
index 7968d30b3..dc74f24cc 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -25,7 +25,7 @@ setup(
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=['jaxlib'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py'],
+    install_requires=['scipy', 'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jaxlib': binary_libs},","diff --git a/build/setup.py b/build/setup.py
index 7968d30b3..dc74f24cc 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -25,7 +25,7 @@ setup(
     author='JAX team',
     author_email='jax-dev@google.com',
     packages=['jaxlib'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py'],
+    install_requires=['scipy', 'numpy>=1.12', 'six', 'protobuf>=3.6.0', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jaxlib': binary_libs},",No
build/build_wheels_macos.sh,build/build_wheels_macos.sh,ce61eb632c7876ecc4ddb3d26ce0346a60666603,27eb26c8db8eac48b24b8327f054c464d297bd69,Delete the virtualenv if it already exists.,"diff --git a/build/build_wheels_macos.sh b/build/build_wheels_macos.sh
index 7f6336415..a56e6ddb5 100755
--- a/build/build_wheels_macos.sh
+++ b/build/build_wheels_macos.sh
@@ -22,6 +22,7 @@ build_jax () {
   echo ""\nBuilding JAX for Python ${PY_VERSION}, tag ${PY_TAG}""
   pyenv install -s ""${PY_VERSION}""
   VENV=""jax-build-${PY_VERSION}""
+  pyenv virtualenv-delete -f ""${VENV}""
   pyenv virtualenv ""${PY_VERSION}"" ""${VENV}""
   pyenv activate ""${VENV}""
   pip install scipy wheel","diff --git a/build/build_wheels_macos.sh b/build/build_wheels_macos.sh
index 7f6336415..a56e6ddb5 100755
--- a/build/build_wheels_macos.sh
+++ b/build/build_wheels_macos.sh
@@ -22,6 +22,7 @@ build_jax () {
   echo ""\nBuilding JAX for Python ${PY_VERSION}, tag ${PY_TAG}""
   pyenv install -s ""${PY_VERSION}""
   VENV=""jax-build-${PY_VERSION}""
+  pyenv virtualenv-delete -f ""${VENV}""
   pyenv virtualenv ""${PY_VERSION}"" ""${VENV}""
   pyenv activate ""${VENV}""
   pip install scipy wheel",No
build/setup.py,build/setup.py,d707426caae0038609fcc71531fd5d7aa61f4f47,59734c0bd9a3706acc661c4139202e4efcc5399a,Bump jaxlib version to 0.1.5 in preparation for building new wheels.,"diff --git a/build/setup.py b/build/setup.py
index dc74f24cc..d47def561 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.4',
+    version='0.1.5',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index dc74f24cc..d47def561 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.4',
+    version='0.1.5',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,e062127d1e03b6d6cdad76a3b1a30d543fa07ab6,d707426caae0038609fcc71531fd5d7aa61f4f47,Update documentation and notebooks for Jaxlib 0.1.5.,"diff --git a/README.md b/README.md
index c684b14dd..b65964c66 100644
--- a/README.md
+++ b/README.md
@@ -96,6 +96,9 @@ cd jax
 pip install scipy
 ```
 
+If you are building on a Mac, make sure XCode and the XCode command line tools
+are installed.
+
 To build XLA with CUDA support, you can run
 
 ```bash
@@ -141,11 +144,11 @@ cloud VM), you can run
 
 ```bash
 # install jaxlib
-PYTHON_VERSION=py2  # alternatives: py2, py3
+PYTHON_VERSION=cp27  # alternatives: cp27, cp36, cp37
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.3-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.5-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```
@@ -160,6 +163,10 @@ nvcc --version
 grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
 ```
 
+The Python version must match your Python interpreter. There are prebuilt wheels
+for Python 2.7, 3.6, and 3.7; for anything else, you must build from source.
+
+
 ## Running the tests
 
 To run all the JAX tests, from the repository root directory run","diff --git a/README.md b/README.md
index c684b14dd..b65964c66 100644
--- a/README.md
+++ b/README.md
@@ -96,6 +96,9 @@ cd jax
 pip install scipy
 ```
 
+If you are building on a Mac, make sure XCode and the XCode command line tools
+are installed.
+
 To build XLA with CUDA support, you can run
 
 ```bash
@@ -141,11 +144,11 @@ cloud VM), you can run
 
 ```bash
 # install jaxlib
-PYTHON_VERSION=py2  # alternatives: py2, py3
+PYTHON_VERSION=cp27  # alternatives: cp27, cp36, cp37
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.3-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.5-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```
@@ -160,6 +163,10 @@ nvcc --version
 grep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path
 ```
 
+The Python version must match your Python interpreter. There are prebuilt wheels
+for Python 2.7, 3.6, and 3.7; for anything else, you must build from source.
+
+
 ## Running the tests
 
 To run all the JAX tests, from the repository root directory run",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,e062127d1e03b6d6cdad76a3b1a30d543fa07ab6,d707426caae0038609fcc71531fd5d7aa61f4f47,Update documentation and notebooks for Jaxlib 0.1.5.,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index cd0fed0a7..52dc1af78 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index cd0fed0a7..52dc1af78 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,e062127d1e03b6d6cdad76a3b1a30d543fa07ab6,d707426caae0038609fcc71531fd5d7aa61f4f47,Update documentation and notebooks for Jaxlib 0.1.5.,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index b86ac9d89..06c60d33d 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index b86ac9d89..06c60d33d 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
jax/lax.py,jax/lax.py,62d946123cea4cf659d4168fa6db41c8157ef4cc,e062127d1e03b6d6cdad76a3b1a30d543fa07ab6,"Use complexfloating instead of complex to suppress NumPy warning.

Fixes #255.","diff --git a/jax/lax.py b/jax/lax.py
index 8aec41764..471067889 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -831,7 +831,7 @@ def _brcast_to(x, shape):
 
 _f32 = {onp.float32}
 _float = {onp.floating}
-_complex = {onp.complex}
+_complex = {onp.complexfloating}
 _complex_elem_types = {onp.float32, onp.float64}
 _int = {onp.integer}
 _bool = {onp.bool_}","diff --git a/jax/lax.py b/jax/lax.py
index 8aec41764..471067889 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -831,7 +831,7 @@ def _brcast_to(x, shape):
 
 _f32 = {onp.float32}
 _float = {onp.floating}
-_complex = {onp.complex}
+_complex = {onp.complexfloating}
 _complex_elem_types = {onp.float32, onp.float64}
 _int = {onp.integer}
 _bool = {onp.bool_}",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,ff249fcaafca3c4dadf40c6b8c3b5bf400770f1e,a223757110ad5ffab90c7d5cb440bc8a82974a6a,Update Jaxlib version in NN example notebook.,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 141cf0bee..b7352686b 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index 141cf0bee..b7352686b 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.4-py3-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
build/build_wheels_macos.sh,build/build_jaxlib_wheels_macos.sh,b2fe46f34766be315ad69cafa286cf8cd9bd06ff,edd920f6e1821734d8565d60953255f3e7d9b86e,"Rename MacOS build script to be more consistent with Linux build script.

Pin particular versions of Numpy and Scipy in the build script to fix Numpy extension version compatibility issue. If jaxlib is built against numpy 1.16 but used against an older version, we see an error ""No module named 'numpy.core._multiarray_umath'"". The fix is to build against numpy 1.15.","diff --git a/build/build_jaxlib_wheels_macos.sh b/build/build_jaxlib_wheels_macos.sh
new file mode 100755
index 000000000..4e56791cd
--- /dev/null
+++ b/build/build_jaxlib_wheels_macos.sh
@@ -0,0 +1,44 @@
+#!/bin/bash
+# Script that builds wheels for a JAX release on Mac OS X.
+# Builds wheels for multiple Python versions, using pyenv instead of Docker.
+# Usage: run from root of JAX source tree as:
+# build/build_wheels_macos.sh
+# The wheels will end up in build/dist.
+#
+# Requires pyenv, pyenv-virtualenv (e.g., from Homebrew). If you have Homebrew
+# installed, you can install these with:
+# brew install pyenv pyenv-virtualenv
+#
+# May also need to install XCode command line tools to fix zlib build problem:
+# https://github.com/pyenv/pyenv/issues/1219
+
+eval ""$(pyenv init -)""
+
+PLATFORM_TAG=""macosx_10_9_x86_64""
+
+build_jax () {
+  PY_VERSION=""$1""
+  PY_TAG=""$2""
+  echo ""\nBuilding JAX for Python ${PY_VERSION}, tag ${PY_TAG}""
+  pyenv install -s ""${PY_VERSION}""
+  VENV=""jax-build-${PY_VERSION}""
+  pyenv virtualenv-delete -f ""${VENV}""
+  pyenv virtualenv ""${PY_VERSION}"" ""${VENV}""
+  pyenv activate ""${VENV}""
+  # We pin the Numpy wheel to a version < 1.16.0, because Numpy extensions built
+  # at 1.16.0 are not backward compatible to earlier Numpy versions.
+  pip install numpy==1.15.4 scipy==1.2.0 wheel
+  rm -fr build/build
+  python build/build.py
+  cd build
+  python setup.py bdist_wheel --python-tag ""${PY_TAG}"" --plat-name ""${PLATFORM_TAG}""
+  cd ..
+  pyenv deactivate
+  pyenv virtualenv-delete -f ""${VENV}""
+}
+
+
+rm -fr build/dist
+build_jax 2.7.15 cp27
+build_jax 3.6.8 cp36
+build_jax 3.7.2 cp37
\ No newline at end of file","diff --git a/build/build_jaxlib_wheels_macos.sh b/build/build_jaxlib_wheels_macos.sh
new file mode 100755
index 000000000..4e56791cd
--- /dev/null
+++ b/build/build_jaxlib_wheels_macos.sh
@@ -0,0 +1,44 @@
+#!/bin/bash
+# Script that builds wheels for a JAX release on Mac OS X.
+# Builds wheels for multiple Python versions, using pyenv instead of Docker.
+# Usage: run from root of JAX source tree as:
+# build/build_wheels_macos.sh
+# The wheels will end up in build/dist.
+#
+# Requires pyenv, pyenv-virtualenv (e.g., from Homebrew). If you have Homebrew
+# installed, you can install these with:
+# brew install pyenv pyenv-virtualenv
+#
+# May also need to install XCode command line tools to fix zlib build problem:
+# https://github.com/pyenv/pyenv/issues/1219
+
+eval ""$(pyenv init -)""
+
+PLATFORM_TAG=""macosx_10_9_x86_64""
+
+build_jax () {
+  PY_VERSION=""$1""
+  PY_TAG=""$2""
+  echo ""\nBuilding JAX for Python ${PY_VERSION}, tag ${PY_TAG}""
+  pyenv install -s ""${PY_VERSION}""
+  VENV=""jax-build-${PY_VERSION}""
+  pyenv virtualenv-delete -f ""${VENV}""
+  pyenv virtualenv ""${PY_VERSION}"" ""${VENV}""
+  pyenv activate ""${VENV}""
+  # We pin the Numpy wheel to a version < 1.16.0, because Numpy extensions built
+  # at 1.16.0 are not backward compatible to earlier Numpy versions.
+  pip install numpy==1.15.4 scipy==1.2.0 wheel
+  rm -fr build/build
+  python build/build.py
+  cd build
+  python setup.py bdist_wheel --python-tag ""${PY_TAG}"" --plat-name ""${PLATFORM_TAG}""
+  cd ..
+  pyenv deactivate
+  pyenv virtualenv-delete -f ""${VENV}""
+}
+
+
+rm -fr build/dist
+build_jax 2.7.15 cp27
+build_jax 3.6.8 cp36
+build_jax 3.7.2 cp37
\ No newline at end of file",No
build/Dockerfile,build/Dockerfile,aaff39a7ee836af786658993224e1149a133f0b8,b2fe46f34766be315ad69cafa286cf8cd9bd06ff,"Change Linux docker wheel build to build separate wheels for each minor Python version.

* Use pyenv to build specific Python releases against which we build wheels. Otherwise we are limited to only those Python releases present in the OS repository, which only correspond to certain major versions of Python.
* Build with docker instead of nvidia-docker. We need a CUDA image to build JAX, but we don't require a GPU or nvidia-docker unless we want to actually run things.
* Various other cleanups.","diff --git a/build/Dockerfile b/build/Dockerfile
index 6b219be3f..be483c367 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -4,16 +4,30 @@ LABEL maintainer ""Matt Johnson <mattjj@google.com>""
 
 RUN apt-get update && apt-get install -y --no-install-recommends \
             dh-autoreconf git curl \
-            python python-pip python-dev \
-            python3 python3-pip python3-dev
-RUN pip install numpy scipy cython setuptools wheel && pip3 install numpy scipy cython setuptools wheel
+            build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev \
+            libsqlite3-dev wget llvm libncurses5-dev xz-utils tk-dev \
+            libxml2-dev libxmlsec1-dev libffi-dev
 
 RUN git clone https://github.com/nixos/patchelf /tmp/patchelf
 WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
+
+WORKDIR /
+RUN git clone https://github.com/pyenv/pyenv.git /pyenv
+ENV PYENV_ROOT /pyenv
+RUN /pyenv/bin/pyenv install 2.7.15
+RUN /pyenv/bin/pyenv install 3.6.8
+RUN /pyenv/bin/pyenv install 3.7.2
+
+# We pin numpy to a version < 1.16 to avoid version compatibility issues.
+RUN eval ""$(/pyenv/bin/pyenv init -)"" && /pyenv/bin/pyenv local 2.7.15 && pip install numpy==1.15.4 scipy cython setuptools wheel
+RUN eval ""$(/pyenv/bin/pyenv init -)"" && /pyenv/bin/pyenv local 3.6.8 && pip install numpy==1.15.4 scipy cython setuptools wheel
+RUN eval ""$(/pyenv/bin/pyenv init -)"" && /pyenv/bin/pyenv local 3.7.2 && pip install numpy==1.15.4 scipy cython setuptools wheel
+
+
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/762abcf29b4a155c3de325c27ecffa5d4a3da28c/build/build_wheel_docker_entrypoint.sh
+COPY build_wheel_docker_entrypoint.sh /build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build","diff --git a/build/Dockerfile b/build/Dockerfile
index 6b219be3f..be483c367 100644
--- a/build/Dockerfile
+++ b/build/Dockerfile
@@ -4,16 +4,30 @@ LABEL maintainer ""Matt Johnson <mattjj@google.com>""
 
 RUN apt-get update && apt-get install -y --no-install-recommends \
             dh-autoreconf git curl \
-            python python-pip python-dev \
-            python3 python3-pip python3-dev
-RUN pip install numpy scipy cython setuptools wheel && pip3 install numpy scipy cython setuptools wheel
+            build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev \
+            libsqlite3-dev wget llvm libncurses5-dev xz-utils tk-dev \
+            libxml2-dev libxmlsec1-dev libffi-dev
 
 RUN git clone https://github.com/nixos/patchelf /tmp/patchelf
 WORKDIR /tmp/patchelf
 RUN bash bootstrap.sh && ./configure && make && make install && rm -r /tmp/patchelf
 
+
 WORKDIR /
-RUN curl -O https://raw.githubusercontent.com/google/jax/762abcf29b4a155c3de325c27ecffa5d4a3da28c/build/build_wheel_docker_entrypoint.sh
+RUN git clone https://github.com/pyenv/pyenv.git /pyenv
+ENV PYENV_ROOT /pyenv
+RUN /pyenv/bin/pyenv install 2.7.15
+RUN /pyenv/bin/pyenv install 3.6.8
+RUN /pyenv/bin/pyenv install 3.7.2
+
+# We pin numpy to a version < 1.16 to avoid version compatibility issues.
+RUN eval ""$(/pyenv/bin/pyenv init -)"" && /pyenv/bin/pyenv local 2.7.15 && pip install numpy==1.15.4 scipy cython setuptools wheel
+RUN eval ""$(/pyenv/bin/pyenv init -)"" && /pyenv/bin/pyenv local 3.6.8 && pip install numpy==1.15.4 scipy cython setuptools wheel
+RUN eval ""$(/pyenv/bin/pyenv init -)"" && /pyenv/bin/pyenv local 3.7.2 && pip install numpy==1.15.4 scipy cython setuptools wheel
+
+
+WORKDIR /
+COPY build_wheel_docker_entrypoint.sh /build_wheel_docker_entrypoint.sh
 RUN chmod +x /build_wheel_docker_entrypoint.sh
 
 WORKDIR /build",Yes
build/build_jaxlib_wheels.sh,build/build_jaxlib_wheels.sh,aaff39a7ee836af786658993224e1149a133f0b8,b2fe46f34766be315ad69cafa286cf8cd9bd06ff,"Change Linux docker wheel build to build separate wheels for each minor Python version.

* Use pyenv to build specific Python releases against which we build wheels. Otherwise we are limited to only those Python releases present in the OS repository, which only correspond to certain major versions of Python.
* Build with docker instead of nvidia-docker. We need a CUDA image to build JAX, but we don't require a GPU or nvidia-docker unless we want to actually run things.
* Various other cleanups.","diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 00987deed..a5b45027e 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -1,10 +1,9 @@
 #!/bin/bash
 set -xev
-JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
 
-PYTHON_VERSIONS=""py2 py3""
+PYTHON_VERSIONS=""2.7.15 3.6.8 3.7.2""
 CUDA_VERSIONS=""9.0 9.2 10.0""
-CUDA_VARIANTS=""cuda""  # ""cuda cuda-included""
+CUDA_VARIANTS=""cuda"" # ""cuda-included""
 
 mkdir -p dist
 
@@ -13,8 +12,11 @@ docker build -t jaxbuild jax/build/
 for PYTHON_VERSION in $PYTHON_VERSIONS
 do
   mkdir -p dist/nocuda/
-  nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
-  mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+  docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
+  for I in $(find dist/nocuda/ -name ""*.whl"")
+  do
+    mv $I $(echo $I | sed -e 's/linux_x86_64/manylinux1_x86_64/')
+  done
 done
 
 # build the cuda linux packages, tagging with linux_x86_64
@@ -26,8 +28,8 @@ do
     for CUDA_VARIANT in $CUDA_VARIANTS
     do
       mkdir -p dist/${CUDA_VARIANT}${CUDA_VERSION//.}
-      nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
-      mv dist/*.whl dist/${CUDA_VARIANT}${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
+      docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
+      mv dist/*.whl dist/${CUDA_VARIANT}${CUDA_VERSION//.}/
     done
   done
-done
+done
\ No newline at end of file","diff --git a/build/build_jaxlib_wheels.sh b/build/build_jaxlib_wheels.sh
index 00987deed..a5b45027e 100755
--- a/build/build_jaxlib_wheels.sh
+++ b/build/build_jaxlib_wheels.sh
@@ -1,10 +1,9 @@
 #!/bin/bash
 set -xev
-JAXLIB_VERSION=$(sed -n ""s/^ \+version=[']\(.*\)['],$/\\1/p"" jax/build/setup.py)
 
-PYTHON_VERSIONS=""py2 py3""
+PYTHON_VERSIONS=""2.7.15 3.6.8 3.7.2""
 CUDA_VERSIONS=""9.0 9.2 10.0""
-CUDA_VARIANTS=""cuda""  # ""cuda cuda-included""
+CUDA_VARIANTS=""cuda"" # ""cuda-included""
 
 mkdir -p dist
 
@@ -13,8 +12,11 @@ docker build -t jaxbuild jax/build/
 for PYTHON_VERSION in $PYTHON_VERSIONS
 do
   mkdir -p dist/nocuda/
-  nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
-  mv dist/*.whl dist/nocuda/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-manylinux1_x86_64.whl
+  docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION nocuda
+  for I in $(find dist/nocuda/ -name ""*.whl"")
+  do
+    mv $I $(echo $I | sed -e 's/linux_x86_64/manylinux1_x86_64/')
+  done
 done
 
 # build the cuda linux packages, tagging with linux_x86_64
@@ -26,8 +28,8 @@ do
     for CUDA_VARIANT in $CUDA_VARIANTS
     do
       mkdir -p dist/${CUDA_VARIANT}${CUDA_VERSION//.}
-      nvidia-docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
-      mv dist/*.whl dist/${CUDA_VARIANT}${CUDA_VERSION//.}/jaxlib-${JAXLIB_VERSION}-${PYTHON_VERSION}-none-linux_x86_64.whl
+      docker run -it --tmpfs /build:exec --rm -v $(pwd)/dist:/dist jaxbuild $PYTHON_VERSION $CUDA_VARIANT
+      mv dist/*.whl dist/${CUDA_VARIANT}${CUDA_VERSION//.}/
     done
   done
-done
+done
\ No newline at end of file",No
build/build_jaxlib_wheels_macos.sh,build/build_jaxlib_wheels_macos.sh,aaff39a7ee836af786658993224e1149a133f0b8,b2fe46f34766be315ad69cafa286cf8cd9bd06ff,"Change Linux docker wheel build to build separate wheels for each minor Python version.

* Use pyenv to build specific Python releases against which we build wheels. Otherwise we are limited to only those Python releases present in the OS repository, which only correspond to certain major versions of Python.
* Build with docker instead of nvidia-docker. We need a CUDA image to build JAX, but we don't require a GPU or nvidia-docker unless we want to actually run things.
* Various other cleanups.","diff --git a/build/build_jaxlib_wheels_macos.sh b/build/build_jaxlib_wheels_macos.sh
index 4e56791cd..095625ace 100755
--- a/build/build_jaxlib_wheels_macos.sh
+++ b/build/build_jaxlib_wheels_macos.sh
@@ -41,4 +41,4 @@ build_jax () {
 rm -fr build/dist
 build_jax 2.7.15 cp27
 build_jax 3.6.8 cp36
-build_jax 3.7.2 cp37
\ No newline at end of file
+build_jax 3.7.2 cp37","diff --git a/build/build_jaxlib_wheels_macos.sh b/build/build_jaxlib_wheels_macos.sh
index 4e56791cd..095625ace 100755
--- a/build/build_jaxlib_wheels_macos.sh
+++ b/build/build_jaxlib_wheels_macos.sh
@@ -41,4 +41,4 @@ build_jax () {
 rm -fr build/dist
 build_jax 2.7.15 cp27
 build_jax 3.6.8 cp36
-build_jax 3.7.2 cp37
\ No newline at end of file
+build_jax 3.7.2 cp37",No
build/build_wheel_docker_entrypoint.sh,build/build_wheel_docker_entrypoint.sh,aaff39a7ee836af786658993224e1149a133f0b8,b2fe46f34766be315ad69cafa286cf8cd9bd06ff,"Change Linux docker wheel build to build separate wheels for each minor Python version.

* Use pyenv to build specific Python releases against which we build wheels. Otherwise we are limited to only those Python releases present in the OS repository, which only correspond to certain major versions of Python.
* Build with docker instead of nvidia-docker. We need a CUDA image to build JAX, but we don't require a GPU or nvidia-docker unless we want to actually run things.
* Various other cleanups.","diff --git a/build/build_wheel_docker_entrypoint.sh b/build/build_wheel_docker_entrypoint.sh
index afc69d211..95fb2bcb9 100755
--- a/build/build_wheel_docker_entrypoint.sh
+++ b/build/build_wheel_docker_entrypoint.sh
@@ -6,6 +6,13 @@ then
   exit 1
 fi
 
+export PYENV_ROOT=""/pyenv""
+export PATH=""$PYENV_ROOT/bin:$PATH""
+eval ""$(pyenv init -)""
+
+PY_VERSION=""$1""
+echo ""Python version $PY_VERSION""
+
 git clone https://github.com/google/jax /build/jax
 cd /build/jax/build
 
@@ -19,15 +26,11 @@ then
   usage
 fi
 
-case $1 in
-  py3)
-    update-alternatives --install /usr/bin/python python /usr/bin/python3 10
-    ;;
-  py2)
-    ;;
-  *)
-    usage
-esac
+# Builds and activates a specific Python version.
+pyenv local ""$PY_VERSION""
+
+PY_TAG=$(python -c ""import wheel; import wheel.pep425tags as t; print(t.get_abbr_impl() + t.get_impl_ver())"")
+echo ""Python tag $PY_TAG""
 
 case $2 in
   cuda-included)
@@ -44,5 +47,5 @@ case $2 in
     usage
 esac
 
-python setup.py bdist_wheel
+python setup.py bdist_wheel --python-tag ""$PY_TAG"" --plat-name ""linux_x86_64""
 cp -r dist/* /dist","diff --git a/build/build_wheel_docker_entrypoint.sh b/build/build_wheel_docker_entrypoint.sh
index afc69d211..95fb2bcb9 100755
--- a/build/build_wheel_docker_entrypoint.sh
+++ b/build/build_wheel_docker_entrypoint.sh
@@ -6,6 +6,13 @@ then
   exit 1
 fi
 
+export PYENV_ROOT=""/pyenv""
+export PATH=""$PYENV_ROOT/bin:$PATH""
+eval ""$(pyenv init -)""
+
+PY_VERSION=""$1""
+echo ""Python version $PY_VERSION""
+
 git clone https://github.com/google/jax /build/jax
 cd /build/jax/build
 
@@ -19,15 +26,11 @@ then
   usage
 fi
 
-case $1 in
-  py3)
-    update-alternatives --install /usr/bin/python python /usr/bin/python3 10
-    ;;
-  py2)
-    ;;
-  *)
-    usage
-esac
+# Builds and activates a specific Python version.
+pyenv local ""$PY_VERSION""
+
+PY_TAG=$(python -c ""import wheel; import wheel.pep425tags as t; print(t.get_abbr_impl() + t.get_impl_ver())"")
+echo ""Python tag $PY_TAG""
 
 case $2 in
   cuda-included)
@@ -44,5 +47,5 @@ case $2 in
     usage
 esac
 
-python setup.py bdist_wheel
+python setup.py bdist_wheel --python-tag ""$PY_TAG"" --plat-name ""linux_x86_64""
 cp -r dist/* /dist",No
examples/advi.py,examples/advi.py,ed087ddbfaa0f0cb26d087c4cf13523bb21f274b,aaff39a7ee836af786658993224e1149a133f0b8,"Minor updates of advi.py

1. fixed some syntax errors
2. hold on the plot in the end","diff --git a/examples/advi.py b/examples/advi.py
index 01983bbd9..740a1bbb3 100644
--- a/examples/advi.py
+++ b/examples/advi.py
@@ -41,7 +41,7 @@ def diag_gaussian_logpdf(x, mean, log_std):
     # Evaluate a single point on a diagonal multivariate Gaussian.
     return np.sum(vmap(norm.logpdf)(x, mean, np.exp(log_std)))
 
-def elbo(logprob, rng, (mean, log_std)):
+def elbo(logprob, rng, mean, log_std):
     # Single-sample Monte Carlo estimate of the variational lower bound.
     sample = diag_gaussian_sample(rng, mean, log_std)
     return logprob(sample) - diag_gaussian_logpdf(sample, mean, log_std)
@@ -49,8 +49,8 @@ def elbo(logprob, rng, (mean, log_std)):
 def batch_elbo(logprob, rng, params, num_samples):
     # Average over a batch of random samples.
     rngs = random.split(rng, num_samples)
-    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None))
-    return np.mean(vectorized_elbo(rngs, params))
+    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None, None))
+    return np.mean(vectorized_elbo(rngs, *params))
 
 
 # ========= Helper function for plotting. =========
@@ -133,6 +133,7 @@ if __name__ == ""__main__"":
     # Main loop.
     print(""Optimizing variational parameters..."")
     for t in range(100):
-      opt_state = update(t, opt_state)
-      params = minmax.get_params(opt_state)
-      callback(params, t)
+        opt_state = update(t, opt_state)
+        params = minmax.get_params(opt_state)
+        callback(params, t)
+    plt.show(block=True)","diff --git a/examples/advi.py b/examples/advi.py
index 01983bbd9..740a1bbb3 100644
--- a/examples/advi.py
+++ b/examples/advi.py
@@ -41,7 +41,7 @@ def diag_gaussian_logpdf(x, mean, log_std):
     # Evaluate a single point on a diagonal multivariate Gaussian.
     return np.sum(vmap(norm.logpdf)(x, mean, np.exp(log_std)))
 
-def elbo(logprob, rng, (mean, log_std)):
+def elbo(logprob, rng, mean, log_std):
     # Single-sample Monte Carlo estimate of the variational lower bound.
     sample = diag_gaussian_sample(rng, mean, log_std)
     return logprob(sample) - diag_gaussian_logpdf(sample, mean, log_std)
@@ -49,8 +49,8 @@ def elbo(logprob, rng, (mean, log_std)):
 def batch_elbo(logprob, rng, params, num_samples):
     # Average over a batch of random samples.
     rngs = random.split(rng, num_samples)
-    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None))
-    return np.mean(vectorized_elbo(rngs, params))
+    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None, None))
+    return np.mean(vectorized_elbo(rngs, *params))
 
 
 # ========= Helper function for plotting. =========
@@ -133,6 +133,7 @@ if __name__ == ""__main__"":
     # Main loop.
     print(""Optimizing variational parameters..."")
     for t in range(100):
-      opt_state = update(t, opt_state)
-      params = minmax.get_params(opt_state)
-      callback(params, t)
+        opt_state = update(t, opt_state)
+        params = minmax.get_params(opt_state)
+        callback(params, t)
+    plt.show(block=True)",No
README.md,README.md,42bd253cbb61520383cc4e92300fd38f02d234e0,ed087ddbfaa0f0cb26d087c4cf13523bb21f274b,add prng design doc in markdown format,"diff --git a/README.md b/README.md
index b65964c66..051775df9 100644
--- a/README.md
+++ b/README.md
@@ -474,7 +474,7 @@ differentiation for fast Jacobian and Hessian matrix calculations in
 
 ## Random numbers are different
 
-JAX needs a functional pseudo-random number generator (PRNG) system to provide
+JAX needs a [functional pseudo-random number generator (PRNG) system](design_notes/prng.md) to provide
 reproducible results invariant to compilation boundaries and backends, while
 also maximizing performance by enabling vectorized generation and
 parallelization across random calls. The `numpy.random` library doesnt have
@@ -524,6 +524,9 @@ There's a gotcha here, which is that it's easy to unintentionally reuse a key
 without splitting. We intend to add a check for this (a sort of dynamic linear
 typing) but for now it's something to be careful about.
 
+For more detailed information on the design and the reasoning behind it, see the
+[PRNG design doc](design_notes/prng.md).
+
 
 ## Mini-libraries
 ","diff --git a/README.md b/README.md
index b65964c66..051775df9 100644
--- a/README.md
+++ b/README.md
@@ -474,7 +474,7 @@ differentiation for fast Jacobian and Hessian matrix calculations in
 
 ## Random numbers are different
 
-JAX needs a functional pseudo-random number generator (PRNG) system to provide
+JAX needs a [functional pseudo-random number generator (PRNG) system](design_notes/prng.md) to provide
 reproducible results invariant to compilation boundaries and backends, while
 also maximizing performance by enabling vectorized generation and
 parallelization across random calls. The `numpy.random` library doesnt have
@@ -524,6 +524,9 @@ There's a gotcha here, which is that it's easy to unintentionally reuse a key
 without splitting. We intend to add a check for this (a sort of dynamic linear
 typing) but for now it's something to be careful about.
 
+For more detailed information on the design and the reasoning behind it, see the
+[PRNG design doc](design_notes/prng.md).
+
 
 ## Mini-libraries
 ",No
jax/api.py,jax/api.py,42bd253cbb61520383cc4e92300fd38f02d234e0,ed087ddbfaa0f0cb26d087c4cf13523bb21f274b,add prng design doc in markdown format,"diff --git a/jax/api.py b/jax/api.py
index 16f15c618..eee6164ab 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -237,11 +237,17 @@ def vmap(fun, in_axes=0, out_axes=0):
   docstr = (""Vectorized version of {fun}. Takes similar arguments as {fun} ""
             ""but with additional array axes over which {fun} is mapped."")
 
+  if (not isinstance(in_axes, (list, tuple, type(None), int))
+      or not isinstance(out_axes, (list, tuple, type(None), int))):
+    msg = (""vmap arguments in_axes and out_axes must each be an integer, None, ""
+           ""or a (nested) tuple of those types, got {} and {} respectively."")
+    raise TypeError(msg.format(type(in_axes), type(out_axes)))
+
   @wraps(fun, docstr=docstr)
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun, kwargs)
-    in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
     in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
     out_flat = batching.batch(jaxtree_fun, in_flat, in_axes_, out_axes)","diff --git a/jax/api.py b/jax/api.py
index 16f15c618..eee6164ab 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -237,11 +237,17 @@ def vmap(fun, in_axes=0, out_axes=0):
   docstr = (""Vectorized version of {fun}. Takes similar arguments as {fun} ""
             ""but with additional array axes over which {fun} is mapped."")
 
+  if (not isinstance(in_axes, (list, tuple, type(None), int))
+      or not isinstance(out_axes, (list, tuple, type(None), int))):
+    msg = (""vmap arguments in_axes and out_axes must each be an integer, None, ""
+           ""or a (nested) tuple of those types, got {} and {} respectively."")
+    raise TypeError(msg.format(type(in_axes), type(out_axes)))
+
   @wraps(fun, docstr=docstr)
   def batched_fun(*args, **kwargs):
     if not isinstance(fun, lu.WrappedFun):
       f = lu.wrap_init(fun, kwargs)
-    in_axes_ = (in_axes,) * len(args) if type(in_axes) is int else in_axes
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
     in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
     out_flat = batching.batch(jaxtree_fun, in_flat, in_axes_, out_axes)",No
WORKSPACE,WORKSPACE,3489b45f8e313799e7c0ba3f121c828cfd16c414,42bd253cbb61520383cc4e92300fd38f02d234e0,"Update XLA release.

Includes https://github.com/tensorflow/tensorflow/commit/7fce32e9be8c43a96f0c55063be6ab215d12b0ae, which fixes #120 once jaxlib is rebuilt.","diff --git a/WORKSPACE b/WORKSPACE
index 9d1623367..05aa9cb2a 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""35d3fdfee69ad53bb12f67a6d37ebb7492fbe18509c4e4b07d43fa2c2808fb78"",
-   strip_prefix = ""tensorflow-bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee"",
+   sha256 = ""2cf7182e8268e144e785c7938d3efd6b43ab087f3cb607522c491cbb1e6b7e1e"",
+   strip_prefix = ""tensorflow-7fce32e9be8c43a96f0c55063be6ab215d12b0ae"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/7fce32e9be8c43a96f0c55063be6ab215d12b0ae.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 9d1623367..05aa9cb2a 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""35d3fdfee69ad53bb12f67a6d37ebb7492fbe18509c4e4b07d43fa2c2808fb78"",
-   strip_prefix = ""tensorflow-bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee"",
+   sha256 = ""2cf7182e8268e144e785c7938d3efd6b43ab087f3cb607522c491cbb1e6b7e1e"",
+   strip_prefix = ""tensorflow-7fce32e9be8c43a96f0c55063be6ab215d12b0ae"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/bf5cd5e750f31b95cd06f8ff75fe9bda30d84bee.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/7fce32e9be8c43a96f0c55063be6ab215d12b0ae.tar.gz"",
    ],
 )
 ",No
WORKSPACE,WORKSPACE,4ea3f2cd8c3eb513e871d1f2def282b0ea8f84b3,12084dfe27d9573b3e313049363d805f6c09656e,"Add complex128 support to LAPACK bindings.

Update XLA to incorporate https://github.com/tensorflow/tensorflow/commit/7a283b835b80808a9cdc6043998f29c8188342a5 .","diff --git a/WORKSPACE b/WORKSPACE
index 05aa9cb2a..859bbff0e 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""2cf7182e8268e144e785c7938d3efd6b43ab087f3cb607522c491cbb1e6b7e1e"",
-   strip_prefix = ""tensorflow-7fce32e9be8c43a96f0c55063be6ab215d12b0ae"",
+   sha256 = ""7cb4e7a3eea5c61c5d4674d20be642e49c77d7cda76113cb3c3bccd805de6c7f"",
+   strip_prefix = ""tensorflow-7a283b835b80808a9cdc6043998f29c8188342a5"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/7fce32e9be8c43a96f0c55063be6ab215d12b0ae.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/7a283b835b80808a9cdc6043998f29c8188342a5.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 05aa9cb2a..859bbff0e 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""2cf7182e8268e144e785c7938d3efd6b43ab087f3cb607522c491cbb1e6b7e1e"",
-   strip_prefix = ""tensorflow-7fce32e9be8c43a96f0c55063be6ab215d12b0ae"",
+   sha256 = ""7cb4e7a3eea5c61c5d4674d20be642e49c77d7cda76113cb3c3bccd805de6c7f"",
+   strip_prefix = ""tensorflow-7a283b835b80808a9cdc6043998f29c8188342a5"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/7fce32e9be8c43a96f0c55063be6ab215d12b0ae.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/7a283b835b80808a9cdc6043998f29c8188342a5.tar.gz"",
    ],
 )
 ",No
jax/lax_linalg.py,jax/lax_linalg.py,4ea3f2cd8c3eb513e871d1f2def282b0ea8f84b3,12084dfe27d9573b3e313049363d805f6c09656e,"Add complex128 support to LAPACK bindings.

Update XLA to incorporate https://github.com/tensorflow/tensorflow/commit/7a283b835b80808a9cdc6043998f29c8188342a5 .","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index a0fadfc77..11634a026 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -65,7 +65,7 @@ def _T(x):
 
 # primitives
 
-_cpu_lapack_types = {np.float32, np.float64, np.complex64}
+_cpu_lapack_types = {np.float32, np.float64, np.complex64, np.complex128}
 
 # Cholesky decomposition
 ","diff --git a/jax/lax_linalg.py b/jax/lax_linalg.py
index a0fadfc77..11634a026 100644
--- a/jax/lax_linalg.py
+++ b/jax/lax_linalg.py
@@ -65,7 +65,7 @@ def _T(x):
 
 # primitives
 
-_cpu_lapack_types = {np.float32, np.float64, np.complex64}
+_cpu_lapack_types = {np.float32, np.float64, np.complex64, np.complex128}
 
 # Cholesky decomposition
 ",No
jaxlib/lapack.pyx,jaxlib/lapack.pyx,4ea3f2cd8c3eb513e871d1f2def282b0ea8f84b3,12084dfe27d9573b3e313049363d805f6c09656e,"Add complex128 support to LAPACK bindings.

Update XLA to incorporate https://github.com/tensorflow/tensorflow/commit/7a283b835b80808a9cdc6043998f29c8188342a5 .","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 4e627682f..ae78bc114 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -25,11 +25,11 @@ from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
-from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
-from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
-from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
-from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
+from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm, ztrsm
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, zgetrf
+from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf, zpotrf
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd, zgesdd
+from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd, zheevd
 
 import numpy as np
 from jaxlib import xla_client
@@ -135,6 +135,36 @@ cdef void blas_ctrsm(void* out, void** data) nogil:
 
 register_cpu_custom_call_target(b""blas_ctrsm"", <void*>(blas_ctrsm))
 
+cdef void blas_ztrsm(void* out, void** data) nogil:
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
+  cdef double complex* alpha = <double complex*>(data[6])
+  cdef double complex* a = <double complex*>(data[7])
+  cdef double complex* b = <double complex*>(data[8])
+
+  cdef double complex* x = <double complex*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(double complex))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
+  ztrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_ztrsm"", <void*>(blas_ztrsm))
+
+
 def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
              conj_a=False, diag=False):
   b_shape = c.GetShape(b)
@@ -153,6 +183,8 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
     fn = b""blas_dtrsm""
   elif dtype == np.complex64:
     fn = b""blas_ctrsm""
+  elif dtype == np.complex128:
+    fn = b""blas_ztrsm""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -236,6 +268,22 @@ cdef void lapack_cgetrf(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_cgetrf"", <void*>(lapack_cgetrf))
 
 
+cdef void lapack_zgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double complex* a_in = <double complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double complex))
+
+  zgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_zgetrf"", <void*>(lapack_zgetrf))
+
 def jax_getrf(c, a):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -248,6 +296,8 @@ def jax_getrf(c, a):
     fn = b""lapack_dgetrf""
   elif dtype == np.complex64:
     fn = b""lapack_cgetrf""
+  elif dtype == np.complex128:
+    fn = b""lapack_zgetrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -355,6 +405,34 @@ cdef void lapack_cpotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_cpotrf"", <void*>(lapack_cpotrf))
 
+cdef void lapack_zpotrf(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double complex* a_in = <double complex*>(data[2])
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef int* info = <int*>(out[1])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(double complex))
+
+  zpotrf(&uplo, &n, a_out, &n, info)
+
+  # zpotrf leaves junk in the part of the triangle that is not written; zero it.
+  cdef int i
+  cdef int j
+  if lower:
+    for i in range(n):
+      for j in range(i):
+        a_out[i * n + j] = 0
+  else:
+    for i in range(n):
+      for j in range(i, n):
+        a_out[i * n + j] = 0
+
+register_cpu_custom_call_target(b""lapack_zpotrf"", <void*>(lapack_zpotrf))
+
 def jax_potrf(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -369,6 +447,8 @@ def jax_potrf(c, a, lower=False):
     fn = b""lapack_dpotrf""
   elif dtype == np.complex64:
     fn = b""lapack_cpotrf""
+  elif dtype == np.complex128:
+    fn = b""lapack_zpotrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -528,7 +608,7 @@ cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
     ldvt = min(m, n)
 
   # First perform a workspace query to get the optimal lwork
-  # NB: We perform a workspace query with malloc and free for the work array, 
+  # NB: We perform a workspace query with malloc and free for the work array,
   # because it is officially recommended in the LAPACK documentation
   cdef float complex wkopt = 0
   cdef int lwork = -1
@@ -543,6 +623,54 @@ cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_cgesdd"", <void*>(lapack_cgesdd))
 
 
+cdef void lapack_zgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef double complex* a_in = <double complex*>(data[4])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef double* s = <double*>(out[1])
+  cdef double complex* u = <double complex*>(out[2])
+  cdef double complex* vt = <double complex*>(out[3])
+  cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  cdef double* rwork = <double*>(out[6])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double complex))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_full_matrices == 0:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
+
+  # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array,
+  # because it is officially recommended in the LAPACK documentation
+  cdef double complex wkopt = 0
+  cdef int lwork = -1
+  zgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, rwork, iwork, info)
+  lwork = <int>(wkopt.real)
+
+  # Now get the actual SVD
+  cdef double complex* work = <double complex*> malloc(lwork * sizeof(double complex))
+  zgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, rwork, iwork, info)
+  free(work)
+
+register_cpu_custom_call_target(b""lapack_zgesdd"", <void*>(lapack_zgesdd))
+
 def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -562,6 +690,11 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
     singular_vals_dtype = np.float32
     workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
                  Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n, int(compute_uv)),), (0,)))
+  elif dtype == np.complex128:
+    fn = b""lapack_zgesdd""
+    singular_vals_dtype = np.float64
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
+                 Shape.array_shape(np.float64, (cgesdd_rwork_size(m, n, int(compute_uv)),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -678,6 +811,33 @@ cdef void lapack_cheevd(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_cheevd"", <void*>(lapack_cheevd))
 
+
+cdef void lapack_zheevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double complex* a_in = <double complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef double* w_out = <double*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  cdef double complex* work = <double complex*>(out[3])
+  cdef double* rwork = <double*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(double complex))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef int lwork = heevd_work_size(n)
+  cdef int lrwork = heevd_rwork_size(n)
+  cdef int liwork = syevd_iwork_size(n)
+  zheevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, rwork, &lrwork,
+         iwork, &liwork, info_out)
+
+register_cpu_custom_call_target(b""lapack_zheevd"", <void*>(lapack_zheevd))
+
 def jax_syevd(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -700,6 +860,12 @@ def jax_syevd(c, a, lower=False):
     workspace = (Shape.array_shape(dtype, (heevd_work_size(n),), (0,)),
                  Shape.array_shape(np.float32, (heevd_rwork_size(n),), (0,)),
                  Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
+  elif dtype == np.complex128:
+    fn = b""lapack_zheevd""
+    eigvals_type = np.float64
+    workspace = (Shape.array_shape(dtype, (heevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.float64, (heevd_rwork_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ","diff --git a/jaxlib/lapack.pyx b/jaxlib/lapack.pyx
index 4e627682f..ae78bc114 100644
--- a/jaxlib/lapack.pyx
+++ b/jaxlib/lapack.pyx
@@ -25,11 +25,11 @@ from libc.string cimport memcpy
 from libcpp.string cimport string
 from cpython.pycapsule cimport PyCapsule_New
 
-from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm
-from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf
-from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf
-from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd
-from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd
+from scipy.linalg.cython_blas cimport strsm, dtrsm, ctrsm, ztrsm
+from scipy.linalg.cython_lapack cimport sgetrf, dgetrf, cgetrf, zgetrf
+from scipy.linalg.cython_lapack cimport spotrf, dpotrf, cpotrf, zpotrf
+from scipy.linalg.cython_lapack cimport sgesdd, dgesdd, cgesdd, zgesdd
+from scipy.linalg.cython_lapack cimport ssyevd, dsyevd, cheevd, zheevd
 
 import numpy as np
 from jaxlib import xla_client
@@ -135,6 +135,36 @@ cdef void blas_ctrsm(void* out, void** data) nogil:
 
 register_cpu_custom_call_target(b""blas_ctrsm"", <void*>(blas_ctrsm))
 
+cdef void blas_ztrsm(void* out, void** data) nogil:
+  cdef int32_t left_side = (<int32_t*>(data[0]))[0]
+  cdef int32_t lower = (<int32_t*>(data[1]))[0]
+  cdef int32_t trans_a = (<int32_t*>(data[2]))[0]
+  cdef int32_t diag = (<int32_t*>(data[3]))[0]
+  cdef int m = (<int32_t*>(data[4]))[0]
+  cdef int n = (<int32_t*>(data[5]))[0]
+  cdef double complex* alpha = <double complex*>(data[6])
+  cdef double complex* a = <double complex*>(data[7])
+  cdef double complex* b = <double complex*>(data[8])
+
+  cdef double complex* x = <double complex*>(out)
+  if x != b:
+    memcpy(x, b, m * n * sizeof(double complex))
+
+  cdef char cside = 'L' if left_side else 'R'
+  cdef char cuplo = 'L' if lower else 'U'
+  cdef char ctransa = 'N'
+  if trans_a == 1:
+    ctransa = 'T'
+  elif trans_a == 2:
+    ctransa = 'C'
+  cdef char cdiag = 'U' if diag else 'N'
+  cdef int lda = m if left_side else n
+  cdef int ldb = m
+  ztrsm(&cside, &cuplo, &ctransa, &cdiag, &m, &n, alpha, a, &lda, x, &ldb)
+
+register_cpu_custom_call_target(b""blas_ztrsm"", <void*>(blas_ztrsm))
+
+
 def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
              conj_a=False, diag=False):
   b_shape = c.GetShape(b)
@@ -153,6 +183,8 @@ def jax_trsm(c, alpha, a, b, left_side=False, lower=False, trans_a=False,
     fn = b""blas_dtrsm""
   elif dtype == np.complex64:
     fn = b""blas_ctrsm""
+  elif dtype == np.complex128:
+    fn = b""blas_ztrsm""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -236,6 +268,22 @@ cdef void lapack_cgetrf(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_cgetrf"", <void*>(lapack_cgetrf))
 
 
+cdef void lapack_zgetrf(void* out_tuple, void** data) nogil:
+  cdef int m = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double complex* a_in = <double complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef int* ipiv = <int*>(out[1])
+  cdef int* info = <int*>(out[2])
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double complex))
+
+  zgetrf(&m, &n, a_out, &m, ipiv, info)
+
+register_cpu_custom_call_target(b""lapack_zgetrf"", <void*>(lapack_zgetrf))
+
 def jax_getrf(c, a):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -248,6 +296,8 @@ def jax_getrf(c, a):
     fn = b""lapack_dgetrf""
   elif dtype == np.complex64:
     fn = b""lapack_cgetrf""
+  elif dtype == np.complex128:
+    fn = b""lapack_zgetrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -355,6 +405,34 @@ cdef void lapack_cpotrf(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_cpotrf"", <void*>(lapack_cpotrf))
 
+cdef void lapack_zpotrf(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double complex* a_in = <double complex*>(data[2])
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef int* info = <int*>(out[1])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(double complex))
+
+  zpotrf(&uplo, &n, a_out, &n, info)
+
+  # zpotrf leaves junk in the part of the triangle that is not written; zero it.
+  cdef int i
+  cdef int j
+  if lower:
+    for i in range(n):
+      for j in range(i):
+        a_out[i * n + j] = 0
+  else:
+    for i in range(n):
+      for j in range(i, n):
+        a_out[i * n + j] = 0
+
+register_cpu_custom_call_target(b""lapack_zpotrf"", <void*>(lapack_zpotrf))
+
 def jax_potrf(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -369,6 +447,8 @@ def jax_potrf(c, a, lower=False):
     fn = b""lapack_dpotrf""
   elif dtype == np.complex64:
     fn = b""lapack_cpotrf""
+  elif dtype == np.complex128:
+    fn = b""lapack_zpotrf""
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -528,7 +608,7 @@ cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
     ldvt = min(m, n)
 
   # First perform a workspace query to get the optimal lwork
-  # NB: We perform a workspace query with malloc and free for the work array, 
+  # NB: We perform a workspace query with malloc and free for the work array,
   # because it is officially recommended in the LAPACK documentation
   cdef float complex wkopt = 0
   cdef int lwork = -1
@@ -543,6 +623,54 @@ cdef void lapack_cgesdd(void* out_tuple, void** data) nogil:
 register_cpu_custom_call_target(b""lapack_cgesdd"", <void*>(lapack_cgesdd))
 
 
+cdef void lapack_zgesdd(void* out_tuple, void** data) nogil:
+  cdef int32_t job_opt_full_matrices = (<int32_t*>(data[0]))[0]
+  cdef int32_t job_opt_compute_uv = (<int32_t*>(data[1]))[0]
+  cdef int m = (<int32_t*>(data[2]))[0]
+  cdef int n = (<int32_t*>(data[3]))[0]
+  cdef double complex* a_in = <double complex*>(data[4])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef double* s = <double*>(out[1])
+  cdef double complex* u = <double complex*>(out[2])
+  cdef double complex* vt = <double complex*>(out[3])
+  cdef int* info = <int*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  cdef double* rwork = <double*>(out[6])
+
+  if a_out != a_in:
+    memcpy(a_out, a_in, m * n * sizeof(double complex))
+
+  # define appropriate job code
+  cdef char jobz = 'A'
+  if job_opt_compute_uv == 0:
+    jobz = 'N'
+  else:
+    if job_opt_full_matrices == 0:
+      jobz = 'S'
+
+  cdef int lda = m
+  cdef int ldu = m
+  cdef int ldvt = n
+  if job_opt_full_matrices == 0:
+    ldvt = min(m, n)
+
+  # First perform a workspace query to get the optimal lwork
+  # NB: We perform a workspace query with malloc and free for the work array,
+  # because it is officially recommended in the LAPACK documentation
+  cdef double complex wkopt = 0
+  cdef int lwork = -1
+  zgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, &wkopt, &lwork, rwork, iwork, info)
+  lwork = <int>(wkopt.real)
+
+  # Now get the actual SVD
+  cdef double complex* work = <double complex*> malloc(lwork * sizeof(double complex))
+  zgesdd(&jobz, &m, &n, a_out, &lda, s, u, &ldu, vt, &ldvt, work, &lwork, rwork, iwork, info)
+  free(work)
+
+register_cpu_custom_call_target(b""lapack_zgesdd"", <void*>(lapack_zgesdd))
+
 def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -562,6 +690,11 @@ def jax_gesdd(c, a, full_matrices=True, compute_uv=True):
     singular_vals_dtype = np.float32
     workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
                  Shape.array_shape(np.float32, (cgesdd_rwork_size(m, n, int(compute_uv)),), (0,)))
+  elif dtype == np.complex128:
+    fn = b""lapack_zgesdd""
+    singular_vals_dtype = np.float64
+    workspace = (Shape.array_shape(np.int32, (gesdd_iwork_size(m, n),), (0,)),
+                 Shape.array_shape(np.float64, (cgesdd_rwork_size(m, n, int(compute_uv)),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 
@@ -678,6 +811,33 @@ cdef void lapack_cheevd(void* out_tuple, void** data) nogil:
 
 register_cpu_custom_call_target(b""lapack_cheevd"", <void*>(lapack_cheevd))
 
+
+cdef void lapack_zheevd(void* out_tuple, void** data) nogil:
+  cdef int32_t lower = (<int32_t*>(data[0]))[0]
+  cdef int n = (<int32_t*>(data[1]))[0]
+  cdef const double complex* a_in = <double complex*>(data[2])
+
+  cdef void** out = <void**>(out_tuple)
+  cdef double complex* a_out = <double complex*>(out[0])
+  cdef double* w_out = <double*>(out[1])
+  cdef int* info_out = <int*>(out[2])
+  cdef double complex* work = <double complex*>(out[3])
+  cdef double* rwork = <double*>(out[4])
+  cdef int* iwork = <int*>(out[5])
+  if a_out != a_in:
+    memcpy(a_out, a_in, n * n * sizeof(double complex))
+
+  cdef char jobz = 'V'
+  cdef char uplo = 'L' if lower else 'U'
+
+  cdef int lwork = heevd_work_size(n)
+  cdef int lrwork = heevd_rwork_size(n)
+  cdef int liwork = syevd_iwork_size(n)
+  zheevd(&jobz, &uplo, &n, a_out, &n, w_out, work, &lwork, rwork, &lrwork,
+         iwork, &liwork, info_out)
+
+register_cpu_custom_call_target(b""lapack_zheevd"", <void*>(lapack_zheevd))
+
 def jax_syevd(c, a, lower=False):
   assert sizeof(int32_t) == sizeof(int)
 
@@ -700,6 +860,12 @@ def jax_syevd(c, a, lower=False):
     workspace = (Shape.array_shape(dtype, (heevd_work_size(n),), (0,)),
                  Shape.array_shape(np.float32, (heevd_rwork_size(n),), (0,)),
                  Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
+  elif dtype == np.complex128:
+    fn = b""lapack_zheevd""
+    eigvals_type = np.float64
+    workspace = (Shape.array_shape(dtype, (heevd_work_size(n),), (0,)),
+                 Shape.array_shape(np.float64, (heevd_rwork_size(n),), (0,)),
+                 Shape.array_shape(np.int32, (syevd_iwork_size(n),), (0,)))
   else:
     raise NotImplementedError(""Unsupported dtype {}"".format(dtype))
 ",No
tests/linalg_test.py,tests/linalg_test.py,4ea3f2cd8c3eb513e871d1f2def282b0ea8f84b3,12084dfe27d9573b3e313049363d805f6c09656e,"Add complex128 support to LAPACK bindings.

Update XLA to incorporate https://github.com/tensorflow/tensorflow/commit/7a283b835b80808a9cdc6043998f29c8188342a5 .","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index d91bb2e6f..9494a5ed6 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -46,6 +46,9 @@ def float_types():
 
 def complex_types():
   return {onp.complex64}
+  # TODO(phawkins): change to the following after another jaxlib release.
+  #return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+  #           for dtype in [onp.complex64, onp.complex128])
 
 
 class NumpyLinalgTest(jtu.JaxTestCase):","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index d91bb2e6f..9494a5ed6 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -46,6 +46,9 @@ def float_types():
 
 def complex_types():
   return {onp.complex64}
+  # TODO(phawkins): change to the following after another jaxlib release.
+  #return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+  #           for dtype in [onp.complex64, onp.complex128])
 
 
 class NumpyLinalgTest(jtu.JaxTestCase):",No
build/setup.py,build/setup.py,82ce80f1e12e9fe326771121684810a5ed62a57b,23abcc98a2bdf77e9930132976bcff1d5243a340,Bump Jaxlib version.,"diff --git a/build/setup.py b/build/setup.py
index d47def561..4ed5505ad 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.5',
+    version='0.1.6',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',","diff --git a/build/setup.py b/build/setup.py
index d47def561..4ed5505ad 100644
--- a/build/setup.py
+++ b/build/setup.py
@@ -20,7 +20,7 @@ binary_libs = [os.path.basename(f) for f in glob('jaxlib/*.so*')]
 
 setup(
     name='jaxlib',
-    version='0.1.5',
+    version='0.1.6',
     description='XLA library for JAX',
     author='JAX team',
     author_email='jax-dev@google.com',",No
README.md,README.md,1d9be64e0129430ad3bfab3d15467e388e6004ab,82ce80f1e12e9fe326771121684810a5ed62a57b,Update Jaxlib references to 0.1.6.,"diff --git a/README.md b/README.md
index 051775df9..812ce6cb5 100644
--- a/README.md
+++ b/README.md
@@ -148,7 +148,7 @@ PYTHON_VERSION=cp27  # alternatives: cp27, cp36, cp37
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.5-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.6-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```","diff --git a/README.md b/README.md
index 051775df9..812ce6cb5 100644
--- a/README.md
+++ b/README.md
@@ -148,7 +148,7 @@ PYTHON_VERSION=cp27  # alternatives: cp27, cp36, cp37
 CUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100
 PLATFORM=linux_x86_64  # alternatives: linux_x86_64
 BASE_URL='https://storage.googleapis.com/jax-wheels'
-pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.5-$PYTHON_VERSION-none-$PLATFORM.whl
+pip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.6-$PYTHON_VERSION-none-$PLATFORM.whl
 
 pip install --upgrade jax  # install jax
 ```",No
notebooks/gufuncs.ipynb,notebooks/gufuncs.ipynb,1d9be64e0129430ad3bfab3d15467e388e6004ab,82ce80f1e12e9fe326771121684810a5ed62a57b,Update Jaxlib references to 0.1.6.,"diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 52dc1af78..521ec68e9 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/gufuncs.ipynb b/notebooks/gufuncs.ipynb
index 52dc1af78..521ec68e9 100644
--- a/notebooks/gufuncs.ipynb
+++ b/notebooks/gufuncs.ipynb
@@ -125,7 +125,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade -q jax""
       ],
       ""execution_count"": 0,",No
notebooks/neural_network_and_data_loading.ipynb,notebooks/neural_network_and_data_loading.ipynb,1d9be64e0129430ad3bfab3d15467e388e6004ab,82ce80f1e12e9fe326771121684810a5ed62a57b,Update Jaxlib references to 0.1.6.,"diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index b7352686b..5ed79589a 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/neural_network_and_data_loading.ipynb b/notebooks/neural_network_and_data_loading.ipynb
index b7352686b..5ed79589a 100644
--- a/notebooks/neural_network_and_data_loading.ipynb
+++ b/notebooks/neural_network_and_data_loading.ipynb
@@ -74,7 +74,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
notebooks/quickstart.ipynb,notebooks/quickstart.ipynb,1d9be64e0129430ad3bfab3d15467e388e6004ab,82ce80f1e12e9fe326771121684810a5ed62a57b,Update Jaxlib references to 0.1.6.,"diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 06c60d33d..ab053686c 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,","diff --git a/notebooks/quickstart.ipynb b/notebooks/quickstart.ipynb
index 06c60d33d..ab053686c 100644
--- a/notebooks/quickstart.ipynb
+++ b/notebooks/quickstart.ipynb
@@ -89,7 +89,7 @@
       },
       ""cell_type"": ""code"",
       ""source"": [
-        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.5-cp36-none-linux_x86_64.whl\n"",
+        ""!pip install --upgrade https://storage.googleapis.com/jax-wheels/cuda92/jaxlib-0.1.6-cp36-none-linux_x86_64.whl\n"",
         ""!pip install --upgrade jax""
       ],
       ""execution_count"": 0,",No
jax/scipy/stats/norm.py,jax/scipy/stats/norm.py,8e9d50136a836a23cc07b1398a35d64745f128a7,e609be4c7fff9c0f6b2e7327c4a826f413379b0c,add scipy.stats.norm.pdf,"diff --git a/jax/scipy/stats/norm.py b/jax/scipy/stats/norm.py
index fe14198c4..22fc8215a 100644
--- a/jax/scipy/stats/norm.py
+++ b/jax/scipy/stats/norm.py
@@ -31,3 +31,7 @@ def logpdf(x, loc=0, scale=1):
   log_normalizer = lax.log(lax.mul(_constant_like(x, 2 * onp.pi), scale_sqrd))
   quadratic = lax.div(lax.pow(lax.sub(x, loc), two), scale_sqrd)
   return lax.div(lax.neg(lax.add(log_normalizer, quadratic)), two)
+
+@_wraps(osp_stats.norm.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))","diff --git a/jax/scipy/stats/norm.py b/jax/scipy/stats/norm.py
index fe14198c4..22fc8215a 100644
--- a/jax/scipy/stats/norm.py
+++ b/jax/scipy/stats/norm.py
@@ -31,3 +31,7 @@ def logpdf(x, loc=0, scale=1):
   log_normalizer = lax.log(lax.mul(_constant_like(x, 2 * onp.pi), scale_sqrd))
   quadratic = lax.div(lax.pow(lax.sub(x, loc), two), scale_sqrd)
   return lax.div(lax.neg(lax.add(log_normalizer, quadratic)), two)
+
+@_wraps(osp_stats.norm.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))",No
jax/scipy/stats/beta.py,jax/scipy/stats/beta.py,8e311059e56916ab903b36c38ba1dab3c260657a,8e9d50136a836a23cc07b1398a35d64745f128a7,add scipy.stats.beta.pdf,"diff --git a/jax/scipy/stats/beta.py b/jax/scipy/stats/beta.py
index 6af0d5996..09f91efaa 100644
--- a/jax/scipy/stats/beta.py
+++ b/jax/scipy/stats/beta.py
@@ -37,3 +37,8 @@ def logpdf(x, a, b, loc=0, scale=1):
   log_probs = lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
   return where(logical_or(lax.ge(x, lax.add(loc, scale)),
                           lax.le(x, loc)), -inf, log_probs)
+
+@_wraps(osp_stats.beta.pdf)
+def pdf(x, a, b, loc=0, scale=1):
+  return lax.exp(logpdf(x, a, b, loc, scale))
+","diff --git a/jax/scipy/stats/beta.py b/jax/scipy/stats/beta.py
index 6af0d5996..09f91efaa 100644
--- a/jax/scipy/stats/beta.py
+++ b/jax/scipy/stats/beta.py
@@ -37,3 +37,8 @@ def logpdf(x, a, b, loc=0, scale=1):
   log_probs = lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
   return where(logical_or(lax.ge(x, lax.add(loc, scale)),
                           lax.le(x, loc)), -inf, log_probs)
+
+@_wraps(osp_stats.beta.pdf)
+def pdf(x, a, b, loc=0, scale=1):
+  return lax.exp(logpdf(x, a, b, loc, scale))
+",No
jax/scipy/stats/expon.py,jax/scipy/stats/expon.py,b2e4882c8a58af7f2c8b207d0941b759471b20a1,8e311059e56916ab903b36c38ba1dab3c260657a,add scipy.stats.expon.pdf,"diff --git a/jax/scipy/stats/expon.py b/jax/scipy/stats/expon.py
index 32e5493ef..bc027247a 100644
--- a/jax/scipy/stats/expon.py
+++ b/jax/scipy/stats/expon.py
@@ -30,3 +30,7 @@ def logpdf(x, loc=0, scale=1):
   linear_term = lax.div(lax.sub(x, loc), scale)
   log_probs = lax.neg(lax.add(linear_term, log_scale))
   return where(lax.le(x, loc), -inf, log_probs)
+
+@_wraps(osp_stats.expon.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))","diff --git a/jax/scipy/stats/expon.py b/jax/scipy/stats/expon.py
index 32e5493ef..bc027247a 100644
--- a/jax/scipy/stats/expon.py
+++ b/jax/scipy/stats/expon.py
@@ -30,3 +30,7 @@ def logpdf(x, loc=0, scale=1):
   linear_term = lax.div(lax.sub(x, loc), scale)
   log_probs = lax.neg(lax.add(linear_term, log_scale))
   return where(lax.le(x, loc), -inf, log_probs)
+
+@_wraps(osp_stats.expon.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))",No
jax/scipy/stats/gamma.py,jax/scipy/stats/gamma.py,beb26626048eb8cc2af6e36fe7df521935ada332,b2e4882c8a58af7f2c8b207d0941b759471b20a1,add scipy.stats.gamma.pdf,"diff --git a/jax/scipy/stats/gamma.py b/jax/scipy/stats/gamma.py
index 9607380f2..c8ff7cb90 100644
--- a/jax/scipy/stats/gamma.py
+++ b/jax/scipy/stats/gamma.py
@@ -34,3 +34,7 @@ def logpdf(x, a, loc=0, scale=1):
   shape_terms = lax.add(gammaln(a), lax.log(scale))
   log_probs = lax.sub(log_linear_term, shape_terms)
   return where(lax.le(x, loc), -inf, log_probs)
+
+@_wraps(osp_stats.gamma.pdf)
+def pdf(x, a, loc=0, scale=1):
+  return lax.exp(logpdf(x, a, loc, scale))","diff --git a/jax/scipy/stats/gamma.py b/jax/scipy/stats/gamma.py
index 9607380f2..c8ff7cb90 100644
--- a/jax/scipy/stats/gamma.py
+++ b/jax/scipy/stats/gamma.py
@@ -34,3 +34,7 @@ def logpdf(x, a, loc=0, scale=1):
   shape_terms = lax.add(gammaln(a), lax.log(scale))
   log_probs = lax.sub(log_linear_term, shape_terms)
   return where(lax.le(x, loc), -inf, log_probs)
+
+@_wraps(osp_stats.gamma.pdf)
+def pdf(x, a, loc=0, scale=1):
+  return lax.exp(logpdf(x, a, loc, scale))",No
jax/scipy/stats/laplace.py,jax/scipy/stats/laplace.py,fc2de383bfc3b4b6938e8419658ad145b9df05d7,beb26626048eb8cc2af6e36fe7df521935ada332,add scipy.stats.laplace.pdf,"diff --git a/jax/scipy/stats/laplace.py b/jax/scipy/stats/laplace.py
index 5dcdd512c..09c049f11 100644
--- a/jax/scipy/stats/laplace.py
+++ b/jax/scipy/stats/laplace.py
@@ -29,3 +29,7 @@ def logpdf(x, loc=0, scale=1):
   two = _constant_like(x, 2)
   linear_term = lax.div(lax.abs(lax.sub(x, loc)), scale)
   return lax.neg(lax.add(linear_term, lax.log(lax.mul(two, scale))))
+
+@_wraps(osp_stats.laplace.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))","diff --git a/jax/scipy/stats/laplace.py b/jax/scipy/stats/laplace.py
index 5dcdd512c..09c049f11 100644
--- a/jax/scipy/stats/laplace.py
+++ b/jax/scipy/stats/laplace.py
@@ -29,3 +29,7 @@ def logpdf(x, loc=0, scale=1):
   two = _constant_like(x, 2)
   linear_term = lax.div(lax.abs(lax.sub(x, loc)), scale)
   return lax.neg(lax.add(linear_term, lax.log(lax.mul(two, scale))))
+
+@_wraps(osp_stats.laplace.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))",No
jax/scipy/stats/uniform.py,jax/scipy/stats/uniform.py,7b6164c20acaf3c553eee3a4ebc4d2a71d9b0894,fc2de383bfc3b4b6938e8419658ad145b9df05d7,add scipy.stats.uniform.pdf,"diff --git a/jax/scipy/stats/uniform.py b/jax/scipy/stats/uniform.py
index 3ac7bc86b..6145c53d9 100644
--- a/jax/scipy/stats/uniform.py
+++ b/jax/scipy/stats/uniform.py
@@ -30,3 +30,7 @@ def logpdf(x, loc=0, scale=1):
   log_probs = lax.broadcast(fill_value, onp.shape(x))
   return where(logical_or(lax.ge(x, lax.add(loc, scale)),
                           lax.le(x, loc)), -inf, log_probs)
+
+@_wraps(osp_stats.uniform.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))","diff --git a/jax/scipy/stats/uniform.py b/jax/scipy/stats/uniform.py
index 3ac7bc86b..6145c53d9 100644
--- a/jax/scipy/stats/uniform.py
+++ b/jax/scipy/stats/uniform.py
@@ -30,3 +30,7 @@ def logpdf(x, loc=0, scale=1):
   log_probs = lax.broadcast(fill_value, onp.shape(x))
   return where(logical_or(lax.ge(x, lax.add(loc, scale)),
                           lax.le(x, loc)), -inf, log_probs)
+
+@_wraps(osp_stats.uniform.pdf)
+def pdf(x, loc=0, scale=1):
+  return lax.exp(logpdf(x, loc, scale))",No
jax/scipy/stats/__init__.py,jax/scipy/stats/__init__.py,dc71e0d965bde8a1805ea7dbf43b74b978db470b,7b6164c20acaf3c553eee3a4ebc4d2a71d9b0894,Update __init__.py,"diff --git a/jax/scipy/stats/__init__.py b/jax/scipy/stats/__init__.py
index 6bae8b090..57a9cfa0d 100644
--- a/jax/scipy/stats/__init__.py
+++ b/jax/scipy/stats/__init__.py
@@ -17,5 +17,6 @@ from . import beta
 from . import expon
 from . import gamma
 from . import laplace
+from . import multivariate_normal
 from . import norm
 from . import uniform","diff --git a/jax/scipy/stats/__init__.py b/jax/scipy/stats/__init__.py
index 6bae8b090..57a9cfa0d 100644
--- a/jax/scipy/stats/__init__.py
+++ b/jax/scipy/stats/__init__.py
@@ -17,5 +17,6 @@ from . import beta
 from . import expon
 from . import gamma
 from . import laplace
+from . import multivariate_normal
 from . import norm
 from . import uniform",No
tests/api_test.py,tests/api_test.py,f6d9a33a864dc4633454b458761f6fa31d6c3476,e609be4c7fff9c0f6b2e7327c4a826f413379b0c,"Undefined name: 'value_and_grad' in ./tests/api_test.py

[flake8](http://flake8.pycqa.org) testing of https://github.com/google/jax on Python 3.7.1

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tests/api_test.py:54:12: F821 undefined name 'value_and_grad'
    assert value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
           ^
./tests/api_test.py:55:12: F821 undefined name 'value_and_grad'
    assert value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
           ^
./tests/api_test.py:56:12: F821 undefined name 'value_and_grad'
    assert value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
           ^
3     F821 undefined name 'value_and_grad'
3
```
__E901,E999,F821,F822,F823__ are the ""_showstopper_"" [flake8](http://flake8.pycqa.org) issues that can halt the runtime with a SyntaxError, NameError, etc. These 5 are different from most other flake8 issues which are merely ""style violations"" -- useful for readability but they do not effect runtime safety.
* F821: undefined name `name`
* F822: undefined name `name` in `__all__`
* F823: local variable name referenced before assignment
* E901: SyntaxError or IndentationError
* E999: SyntaxError -- failed to compile a file into an Abstract Syntax Tree","diff --git a/tests/api_test.py b/tests/api_test.py
index 6f51788e6..e5272731f 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -51,9 +51,9 @@ class APITest(jtu.JaxTestCase):
       return 1.0 * x + 2.0 * y + 3.0 * z
 
     y = f(1.0, 1.0, 1.0, flag=True)
-    assert value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
-    assert value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
-    assert value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
+    assert api.value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
+    assert api.value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
+    assert api.value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
 
   def test_jit_static_args(self):
     side = []","diff --git a/tests/api_test.py b/tests/api_test.py
index 6f51788e6..e5272731f 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -51,9 +51,9 @@ class APITest(jtu.JaxTestCase):
       return 1.0 * x + 2.0 * y + 3.0 * z
 
     y = f(1.0, 1.0, 1.0, flag=True)
-    assert value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
-    assert value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
-    assert value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
+    assert api.value_and_grad(f)(1.0, 1.0, 1.0, flag=True) == (y, 1.0)
+    assert api.value_and_grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == (y, 2.0)
+    assert api.value_and_grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (y, (3.0, 1.0))
 
   def test_jit_static_args(self):
     side = []",No
tests/api_test.py,tests/api_test.py,b95669eb79c1e2148397cd46a4d4b6f2c35c72b8,8565a3486adf16beb388b2364c9cd930d7a0d92d,enable value-and-grad api test,"diff --git a/tests/api_test.py b/tests/api_test.py
index e5272731f..ddafd4df3 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -45,7 +45,7 @@ class APITest(jtu.JaxTestCase):
     assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
     assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
 
-  def value_and_grad_argnums(self):
+  def test_value_and_grad_argnums(self):
     def f(x, y, z, flag=False):
       assert flag
       return 1.0 * x + 2.0 * y + 3.0 * z","diff --git a/tests/api_test.py b/tests/api_test.py
index e5272731f..ddafd4df3 100644
--- a/tests/api_test.py
+++ b/tests/api_test.py
@@ -45,7 +45,7 @@ class APITest(jtu.JaxTestCase):
     assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
     assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
 
-  def value_and_grad_argnums(self):
+  def test_value_and_grad_argnums(self):
     def f(x, y, z, flag=False):
       assert flag
       return 1.0 * x + 2.0 * y + 3.0 * z",No
docs/conf.py,docs/conf.py,326c3dbb990d3241c981e67090b4f82bf2389e12,fab3a7a7298d1b8be0e6589242ebf3d4eda6954a,Add license header to docs/conf.py.,"diff --git a/docs/conf.py b/docs/conf.py
index 7cab6174c..5f0a40527 100644
--- a/docs/conf.py
+++ b/docs/conf.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
 # -*- coding: utf-8 -*-
 #
 # Configuration file for the Sphinx documentation builder.","diff --git a/docs/conf.py b/docs/conf.py
index 7cab6174c..5f0a40527 100644
--- a/docs/conf.py
+++ b/docs/conf.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
 # -*- coding: utf-8 -*-
 #
 # Configuration file for the Sphinx documentation builder.",No
jax/api.py,jax/api.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/jax/api.py b/jax/api.py
index eee6164ab..3ed2a42d2 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -45,12 +45,13 @@ from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
 from .interpreters import batching
+from .interpreters import parallel
 
 map = safe_map
 zip = safe_zip
 
 
-def jit(fun, static_argnums=()):
+def jit(fun, static_argnums=(), **params):
   """"""Sets up `fun` for just-in-time compilation with XLA.
 
   Args:
@@ -75,7 +76,7 @@ def jit(fun, static_argnums=()):
     args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
     check_args(args_flat)
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    out_flat = xla.xla_call(jaxtree_fun, *args_flat)
+    out_flat = xla.xla_call(jaxtree_fun, *args_flat, **params)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
@@ -255,6 +256,35 @@ def vmap(fun, in_axes=0, out_axes=0):
 
   return batched_fun
 
+
+def pmap(fun, axis_name, in_axes=0, out_axes=0):
+  """"""Vectorizing pseudo-map for single-program multiple-data (SPMD) functions.""""""
+  def pmap_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = parallel.pmap(jaxtree_fun, axis_name, args, in_axes_, out_axes)
+    return build_tree(out_tree(), out_flat)
+
+  return pmap_fun
+
+
+def papply(fun, in_axes=0):
+  """"""Apply a function using parallel computation by sharding inputs.""""""
+  axis_name = parallel.newvar()
+
+  def papply_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
+    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = parallel.papply(jaxtree_fun, axis_name, args_flat, in_axes_)
+    return build_tree(out_tree(), out_flat)
+
+  return papply_fun, axis_name
+
+
 def jvp(fun, primals, tangents):
   def trim_arg(primal, tangent):
     primal_jtuple, tree_def = pytree_to_jaxtupletree(primal)","diff --git a/jax/api.py b/jax/api.py
index eee6164ab..3ed2a42d2 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -45,12 +45,13 @@ from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
 from .interpreters import batching
+from .interpreters import parallel
 
 map = safe_map
 zip = safe_zip
 
 
-def jit(fun, static_argnums=()):
+def jit(fun, static_argnums=(), **params):
   """"""Sets up `fun` for just-in-time compilation with XLA.
 
   Args:
@@ -75,7 +76,7 @@ def jit(fun, static_argnums=()):
     args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
     check_args(args_flat)
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    out_flat = xla.xla_call(jaxtree_fun, *args_flat)
+    out_flat = xla.xla_call(jaxtree_fun, *args_flat, **params)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
@@ -255,6 +256,35 @@ def vmap(fun, in_axes=0, out_axes=0):
 
   return batched_fun
 
+
+def pmap(fun, axis_name, in_axes=0, out_axes=0):
+  """"""Vectorizing pseudo-map for single-program multiple-data (SPMD) functions.""""""
+  def pmap_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = parallel.pmap(jaxtree_fun, axis_name, args, in_axes_, out_axes)
+    return build_tree(out_tree(), out_flat)
+
+  return pmap_fun
+
+
+def papply(fun, in_axes=0):
+  """"""Apply a function using parallel computation by sharding inputs.""""""
+  axis_name = parallel.newvar()
+
+  def papply_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
+    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = parallel.papply(jaxtree_fun, axis_name, args_flat, in_axes_)
+    return build_tree(out_tree(), out_flat)
+
+  return papply_fun, axis_name
+
+
 def jvp(fun, primals, tangents):
   def trim_arg(primal, tangent):
     primal_jtuple, tree_def = pytree_to_jaxtupletree(primal)",No
jax/interpreters/batching.py,jax/interpreters/batching.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 37080c60d..842a9122a 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -221,14 +221,14 @@ def broadcast_batcher(prim, batched_args, batch_dims, **params):
 def defreducer(prim):
   primitive_batchers[prim] = partial(reducer_batcher, prim)
 
-def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
+def reducer_batcher(prim, batched_args, batch_dims, axes, **params):
   operand, = batched_args
   bdim, = batch_dims
   axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
   bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
-  if 'input_shape' in kwargs:
-    kwargs['input_shape'] = operand.shape
-  return prim.bind(operand, axes=axes, **kwargs), bdim_out
+  if 'input_shape' in params:
+    params = dict(params, input_shape=operand.shape)
+  return prim.bind(operand, axes=axes, **params), bdim_out
 
 # set up primitive batches for ad_util primitives
 ","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 37080c60d..842a9122a 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -221,14 +221,14 @@ def broadcast_batcher(prim, batched_args, batch_dims, **params):
 def defreducer(prim):
   primitive_batchers[prim] = partial(reducer_batcher, prim)
 
-def reducer_batcher(prim, batched_args, batch_dims, axes, **kwargs):
+def reducer_batcher(prim, batched_args, batch_dims, axes, **params):
   operand, = batched_args
   bdim, = batch_dims
   axes = tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
   bdim_out = list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim)
-  if 'input_shape' in kwargs:
-    kwargs['input_shape'] = operand.shape
-  return prim.bind(operand, axes=axes, **kwargs), bdim_out
+  if 'input_shape' in params:
+    params = dict(params, input_shape=operand.shape)
+  return prim.bind(operand, axes=axes, **params), bdim_out
 
 # set up primitive batches for ad_util primitives
 ",No
jax/interpreters/partial_eval.py,jax/interpreters/partial_eval.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index 610963c88..dcec8638e 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -260,7 +260,7 @@ def trace_to_jaxpr(fun, pvals, **kwargs):
   return jaxpr, out_pval, consts
 
 @transformation
-def trace_to_subjaxpr(master, pvals, **kwargs):
+def trace_to_subjaxpr(master, pvals):
   assert all([isinstance(pv, PartialVal) for pv in pvals]), pvals
   trace = JaxprTrace(master, core.cur_sublevel())
   in_tracers = map(trace.new_arg, pvals)","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index 610963c88..dcec8638e 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -260,7 +260,7 @@ def trace_to_jaxpr(fun, pvals, **kwargs):
   return jaxpr, out_pval, consts
 
 @transformation
-def trace_to_subjaxpr(master, pvals, **kwargs):
+def trace_to_subjaxpr(master, pvals):
   assert all([isinstance(pv, PartialVal) for pv in pvals]), pvals
   trace = JaxprTrace(master, core.cur_sublevel())
   in_tracers = map(trace.new_arg, pvals)",No
jax/interpreters/xla.py,jax/interpreters/xla.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index f6a6891da..2a5b9f569 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -29,6 +29,7 @@ from six.moves import xrange
 from ..config import flags
 from .. import core
 from .. import ad_util
+from .. import tree_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types
 from ..core import AbstractTuple, JaxTuple, pack, valid_jaxtype
 from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map, prod
@@ -73,10 +74,11 @@ def aval_from_xla_shape(shape):
   return ShapedArray(shape.dimensions(), shape.element_type())
 
 def execute_compiled_primitive(compiled, result_handler, *args):
-  input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
+  input_bufs = [device_put(x) for x in args]
   return result_handler(compiled.Execute(input_bufs, not core.skip_checks))
 
 def device_put(x):
+  x = canonicalize_pyval_dtype(x)
   if type(x) is DeviceArray:
     return x.device_buffer
   elif isinstance(x, DeviceConstant):
@@ -417,25 +419,28 @@ def xla_call_impl(fun, *args):
 
 @linear_memoize
 def xla_callable(fun, *abstract_args):
+  pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
-    pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
     assert not env  # no subtraces here (though cond might eventually need them)
     compiled, result_shape = compile_jaxpr(jaxpr, consts, *abstract_args)
-    del master, pvals, consts, jaxpr, env
-    handle_result = result_handler(result_shape)
-    return partial(execute_compiled, compiled, pval, handle_result)
+    del master, consts, jaxpr, env
+  handle_result = result_handler(result_shape)
+  return partial(execute_compiled, compiled, pval, handle_result)
 
 def execute_compiled(compiled, pval, handle_result, *args):
-  input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
+  input_bufs = [device_put(x) for x in args]
   out_buf = compiled.Execute(input_bufs, not core.skip_checks)
   return merge_pvals(handle_result(out_buf), pval)
 
 
+def xla_call_translation_rule(c, subc_a1, *a2):
+  subc, a1 = subc_a1
+  return c.Call(subc, a1 + a2)
+
 xla_call_p = core.Primitive('xla_call')
 xla_call = partial(core.call_bind, xla_call_p)
 xla_call_p.def_custom_bind(xla_call)
 xla_call_p.def_impl(xla_call_impl)
 
-translations[xla_call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
-                                                          subc_a1[1] + a2)
+translations[xla_call_p] = xla_call_translation_rule","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index f6a6891da..2a5b9f569 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -29,6 +29,7 @@ from six.moves import xrange
 from ..config import flags
 from .. import core
 from .. import ad_util
+from .. import tree_util
 from ..abstract_arrays import ConcreteArray, ShapedArray, make_shaped_array, array_types
 from ..core import AbstractTuple, JaxTuple, pack, valid_jaxtype
 from ..util import partial, partialmethod, memoize, unzip2, concatenate, safe_map, prod
@@ -73,10 +74,11 @@ def aval_from_xla_shape(shape):
   return ShapedArray(shape.dimensions(), shape.element_type())
 
 def execute_compiled_primitive(compiled, result_handler, *args):
-  input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
+  input_bufs = [device_put(x) for x in args]
   return result_handler(compiled.Execute(input_bufs, not core.skip_checks))
 
 def device_put(x):
+  x = canonicalize_pyval_dtype(x)
   if type(x) is DeviceArray:
     return x.device_buffer
   elif isinstance(x, DeviceConstant):
@@ -417,25 +419,28 @@ def xla_call_impl(fun, *args):
 
 @linear_memoize
 def xla_callable(fun, *abstract_args):
+  pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
-    pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
     assert not env  # no subtraces here (though cond might eventually need them)
     compiled, result_shape = compile_jaxpr(jaxpr, consts, *abstract_args)
-    del master, pvals, consts, jaxpr, env
-    handle_result = result_handler(result_shape)
-    return partial(execute_compiled, compiled, pval, handle_result)
+    del master, consts, jaxpr, env
+  handle_result = result_handler(result_shape)
+  return partial(execute_compiled, compiled, pval, handle_result)
 
 def execute_compiled(compiled, pval, handle_result, *args):
-  input_bufs = [device_put(canonicalize_pyval_dtype(x)) for x in args]
+  input_bufs = [device_put(x) for x in args]
   out_buf = compiled.Execute(input_bufs, not core.skip_checks)
   return merge_pvals(handle_result(out_buf), pval)
 
 
+def xla_call_translation_rule(c, subc_a1, *a2):
+  subc, a1 = subc_a1
+  return c.Call(subc, a1 + a2)
+
 xla_call_p = core.Primitive('xla_call')
 xla_call = partial(core.call_bind, xla_call_p)
 xla_call_p.def_custom_bind(xla_call)
 xla_call_p.def_impl(xla_call_impl)
 
-translations[xla_call_p] = lambda c, subc_a1, *a2: c.Call(subc_a1[0],
-                                                          subc_a1[1] + a2)
+translations[xla_call_p] = xla_call_translation_rule",No
jax/lax.py,jax/lax.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/jax/lax.py b/jax/lax.py
index 471067889..3372bac17 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -40,6 +40,7 @@ from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
 from .interpreters import batching
+from .interpreters import parallel
 from .util import curry, safe_zip, unzip2, prod
 from .tree_util import build_tree
 from .lib import xla_bridge
@@ -758,6 +759,7 @@ def unop(result_dtype, accepted_dtypes, name):
   dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
   prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)
   batching.defvectorized(prim)
+  parallel.defvectorized(prim)
   return prim
 standard_unop = partial(unop, identity)
 _attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)
@@ -797,6 +799,7 @@ def binop(result_dtype, accepted_dtypes, name):
   shape_rule = partial(broadcasting_shape_rule, name)
   prim = standard_primitive(shape_rule, dtype_rule, name)
   batching.defbroadcasting(prim)
+  parallel.defbroadcasting(prim)
   return prim
 standard_binop = partial(binop, _input_dtype)
 
@@ -1570,10 +1573,24 @@ def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused
     dimensions = (0,) + tuple(onp.add(1, dimensions))
   return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0
 
+def reshape_papply_rule(name, vals, axes, new_sizes, dimensions, old_sizes):
+  operand, = vals
+  axis, = axes
+
+  if dimensions is None:
+    # if there is an i such that prod(new_sizes[:i]) == prod(old_sizes[:axis])
+    # and new_sizes[i] == old_sizes[axis], then we can maintain sharding.
+    # otherwise, it's ambiguous, and we could either gather or just make up a
+    # way to rescatter.
+    raise NotImplementedError  # TODO
+  else:
+    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
+
 reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                                'reshape', reshape_translation_rule)
 ad.deflinear(reshape_p, reshape_transpose_rule)
 batching.primitive_batchers[reshape_p] = reshape_batch_rule
+parallel.papply_primitive_rules[reshape_p] = reshape_papply_rule
 
 
 def rev_shape_rule(operand, dimensions):
@@ -2155,6 +2172,7 @@ reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                   'reduce_sum', reduce_sum_translation_rule)
 ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
 batching.defreducer(reduce_sum_p)
+parallel.defreducer(reduce_sum_p, parallel.psum_p)
 
 
 def reduce_chooser_shape_rule(operand, axes):","diff --git a/jax/lax.py b/jax/lax.py
index 471067889..3372bac17 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -40,6 +40,7 @@ from .interpreters import partial_eval as pe
 from .interpreters import xla
 from .interpreters import ad
 from .interpreters import batching
+from .interpreters import parallel
 from .util import curry, safe_zip, unzip2, prod
 from .tree_util import build_tree
 from .lib import xla_bridge
@@ -758,6 +759,7 @@ def unop(result_dtype, accepted_dtypes, name):
   dtype_rule = partial(unop_dtype_rule, result_dtype, accepted_dtypes, name)
   prim = standard_primitive(_attrgetter('shape'), dtype_rule, name)
   batching.defvectorized(prim)
+  parallel.defvectorized(prim)
   return prim
 standard_unop = partial(unop, identity)
 _attrgetter = lambda name: lambda x, **kwargs: getattr(x, name)
@@ -797,6 +799,7 @@ def binop(result_dtype, accepted_dtypes, name):
   shape_rule = partial(broadcasting_shape_rule, name)
   prim = standard_primitive(shape_rule, dtype_rule, name)
   batching.defbroadcasting(prim)
+  parallel.defbroadcasting(prim)
   return prim
 standard_binop = partial(binop, _input_dtype)
 
@@ -1570,10 +1573,24 @@ def reshape_batch_rule(batched_args, batch_dims, new_sizes, dimensions, **unused
     dimensions = (0,) + tuple(onp.add(1, dimensions))
   return reshape(operand, operand.shape[:1] + new_sizes, dimensions), 0
 
+def reshape_papply_rule(name, vals, axes, new_sizes, dimensions, old_sizes):
+  operand, = vals
+  axis, = axes
+
+  if dimensions is None:
+    # if there is an i such that prod(new_sizes[:i]) == prod(old_sizes[:axis])
+    # and new_sizes[i] == old_sizes[axis], then we can maintain sharding.
+    # otherwise, it's ambiguous, and we could either gather or just make up a
+    # way to rescatter.
+    raise NotImplementedError  # TODO
+  else:
+    raise NotImplementedError  # TODO(mattjj): handle reshape w/ dimensions
+
 reshape_p = standard_primitive(reshape_shape_rule, reshape_dtype_rule,
                                'reshape', reshape_translation_rule)
 ad.deflinear(reshape_p, reshape_transpose_rule)
 batching.primitive_batchers[reshape_p] = reshape_batch_rule
+parallel.papply_primitive_rules[reshape_p] = reshape_papply_rule
 
 
 def rev_shape_rule(operand, dimensions):
@@ -2155,6 +2172,7 @@ reduce_sum_p = standard_primitive(reduce_sum_shape_rule, _input_dtype,
                                   'reduce_sum', reduce_sum_translation_rule)
 ad.deflinear(reduce_sum_p, reduce_sum_transpose_rule)
 batching.defreducer(reduce_sum_p)
+parallel.defreducer(reduce_sum_p, parallel.psum_p)
 
 
 def reduce_chooser_shape_rule(operand, axes):",No
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index c38b8226e..3dd4ea1de 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -165,10 +165,9 @@ def _get_backend():
                                 FLAGS.jax_backend_target)
 
 
-def device_put(pyval):
-  # TODO(frostig): Accept a replica id for placement. For now, this places on
-  # the first replica only.
-  return get_xla_client().LocalBuffer.from_pyval(pyval, backend=_get_backend())
+def device_put(pyval, replica=0):
+  client = get_xla_client()
+  return client.LocalBuffer.from_pyval(pyval, replica, backend=_get_backend())
 
 
 Shape = xla_client.Shape        # pylint: disable=invalid-name","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index c38b8226e..3dd4ea1de 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -165,10 +165,9 @@ def _get_backend():
                                 FLAGS.jax_backend_target)
 
 
-def device_put(pyval):
-  # TODO(frostig): Accept a replica id for placement. For now, this places on
-  # the first replica only.
-  return get_xla_client().LocalBuffer.from_pyval(pyval, backend=_get_backend())
+def device_put(pyval, replica=0):
+  client = get_xla_client()
+  return client.LocalBuffer.from_pyval(pyval, replica, backend=_get_backend())
 
 
 Shape = xla_client.Shape        # pylint: disable=invalid-name",No
tests/batching_test.py,tests/batching_test.py,c7ce4420843846466d7262eb3b8c53d258c3ff0a,326c3dbb990d3241c981e67090b4f82bf2389e12,"initial pmap/pxla code, pair-coded w/ @dougalm","diff --git a/tests/batching_test.py b/tests/batching_test.py
index db4874580..9bf6cfa30 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -34,6 +34,7 @@ from jax.util import partial, curry
 from jax.config import config
 config.parse_flags_with_absl()
 
+
 class BatchingTest(jtu.JaxTestCase):
 
   def testConstantFunction(self):","diff --git a/tests/batching_test.py b/tests/batching_test.py
index db4874580..9bf6cfa30 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -34,6 +34,7 @@ from jax.util import partial, curry
 from jax.config import config
 config.parse_flags_with_absl()
 
+
 class BatchingTest(jtu.JaxTestCase):
 
   def testConstantFunction(self):",No
tests/parallel_test.py,tests/parallel_test.py,781577226dbddd28b33c8c80e16424ca8a863e7d,c7ce4420843846466d7262eb3b8c53d258c3ff0a,clean up parallel_test.py file,"diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index f78695c34..9869281e1 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -31,115 +31,88 @@ from jax.config import config
 config.parse_flags_with_absl()
 
 
-# class PmapTest(jtu.JaxTestCase):
+class PmapTest(jtu.JaxTestCase):
 
-#   def testConstantFunction(self):
-#     f = lambda x: 3
-#     ans = pmap(f, axis_name='i')(onp.ones(4))
-#     expected = 3 * onp.ones(4)
-#     self.assertAllClose(ans, expected, check_dtypes=False)
-
-#   def testReduceSum(self):
-#     f = lambda x: psum(x, 'i')
-#     ans = pmap(f, axis_name='i')(onp.ones(4))
-#     expected = 4 * onp.ones(4)
-#     self.assertAllClose(ans, expected, check_dtypes=False)
-
-#   def testLogSoftmax(self):
-#     f = lambda x: x - np.log(psum(np.exp(x), 'i'))
-#     x = onp.log(onp.arange(1., 10., dtype=onp.float32))
-#     ans = pmap(f, axis_name='i')(x)
-#     expected = x - onp.log(onp.sum(onp.exp(x)))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
-
-#   # def testPmapOfJit(self):
-#   #   f = jit(lambda x: psum(x, 'i'))
-#   #   x = onp.arange(12.)
-#   #   ans = pmap(f, axis_name='i')(x)
-#   #   expected = onp.sum(x)
-#   #   self.assertAllClose(ans, expected, check_dtypes=False)
-
-
-# class PapplyTest(jtu.JaxTestCase):
-
-#   def testIdentity(self):
-#     pfun, axis_name = papply(lambda x: x)
-#     ans = pfun(onp.arange(3))
-#     expected = onp.arange(3)
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  def testConstantFunction(self):
+    f = lambda x: 3
+    ans = pmap(f, axis_name='i')(onp.ones(4))
+    expected = 3 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testMap(self):
-#     pfun, axis_name = papply(np.sin)
-#     ans = pfun(onp.arange(3.))
-#     expected = onp.sin(onp.arange(3.))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  def testReduceSum(self):
+    f = lambda x: psum(x, 'i')
+    ans = pmap(f, axis_name='i')(onp.ones(4))
+    expected = 4 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testSum(self):
-#     pfun, axis_name = papply(np.sum)
+  def testLogSoftmax(self):
+    f = lambda x: x - np.log(psum(np.exp(x), 'i'))
+    x = onp.log(onp.arange(1., 10., dtype=onp.float32))
+    ans = pmap(f, axis_name='i')(x)
+    expected = x - onp.log(onp.sum(onp.exp(x)))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#     jaxpr = make_jaxpr(pfun)(onp.zeros(5))
-#     expected_jaxpr = make_jaxpr(lambda x: psum(x, axis_name))(onp.zeros(5))
-#     assert repr(jaxpr) == repr(expected_jaxpr)
 
-#     ans = pmap(pfun, axis_name)(onp.arange(3.))
-#     expected = onp.sum(onp.arange(3.))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+class PapplyTest(jtu.JaxTestCase):
 
-#   def testLogSoftmax(self):
+  def testIdentity(self):
+    pfun, axis_name = papply(lambda x: x)
+    ans = pfun(onp.arange(3))
+    expected = onp.arange(3)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#     def fun(x):
-#       return x - np.log(np.sum(np.exp(x)))
+  def testMap(self):
+    pfun, axis_name = papply(np.sin)
+    ans = pfun(onp.arange(3.))
+    expected = onp.sin(onp.arange(3.))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#     pfun, axis_name = papply(fun)
+  def testSum(self):
+    pfun, axis_name = papply(np.sum)
 
-#     jaxpr = make_jaxpr(pfun)(onp.zeros(5))
-#     expected_jaxpr = make_jaxpr(lambda x: x - np.log(psum(np.exp(x), axis_name))
-#                                 )(onp.zeros(5))
-#     assert repr(jaxpr) == repr(expected_jaxpr)
+    jaxpr = make_jaxpr(pfun)(onp.zeros(5))
+    expected_jaxpr = make_jaxpr(lambda x: psum(x, axis_name))(onp.zeros(5))
+    assert repr(jaxpr) == repr(expected_jaxpr)
 
-#     ans = pmap(pfun, axis_name)(onp.arange(1., 5.))
-#     expected = fun(onp.arange(1., 5.))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+    ans = pmap(pfun, axis_name)(onp.arange(3.))
+    expected = onp.sum(onp.arange(3.))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testAdd(self):
-#     x = onp.array([[1, 2, 3], [4, 5, 6]])
-#     expected = x + x
+  def testLogSoftmax(self):
 
-#     pfun, axis_name = papply(np.add)
-#     ans = pmap(pfun, axis_name)(x, x)
-#     self.assertAllClose(ans, expected, check_dtypes=True)
+    def fun(x):
+      return x - np.log(np.sum(np.exp(x)))
 
-# #   def testAddDifferentSharding(self):
-# #     x = onp.array([[1, 2, 3], [4, 5, 6]])
-# #     expected = x + x
+    pfun, axis_name = papply(fun)
 
-# #     pfun, axis_name = papply(np.add, (0, 1))
-# #     ans = pmap(pfun, axis_name)(x, x)
-# #     self.assertAllClose(ans, expected, check_dtypes=True)
+    jaxpr = make_jaxpr(pfun)(onp.zeros(5))
+    expected_jaxpr = make_jaxpr(lambda x: x - np.log(psum(np.exp(x), axis_name))
+                                )(onp.zeros(5))
+    assert repr(jaxpr) == repr(expected_jaxpr)
 
-# #   def testScatterLike(self):
-# #     def fun(y):
-# #       x_scattered = scatter_like(x, y)
-# #       return lax.add(x_scattered, y)  # TODO replace with x + y
+    ans = pmap(pfun, axis_name)(onp.arange(1., 5.))
+    expected = fun(onp.arange(1., 5.))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-# #     x = onp.array([[1, 2], [3, 4]])
-# #     expected = x + x
+  def testAdd(self):
+    x = onp.array([[1, 2, 3], [4, 5, 6]])
+    expected = x + x
 
-# #     pfun, axis_name = papply(fun)
-# #     ans = pmap(pfun, axis_name)(x)
-# #     self.assertAllClose(ans, expected, check_dtypes=True)
+    pfun, axis_name = papply(np.add)
+    ans = pmap(pfun, axis_name)(x, x)
+    self.assertAllClose(ans, expected, check_dtypes=True)
 
-#   def testAddBroadcasting(self):
+  def testAddBroadcasting(self):
 
-#     def fun(x):
-#       return x + 3
+    def fun(x):
+      return x + 3
 
-#     x = onp.array([[1, 2], [3, 4]])
-#     expected = x + 3
+    x = onp.array([[1, 2], [3, 4]])
+    expected = x + 3
 
-#     pfun, axis_name = papply(fun)
-#     ans = pmap(pfun, axis_name)(x)
-#     self.assertAllClose(ans, expected, check_dtypes=True)
+    pfun, axis_name = papply(fun)
+    ans = pmap(pfun, axis_name)(x)
+    self.assertAllClose(ans, expected, check_dtypes=True)
 
 
 class ChunkTest(jtu.JaxTestCase):","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index f78695c34..9869281e1 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -31,115 +31,88 @@ from jax.config import config
 config.parse_flags_with_absl()
 
 
-# class PmapTest(jtu.JaxTestCase):
+class PmapTest(jtu.JaxTestCase):
 
-#   def testConstantFunction(self):
-#     f = lambda x: 3
-#     ans = pmap(f, axis_name='i')(onp.ones(4))
-#     expected = 3 * onp.ones(4)
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  def testConstantFunction(self):
+    f = lambda x: 3
+    ans = pmap(f, axis_name='i')(onp.ones(4))
+    expected = 3 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testReduceSum(self):
-#     f = lambda x: psum(x, 'i')
-#     ans = pmap(f, axis_name='i')(onp.ones(4))
-#     expected = 4 * onp.ones(4)
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  def testReduceSum(self):
+    f = lambda x: psum(x, 'i')
+    ans = pmap(f, axis_name='i')(onp.ones(4))
+    expected = 4 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testLogSoftmax(self):
-#     f = lambda x: x - np.log(psum(np.exp(x), 'i'))
-#     x = onp.log(onp.arange(1., 10., dtype=onp.float32))
-#     ans = pmap(f, axis_name='i')(x)
-#     expected = x - onp.log(onp.sum(onp.exp(x)))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
-
-#   # def testPmapOfJit(self):
-#   #   f = jit(lambda x: psum(x, 'i'))
-#   #   x = onp.arange(12.)
-#   #   ans = pmap(f, axis_name='i')(x)
-#   #   expected = onp.sum(x)
-#   #   self.assertAllClose(ans, expected, check_dtypes=False)
+  def testLogSoftmax(self):
+    f = lambda x: x - np.log(psum(np.exp(x), 'i'))
+    x = onp.log(onp.arange(1., 10., dtype=onp.float32))
+    ans = pmap(f, axis_name='i')(x)
+    expected = x - onp.log(onp.sum(onp.exp(x)))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
 
-# class PapplyTest(jtu.JaxTestCase):
+class PapplyTest(jtu.JaxTestCase):
 
-#   def testIdentity(self):
-#     pfun, axis_name = papply(lambda x: x)
-#     ans = pfun(onp.arange(3))
-#     expected = onp.arange(3)
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  def testIdentity(self):
+    pfun, axis_name = papply(lambda x: x)
+    ans = pfun(onp.arange(3))
+    expected = onp.arange(3)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testMap(self):
-#     pfun, axis_name = papply(np.sin)
-#     ans = pfun(onp.arange(3.))
-#     expected = onp.sin(onp.arange(3.))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  def testMap(self):
+    pfun, axis_name = papply(np.sin)
+    ans = pfun(onp.arange(3.))
+    expected = onp.sin(onp.arange(3.))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testSum(self):
-#     pfun, axis_name = papply(np.sum)
+  def testSum(self):
+    pfun, axis_name = papply(np.sum)
 
-#     jaxpr = make_jaxpr(pfun)(onp.zeros(5))
-#     expected_jaxpr = make_jaxpr(lambda x: psum(x, axis_name))(onp.zeros(5))
-#     assert repr(jaxpr) == repr(expected_jaxpr)
+    jaxpr = make_jaxpr(pfun)(onp.zeros(5))
+    expected_jaxpr = make_jaxpr(lambda x: psum(x, axis_name))(onp.zeros(5))
+    assert repr(jaxpr) == repr(expected_jaxpr)
 
-#     ans = pmap(pfun, axis_name)(onp.arange(3.))
-#     expected = onp.sum(onp.arange(3.))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+    ans = pmap(pfun, axis_name)(onp.arange(3.))
+    expected = onp.sum(onp.arange(3.))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testLogSoftmax(self):
+  def testLogSoftmax(self):
 
-#     def fun(x):
-#       return x - np.log(np.sum(np.exp(x)))
+    def fun(x):
+      return x - np.log(np.sum(np.exp(x)))
 
-#     pfun, axis_name = papply(fun)
+    pfun, axis_name = papply(fun)
 
-#     jaxpr = make_jaxpr(pfun)(onp.zeros(5))
-#     expected_jaxpr = make_jaxpr(lambda x: x - np.log(psum(np.exp(x), axis_name))
-#                                 )(onp.zeros(5))
-#     assert repr(jaxpr) == repr(expected_jaxpr)
+    jaxpr = make_jaxpr(pfun)(onp.zeros(5))
+    expected_jaxpr = make_jaxpr(lambda x: x - np.log(psum(np.exp(x), axis_name))
+                                )(onp.zeros(5))
+    assert repr(jaxpr) == repr(expected_jaxpr)
 
-#     ans = pmap(pfun, axis_name)(onp.arange(1., 5.))
-#     expected = fun(onp.arange(1., 5.))
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+    ans = pmap(pfun, axis_name)(onp.arange(1., 5.))
+    expected = fun(onp.arange(1., 5.))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#   def testAdd(self):
-#     x = onp.array([[1, 2, 3], [4, 5, 6]])
-#     expected = x + x
+  def testAdd(self):
+    x = onp.array([[1, 2, 3], [4, 5, 6]])
+    expected = x + x
 
-#     pfun, axis_name = papply(np.add)
-#     ans = pmap(pfun, axis_name)(x, x)
-#     self.assertAllClose(ans, expected, check_dtypes=True)
+    pfun, axis_name = papply(np.add)
+    ans = pmap(pfun, axis_name)(x, x)
+    self.assertAllClose(ans, expected, check_dtypes=True)
 
-# #   def testAddDifferentSharding(self):
-# #     x = onp.array([[1, 2, 3], [4, 5, 6]])
-# #     expected = x + x
+  def testAddBroadcasting(self):
 
-# #     pfun, axis_name = papply(np.add, (0, 1))
-# #     ans = pmap(pfun, axis_name)(x, x)
-# #     self.assertAllClose(ans, expected, check_dtypes=True)
+    def fun(x):
+      return x + 3
 
-# #   def testScatterLike(self):
-# #     def fun(y):
-# #       x_scattered = scatter_like(x, y)
-# #       return lax.add(x_scattered, y)  # TODO replace with x + y
+    x = onp.array([[1, 2], [3, 4]])
+    expected = x + 3
 
-# #     x = onp.array([[1, 2], [3, 4]])
-# #     expected = x + x
-
-# #     pfun, axis_name = papply(fun)
-# #     ans = pmap(pfun, axis_name)(x)
-# #     self.assertAllClose(ans, expected, check_dtypes=True)
-
-#   def testAddBroadcasting(self):
-
-#     def fun(x):
-#       return x + 3
-
-#     x = onp.array([[1, 2], [3, 4]])
-#     expected = x + 3
-
-#     pfun, axis_name = papply(fun)
-#     ans = pmap(pfun, axis_name)(x)
-#     self.assertAllClose(ans, expected, check_dtypes=True)
+    pfun, axis_name = papply(fun)
+    ans = pmap(pfun, axis_name)(x)
+    self.assertAllClose(ans, expected, check_dtypes=True)
 
 
 class ChunkTest(jtu.JaxTestCase):",Yes
jax/interpreters/parallel.py,jax/interpreters/parallel.py,b34c87dc221163d224ced455b61abc8c7fd3a3c8,781577226dbddd28b33c8c80e16424ca8a863e7d,remove 'chunk' for now,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index e56a66da5..99462b7a1 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -220,133 +220,6 @@ scatter_p = PmapPrimitive('scatter')
 pmap_primitive_rules[scatter_p] = scatter_pmap_rule
 
 
-### chunk
-# TODO expreses chunk in terms of a index-split composed with pmap
-
-# chunk :: spmd_traceable{G, i}[N] -> i -> axis -> chunksize
-#                                  -> spmd_traceable{G, i}[N // chunksize]
-
-def chunk(fun, chunksize, name, in_vals, in_axes, out_axis_target):
-  sizes = reduce(set.union, map(batching.dimsize, in_axes, in_vals))
-  if not sizes:
-    return fun.call_wrapped(*in_vals)
-  elif len(sizes) == 1:
-    out_val, out_axis = chunk_transform(fun).call_wrapped(
-        name, in_vals, in_axes, chunksize)
-    return batching.moveaxis(sizes.pop(), out_axis_target, out_axis, out_val)
-  else:
-    raise TypeError(""got inconsistent map dimension sizes: {}"".format(sizes))
-
-
-@lu.transformation
-def chunk_transform(name, vals, axes, chunksize):
-  with new_master(ChunkTrace) as master:
-    trace = ChunkTrace(master, core.cur_sublevel())
-    in_tracers = map(partial(ChunkTracer, trace, name, chunksize), vals, axes)
-    ans = yield in_tracers
-    out_tracer = trace.full_raise(ans)
-    out_val, out_axis = out_tracer.val, out_tracer.axis
-    del master
-  yield out_val, out_axis
-
-
-class ChunkTracer(Tracer):
-  def __init__(self, trace, name, chunksize, val, axis):
-    self.trace = trace
-    self.name = name
-    self.chunksize = chunksize
-    self.val = val
-    self.axis = axis
-
-  @property
-  def aval(self):
-    batched_aval = batching.get_aval(self.val)
-    return batching.remove_batch_dim_from_aval(self.axis, batched_aval)
-
-  def unpack(self):
-    t = type(self.axis)
-    if t is tuple:
-      axes = self.axis
-    elif t is int:
-      axes = [self.axis] * len(self.val)
-    elif t is type(None):
-      return tuple(self.val)
-    else:
-      raise TypeError(t)
-    new_tracer = partial(ChunkTracer, self.trace, self.name, self.chunksize)
-    return map(new_tracer, self.val, axes)
-
-  def full_lower(self):
-    if self.axis is None:
-      return core.full_lower(self.val)
-    else:
-      return self
-
-class ChunkTrace(Trace):
-  def pure(self, val):
-    return ChunkTracer(self, None, None, val, None)
-
-  def lift(self, val):
-    return ChunkTracer(self, None, None, val, None)
-
-  def sublift(self, val):
-    return ChunkTracer(self, val.name, val.chunksize, val.val, val.axis)
-
-  def process_primitive(self, primitive, tracers, params):
-    names_in, vals_in, axes_in = unzip3((t.name, t.val, t.axis) for t in tracers)
-    if all(axis is None for axis in axes_in):
-      return primitive.bind(*vals_in, **params)
-    else:
-      name = next(name for name in names_in if name is not None)  # all same
-      if primitive in chunk_primitive_rules:
-        if name == params['axis_name']:
-          chunksize = next(t.chunksize for t in tracers if t.chunksize)
-          rule = chunk_primitive_rules[primitive]
-          params = {k: params[k] for k in params if k != 'axis_name'}
-          val_out, axis_out = rule(chunksize, name, vals_in, axes_in, **params)
-          return ChunkTracer(self, name, chunksize, val_out, axis_out)
-        else:
-          return primitive.bind(val, **params)
-      else:
-        rule = batching.get_primitive_batcher(primitive)
-        val_out, axis_out = rule(vals_in, axes_in, **params)
-        return ChunkTracer(self, name, chunksize, val_out, axis_out)
-
-  def process_call(self, call_primitive, f, tracers, params):
-    names, vals, axes = unzip3((t.name, t.val, t.axis) for t in tracers)
-    if all(axis is None for axis in axes):
-      return call_primitive.bind(f, *vals, **params)
-    else:
-      name = next(name for name in names if name is not None)  # all same
-      chunksize = next(t.chunksize for t in tracers if t.chunksize)
-      f, axis_out = chunk_subtrace(f, self.master, name, chunksize, axes)
-      val_out = call_primitive.bind(f, *vals, **params)
-      return ChunkTracer(self, name, chunksize, val_out, axis_out())
-
-  def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
-
-  def pack(self, tracers):
-    vals = pack([t.val for t in tracers])
-    axis = tuple(t.axis for t in tracers)
-    name = next(t.name for t in tracers if t.name)
-    chunksize = next(t.chunksize for t in tracers if t.chunksize)
-    return ChunkTracer(self, name, chunksize, vals, axis)
-
-
-chunk_primitive_rules = {}
-
-
-def psum_chunk_rule(chunksize, name, vals, axes):
-  val, = vals
-  axis, = axes
-  shape = list(val.shape)
-  shape[axis] = val.shape[axis] // chunksize
-  shape.insert(axis, chunksize)
-  return psum(val.sum(axis), name), None
-chunk_primitive_rules[psum_p] = psum_chunk_rule
-
-
 ### papply
 
 ","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index e56a66da5..99462b7a1 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -220,133 +220,6 @@ scatter_p = PmapPrimitive('scatter')
 pmap_primitive_rules[scatter_p] = scatter_pmap_rule
 
 
-### chunk
-# TODO expreses chunk in terms of a index-split composed with pmap
-
-# chunk :: spmd_traceable{G, i}[N] -> i -> axis -> chunksize
-#                                  -> spmd_traceable{G, i}[N // chunksize]
-
-def chunk(fun, chunksize, name, in_vals, in_axes, out_axis_target):
-  sizes = reduce(set.union, map(batching.dimsize, in_axes, in_vals))
-  if not sizes:
-    return fun.call_wrapped(*in_vals)
-  elif len(sizes) == 1:
-    out_val, out_axis = chunk_transform(fun).call_wrapped(
-        name, in_vals, in_axes, chunksize)
-    return batching.moveaxis(sizes.pop(), out_axis_target, out_axis, out_val)
-  else:
-    raise TypeError(""got inconsistent map dimension sizes: {}"".format(sizes))
-
-
-@lu.transformation
-def chunk_transform(name, vals, axes, chunksize):
-  with new_master(ChunkTrace) as master:
-    trace = ChunkTrace(master, core.cur_sublevel())
-    in_tracers = map(partial(ChunkTracer, trace, name, chunksize), vals, axes)
-    ans = yield in_tracers
-    out_tracer = trace.full_raise(ans)
-    out_val, out_axis = out_tracer.val, out_tracer.axis
-    del master
-  yield out_val, out_axis
-
-
-class ChunkTracer(Tracer):
-  def __init__(self, trace, name, chunksize, val, axis):
-    self.trace = trace
-    self.name = name
-    self.chunksize = chunksize
-    self.val = val
-    self.axis = axis
-
-  @property
-  def aval(self):
-    batched_aval = batching.get_aval(self.val)
-    return batching.remove_batch_dim_from_aval(self.axis, batched_aval)
-
-  def unpack(self):
-    t = type(self.axis)
-    if t is tuple:
-      axes = self.axis
-    elif t is int:
-      axes = [self.axis] * len(self.val)
-    elif t is type(None):
-      return tuple(self.val)
-    else:
-      raise TypeError(t)
-    new_tracer = partial(ChunkTracer, self.trace, self.name, self.chunksize)
-    return map(new_tracer, self.val, axes)
-
-  def full_lower(self):
-    if self.axis is None:
-      return core.full_lower(self.val)
-    else:
-      return self
-
-class ChunkTrace(Trace):
-  def pure(self, val):
-    return ChunkTracer(self, None, None, val, None)
-
-  def lift(self, val):
-    return ChunkTracer(self, None, None, val, None)
-
-  def sublift(self, val):
-    return ChunkTracer(self, val.name, val.chunksize, val.val, val.axis)
-
-  def process_primitive(self, primitive, tracers, params):
-    names_in, vals_in, axes_in = unzip3((t.name, t.val, t.axis) for t in tracers)
-    if all(axis is None for axis in axes_in):
-      return primitive.bind(*vals_in, **params)
-    else:
-      name = next(name for name in names_in if name is not None)  # all same
-      if primitive in chunk_primitive_rules:
-        if name == params['axis_name']:
-          chunksize = next(t.chunksize for t in tracers if t.chunksize)
-          rule = chunk_primitive_rules[primitive]
-          params = {k: params[k] for k in params if k != 'axis_name'}
-          val_out, axis_out = rule(chunksize, name, vals_in, axes_in, **params)
-          return ChunkTracer(self, name, chunksize, val_out, axis_out)
-        else:
-          return primitive.bind(val, **params)
-      else:
-        rule = batching.get_primitive_batcher(primitive)
-        val_out, axis_out = rule(vals_in, axes_in, **params)
-        return ChunkTracer(self, name, chunksize, val_out, axis_out)
-
-  def process_call(self, call_primitive, f, tracers, params):
-    names, vals, axes = unzip3((t.name, t.val, t.axis) for t in tracers)
-    if all(axis is None for axis in axes):
-      return call_primitive.bind(f, *vals, **params)
-    else:
-      name = next(name for name in names if name is not None)  # all same
-      chunksize = next(t.chunksize for t in tracers if t.chunksize)
-      f, axis_out = chunk_subtrace(f, self.master, name, chunksize, axes)
-      val_out = call_primitive.bind(f, *vals, **params)
-      return ChunkTracer(self, name, chunksize, val_out, axis_out())
-
-  def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
-
-  def pack(self, tracers):
-    vals = pack([t.val for t in tracers])
-    axis = tuple(t.axis for t in tracers)
-    name = next(t.name for t in tracers if t.name)
-    chunksize = next(t.chunksize for t in tracers if t.chunksize)
-    return ChunkTracer(self, name, chunksize, vals, axis)
-
-
-chunk_primitive_rules = {}
-
-
-def psum_chunk_rule(chunksize, name, vals, axes):
-  val, = vals
-  axis, = axes
-  shape = list(val.shape)
-  shape[axis] = val.shape[axis] // chunksize
-  shape.insert(axis, chunksize)
-  return psum(val.sum(axis), name), None
-chunk_primitive_rules[psum_p] = psum_chunk_rule
-
-
 ### papply
 
 ",No
tests/parallel_test.py,tests/parallel_test.py,b34c87dc221163d224ced455b61abc8c7fd3a3c8,781577226dbddd28b33c8c80e16424ca8a863e7d,remove 'chunk' for now,"diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 9869281e1..78f2cbf66 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -25,7 +25,7 @@ from jax import test_util as jtu
 from jax import lax
 from jax.api import pmap, papply, jit, make_jaxpr
 from jax.linear_util import wrap_init
-from jax.interpreters.parallel import psum, scatter_like, chunk
+from jax.interpreters.parallel import psum, scatter_like
 
 from jax.config import config
 config.parse_flags_with_absl()
@@ -115,16 +115,16 @@ class PapplyTest(jtu.JaxTestCase):
     self.assertAllClose(ans, expected, check_dtypes=True)
 
 
-class ChunkTest(jtu.JaxTestCase):
+# class ChunkTest(jtu.JaxTestCase):
 
-  def testChunkingSum(self):
-    f = lambda x: psum(x, 'i')
+#   def testChunkingSum(self):
+#     f = lambda x: psum(x, 'i')
 
-    x = 3 * onp.ones((4, 2))
-    ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
-    expected = 24
+#     x = 3 * onp.ones((4, 2))
+#     ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
+#     expected = 24
 
-    self.assertAllClose(ans, expected, check_dtypes=False)
+#     self.assertAllClose(ans, expected, check_dtypes=False)
 
 
 if __name__ == '__main__':","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 9869281e1..78f2cbf66 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -25,7 +25,7 @@ from jax import test_util as jtu
 from jax import lax
 from jax.api import pmap, papply, jit, make_jaxpr
 from jax.linear_util import wrap_init
-from jax.interpreters.parallel import psum, scatter_like, chunk
+from jax.interpreters.parallel import psum, scatter_like
 
 from jax.config import config
 config.parse_flags_with_absl()
@@ -115,16 +115,16 @@ class PapplyTest(jtu.JaxTestCase):
     self.assertAllClose(ans, expected, check_dtypes=True)
 
 
-class ChunkTest(jtu.JaxTestCase):
+# class ChunkTest(jtu.JaxTestCase):
 
-  def testChunkingSum(self):
-    f = lambda x: psum(x, 'i')
+#   def testChunkingSum(self):
+#     f = lambda x: psum(x, 'i')
 
-    x = 3 * onp.ones((4, 2))
-    ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
-    expected = 24
+#     x = 3 * onp.ones((4, 2))
+#     ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
+#     expected = 24
 
-    self.assertAllClose(ans, expected, check_dtypes=False)
+#     self.assertAllClose(ans, expected, check_dtypes=False)
 
 
 if __name__ == '__main__':",No
jax/interpreters/ad.py,jax/interpreters/ad.py,7e05f94c48c69863f6988e038afb87576764b12d,b34c87dc221163d224ced455b61abc8c7fd3a3c8,"fix pmap nesting bug, sketch out index splitting","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 683413e87..55e193309 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -164,7 +164,7 @@ class JVPTrace(Trace):
   def lift(self, val):
     return JVPTracer(self, val, zero)
 
-  def sublift(self,val):
+  def sublift(self, val):
     return JVPTracer(self, val.primal, val.tangent)
 
   def process_primitive(self, primitive, tracers, params):","diff --git a/jax/interpreters/ad.py b/jax/interpreters/ad.py
index 683413e87..55e193309 100644
--- a/jax/interpreters/ad.py
+++ b/jax/interpreters/ad.py
@@ -164,7 +164,7 @@ class JVPTrace(Trace):
   def lift(self, val):
     return JVPTracer(self, val, zero)
 
-  def sublift(self,val):
+  def sublift(self, val):
     return JVPTracer(self, val.primal, val.tangent)
 
   def process_primitive(self, primitive, tracers, params):",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,7e05f94c48c69863f6988e038afb87576764b12d,b34c87dc221163d224ced455b61abc8c7fd3a3c8,"fix pmap nesting bug, sketch out index splitting","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 99462b7a1..1d91dfc66 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -116,14 +116,21 @@ class PmapTrace(Trace):
     else:
       name = next(name for name in names_in if name is not None)  # all same
       if primitive in pmap_primitive_rules:
+        # if it's a pmap collective primitive, do something special
+        val_in, = vals_in
+        axis_in, = axes_in
         if name == params['axis_name']:
+          # if the name matches this tracer's name, apply the pmap rule
           rule = pmap_primitive_rules[primitive]
           params = {k: params[k] for k in params if k != 'axis_name'}
-          val_out, axis_out = rule(vals_in, axes_in, **params)
+          val_out, axis_out = rule(val_in, axis_in, **params)
           return PmapTracer(self, name, val_out, axis_out)
         else:
-          return primitive.bind(val, **params)
+          # if not, bind the primitive so that any other pmap tracers can see it
+          val_out = primitive.bind(val_in, **params)
+          return PmapTracer(self, name, val_out, axis_in)
       else:
+        # if it's not a pmap collective primitive, act just like vmap
         rule = batching.get_primitive_batcher(primitive)
         val_out, axis_out = rule(vals_in, axes_in, **params)
         return PmapTracer(self, name, val_out, axis_out)
@@ -166,13 +173,10 @@ pmap_primitive_rules = {}
 def psum(x, axis_name):
   return psum_p.bind(x, axis_name=axis_name)
 
-def psum_pmap_rule(vals, axes):
-  val, = vals
-  axis, = axes
+def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
-def psum_parallel_translation_rule(c, in_nodes, device_grp):
-  val, = in_nodes
+def psum_parallel_translation_rule(c, val, device_grp):
   # return c.CrossReplicaSum(val, device_grp)  # TODO
   return c.CrossReplicaSum(val)
 
@@ -184,40 +188,99 @@ pxla.parallel_translation_rules[psum_p] = psum_parallel_translation_rule
 def gather(x, axis_name):
   return gather_p.bind(x, axis_name=axis_name)
 
-def gather_pmap_rule(vals, axes):
-  val, = vals
+def gather_pmap_rule(val, axis):
   return val, None
 
 gather_p = PmapPrimitive('gather')
 pmap_primitive_rules[gather_p] = gather_pmap_rule
 
 
-def rescatter(x, new_axis, axis_name):
-  return rescatter_p.bind(x, new_axis=new_axis, axis_name=axis_name)
+### axis variable splitting and computation chunking
 
-def rescatter_pmap_rule(vals, axes, new_axis):
-  val, = vals
-  axis, = axes
-  raise NotImplementedError  # TODO why the transpose, instead of identity?
-  return batching.moveaxis(None, new_axis, axis, val), new_axis
 
-rescatter_p = PmapPrimitive('rescatter')
-pmap_primitive_rules[rescatter_p] = rescatter_pmap_rule
+@lu.transformation
+def axisvar_split_transform(name, new_names, *args):
+  with new_master(SplitTrace) as master:
+    trace = Trace(master, core.cur_sublevel())
+    in_tracers = map(partial(SplitTracer, name, new_names), args)
+    ans = yield in_tracers
+    out_tracer = trace.full_raise(ans)
+    out_val = out_tracer.val
+    del master
+  yield out_val
+
+class SplitTracer(Tracer):
+  def __init__(self, trace, name, new_names, val):
+    self.trace = trace
+    self.name = name
+    self.new_names = new_names
+    self.val = val
+
+  @property
+  def aval(self):
+    return core.get_aval(self.val)
 
+  def unpack(self):
+    raise NotImplementedError  # TODO(mattjj)
 
-# TODO maybe this isn't a great primitive, and it's the only one that needs more
-# than one operand at the moment
-def _scatter(source, dummy, target_axis, axis_name):
-  return scatter_p.bind(source, dummy, target_axis=target_axis, axis_name=axis_name)
+  def full_lower(self):
+    if self.name is None:
+      return core.full_lower(self.val)
+    else:
+      return self
 
-def scatter_pmap_rule(vals, axes, target_axis):
-  source, _ = vals
-  source_axis, _ = axes
-  assert source_axis is None
-  return source, target_axis
+class SplitTrace(Trace):
+  def pure(self, val):
+    return SplitTracer(self, None, (), val)
 
-scatter_p = PmapPrimitive('scatter')
-pmap_primitive_rules[scatter_p] = scatter_pmap_rule
+  def lift(self, val):
+    return SplitTracer(self, None, (), val)
+
+  def sublift(self, val):
+    return SplitTracer(self, val.name, val.new_names, val.val)
+
+  def process_primitive(self, primitive, tracers, params):
+    names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
+    if all(name is None for name in names_in):
+      return primitive.bind(*vals_in, **params)
+    else:
+      name = next(name for name in names if name is not None)
+      new_names = next(t.new_names for t in tracers if t.name is not None)
+      if primitive in pmap_primitive_rules:
+        val_in, = vals_in
+        if name == params['axis_name']:
+          new_params = {k: params[k] for k in params if k != 'axis_name'}
+          val = val_in
+          for new_name in new_names:
+            val = primitive.bind(val, axis_name=new_name, **new_params)
+          val_out = val
+          return SplitTracer(self, name, new_names, val_out)
+        else:
+          val_out = primitive.bind(val_in, **params)
+          return SplitTracer(self, name, new_names, val_out)
+      else:
+        val_out = primitive.bind(*vals_in, **params)
+        return SplitTracer(self, name, new_names, val_out)
+
+  def process_call(self, call_primitive, f, tracers, params):
+    names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
+    if all(name is None for name in names_in):
+      return call_primitive.bind(f, *vals, **params)
+    else:
+      name = next(name for name in names if name is not None)
+      new_names = next(t.new_names for t in tracers if t.name is not None)
+      f = axisvar_split_subtrace(f, self.master, name, new_names)
+      val_out = call_primitive.bind(f, *vals, **params)
+      return SplitTracer(self, name, new_names, val_out)
+
+  def post_process_call(self, _, out_tracer):
+    raise NotImplementedError  # TODO(mattjj,dougalm)
+
+  def pack(self, tracers):
+    vals = pack([t.val for t in tracers])
+    name = next(t.name for t in tracers if t.name is not None)
+    new_names = next(t.new_names for t in tracers if t.name is not None)
+    return SplitTracer(self, name, new_names, vals)
 
 
 ### papply
@@ -351,7 +414,6 @@ def broadcasting_papply(prim, name, vals, axes, **params):
   elif xdim == ydim:
     return prim.bind(x, y, **params), xdim
   else:
-    # TODO rescatter based on sizes
     raise NotImplementedError  # this isn't right, need to think about names
     x = rescatter(x, ydim, name)
     return prim.bind(x, y, **params), ydim","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 99462b7a1..1d91dfc66 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -116,14 +116,21 @@ class PmapTrace(Trace):
     else:
       name = next(name for name in names_in if name is not None)  # all same
       if primitive in pmap_primitive_rules:
+        # if it's a pmap collective primitive, do something special
+        val_in, = vals_in
+        axis_in, = axes_in
         if name == params['axis_name']:
+          # if the name matches this tracer's name, apply the pmap rule
           rule = pmap_primitive_rules[primitive]
           params = {k: params[k] for k in params if k != 'axis_name'}
-          val_out, axis_out = rule(vals_in, axes_in, **params)
+          val_out, axis_out = rule(val_in, axis_in, **params)
           return PmapTracer(self, name, val_out, axis_out)
         else:
-          return primitive.bind(val, **params)
+          # if not, bind the primitive so that any other pmap tracers can see it
+          val_out = primitive.bind(val_in, **params)
+          return PmapTracer(self, name, val_out, axis_in)
       else:
+        # if it's not a pmap collective primitive, act just like vmap
         rule = batching.get_primitive_batcher(primitive)
         val_out, axis_out = rule(vals_in, axes_in, **params)
         return PmapTracer(self, name, val_out, axis_out)
@@ -166,13 +173,10 @@ pmap_primitive_rules = {}
 def psum(x, axis_name):
   return psum_p.bind(x, axis_name=axis_name)
 
-def psum_pmap_rule(vals, axes):
-  val, = vals
-  axis, = axes
+def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
-def psum_parallel_translation_rule(c, in_nodes, device_grp):
-  val, = in_nodes
+def psum_parallel_translation_rule(c, val, device_grp):
   # return c.CrossReplicaSum(val, device_grp)  # TODO
   return c.CrossReplicaSum(val)
 
@@ -184,40 +188,99 @@ pxla.parallel_translation_rules[psum_p] = psum_parallel_translation_rule
 def gather(x, axis_name):
   return gather_p.bind(x, axis_name=axis_name)
 
-def gather_pmap_rule(vals, axes):
-  val, = vals
+def gather_pmap_rule(val, axis):
   return val, None
 
 gather_p = PmapPrimitive('gather')
 pmap_primitive_rules[gather_p] = gather_pmap_rule
 
 
-def rescatter(x, new_axis, axis_name):
-  return rescatter_p.bind(x, new_axis=new_axis, axis_name=axis_name)
-
-def rescatter_pmap_rule(vals, axes, new_axis):
-  val, = vals
-  axis, = axes
-  raise NotImplementedError  # TODO why the transpose, instead of identity?
-  return batching.moveaxis(None, new_axis, axis, val), new_axis
-
-rescatter_p = PmapPrimitive('rescatter')
-pmap_primitive_rules[rescatter_p] = rescatter_pmap_rule
+### axis variable splitting and computation chunking
 
 
-# TODO maybe this isn't a great primitive, and it's the only one that needs more
-# than one operand at the moment
-def _scatter(source, dummy, target_axis, axis_name):
-  return scatter_p.bind(source, dummy, target_axis=target_axis, axis_name=axis_name)
+@lu.transformation
+def axisvar_split_transform(name, new_names, *args):
+  with new_master(SplitTrace) as master:
+    trace = Trace(master, core.cur_sublevel())
+    in_tracers = map(partial(SplitTracer, name, new_names), args)
+    ans = yield in_tracers
+    out_tracer = trace.full_raise(ans)
+    out_val = out_tracer.val
+    del master
+  yield out_val
 
-def scatter_pmap_rule(vals, axes, target_axis):
-  source, _ = vals
-  source_axis, _ = axes
-  assert source_axis is None
-  return source, target_axis
+class SplitTracer(Tracer):
+  def __init__(self, trace, name, new_names, val):
+    self.trace = trace
+    self.name = name
+    self.new_names = new_names
+    self.val = val
 
-scatter_p = PmapPrimitive('scatter')
-pmap_primitive_rules[scatter_p] = scatter_pmap_rule
+  @property
+  def aval(self):
+    return core.get_aval(self.val)
+
+  def unpack(self):
+    raise NotImplementedError  # TODO(mattjj)
+
+  def full_lower(self):
+    if self.name is None:
+      return core.full_lower(self.val)
+    else:
+      return self
+
+class SplitTrace(Trace):
+  def pure(self, val):
+    return SplitTracer(self, None, (), val)
+
+  def lift(self, val):
+    return SplitTracer(self, None, (), val)
+
+  def sublift(self, val):
+    return SplitTracer(self, val.name, val.new_names, val.val)
+
+  def process_primitive(self, primitive, tracers, params):
+    names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
+    if all(name is None for name in names_in):
+      return primitive.bind(*vals_in, **params)
+    else:
+      name = next(name for name in names if name is not None)
+      new_names = next(t.new_names for t in tracers if t.name is not None)
+      if primitive in pmap_primitive_rules:
+        val_in, = vals_in
+        if name == params['axis_name']:
+          new_params = {k: params[k] for k in params if k != 'axis_name'}
+          val = val_in
+          for new_name in new_names:
+            val = primitive.bind(val, axis_name=new_name, **new_params)
+          val_out = val
+          return SplitTracer(self, name, new_names, val_out)
+        else:
+          val_out = primitive.bind(val_in, **params)
+          return SplitTracer(self, name, new_names, val_out)
+      else:
+        val_out = primitive.bind(*vals_in, **params)
+        return SplitTracer(self, name, new_names, val_out)
+
+  def process_call(self, call_primitive, f, tracers, params):
+    names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
+    if all(name is None for name in names_in):
+      return call_primitive.bind(f, *vals, **params)
+    else:
+      name = next(name for name in names if name is not None)
+      new_names = next(t.new_names for t in tracers if t.name is not None)
+      f = axisvar_split_subtrace(f, self.master, name, new_names)
+      val_out = call_primitive.bind(f, *vals, **params)
+      return SplitTracer(self, name, new_names, val_out)
+
+  def post_process_call(self, _, out_tracer):
+    raise NotImplementedError  # TODO(mattjj,dougalm)
+
+  def pack(self, tracers):
+    vals = pack([t.val for t in tracers])
+    name = next(t.name for t in tracers if t.name is not None)
+    new_names = next(t.new_names for t in tracers if t.name is not None)
+    return SplitTracer(self, name, new_names, vals)
 
 
 ### papply
@@ -351,7 +414,6 @@ def broadcasting_papply(prim, name, vals, axes, **params):
   elif xdim == ydim:
     return prim.bind(x, y, **params), xdim
   else:
-    # TODO rescatter based on sizes
     raise NotImplementedError  # this isn't right, need to think about names
     x = rescatter(x, ydim, name)
     return prim.bind(x, y, **params), ydim",Yes
tests/parallel_test.py,tests/parallel_test.py,7e05f94c48c69863f6988e038afb87576764b12d,b34c87dc221163d224ced455b61abc8c7fd3a3c8,"fix pmap nesting bug, sketch out index splitting","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 78f2cbf66..92d7c1fb3 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -52,6 +52,15 @@ class PmapTest(jtu.JaxTestCase):
     expected = x - onp.log(onp.sum(onp.exp(x)))
     self.assertAllClose(ans, expected, check_dtypes=False)
 
+  def testNested(self):
+    f = lambda x: psum(psum(x, 'i'), 'j')
+    x = onp.ones((2, 2))
+    ans1 = pmap(pmap(f, 'i'), 'j')(x)
+    ans2 = pmap(pmap(f, 'j'), 'i')(x)
+    expected = 4 * onp.ones((2, 2))
+    self.assertAllClose(ans1, expected, check_dtypes=False)
+    self.assertAllClose(ans2, expected, check_dtypes=False)
+
 
 class PapplyTest(jtu.JaxTestCase):
 ","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 78f2cbf66..92d7c1fb3 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -52,6 +52,15 @@ class PmapTest(jtu.JaxTestCase):
     expected = x - onp.log(onp.sum(onp.exp(x)))
     self.assertAllClose(ans, expected, check_dtypes=False)
 
+  def testNested(self):
+    f = lambda x: psum(psum(x, 'i'), 'j')
+    x = onp.ones((2, 2))
+    ans1 = pmap(pmap(f, 'i'), 'j')(x)
+    ans2 = pmap(pmap(f, 'j'), 'i')(x)
+    expected = 4 * onp.ones((2, 2))
+    self.assertAllClose(ans1, expected, check_dtypes=False)
+    self.assertAllClose(ans2, expected, check_dtypes=False)
+
 
 class PapplyTest(jtu.JaxTestCase):
 ",No
jax/api.py,jax/api.py,ae86b9a640acbed02fc055cc440e908d29ac3386,7e05f94c48c69863f6988e038afb87576764b12d,fix split tracer bugs,"diff --git a/jax/api.py b/jax/api.py
index 3ed2a42d2..fc0302e8e 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -270,6 +270,18 @@ def pmap(fun, axis_name, in_axes=0, out_axes=0):
   return pmap_fun
 
 
+def axisvar_split(fun, name, new_names):
+  """"""Split axis variable names into new names.""""""
+  def split_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = parallel.axisvar_split(jaxtree_fun, name, new_names).call_wrapped(*args)
+    return build_tree(out_tree(), out_flat)
+
+  return split_fun
+
+
 def papply(fun, in_axes=0):
   """"""Apply a function using parallel computation by sharding inputs.""""""
   axis_name = parallel.newvar()","diff --git a/jax/api.py b/jax/api.py
index 3ed2a42d2..fc0302e8e 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -270,6 +270,18 @@ def pmap(fun, axis_name, in_axes=0, out_axes=0):
   return pmap_fun
 
 
+def axisvar_split(fun, name, new_names):
+  """"""Split axis variable names into new names.""""""
+  def split_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = parallel.axisvar_split(jaxtree_fun, name, new_names).call_wrapped(*args)
+    return build_tree(out_tree(), out_flat)
+
+  return split_fun
+
+
 def papply(fun, in_axes=0):
   """"""Apply a function using parallel computation by sharding inputs.""""""
   axis_name = parallel.newvar()",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,ae86b9a640acbed02fc055cc440e908d29ac3386,7e05f94c48c69863f6988e038afb87576764b12d,fix split tracer bugs,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 1d91dfc66..aa6e55c03 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -199,10 +199,10 @@ pmap_primitive_rules[gather_p] = gather_pmap_rule
 
 
 @lu.transformation
-def axisvar_split_transform(name, new_names, *args):
+def axisvar_split(name, new_names, *args):
   with new_master(SplitTrace) as master:
-    trace = Trace(master, core.cur_sublevel())
-    in_tracers = map(partial(SplitTracer, name, new_names), args)
+    trace = SplitTrace(master, core.cur_sublevel())
+    in_tracers = map(partial(SplitTracer, trace, name, new_names), args)
     ans = yield in_tracers
     out_tracer = trace.full_raise(ans)
     out_val = out_tracer.val
@@ -244,7 +244,7 @@ class SplitTrace(Trace):
     if all(name is None for name in names_in):
       return primitive.bind(*vals_in, **params)
     else:
-      name = next(name for name in names if name is not None)
+      name = next(name for name in names_in if name is not None)
       new_names = next(t.new_names for t in tracers if t.name is not None)
       if primitive in pmap_primitive_rules:
         val_in, = vals_in
@@ -267,7 +267,7 @@ class SplitTrace(Trace):
     if all(name is None for name in names_in):
       return call_primitive.bind(f, *vals, **params)
     else:
-      name = next(name for name in names if name is not None)
+      name = next(name for name in names_in if name is not None)
       new_names = next(t.new_names for t in tracers if t.name is not None)
       f = axisvar_split_subtrace(f, self.master, name, new_names)
       val_out = call_primitive.bind(f, *vals, **params)","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 1d91dfc66..aa6e55c03 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -199,10 +199,10 @@ pmap_primitive_rules[gather_p] = gather_pmap_rule
 
 
 @lu.transformation
-def axisvar_split_transform(name, new_names, *args):
+def axisvar_split(name, new_names, *args):
   with new_master(SplitTrace) as master:
-    trace = Trace(master, core.cur_sublevel())
-    in_tracers = map(partial(SplitTracer, name, new_names), args)
+    trace = SplitTrace(master, core.cur_sublevel())
+    in_tracers = map(partial(SplitTracer, trace, name, new_names), args)
     ans = yield in_tracers
     out_tracer = trace.full_raise(ans)
     out_val = out_tracer.val
@@ -244,7 +244,7 @@ class SplitTrace(Trace):
     if all(name is None for name in names_in):
       return primitive.bind(*vals_in, **params)
     else:
-      name = next(name for name in names if name is not None)
+      name = next(name for name in names_in if name is not None)
       new_names = next(t.new_names for t in tracers if t.name is not None)
       if primitive in pmap_primitive_rules:
         val_in, = vals_in
@@ -267,7 +267,7 @@ class SplitTrace(Trace):
     if all(name is None for name in names_in):
       return call_primitive.bind(f, *vals, **params)
     else:
-      name = next(name for name in names if name is not None)
+      name = next(name for name in names_in if name is not None)
       new_names = next(t.new_names for t in tracers if t.name is not None)
       f = axisvar_split_subtrace(f, self.master, name, new_names)
       val_out = call_primitive.bind(f, *vals, **params)",No
tests/parallel_test.py,tests/parallel_test.py,c1be2aa2fa5acee18aa8b0c8e317221161303c79,ae86b9a640acbed02fc055cc440e908d29ac3386,add basic split test,"diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 92d7c1fb3..ae4da25d0 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax import lax
-from jax.api import pmap, papply, jit, make_jaxpr
+from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split
 from jax.linear_util import wrap_init
 from jax.interpreters.parallel import psum, scatter_like
 
@@ -124,16 +124,25 @@ class PapplyTest(jtu.JaxTestCase):
     self.assertAllClose(ans, expected, check_dtypes=True)
 
 
-# class ChunkTest(jtu.JaxTestCase):
+class SplitTest(jtu.JaxTestCase):
 
-#   def testChunkingSum(self):
-#     f = lambda x: psum(x, 'i')
+  def testSplitBasic(self):
+    f = lambda x: psum(np.sin(x), 'i')
+    x = onp.ones((2, 2))
+    fsplit = axisvar_split(f, 'i', ('j', 'k'))
+    ans = pmap(pmap(fsplit, 'j'), 'k')(x)
+    expected = onp.sum(onp.sin(x))
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+
+  # def testChunkingSum(self):
+  #   f = lambda x: psum(x, 'i')
 
-#     x = 3 * onp.ones((4, 2))
-#     ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
-#     expected = 24
+  #   x = 3 * onp.ones((4, 2))
+  #   ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
+  #   expected = 24
 
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  #   self.assertAllClose(ans, expected, check_dtypes=False)
 
 
 if __name__ == '__main__':","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 92d7c1fb3..ae4da25d0 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax import lax
-from jax.api import pmap, papply, jit, make_jaxpr
+from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split
 from jax.linear_util import wrap_init
 from jax.interpreters.parallel import psum, scatter_like
 
@@ -124,16 +124,25 @@ class PapplyTest(jtu.JaxTestCase):
     self.assertAllClose(ans, expected, check_dtypes=True)
 
 
-# class ChunkTest(jtu.JaxTestCase):
+class SplitTest(jtu.JaxTestCase):
 
-#   def testChunkingSum(self):
-#     f = lambda x: psum(x, 'i')
+  def testSplitBasic(self):
+    f = lambda x: psum(np.sin(x), 'i')
+    x = onp.ones((2, 2))
+    fsplit = axisvar_split(f, 'i', ('j', 'k'))
+    ans = pmap(pmap(fsplit, 'j'), 'k')(x)
+    expected = onp.sum(onp.sin(x))
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
-#     x = 3 * onp.ones((4, 2))
-#     ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
-#     expected = 24
 
-#     self.assertAllClose(ans, expected, check_dtypes=False)
+  # def testChunkingSum(self):
+  #   f = lambda x: psum(x, 'i')
+
+  #   x = 3 * onp.ones((4, 2))
+  #   ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
+  #   expected = 24
+
+  #   self.assertAllClose(ans, expected, check_dtypes=False)
 
 
 if __name__ == '__main__':",Yes
jax/api.py,jax/api.py,e546f13b03d60ed0c1842dfc6b0baeab2697d7c2,c1be2aa2fa5acee18aa8b0c8e317221161303c79,add chunking transform,"diff --git a/jax/api.py b/jax/api.py
index fc0302e8e..d305bd0b3 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -271,7 +271,7 @@ def pmap(fun, axis_name, in_axes=0, out_axes=0):
 
 
 def axisvar_split(fun, name, new_names):
-  """"""Split axis variable names into new names.""""""
+  """"""Split axis variable names into new names in an SPMD function.""""""
   def split_fun(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
@@ -282,6 +282,24 @@ def axisvar_split(fun, name, new_names):
   return split_fun
 
 
+def chunk(fun, name, chunksize, in_axes=0, out_axes=0):
+  """"""Stage SPMD primitives to first operate on chunks, then use collectives.""""""
+  temp_name = object()
+
+  def chunked_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    f, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    f = parallel.axisvar_split(f, name, (temp_name, name))
+    reshape = partial(parallel.reshape_axis, chunksize)
+    reshaped_args = map(reshape, in_axes_, args)
+    out_flat = parallel.pmap(f, temp_name, reshaped_args, in_axes_, out_axes)
+    return build_tree(out_tree(), out_flat)
+
+  return chunked_fun
+
+
 def papply(fun, in_axes=0):
   """"""Apply a function using parallel computation by sharding inputs.""""""
   axis_name = parallel.newvar()","diff --git a/jax/api.py b/jax/api.py
index fc0302e8e..d305bd0b3 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -271,7 +271,7 @@ def pmap(fun, axis_name, in_axes=0, out_axes=0):
 
 
 def axisvar_split(fun, name, new_names):
-  """"""Split axis variable names into new names.""""""
+  """"""Split axis variable names into new names in an SPMD function.""""""
   def split_fun(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
     in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
@@ -282,6 +282,24 @@ def axisvar_split(fun, name, new_names):
   return split_fun
 
 
+def chunk(fun, name, chunksize, in_axes=0, out_axes=0):
+  """"""Stage SPMD primitives to first operate on chunks, then use collectives.""""""
+  temp_name = object()
+
+  def chunked_fun(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
+    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
+    f, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    f = parallel.axisvar_split(f, name, (temp_name, name))
+    reshape = partial(parallel.reshape_axis, chunksize)
+    reshaped_args = map(reshape, in_axes_, args)
+    out_flat = parallel.pmap(f, temp_name, reshaped_args, in_axes_, out_axes)
+    return build_tree(out_tree(), out_flat)
+
+  return chunked_fun
+
+
 def papply(fun, in_axes=0):
   """"""Apply a function using parallel computation by sharding inputs.""""""
   axis_name = parallel.newvar()",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,e546f13b03d60ed0c1842dfc6b0baeab2697d7c2,c1be2aa2fa5acee18aa8b0c8e317221161303c79,add chunking transform,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index aa6e55c03..24ce58ad2 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -282,6 +282,23 @@ class SplitTrace(Trace):
     new_names = next(t.new_names for t in tracers if t.name is not None)
     return SplitTracer(self, name, new_names, vals)
 
+def reshape_axis(chunksize, in_axis, arg):
+  aval = core.get_aval(arg)
+  if type(aval) is core.AbstractTuple:
+    if type(in_axis) is int:
+      return core.pack(map(partial(reshape_axis, chunksize, in_axis), arg))
+    elif isinstance(in_axis, (list, tuple)):
+      return core.pack(map(partial(reshape_axis, chunksize), in_axis, arg))
+    else:
+      raise TypeError(""unexpected in_axis type: {}"".format(type(in_axis)))
+  elif isinstance(aval, ShapedArray):
+    in_axis = in_axis % arg.ndim
+    split_shape = (arg.shape[in_axis] // chunksize, chunksize)
+    new_shape = arg.shape[:in_axis] + split_shape + arg.shape[in_axis+1:]
+    return arg.reshape(new_shape)
+  else:
+    raise TypeError(type(arg))
+
 
 ### papply
 ","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index aa6e55c03..24ce58ad2 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -282,6 +282,23 @@ class SplitTrace(Trace):
     new_names = next(t.new_names for t in tracers if t.name is not None)
     return SplitTracer(self, name, new_names, vals)
 
+def reshape_axis(chunksize, in_axis, arg):
+  aval = core.get_aval(arg)
+  if type(aval) is core.AbstractTuple:
+    if type(in_axis) is int:
+      return core.pack(map(partial(reshape_axis, chunksize, in_axis), arg))
+    elif isinstance(in_axis, (list, tuple)):
+      return core.pack(map(partial(reshape_axis, chunksize), in_axis, arg))
+    else:
+      raise TypeError(""unexpected in_axis type: {}"".format(type(in_axis)))
+  elif isinstance(aval, ShapedArray):
+    in_axis = in_axis % arg.ndim
+    split_shape = (arg.shape[in_axis] // chunksize, chunksize)
+    new_shape = arg.shape[:in_axis] + split_shape + arg.shape[in_axis+1:]
+    return arg.reshape(new_shape)
+  else:
+    raise TypeError(type(arg))
+
 
 ### papply
 ",No
tests/parallel_test.py,tests/parallel_test.py,e546f13b03d60ed0c1842dfc6b0baeab2697d7c2,c1be2aa2fa5acee18aa8b0c8e317221161303c79,add chunking transform,"diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index ae4da25d0..0293f9042 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax import lax
-from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split
+from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split, chunk
 from jax.linear_util import wrap_init
 from jax.interpreters.parallel import psum, scatter_like
 
@@ -134,15 +134,13 @@ class SplitTest(jtu.JaxTestCase):
     expected = onp.sum(onp.sin(x))
     self.assertAllClose(ans, expected, check_dtypes=False)
 
-
-  # def testChunkingSum(self):
-  #   f = lambda x: psum(x, 'i')
-
-  #   x = 3 * onp.ones((4, 2))
-  #   ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
-  #   expected = 24
-
-  #   self.assertAllClose(ans, expected, check_dtypes=False)
+  def testChunkingSum(self):
+    f = lambda x: x - psum(x, 'i')
+    x = onp.ones((4, 2))
+    fchunked = chunk(f, 'i', 2)
+    ans = pmap(fchunked, 'i')(x)
+    expected = pmap(f, 'i')(x)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
 
 if __name__ == '__main__':","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index ae4da25d0..0293f9042 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax import lax
-from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split
+from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split, chunk
 from jax.linear_util import wrap_init
 from jax.interpreters.parallel import psum, scatter_like
 
@@ -134,15 +134,13 @@ class SplitTest(jtu.JaxTestCase):
     expected = onp.sum(onp.sin(x))
     self.assertAllClose(ans, expected, check_dtypes=False)
 
-
-  # def testChunkingSum(self):
-  #   f = lambda x: psum(x, 'i')
-
-  #   x = 3 * onp.ones((4, 2))
-  #   ans = pmap(lambda x: chunk(wrap_init(f), 2, 'i', (x,), (0,), 0), 'i')(x)
-  #   expected = 24
-
-  #   self.assertAllClose(ans, expected, check_dtypes=False)
+  def testChunkingSum(self):
+    f = lambda x: x - psum(x, 'i')
+    x = onp.ones((4, 2))
+    fchunked = chunk(f, 'i', 2)
+    ans = pmap(fchunked, 'i')(x)
+    expected = pmap(f, 'i')(x)
+    self.assertAllClose(ans, expected, check_dtypes=False)
 
 
 if __name__ == '__main__':",No
jax/api.py,jax/api.py,d321e2a3700996a55f5c2a0a9732f0d867ff432b,e546f13b03d60ed0c1842dfc6b0baeab2697d7c2,add pjit to api.py,"diff --git a/jax/api.py b/jax/api.py
index d305bd0b3..b6335d6ec 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -43,6 +43,7 @@ from .lib.xla_bridge import canonicalize_dtype
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
+from .interpreters import pxla
 from .interpreters import ad
 from .interpreters import batching
 from .interpreters import parallel
@@ -51,7 +52,7 @@ map = safe_map
 zip = safe_zip
 
 
-def jit(fun, static_argnums=(), **params):
+def jit(fun, static_argnums=()):
   """"""Sets up `fun` for just-in-time compilation with XLA.
 
   Args:
@@ -76,7 +77,7 @@ def jit(fun, static_argnums=(), **params):
     args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
     check_args(args_flat)
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    out_flat = xla.xla_call(jaxtree_fun, *args_flat, **params)
+    out_flat = xla.xla_call(jaxtree_fun, *args_flat)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
@@ -257,6 +258,25 @@ def vmap(fun, in_axes=0, out_axes=0):
   return batched_fun
 
 
+def pjit(fun, axis_map, out_axis_map, mesh_spec, mesh_map, static_argnums=()):
+  """"""Set up SPMD function for JIT compilation and parallel execution with XLA.""""""
+  @wraps(fun)
+  def f_jitted(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
+    f, dyn_args = argnums_partial(f, dyn_argnums, args)
+    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
+    check_args(args_flat)
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = pxla.xla_pcall(jaxtree_fun, *args_flat,
+                              axis_map=axis_map, out_axis_map=out_axis_map,
+                              mesh_map=mesh_map, mesh_spec=mesh_spec)
+    return build_tree(out_tree(), out_flat)
+
+  f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)
+  return f_jitted
+
+
 def pmap(fun, axis_name, in_axes=0, out_axes=0):
   """"""Vectorizing pseudo-map for single-program multiple-data (SPMD) functions.""""""
   def pmap_fun(*args, **kwargs):","diff --git a/jax/api.py b/jax/api.py
index d305bd0b3..b6335d6ec 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -43,6 +43,7 @@ from .lib.xla_bridge import canonicalize_dtype
 from .abstract_arrays import ShapedArray
 from .interpreters import partial_eval as pe
 from .interpreters import xla
+from .interpreters import pxla
 from .interpreters import ad
 from .interpreters import batching
 from .interpreters import parallel
@@ -51,7 +52,7 @@ map = safe_map
 zip = safe_zip
 
 
-def jit(fun, static_argnums=(), **params):
+def jit(fun, static_argnums=()):
   """"""Sets up `fun` for just-in-time compilation with XLA.
 
   Args:
@@ -76,7 +77,7 @@ def jit(fun, static_argnums=(), **params):
     args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
     check_args(args_flat)
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    out_flat = xla.xla_call(jaxtree_fun, *args_flat, **params)
+    out_flat = xla.xla_call(jaxtree_fun, *args_flat)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""jit({})"".format(f_jitted.__name__)
@@ -257,6 +258,25 @@ def vmap(fun, in_axes=0, out_axes=0):
   return batched_fun
 
 
+def pjit(fun, axis_map, out_axis_map, mesh_spec, mesh_map, static_argnums=()):
+  """"""Set up SPMD function for JIT compilation and parallel execution with XLA.""""""
+  @wraps(fun)
+  def f_jitted(*args, **kwargs):
+    f = lu.wrap_init(fun, kwargs)
+    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
+    f, dyn_args = argnums_partial(f, dyn_argnums, args)
+    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
+    check_args(args_flat)
+    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
+    out_flat = pxla.xla_pcall(jaxtree_fun, *args_flat,
+                              axis_map=axis_map, out_axis_map=out_axis_map,
+                              mesh_map=mesh_map, mesh_spec=mesh_spec)
+    return build_tree(out_tree(), out_flat)
+
+  f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)
+  return f_jitted
+
+
 def pmap(fun, axis_name, in_axes=0, out_axes=0):
   """"""Vectorizing pseudo-map for single-program multiple-data (SPMD) functions.""""""
   def pmap_fun(*args, **kwargs):",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,d321e2a3700996a55f5c2a0a9732f0d867ff432b,e546f13b03d60ed0c1842dfc6b0baeab2697d7c2,add pjit to api.py,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 24ce58ad2..7ad3b6ab8 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -176,7 +176,7 @@ def psum(x, axis_name):
 def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
-def psum_parallel_translation_rule(c, val, device_grp):
+def psum_parallel_translation_rule(c, val, device_groups):
   # return c.CrossReplicaSum(val, device_grp)  # TODO
   return c.CrossReplicaSum(val)
 ","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 24ce58ad2..7ad3b6ab8 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -176,7 +176,7 @@ def psum(x, axis_name):
 def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
-def psum_parallel_translation_rule(c, val, device_grp):
+def psum_parallel_translation_rule(c, val, device_groups):
   # return c.CrossReplicaSum(val, device_grp)  # TODO
   return c.CrossReplicaSum(val)
 ",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,d321e2a3700996a55f5c2a0a9732f0d867ff432b,e546f13b03d60ed0c1842dfc6b0baeab2697d7c2,add pjit to api.py,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 7f329c346..4e480a522 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -189,7 +189,7 @@ def replicated_jaxpr_computation(jaxpr, devicegrps,
       rule = parallel_translation_rules[eqn.primitive]
       axis_name = eqn.params['axis_name']
       params = {k: eqn.params[k] for k in eqn.params if k != 'axis_name'}
-      ans = rule(c, in_nodes, devicegrps[axis_name], **params)
+      ans = rule(c, *in_nodes, device_groups=devicegrps[axis_name], **params)
     else:
       if eqn.bound_subjaxprs: raise NotImplementedError  # TODO check primitive
       ans = translation_rule(eqn.primitive)(c, *in_nodes, **eqn.params)
@@ -260,10 +260,11 @@ def execute_replicated(axis_map, mesh_map, mesh_spec, compiled, pval,
   input_bufs = map(partial(shard_array, mesh_spec, mesh_map), axis_maps, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
   if out_tree is leaf:
+    # TODO sharded device persistence
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
   else:
-    raise NotImplementedError
+    raise NotImplementedError  # TODO
 
 
 xla_pcall_p = core.Primitive('xla_pcall')","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 7f329c346..4e480a522 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -189,7 +189,7 @@ def replicated_jaxpr_computation(jaxpr, devicegrps,
       rule = parallel_translation_rules[eqn.primitive]
       axis_name = eqn.params['axis_name']
       params = {k: eqn.params[k] for k in eqn.params if k != 'axis_name'}
-      ans = rule(c, in_nodes, devicegrps[axis_name], **params)
+      ans = rule(c, *in_nodes, device_groups=devicegrps[axis_name], **params)
     else:
       if eqn.bound_subjaxprs: raise NotImplementedError  # TODO check primitive
       ans = translation_rule(eqn.primitive)(c, *in_nodes, **eqn.params)
@@ -260,10 +260,11 @@ def execute_replicated(axis_map, mesh_map, mesh_spec, compiled, pval,
   input_bufs = map(partial(shard_array, mesh_spec, mesh_map), axis_maps, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
   if out_tree is leaf:
+    # TODO sharded device persistence
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
   else:
-    raise NotImplementedError
+    raise NotImplementedError  # TODO
 
 
 xla_pcall_p = core.Primitive('xla_pcall')",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,1691ae0a483845f78e1273b09e9c820bab3fc23f,326c3dbb990d3241c981e67090b4f82bf2389e12,added support for np.newaxis to jax.numpy,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 1ddc4b18e..612788fb0 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -42,6 +42,7 @@ else:
   def removechars(s, chars):
     return s.translate(None, ''.join(chars))
 
+newaxis = None
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
 _abs = builtins.abs","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 1ddc4b18e..612788fb0 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -42,6 +42,7 @@ else:
   def removechars(s, chars):
     return s.translate(None, ''.join(chars))
 
+newaxis = None
 
 # We replace some builtin names to follow Numpy's API, so we capture here.
 _abs = builtins.abs",No
jax/api.py,jax/api.py,8a6c09491b9264f6eaa947bc59fca2a9d8fb57b6,d321e2a3700996a55f5c2a0a9732f0d867ff432b,promising... but why do we need post_process_call?,"diff --git a/jax/api.py b/jax/api.py
index b6335d6ec..ddec1d539 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -258,7 +258,7 @@ def vmap(fun, in_axes=0, out_axes=0):
   return batched_fun
 
 
-def pjit(fun, axis_map, out_axis_map, mesh_spec, mesh_map, static_argnums=()):
+def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0, static_argnums=()):
   """"""Set up SPMD function for JIT compilation and parallel execution with XLA.""""""
   @wraps(fun)
   def f_jitted(*args, **kwargs):
@@ -269,8 +269,8 @@ def pjit(fun, axis_map, out_axis_map, mesh_spec, mesh_map, static_argnums=()):
     check_args(args_flat)
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
     out_flat = pxla.xla_pcall(jaxtree_fun, *args_flat,
-                              axis_map=axis_map, out_axis_map=out_axis_map,
-                              mesh_map=mesh_map, mesh_spec=mesh_spec)
+                              axis_name=axis_name, in_axes=in_axes,
+                              out_axes=out_axes, mesh_axis=mesh_axis)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)","diff --git a/jax/api.py b/jax/api.py
index b6335d6ec..ddec1d539 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -258,7 +258,7 @@ def vmap(fun, in_axes=0, out_axes=0):
   return batched_fun
 
 
-def pjit(fun, axis_map, out_axis_map, mesh_spec, mesh_map, static_argnums=()):
+def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0, static_argnums=()):
   """"""Set up SPMD function for JIT compilation and parallel execution with XLA.""""""
   @wraps(fun)
   def f_jitted(*args, **kwargs):
@@ -269,8 +269,8 @@ def pjit(fun, axis_map, out_axis_map, mesh_spec, mesh_map, static_argnums=()):
     check_args(args_flat)
     jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
     out_flat = pxla.xla_pcall(jaxtree_fun, *args_flat,
-                              axis_map=axis_map, out_axis_map=out_axis_map,
-                              mesh_map=mesh_map, mesh_spec=mesh_spec)
+                              axis_name=axis_name, in_axes=in_axes,
+                              out_axes=out_axes, mesh_axis=mesh_axis)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,8a6c09491b9264f6eaa947bc59fca2a9d8fb57b6,d321e2a3700996a55f5c2a0a9732f0d867ff432b,promising... but why do we need post_process_call?,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 7ad3b6ab8..73d29aac1 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -29,7 +29,6 @@ from ..abstract_arrays import ShapedArray, ConcreteArray, make_shaped_array
 from ..util import safe_zip, unzip2, unzip3, partialmethod, prod
 from ..lib import xla_bridge as xb
 from . import partial_eval as pe
-from . import pxla
 from . import batching
 
 zip = safe_zip
@@ -45,13 +44,14 @@ def pmap(fun, name, in_vals, in_axes, out_axis_target):
   if not sizes:
     return fun.call_wrapped(*in_vals)
   elif len(sizes) == 1:
-    out_val, out_axis = pmap_transform(fun).call_wrapped(name, in_vals, in_axes)
-    return batching.moveaxis(sizes.pop(), out_axis_target, out_axis, out_val)
+    fun, out_axis = pmap_transform(fun, name, in_axes)
+    out_val = fun.call_wrapped(*in_vals)
+    return batching.moveaxis(sizes.pop(), out_axis_target, out_axis(), out_val)
   else:
     raise TypeError(""got inconsistent map dimension sizes: {}"".format(sizes))
 
-@lu.transformation
-def pmap_transform(name, vals, axes):
+@lu.transformation_with_aux
+def pmap_transform(name, axes, *vals):
   with new_master(PmapTrace) as master:
     trace = PmapTrace(master, core.cur_sublevel())
     in_tracers = map(partial(PmapTracer, trace, name), vals, axes)
@@ -168,6 +168,7 @@ def PmapPrimitive(name):
 
 
 pmap_primitive_rules = {}
+parallel_translation_rules = {}
 
 
 def psum(x, axis_name):
@@ -182,7 +183,7 @@ def psum_parallel_translation_rule(c, val, device_groups):
 
 psum_p = PmapPrimitive('psum')
 pmap_primitive_rules[psum_p] = psum_pmap_rule
-pxla.parallel_translation_rules[psum_p] = psum_parallel_translation_rule
+parallel_translation_rules[psum_p] = psum_parallel_translation_rule
 
 
 def gather(x, axis_name):","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 7ad3b6ab8..73d29aac1 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -29,7 +29,6 @@ from ..abstract_arrays import ShapedArray, ConcreteArray, make_shaped_array
 from ..util import safe_zip, unzip2, unzip3, partialmethod, prod
 from ..lib import xla_bridge as xb
 from . import partial_eval as pe
-from . import pxla
 from . import batching
 
 zip = safe_zip
@@ -45,13 +44,14 @@ def pmap(fun, name, in_vals, in_axes, out_axis_target):
   if not sizes:
     return fun.call_wrapped(*in_vals)
   elif len(sizes) == 1:
-    out_val, out_axis = pmap_transform(fun).call_wrapped(name, in_vals, in_axes)
-    return batching.moveaxis(sizes.pop(), out_axis_target, out_axis, out_val)
+    fun, out_axis = pmap_transform(fun, name, in_axes)
+    out_val = fun.call_wrapped(*in_vals)
+    return batching.moveaxis(sizes.pop(), out_axis_target, out_axis(), out_val)
   else:
     raise TypeError(""got inconsistent map dimension sizes: {}"".format(sizes))
 
-@lu.transformation
-def pmap_transform(name, vals, axes):
+@lu.transformation_with_aux
+def pmap_transform(name, axes, *vals):
   with new_master(PmapTrace) as master:
     trace = PmapTrace(master, core.cur_sublevel())
     in_tracers = map(partial(PmapTracer, trace, name), vals, axes)
@@ -168,6 +168,7 @@ def PmapPrimitive(name):
 
 
 pmap_primitive_rules = {}
+parallel_translation_rules = {}
 
 
 def psum(x, axis_name):
@@ -182,7 +183,7 @@ def psum_parallel_translation_rule(c, val, device_groups):
 
 psum_p = PmapPrimitive('psum')
 pmap_primitive_rules[psum_p] = psum_pmap_rule
-pxla.parallel_translation_rules[psum_p] = psum_parallel_translation_rule
+parallel_translation_rules[psum_p] = psum_parallel_translation_rule
 
 
 def gather(x, axis_name):",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,8a6c09491b9264f6eaa947bc59fca2a9d8fb57b6,d321e2a3700996a55f5c2a0a9732f0d867ff432b,promising... but why do we need post_process_call?,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 4e480a522..e8c699674 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -37,6 +37,8 @@ from .xla import (flatten_fun, tree_flatten, build_tree, leaf, xla_shape,
                   xla_destructure, translation_rule, abstractify,
                   xla_shape_to_result_shape)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
+from .parallel import parallel_translation_rules
+from . import parallel
 
 map = safe_map
 
@@ -44,6 +46,26 @@ map = safe_map
 ### util
 
 
+def chunk_transform(fun, name, in_axes, out_axes_dst):
+  temp_name = object()  # TODO gensym
+  fun = parallel.axisvar_split(fun, name, (temp_name, name))
+  fun, out_axes_src = parallel.pmap_transform(fun, temp_name, in_axes)
+  fun = move_output_axis_transform(fun, out_axes_src, out_axes_dst)
+  return fun
+
+@lu.transformation
+def move_output_axis_transform(src, dst, *args):
+  ans = yield args
+  yield batching.moveaxis(None, dst(), src(), ans)
+
+def chunk_aval(chunksize, aval, axis):
+  if axis is None:
+    return aval
+  else:
+    shape = list(aval.shape)
+    shape[axis] = chunksize
+    return ShapedArray(tuple(shape), aval.dtype)
+
 def canonicalize_axis_spec(in_trees, spec):
   spec = (spec,) * len(in_trees) if type(spec) is int else spec
   spec = map(build_axis_spec_tree, spec, in_trees)
@@ -200,48 +222,66 @@ def replicated_jaxpr_computation(jaxpr, devicegrps,
 
 
 def xla_pcall_impl(fun, *args, **params):
-  axis_map = params.pop('axis_map')    # e.g. {'i': 0, 'j': (None, 1)}
-  mesh_map = params.pop('mesh_map')    # e.g. {'i': 0, 'j': 2}
-  mesh_spec = params.pop('mesh_spec')  # e.g. (2, 2, 2)
-  out_axis_map = params.pop('out_axis_map')    # e.g. {'i': 0, 'j': (None, 1)}
+  axis_name = params.pop('axis_name')  # e.g. 'i'
+  in_axes = params.pop('in_axes')      # e.g. 0 or (0, None)
+  out_axes = params.pop('out_axes')    # e.g. 0 or (None, 1)
+  mesh_axis = params.pop('mesh_axis')  # e.g. 0 or 1
   assert not params
 
   flat_args, in_trees = unzip2(map(tree_flatten, args))
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
-  mesh_map = tuple(sorted(mesh_map.items()))
-  axis_map = tuple((axis_name, canonicalize_axis_spec(in_trees, spec))
-                   for axis_name, spec in sorted(axis_map.items()))
-  compiled_fun = xla_parallel_callable(fun, axis_map, mesh_map, mesh_spec,
-                                       *map(abstractify, flat_args))
+  @HideFromMemoizer
+  def out_axes_thunk():
+    return canonicalize_axis_spec([out_tree()], out_axes)[0]
+
+  in_axes = canonicalize_axis_spec(in_trees, in_axes)
+  compiled_fun = xla_parallel_callable(
+      fun, axis_name, in_axes, out_axes_thunk,
+      mesh_axis, mesh_spec, *map(abstractify, flat_args))
 
-  out_axis_map = {axis_name : canonicalize_axis_spec([out_tree()], spec)[0]
-                  for axis_name, spec in out_axis_map.items()}
-  flat_ans = compiled_fun(out_tree(), out_axis_map, *args)
+  leaf_out = out_tree() is leaf
+  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
 
-  if out_tree() is leaf:
+  if leaf_out:
     return flat_ans
   else:
     return build_tree(iter(flat_ans), out_tree())
 
-@lu.memoize
-def xla_parallel_callable(fun, axis_map, mesh_map, mesh_spec, *abstract_args):
-  axis_map, mesh_map = dict(axis_map), dict(mesh_map)
+class HideFromMemoizer(object):
+  def __init__(self, val):
+    self.val = val
+  def __call__(self):
+    return self.val()
+  def __hash__(self):
+    return 0
+  def __eq__(self, other):
+    return type(other) is HideFromMemoizer
 
-  # check that all mapped axes have the right size
-  for axis_name in axis_map:
-    if not all(axis is None or arg.shape[axis] == mesh_spec[mesh_map[axis_name]]
-               for arg, axis in zip(abstract_args, axis_map[axis_name])):
-      msg = ""axis size does not match mesh size for axis name {}""
-      raise ValueError(msg.format(axis_name))
-
-  # construct abstract args with sharded dimensions removed
-  abstract_args = map(remove_mapped_dims, abstract_args, *axis_map.values())
-
-  # process mesh_spec and mesh_map into a mapping to device groups
-  devicegrps = {axis_name : meshgroups(mesh_spec, mesh_map[axis_name])
-                for axis_name in mesh_map}
+@lu.memoize
+def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
+                          mesh_axis, mesh_spec, *abstract_args):
+  axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
+                if axis is not None}
+  if len(axis_sizes) == 0:
+    msg = ""axis name '{}' not bound to any input axes.""
+    raise ValueError(msg.format(axis_name))
+  elif len(axis_sizes) > 1:
+    msg = ""axis name '{}' bound to multiple axes with different sizes: {}.""
+    raise ValueError(msg.format(axis_name, axis_sizes))
+  else:
+    axis_size = axis_sizes.pop()
+    if axis_size % mesh_spec[mesh_axis]:
+      msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
+             ""axis index {} with size {}, which does not evenly divide {}."")
+      raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
+                                  mesh_spec[mesh_axis], axis_size))
+
+  chunksize = axis_size // mesh_spec[mesh_axis]
+  abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
+  fun = chunk_transform(fun, axis_name, in_axes, out_axes)
+  device_groups = meshgroups(mesh_spec, mesh_axis)
 
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
@@ -250,17 +290,17 @@ def xla_parallel_callable(fun, axis_map, mesh_map, mesh_spec, *abstract_args):
     compiled, result_shape = compile_replicated(jaxpr, devicegrps,
                                                 consts, *abstract_args)
     del master, consts, jaxpr, env
+  return partial(execute_replicated, in_axes, mesh_axis, mesh_spec, compiled, pval)
 
-  return partial(execute_replicated, axis_map, mesh_map, mesh_spec, compiled, pval)
-
-def execute_replicated(axis_map, mesh_map, mesh_spec, compiled, pval,
-                       out_tree, out_axis_map, *args):
+def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
+                       leaf_out, out_axes, *args):
+  assert False
   axis_maps = [{axis_name : axes[i] for axis_name, axes in axis_map.items()}
                for i in range(len(args))]
   input_bufs = map(partial(shard_array, mesh_spec, mesh_map), axis_maps, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
-  if out_tree is leaf:
-    # TODO sharded device persistence
+  if leaf_out:
+    # TODO sharded device persistence, remove the .to_py()
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
   else:","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 4e480a522..e8c699674 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -37,6 +37,8 @@ from .xla import (flatten_fun, tree_flatten, build_tree, leaf, xla_shape,
                   xla_destructure, translation_rule, abstractify,
                   xla_shape_to_result_shape)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
+from .parallel import parallel_translation_rules
+from . import parallel
 
 map = safe_map
 
@@ -44,6 +46,26 @@ map = safe_map
 ### util
 
 
+def chunk_transform(fun, name, in_axes, out_axes_dst):
+  temp_name = object()  # TODO gensym
+  fun = parallel.axisvar_split(fun, name, (temp_name, name))
+  fun, out_axes_src = parallel.pmap_transform(fun, temp_name, in_axes)
+  fun = move_output_axis_transform(fun, out_axes_src, out_axes_dst)
+  return fun
+
+@lu.transformation
+def move_output_axis_transform(src, dst, *args):
+  ans = yield args
+  yield batching.moveaxis(None, dst(), src(), ans)
+
+def chunk_aval(chunksize, aval, axis):
+  if axis is None:
+    return aval
+  else:
+    shape = list(aval.shape)
+    shape[axis] = chunksize
+    return ShapedArray(tuple(shape), aval.dtype)
+
 def canonicalize_axis_spec(in_trees, spec):
   spec = (spec,) * len(in_trees) if type(spec) is int else spec
   spec = map(build_axis_spec_tree, spec, in_trees)
@@ -200,48 +222,66 @@ def replicated_jaxpr_computation(jaxpr, devicegrps,
 
 
 def xla_pcall_impl(fun, *args, **params):
-  axis_map = params.pop('axis_map')    # e.g. {'i': 0, 'j': (None, 1)}
-  mesh_map = params.pop('mesh_map')    # e.g. {'i': 0, 'j': 2}
-  mesh_spec = params.pop('mesh_spec')  # e.g. (2, 2, 2)
-  out_axis_map = params.pop('out_axis_map')    # e.g. {'i': 0, 'j': (None, 1)}
+  axis_name = params.pop('axis_name')  # e.g. 'i'
+  in_axes = params.pop('in_axes')      # e.g. 0 or (0, None)
+  out_axes = params.pop('out_axes')    # e.g. 0 or (None, 1)
+  mesh_axis = params.pop('mesh_axis')  # e.g. 0 or 1
   assert not params
 
   flat_args, in_trees = unzip2(map(tree_flatten, args))
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
-  mesh_map = tuple(sorted(mesh_map.items()))
-  axis_map = tuple((axis_name, canonicalize_axis_spec(in_trees, spec))
-                   for axis_name, spec in sorted(axis_map.items()))
-  compiled_fun = xla_parallel_callable(fun, axis_map, mesh_map, mesh_spec,
-                                       *map(abstractify, flat_args))
+  @HideFromMemoizer
+  def out_axes_thunk():
+    return canonicalize_axis_spec([out_tree()], out_axes)[0]
 
-  out_axis_map = {axis_name : canonicalize_axis_spec([out_tree()], spec)[0]
-                  for axis_name, spec in out_axis_map.items()}
-  flat_ans = compiled_fun(out_tree(), out_axis_map, *args)
+  in_axes = canonicalize_axis_spec(in_trees, in_axes)
+  compiled_fun = xla_parallel_callable(
+      fun, axis_name, in_axes, out_axes_thunk,
+      mesh_axis, mesh_spec, *map(abstractify, flat_args))
 
-  if out_tree() is leaf:
+  leaf_out = out_tree() is leaf
+  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
+
+  if leaf_out:
     return flat_ans
   else:
     return build_tree(iter(flat_ans), out_tree())
 
+class HideFromMemoizer(object):
+  def __init__(self, val):
+    self.val = val
+  def __call__(self):
+    return self.val()
+  def __hash__(self):
+    return 0
+  def __eq__(self, other):
+    return type(other) is HideFromMemoizer
+
 @lu.memoize
-def xla_parallel_callable(fun, axis_map, mesh_map, mesh_spec, *abstract_args):
-  axis_map, mesh_map = dict(axis_map), dict(mesh_map)
+def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
+                          mesh_axis, mesh_spec, *abstract_args):
+  axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
+                if axis is not None}
+  if len(axis_sizes) == 0:
+    msg = ""axis name '{}' not bound to any input axes.""
+    raise ValueError(msg.format(axis_name))
+  elif len(axis_sizes) > 1:
+    msg = ""axis name '{}' bound to multiple axes with different sizes: {}.""
+    raise ValueError(msg.format(axis_name, axis_sizes))
+  else:
+    axis_size = axis_sizes.pop()
+    if axis_size % mesh_spec[mesh_axis]:
+      msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
+             ""axis index {} with size {}, which does not evenly divide {}."")
+      raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
+                                  mesh_spec[mesh_axis], axis_size))
 
-  # check that all mapped axes have the right size
-  for axis_name in axis_map:
-    if not all(axis is None or arg.shape[axis] == mesh_spec[mesh_map[axis_name]]
-               for arg, axis in zip(abstract_args, axis_map[axis_name])):
-      msg = ""axis size does not match mesh size for axis name {}""
-      raise ValueError(msg.format(axis_name))
-
-  # construct abstract args with sharded dimensions removed
-  abstract_args = map(remove_mapped_dims, abstract_args, *axis_map.values())
-
-  # process mesh_spec and mesh_map into a mapping to device groups
-  devicegrps = {axis_name : meshgroups(mesh_spec, mesh_map[axis_name])
-                for axis_name in mesh_map}
+  chunksize = axis_size // mesh_spec[mesh_axis]
+  abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
+  fun = chunk_transform(fun, axis_name, in_axes, out_axes)
+  device_groups = meshgroups(mesh_spec, mesh_axis)
 
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
@@ -250,17 +290,17 @@ def xla_parallel_callable(fun, axis_map, mesh_map, mesh_spec, *abstract_args):
     compiled, result_shape = compile_replicated(jaxpr, devicegrps,
                                                 consts, *abstract_args)
     del master, consts, jaxpr, env
+  return partial(execute_replicated, in_axes, mesh_axis, mesh_spec, compiled, pval)
 
-  return partial(execute_replicated, axis_map, mesh_map, mesh_spec, compiled, pval)
-
-def execute_replicated(axis_map, mesh_map, mesh_spec, compiled, pval,
-                       out_tree, out_axis_map, *args):
+def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
+                       leaf_out, out_axes, *args):
+  assert False
   axis_maps = [{axis_name : axes[i] for axis_name, axes in axis_map.items()}
                for i in range(len(args))]
   input_bufs = map(partial(shard_array, mesh_spec, mesh_map), axis_maps, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
-  if out_tree is leaf:
-    # TODO sharded device persistence
+  if leaf_out:
+    # TODO sharded device persistence, remove the .to_py()
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
   else:",Yes
jax/interpreters/parallel.py,jax/interpreters/parallel.py,6494504aed4d8748e9c56dd32e1e84a838ae8f6b,8a6c09491b9264f6eaa947bc59fca2a9d8fb57b6,"woo post_process_call, it scrolls like butter","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 73d29aac1..87232eb8b 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -146,7 +146,13 @@ class PmapTrace(Trace):
       return PmapTracer(self, name, val_out, axis_out())
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
+    name, val, axis = out_tracer.name, out_tracer.val, out_tracer.axis
+    master = self.master
+    def todo(x):
+      trace = PmapTrace(master, core.cur_sublevel())
+      return PmapTracer(trace, name, x, axis)
+
+    return val, todo
 
   def pack(self, tracers):
     vals = pack([t.val for t in tracers])
@@ -275,7 +281,13 @@ class SplitTrace(Trace):
       return SplitTracer(self, name, new_names, val_out)
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
+    name, new_names, val = out_tracer.name, out_tracer.new_names, out_tracer.val
+    master = self.master
+    def todo(x):
+      trace = SplitTrace(master, core.cur_sublevel())
+      return SplitTracer(trace, name, new_names, x)
+
+    return val, todo
 
   def pack(self, tracers):
     vals = pack([t.val for t in tracers])","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 73d29aac1..87232eb8b 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -146,7 +146,13 @@ class PmapTrace(Trace):
       return PmapTracer(self, name, val_out, axis_out())
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
+    name, val, axis = out_tracer.name, out_tracer.val, out_tracer.axis
+    master = self.master
+    def todo(x):
+      trace = PmapTrace(master, core.cur_sublevel())
+      return PmapTracer(trace, name, x, axis)
+
+    return val, todo
 
   def pack(self, tracers):
     vals = pack([t.val for t in tracers])
@@ -275,7 +281,13 @@ class SplitTrace(Trace):
       return SplitTracer(self, name, new_names, val_out)
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
+    name, new_names, val = out_tracer.name, out_tracer.new_names, out_tracer.val
+    master = self.master
+    def todo(x):
+      trace = SplitTrace(master, core.cur_sublevel())
+      return SplitTracer(trace, name, new_names, x)
+
+    return val, todo
 
   def pack(self, tracers):
     vals = pack([t.val for t in tracers])",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,6494504aed4d8748e9c56dd32e1e84a838ae8f6b,8a6c09491b9264f6eaa947bc59fca2a9d8fb57b6,"woo post_process_call, it scrolls like butter","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index e8c699674..eb61fd671 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -38,6 +38,7 @@ from .xla import (flatten_fun, tree_flatten, build_tree, leaf, xla_shape,
                   xla_shape_to_result_shape)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 from .parallel import parallel_translation_rules
+from .batching import moveaxis
 from . import parallel
 
 map = safe_map
@@ -56,7 +57,7 @@ def chunk_transform(fun, name, in_axes, out_axes_dst):
 @lu.transformation
 def move_output_axis_transform(src, dst, *args):
   ans = yield args
-  yield batching.moveaxis(None, dst(), src(), ans)
+  yield moveaxis(1, dst(), src(), ans)  # inserts singleton if src is None
 
 def chunk_aval(chunksize, aval, axis):
   if axis is None:
@@ -181,14 +182,14 @@ def split_array(x, axis):
 ### xla_pcall
 
 
-def compile_replicated(jaxpr, devicegrps, consts, *abstract_args):
+def compile_replicated(jaxpr, device_groups, consts, *abstract_args):
   arg_shapes = list(map(xla_shape, abstract_args))
-  built_c = replicated_jaxpr_computation(jaxpr, devicegrps, consts, (),
+  built_c = replicated_jaxpr_computation(jaxpr, device_groups, consts, (),
                                          *arg_shapes)
   result_shape = xla_shape_to_result_shape(built_c.GetReturnValueShape())
   return built_c.Compile(arg_shapes, xb.get_compile_options()), result_shape
 
-def replicated_jaxpr_computation(jaxpr, devicegrps,
+def replicated_jaxpr_computation(jaxpr, device_groups,
                                  const_vals, freevar_shapes, *arg_shapes):
   c = xb.make_computation_builder(""replicated_jaxpr_computation"")
 
@@ -211,7 +212,7 @@ def replicated_jaxpr_computation(jaxpr, devicegrps,
       rule = parallel_translation_rules[eqn.primitive]
       axis_name = eqn.params['axis_name']
       params = {k: eqn.params[k] for k in eqn.params if k != 'axis_name'}
-      ans = rule(c, *in_nodes, device_groups=devicegrps[axis_name], **params)
+      ans = rule(c, *in_nodes, device_groups=device_groups, **params)
     else:
       if eqn.bound_subjaxprs: raise NotImplementedError  # TODO check primitive
       ans = translation_rule(eqn.primitive)(c, *in_nodes, **eqn.params)
@@ -287,7 +288,7 @@ def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
     assert not env
-    compiled, result_shape = compile_replicated(jaxpr, devicegrps,
+    compiled, result_shape = compile_replicated(jaxpr, device_groups,
                                                 consts, *abstract_args)
     del master, consts, jaxpr, env
   return partial(execute_replicated, in_axes, mesh_axis, mesh_spec, compiled, pval)
@@ -311,6 +312,3 @@ xla_pcall_p = core.Primitive('xla_pcall')
 xla_pcall = partial(core.call_bind, xla_pcall_p)
 xla_pcall_p.def_custom_bind(xla_pcall)
 xla_pcall_p.def_impl(xla_pcall_impl)
-
-
-parallel_translation_rules = {}","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index e8c699674..eb61fd671 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -38,6 +38,7 @@ from .xla import (flatten_fun, tree_flatten, build_tree, leaf, xla_shape,
                   xla_shape_to_result_shape)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 from .parallel import parallel_translation_rules
+from .batching import moveaxis
 from . import parallel
 
 map = safe_map
@@ -56,7 +57,7 @@ def chunk_transform(fun, name, in_axes, out_axes_dst):
 @lu.transformation
 def move_output_axis_transform(src, dst, *args):
   ans = yield args
-  yield batching.moveaxis(None, dst(), src(), ans)
+  yield moveaxis(1, dst(), src(), ans)  # inserts singleton if src is None
 
 def chunk_aval(chunksize, aval, axis):
   if axis is None:
@@ -181,14 +182,14 @@ def split_array(x, axis):
 ### xla_pcall
 
 
-def compile_replicated(jaxpr, devicegrps, consts, *abstract_args):
+def compile_replicated(jaxpr, device_groups, consts, *abstract_args):
   arg_shapes = list(map(xla_shape, abstract_args))
-  built_c = replicated_jaxpr_computation(jaxpr, devicegrps, consts, (),
+  built_c = replicated_jaxpr_computation(jaxpr, device_groups, consts, (),
                                          *arg_shapes)
   result_shape = xla_shape_to_result_shape(built_c.GetReturnValueShape())
   return built_c.Compile(arg_shapes, xb.get_compile_options()), result_shape
 
-def replicated_jaxpr_computation(jaxpr, devicegrps,
+def replicated_jaxpr_computation(jaxpr, device_groups,
                                  const_vals, freevar_shapes, *arg_shapes):
   c = xb.make_computation_builder(""replicated_jaxpr_computation"")
 
@@ -211,7 +212,7 @@ def replicated_jaxpr_computation(jaxpr, devicegrps,
       rule = parallel_translation_rules[eqn.primitive]
       axis_name = eqn.params['axis_name']
       params = {k: eqn.params[k] for k in eqn.params if k != 'axis_name'}
-      ans = rule(c, *in_nodes, device_groups=devicegrps[axis_name], **params)
+      ans = rule(c, *in_nodes, device_groups=device_groups, **params)
     else:
       if eqn.bound_subjaxprs: raise NotImplementedError  # TODO check primitive
       ans = translation_rule(eqn.primitive)(c, *in_nodes, **eqn.params)
@@ -287,7 +288,7 @@ def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
     assert not env
-    compiled, result_shape = compile_replicated(jaxpr, devicegrps,
+    compiled, result_shape = compile_replicated(jaxpr, device_groups,
                                                 consts, *abstract_args)
     del master, consts, jaxpr, env
   return partial(execute_replicated, in_axes, mesh_axis, mesh_spec, compiled, pval)
@@ -311,6 +312,3 @@ xla_pcall_p = core.Primitive('xla_pcall')
 xla_pcall = partial(core.call_bind, xla_pcall_p)
 xla_pcall_p.def_custom_bind(xla_pcall)
 xla_pcall_p.def_impl(xla_pcall_impl)
-
-
-parallel_translation_rules = {}",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,8bc579aebc3c150c8fe9b470f7ecca154743e58a,6494504aed4d8748e9c56dd32e1e84a838ae8f6b,chunk transform outside of xla_parallel_callable,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index eb61fd671..019155493 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -233,36 +233,25 @@ def xla_pcall_impl(fun, *args, **params):
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
-  @HideFromMemoizer
-  def out_axes_thunk():
+  in_axes = canonicalize_axis_spec(in_trees, in_axes)
+  def canonicalized_out_axes():
     return canonicalize_axis_spec([out_tree()], out_axes)[0]
+  fun = chunk_transform(fun, axis_name, in_axes, canonicalized_out_axes)
 
-  in_axes = canonicalize_axis_spec(in_trees, in_axes)
-  compiled_fun = xla_parallel_callable(
-      fun, axis_name, in_axes, out_axes_thunk,
-      mesh_axis, mesh_spec, *map(abstractify, flat_args))
+  compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
+                                       mesh_spec, *map(abstractify, flat_args))
 
   leaf_out = out_tree() is leaf
-  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
+  flat_ans = compiled_fun(leaf_out, canonicalized_out_axes(), *args)
 
   if leaf_out:
     return flat_ans
   else:
     return build_tree(iter(flat_ans), out_tree())
 
-class HideFromMemoizer(object):
-  def __init__(self, val):
-    self.val = val
-  def __call__(self):
-    return self.val()
-  def __hash__(self):
-    return 0
-  def __eq__(self, other):
-    return type(other) is HideFromMemoizer
-
 @lu.memoize
-def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
-                          mesh_axis, mesh_spec, *abstract_args):
+def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
+                          *abstract_args):
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -281,7 +270,6 @@ def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
 
   chunksize = axis_size // mesh_spec[mesh_axis]
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
-  fun = chunk_transform(fun, axis_name, in_axes, out_axes)
   device_groups = meshgroups(mesh_spec, mesh_axis)
 
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index eb61fd671..019155493 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -233,36 +233,25 @@ def xla_pcall_impl(fun, *args, **params):
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
-  @HideFromMemoizer
-  def out_axes_thunk():
-    return canonicalize_axis_spec([out_tree()], out_axes)[0]
-
   in_axes = canonicalize_axis_spec(in_trees, in_axes)
-  compiled_fun = xla_parallel_callable(
-      fun, axis_name, in_axes, out_axes_thunk,
-      mesh_axis, mesh_spec, *map(abstractify, flat_args))
+  def canonicalized_out_axes():
+    return canonicalize_axis_spec([out_tree()], out_axes)[0]
+  fun = chunk_transform(fun, axis_name, in_axes, canonicalized_out_axes)
+
+  compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
+                                       mesh_spec, *map(abstractify, flat_args))
 
   leaf_out = out_tree() is leaf
-  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
+  flat_ans = compiled_fun(leaf_out, canonicalized_out_axes(), *args)
 
   if leaf_out:
     return flat_ans
   else:
     return build_tree(iter(flat_ans), out_tree())
 
-class HideFromMemoizer(object):
-  def __init__(self, val):
-    self.val = val
-  def __call__(self):
-    return self.val()
-  def __hash__(self):
-    return 0
-  def __eq__(self, other):
-    return type(other) is HideFromMemoizer
-
 @lu.memoize
-def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
-                          mesh_axis, mesh_spec, *abstract_args):
+def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
+                          *abstract_args):
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -281,7 +270,6 @@ def xla_parallel_callable(fun, axis_name, in_axes, out_axes,
 
   chunksize = axis_size // mesh_spec[mesh_axis]
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
-  fun = chunk_transform(fun, axis_name, in_axes, out_axes)
   device_groups = meshgroups(mesh_spec, mesh_axis)
 
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]",Yes
jax/interpreters/pxla.py,jax/interpreters/pxla.py,da724d315927d0f478e014d62336ad5073c12d88,8bc579aebc3c150c8fe9b470f7ecca154743e58a,single-axis-var version runs! and some cleanup,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 019155493..8a5bc63a0 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -33,9 +33,9 @@ from .. import linear_util as lu
 from ..abstract_arrays import ShapedArray
 from ..util import partial, unzip2, concatenate, safe_map, prod
 from ..lib import xla_bridge as xb
-from .xla import (flatten_fun, tree_flatten, build_tree, leaf, xla_shape,
+from .xla import (flatten_fun, tree_flatten, build_tree, xla_shape,
                   xla_destructure, translation_rule, abstractify,
-                  xla_shape_to_result_shape)
+                  xla_shape_to_result_shape, leaf, JTupleTreeDef)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 from .parallel import parallel_translation_rules
 from .batching import moveaxis
@@ -44,10 +44,14 @@ from . import parallel
 map = safe_map
 
 
+mesh_spec = None
+
+
 ### util
 
 
 def chunk_transform(fun, name, in_axes, out_axes_dst):
+  """"""Rewrite SPMD operations to act first on local chunks then cross-replica.""""""
   temp_name = object()  # TODO gensym
   fun = parallel.axisvar_split(fun, name, (temp_name, name))
   fun, out_axes_src = parallel.pmap_transform(fun, temp_name, in_axes)
@@ -56,10 +60,13 @@ def chunk_transform(fun, name, in_axes, out_axes_dst):
 
 @lu.transformation
 def move_output_axis_transform(src, dst, *args):
+  """"""Function transformation that moves output axes from src to dst.""""""
   ans = yield args
+  # TODO singleton or chunksize? i think chunksize...
   yield moveaxis(1, dst(), src(), ans)  # inserts singleton if src is None
 
 def chunk_aval(chunksize, aval, axis):
+  """"""Transform an abstract value's shape to have chunksize extent along axis.""""""
   if axis is None:
     return aval
   else:
@@ -67,116 +74,74 @@ def chunk_aval(chunksize, aval, axis):
     shape[axis] = chunksize
     return ShapedArray(tuple(shape), aval.dtype)
 
-def canonicalize_axis_spec(in_trees, spec):
+def canonicalize_in_axis_spec(in_trees, spec):
+  """"""Given argument list in_trees, canonicalize and flatten an in_axes spec.""""""
   spec = (spec,) * len(in_trees) if type(spec) is int else spec
   spec = map(build_axis_spec_tree, spec, in_trees)
-  spec = tuple(tree_util.tree_flatten(spec)[0])
-  return spec
+  return tuple(tree_util.tree_flatten(spec)[0])
+
+def canonicalize_out_axis_spec(out_tree, spec):
+  """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
+  if out_tree is leaf:
+    return spec
+  else:
+    spec = build_axis_spec_tree(spec, out_tree)
+    return tuple(tree_util.tree_flatten(spec)[0])
 
-def build_axis_spec_tree(spec, in_tree):
-  if in_tree is leaf:
+def build_axis_spec_tree(spec, tree):
+  """"""Given a JaxTuple treedef, canonicalize an axis spec for that tree.""""""
+  if tree is leaf:
     assert type(spec) is int
     return spec
-  elif type(in_tree) is JTupleTreeDef:
+  elif type(tree) is JTupleTreeDef:
     spec_type = type(spec)
     if spec_type is int:
-      return tuple(map(partial(build_axis_spec_tree, spec), in_tree.child_specs))
+      return tuple(map(partial(build_axis_spec_tree, spec), tree.child_specs))
     elif spec_type is tuple:
-      return tuple(map(build_axis_spec_tree, spec, in_tree.child_specs))
+      return tuple(map(build_axis_spec_tree, spec, tree.child_specs))
     else:
       raise TypeError(spec_type)
   else:
-    raise TypeError(type(in_tree))
-
-def remove_mapped_dims(aval, *axes):
-  assert type(aval) is ShapedArray
-  axes = [d for d in axes if d is not None]
-  if len(set(axes)) != len(axes):
-    raise ValueError(""multiple names mapped to the same axis"")
-  shape = tuple(onp.delete(aval.shape, axes))
-  return ShapedArray(shape, aval.dtype)
-
-def meshgroups(mesh_spec, mesh_axis):
+    raise TypeError(type(tree))
+
+def shard_arg(mesh_spec, mesh_axis, axis, arg):
+  """"""Shard and device_put an input array argument along a logical axis.""""""
+  num_replicas = xb.get_replica_count()
+  if prod(mesh_spec) != num_replicas:
+    msg = ""mesh spec {} total size of {} doesn't match number of replicas {}.""
+    raise ValueError(msg.format(mesh_spec, prod(mesh_spec), num_replicas))
+  shards = split_array(arg, mesh_spec[mesh_axis], axis)
+  replica_shards = [shards[i] for i in shard_assignments(mesh_spec, mesh_axis)]
+  return map(xb.device_put, replica_shards, range(num_replicas))
+
+def unshard_output(mesh_spec, mesh_axis, out_axis, out_shards):
+  """"""Collect and concatenate sharded device results.""""""
+  _, ids = onp.unique(shard_assignments(mesh_spec, mesh_axis), return_index=True)
+  shards = [out_shards[i] for i in ids]
+  return onp.concatenate(shards, out_axis)
+
+def shard_assignments(mesh_spec, mesh_axis):
+  """"""Given a mesh axis long which to shard data, compute replica assignments.""""""
+  indices_shape = [1] * len(mesh_spec)
+  indices_shape[mesh_axis] = mesh_spec[mesh_axis]
+  indices = onp.arange(mesh_spec[mesh_axis]).reshape(indices_shape)
+  return tuple(onp.broadcast_to(indices, mesh_spec).ravel())
+
+def replica_groups(mesh_spec, mesh_axis):
+  """"""Given a mesh axis along which to operate, compute XLA replica_groups.""""""
   groups = onp.split(onp.arange(prod(mesh_spec)).reshape(mesh_spec),
                      mesh_spec[mesh_axis], axis=mesh_axis)
   return tuple(tuple(group.ravel()) for group in groups)
 
-def shard_array(mesh_spec, mesh_map, axis_map, x):
-  # axis_map , e.g. {'i': 0, 'j': None}  (axis indices)
-  # mesh_map , e.g. {'i': 0, 'k': 2}     (mesh indices)
-  # mesh_spec, e.g. (2, 4, 4)
-  # return flat list of device buffers - one per replica
-  mesh_ndim = len(mesh_spec)
-  mesh_size = onp.prod(mesh_spec)
-  mesh_map_inverted = {v : k for k, v in mesh_map.items()}
-  ordered_idx_names = map(mesh_map_inverted.get, range(mesh_ndim))  # [i,None,k]
-  axes = map(axis_map.get, ordered_idx_names)
-  xs = list_ravel(unstack_axes(mesh_spec, axes, x))
-  return map(xb.device_put, xs, range(mesh_size))
-
-def unstack_axes(mesh_spec, axes, x):
-  # axes: list of ints. logical axes of x to split, ordered as in mesh_spec
-  #  e.g. zeros(4, 10, 2, 6)
-  # axes  (2, 0, None[broadcast_size=10])
-  # results in (8,) nested lists each with (10,6) arrays
-  if axes:
-    ax0 = axes[0]
-    recur = partial(unstack_axes, mesh_spec[1:], axes[1:])
-    if ax0 is None:
-      return [recur(x)]  * mesh_spec[0]
-    else:
-      assert x.shape[ax0] == mesh_spec[0]
-      xs = split_array(x, axes[0])
-      return list(map(recur, xs))
-  else:
-    return x
-
-def unshard_array(mesh_spec, mesh_map, axis_map, xs):
-  mesh_ndim = len(mesh_spec)
-  mesh_size = onp.prod(mesh_spec)
-  mesh_map_inverted = {v : k for k, v in mesh_map.items()}
-  ordered_idx_names = map(mesh_map_inverted.get, range(mesh_ndim))  # [i,None,k]
-  axes = map(axis_map.get, ordered_idx_names)
-
-  example_shard = xs[0]
-  num_splits = len([i for i in axes if i is not None])
-  shape = iter(example_shard.shape)
-  newshape = [1 if i in axes else next(shape)
-              for i in range(num_splits + example_shard.ndim)]
-
-  xs = [x.reshape(newshape) for x in xs]
-  xs = list_reshape(mesh_spec, iter(xs))
-  x = stack_axes(mesh_spec, axes, xs)
-  return x
-
-def list_reshape(mesh_spec, flat_xs):
-  if mesh_spec:
-    return [list_reshape(mesh_spec[1:], flat_xs) for _ in range(mesh_spec[0])]
-  else:
-    return next(flat_xs)
-
-def stack_axes(mesh_spec, axes, xs):
-  if mesh_spec:
-    if mesh_spec[0] is None:
-      return stack_axes(mesh_spec[1:], axes[1:], xs[0])
-    else:
-      components = map(partial(stack_axes, mesh_spec[1:], axes[1:]), xs)
-      return onp.concatenate(components, axis=axes[0])
-  else:
-    return xs
-
-def list_ravel(xs):
-  if isinstance(xs, list):
-    return concatenate(map(list_ravel, xs))
-  else:
-    return [xs]
-
-def split_array(x, axis):
+def split_array(x, num_splits, axis):
+  """"""A special-case of numpy.split implemented in terms of indexing.""""""
+  assert x.shape[axis] % num_splits == 0
+  split_size = x.shape[axis] // num_splits
   def get_nth_subarray(n):
     idx = [slice(None)] * x.ndim
-    idx[axis] = n
+    idx[axis] = slice(n * split_size, (n+1) * split_size)
     return x[tuple(idx)]
-  return map(get_nth_subarray, range(x.shape[axis]))
+  return map(get_nth_subarray, range(num_splits))
 
 
 ### xla_pcall
@@ -229,20 +194,21 @@ def xla_pcall_impl(fun, *args, **params):
   mesh_axis = params.pop('mesh_axis')  # e.g. 0 or 1
   assert not params
 
+  global mesh_spec
+  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
+
   flat_args, in_trees = unzip2(map(tree_flatten, args))
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
-  in_axes = canonicalize_axis_spec(in_trees, in_axes)
-  def canonicalized_out_axes():
-    return canonicalize_axis_spec([out_tree()], out_axes)[0]
-  fun = chunk_transform(fun, axis_name, in_axes, canonicalized_out_axes)
+  in_axes = canonicalize_in_axis_spec(in_trees, in_axes)
+  out_axes_thunk = lambda: canonicalize_out_axis_spec(out_tree(), out_axes)
+  fun = chunk_transform(fun, axis_name, in_axes, out_axes_thunk)
 
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec, *map(abstractify, flat_args))
-
+                                       mesh_spec_, *map(abstractify, flat_args))
   leaf_out = out_tree() is leaf
-  flat_ans = compiled_fun(leaf_out, canonicalized_out_axes(), *args)
+  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
 
   if leaf_out:
     return flat_ans
@@ -270,7 +236,7 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
 
   chunksize = axis_size // mesh_spec[mesh_axis]
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
-  device_groups = meshgroups(mesh_spec, mesh_axis)
+  device_groups = replica_groups(mesh_spec, mesh_axis)
 
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
@@ -283,17 +249,13 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
 
 def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
                        leaf_out, out_axes, *args):
-  assert False
-  axis_maps = [{axis_name : axes[i] for axis_name, axes in axis_map.items()}
-               for i in range(len(args))]
-  input_bufs = map(partial(shard_array, mesh_spec, mesh_map), axis_maps, args)
+  input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
   if leaf_out:
-    # TODO sharded device persistence, remove the .to_py()
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
-    return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
+    return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)
   else:
-    raise NotImplementedError  # TODO
+    raise NotImplementedError  # TODO zip*
 
 
 xla_pcall_p = core.Primitive('xla_pcall')","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 019155493..8a5bc63a0 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -33,9 +33,9 @@ from .. import linear_util as lu
 from ..abstract_arrays import ShapedArray
 from ..util import partial, unzip2, concatenate, safe_map, prod
 from ..lib import xla_bridge as xb
-from .xla import (flatten_fun, tree_flatten, build_tree, leaf, xla_shape,
+from .xla import (flatten_fun, tree_flatten, build_tree, xla_shape,
                   xla_destructure, translation_rule, abstractify,
-                  xla_shape_to_result_shape)
+                  xla_shape_to_result_shape, leaf, JTupleTreeDef)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 from .parallel import parallel_translation_rules
 from .batching import moveaxis
@@ -44,10 +44,14 @@ from . import parallel
 map = safe_map
 
 
+mesh_spec = None
+
+
 ### util
 
 
 def chunk_transform(fun, name, in_axes, out_axes_dst):
+  """"""Rewrite SPMD operations to act first on local chunks then cross-replica.""""""
   temp_name = object()  # TODO gensym
   fun = parallel.axisvar_split(fun, name, (temp_name, name))
   fun, out_axes_src = parallel.pmap_transform(fun, temp_name, in_axes)
@@ -56,10 +60,13 @@ def chunk_transform(fun, name, in_axes, out_axes_dst):
 
 @lu.transformation
 def move_output_axis_transform(src, dst, *args):
+  """"""Function transformation that moves output axes from src to dst.""""""
   ans = yield args
+  # TODO singleton or chunksize? i think chunksize...
   yield moveaxis(1, dst(), src(), ans)  # inserts singleton if src is None
 
 def chunk_aval(chunksize, aval, axis):
+  """"""Transform an abstract value's shape to have chunksize extent along axis.""""""
   if axis is None:
     return aval
   else:
@@ -67,116 +74,74 @@ def chunk_aval(chunksize, aval, axis):
     shape[axis] = chunksize
     return ShapedArray(tuple(shape), aval.dtype)
 
-def canonicalize_axis_spec(in_trees, spec):
+def canonicalize_in_axis_spec(in_trees, spec):
+  """"""Given argument list in_trees, canonicalize and flatten an in_axes spec.""""""
   spec = (spec,) * len(in_trees) if type(spec) is int else spec
   spec = map(build_axis_spec_tree, spec, in_trees)
-  spec = tuple(tree_util.tree_flatten(spec)[0])
-  return spec
+  return tuple(tree_util.tree_flatten(spec)[0])
 
-def build_axis_spec_tree(spec, in_tree):
-  if in_tree is leaf:
+def canonicalize_out_axis_spec(out_tree, spec):
+  """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
+  if out_tree is leaf:
+    return spec
+  else:
+    spec = build_axis_spec_tree(spec, out_tree)
+    return tuple(tree_util.tree_flatten(spec)[0])
+
+def build_axis_spec_tree(spec, tree):
+  """"""Given a JaxTuple treedef, canonicalize an axis spec for that tree.""""""
+  if tree is leaf:
     assert type(spec) is int
     return spec
-  elif type(in_tree) is JTupleTreeDef:
+  elif type(tree) is JTupleTreeDef:
     spec_type = type(spec)
     if spec_type is int:
-      return tuple(map(partial(build_axis_spec_tree, spec), in_tree.child_specs))
+      return tuple(map(partial(build_axis_spec_tree, spec), tree.child_specs))
     elif spec_type is tuple:
-      return tuple(map(build_axis_spec_tree, spec, in_tree.child_specs))
+      return tuple(map(build_axis_spec_tree, spec, tree.child_specs))
     else:
       raise TypeError(spec_type)
   else:
-    raise TypeError(type(in_tree))
+    raise TypeError(type(tree))
 
-def remove_mapped_dims(aval, *axes):
-  assert type(aval) is ShapedArray
-  axes = [d for d in axes if d is not None]
-  if len(set(axes)) != len(axes):
-    raise ValueError(""multiple names mapped to the same axis"")
-  shape = tuple(onp.delete(aval.shape, axes))
-  return ShapedArray(shape, aval.dtype)
+def shard_arg(mesh_spec, mesh_axis, axis, arg):
+  """"""Shard and device_put an input array argument along a logical axis.""""""
+  num_replicas = xb.get_replica_count()
+  if prod(mesh_spec) != num_replicas:
+    msg = ""mesh spec {} total size of {} doesn't match number of replicas {}.""
+    raise ValueError(msg.format(mesh_spec, prod(mesh_spec), num_replicas))
+  shards = split_array(arg, mesh_spec[mesh_axis], axis)
+  replica_shards = [shards[i] for i in shard_assignments(mesh_spec, mesh_axis)]
+  return map(xb.device_put, replica_shards, range(num_replicas))
 
-def meshgroups(mesh_spec, mesh_axis):
+def unshard_output(mesh_spec, mesh_axis, out_axis, out_shards):
+  """"""Collect and concatenate sharded device results.""""""
+  _, ids = onp.unique(shard_assignments(mesh_spec, mesh_axis), return_index=True)
+  shards = [out_shards[i] for i in ids]
+  return onp.concatenate(shards, out_axis)
+
+def shard_assignments(mesh_spec, mesh_axis):
+  """"""Given a mesh axis long which to shard data, compute replica assignments.""""""
+  indices_shape = [1] * len(mesh_spec)
+  indices_shape[mesh_axis] = mesh_spec[mesh_axis]
+  indices = onp.arange(mesh_spec[mesh_axis]).reshape(indices_shape)
+  return tuple(onp.broadcast_to(indices, mesh_spec).ravel())
+
+def replica_groups(mesh_spec, mesh_axis):
+  """"""Given a mesh axis along which to operate, compute XLA replica_groups.""""""
   groups = onp.split(onp.arange(prod(mesh_spec)).reshape(mesh_spec),
                      mesh_spec[mesh_axis], axis=mesh_axis)
   return tuple(tuple(group.ravel()) for group in groups)
 
-def shard_array(mesh_spec, mesh_map, axis_map, x):
-  # axis_map , e.g. {'i': 0, 'j': None}  (axis indices)
-  # mesh_map , e.g. {'i': 0, 'k': 2}     (mesh indices)
-  # mesh_spec, e.g. (2, 4, 4)
-  # return flat list of device buffers - one per replica
-  mesh_ndim = len(mesh_spec)
-  mesh_size = onp.prod(mesh_spec)
-  mesh_map_inverted = {v : k for k, v in mesh_map.items()}
-  ordered_idx_names = map(mesh_map_inverted.get, range(mesh_ndim))  # [i,None,k]
-  axes = map(axis_map.get, ordered_idx_names)
-  xs = list_ravel(unstack_axes(mesh_spec, axes, x))
-  return map(xb.device_put, xs, range(mesh_size))
-
-def unstack_axes(mesh_spec, axes, x):
-  # axes: list of ints. logical axes of x to split, ordered as in mesh_spec
-  #  e.g. zeros(4, 10, 2, 6)
-  # axes  (2, 0, None[broadcast_size=10])
-  # results in (8,) nested lists each with (10,6) arrays
-  if axes:
-    ax0 = axes[0]
-    recur = partial(unstack_axes, mesh_spec[1:], axes[1:])
-    if ax0 is None:
-      return [recur(x)]  * mesh_spec[0]
-    else:
-      assert x.shape[ax0] == mesh_spec[0]
-      xs = split_array(x, axes[0])
-      return list(map(recur, xs))
-  else:
-    return x
-
-def unshard_array(mesh_spec, mesh_map, axis_map, xs):
-  mesh_ndim = len(mesh_spec)
-  mesh_size = onp.prod(mesh_spec)
-  mesh_map_inverted = {v : k for k, v in mesh_map.items()}
-  ordered_idx_names = map(mesh_map_inverted.get, range(mesh_ndim))  # [i,None,k]
-  axes = map(axis_map.get, ordered_idx_names)
-
-  example_shard = xs[0]
-  num_splits = len([i for i in axes if i is not None])
-  shape = iter(example_shard.shape)
-  newshape = [1 if i in axes else next(shape)
-              for i in range(num_splits + example_shard.ndim)]
-
-  xs = [x.reshape(newshape) for x in xs]
-  xs = list_reshape(mesh_spec, iter(xs))
-  x = stack_axes(mesh_spec, axes, xs)
-  return x
-
-def list_reshape(mesh_spec, flat_xs):
-  if mesh_spec:
-    return [list_reshape(mesh_spec[1:], flat_xs) for _ in range(mesh_spec[0])]
-  else:
-    return next(flat_xs)
-
-def stack_axes(mesh_spec, axes, xs):
-  if mesh_spec:
-    if mesh_spec[0] is None:
-      return stack_axes(mesh_spec[1:], axes[1:], xs[0])
-    else:
-      components = map(partial(stack_axes, mesh_spec[1:], axes[1:]), xs)
-      return onp.concatenate(components, axis=axes[0])
-  else:
-    return xs
-
-def list_ravel(xs):
-  if isinstance(xs, list):
-    return concatenate(map(list_ravel, xs))
-  else:
-    return [xs]
-
-def split_array(x, axis):
+def split_array(x, num_splits, axis):
+  """"""A special-case of numpy.split implemented in terms of indexing.""""""
+  assert x.shape[axis] % num_splits == 0
+  split_size = x.shape[axis] // num_splits
   def get_nth_subarray(n):
     idx = [slice(None)] * x.ndim
-    idx[axis] = n
+    idx[axis] = slice(n * split_size, (n+1) * split_size)
     return x[tuple(idx)]
-  return map(get_nth_subarray, range(x.shape[axis]))
+  return map(get_nth_subarray, range(num_splits))
 
 
 ### xla_pcall
@@ -229,20 +194,21 @@ def xla_pcall_impl(fun, *args, **params):
   mesh_axis = params.pop('mesh_axis')  # e.g. 0 or 1
   assert not params
 
+  global mesh_spec
+  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
+
   flat_args, in_trees = unzip2(map(tree_flatten, args))
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
-  in_axes = canonicalize_axis_spec(in_trees, in_axes)
-  def canonicalized_out_axes():
-    return canonicalize_axis_spec([out_tree()], out_axes)[0]
-  fun = chunk_transform(fun, axis_name, in_axes, canonicalized_out_axes)
+  in_axes = canonicalize_in_axis_spec(in_trees, in_axes)
+  out_axes_thunk = lambda: canonicalize_out_axis_spec(out_tree(), out_axes)
+  fun = chunk_transform(fun, axis_name, in_axes, out_axes_thunk)
 
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec, *map(abstractify, flat_args))
-
+                                       mesh_spec_, *map(abstractify, flat_args))
   leaf_out = out_tree() is leaf
-  flat_ans = compiled_fun(leaf_out, canonicalized_out_axes(), *args)
+  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
 
   if leaf_out:
     return flat_ans
@@ -270,7 +236,7 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
 
   chunksize = axis_size // mesh_spec[mesh_axis]
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
-  device_groups = meshgroups(mesh_spec, mesh_axis)
+  device_groups = replica_groups(mesh_spec, mesh_axis)
 
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
@@ -283,17 +249,13 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
 
 def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
                        leaf_out, out_axes, *args):
-  assert False
-  axis_maps = [{axis_name : axes[i] for axis_name, axes in axis_map.items()}
-               for i in range(len(args))]
-  input_bufs = map(partial(shard_array, mesh_spec, mesh_map), axis_maps, args)
+  input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
   if leaf_out:
-    # TODO sharded device persistence, remove the .to_py()
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
-    return unshard_array(mesh_spec, mesh_map, out_axis_map, out_shards)
+    return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)
   else:
-    raise NotImplementedError  # TODO
+    raise NotImplementedError  # TODO zip*
 
 
 xla_pcall_p = core.Primitive('xla_pcall')",Yes
jax/interpreters/pxla.py,jax/interpreters/pxla.py,9c8018a4560d439b34ab12ddec5e3944b4f10d2e,da724d315927d0f478e014d62336ad5073c12d88,more cleanup,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 8a5bc63a0..5665d7615 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -118,7 +118,7 @@ def unshard_output(mesh_spec, mesh_axis, out_axis, out_shards):
   """"""Collect and concatenate sharded device results.""""""
   _, ids = onp.unique(shard_assignments(mesh_spec, mesh_axis), return_index=True)
   shards = [out_shards[i] for i in ids]
-  return onp.concatenate(shards, out_axis)
+  return onp.concatenate(shards, out_axis)  # TODO device persistence
 
 def shard_assignments(mesh_spec, mesh_axis):
   """"""Given a mesh axis long which to shard data, compute replica assignments.""""""
@@ -143,6 +143,24 @@ def split_array(x, num_splits, axis):
     return x[tuple(idx)]
   return map(get_nth_subarray, range(num_splits))
 
+def axis_size(mesh_spec, mesh_axis, in_axes, abstract_args):
+  """"""Compute the size of mapped axes, checking for errors.""""""
+  axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
+                if axis is not None}
+  if len(axis_sizes) == 0:
+    msg = ""axis name '{}' not bound to any input axes.""
+    raise ValueError(msg.format(axis_name))
+  elif len(axis_sizes) > 1:
+    msg = ""axis name '{}' bound to multiple axes with different sizes: {}.""
+    raise ValueError(msg.format(axis_name, axis_sizes))
+  else:
+    axis_size = axis_sizes.pop()
+    if axis_size % mesh_spec[mesh_axis]:
+      msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
+             ""axis index {} with size {}, which does not evenly divide {}."")
+      raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
+                                  mesh_spec[mesh_axis], axis_size))
+  return axis_size
 
 ### xla_pcall
 
@@ -218,26 +236,10 @@ def xla_pcall_impl(fun, *args, **params):
 @lu.memoize
 def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
                           *abstract_args):
-  axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
-                if axis is not None}
-  if len(axis_sizes) == 0:
-    msg = ""axis name '{}' not bound to any input axes.""
-    raise ValueError(msg.format(axis_name))
-  elif len(axis_sizes) > 1:
-    msg = ""axis name '{}' bound to multiple axes with different sizes: {}.""
-    raise ValueError(msg.format(axis_name, axis_sizes))
-  else:
-    axis_size = axis_sizes.pop()
-    if axis_size % mesh_spec[mesh_axis]:
-      msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
-             ""axis index {} with size {}, which does not evenly divide {}."")
-      raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
-                                  mesh_spec[mesh_axis], axis_size))
-
-  chunksize = axis_size // mesh_spec[mesh_axis]
+  ax_size = axis_size(mesh_spec, mesh_axis, in_axes, abstract_args)
+  chunksize = ax_size // mesh_spec[mesh_axis]
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
   device_groups = replica_groups(mesh_spec, mesh_axis)
-
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
@@ -252,6 +254,7 @@ def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
   input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
   if leaf_out:
+    # TODO device persistence
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)
   else:","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 8a5bc63a0..5665d7615 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -118,7 +118,7 @@ def unshard_output(mesh_spec, mesh_axis, out_axis, out_shards):
   """"""Collect and concatenate sharded device results.""""""
   _, ids = onp.unique(shard_assignments(mesh_spec, mesh_axis), return_index=True)
   shards = [out_shards[i] for i in ids]
-  return onp.concatenate(shards, out_axis)
+  return onp.concatenate(shards, out_axis)  # TODO device persistence
 
 def shard_assignments(mesh_spec, mesh_axis):
   """"""Given a mesh axis long which to shard data, compute replica assignments.""""""
@@ -143,6 +143,24 @@ def split_array(x, num_splits, axis):
     return x[tuple(idx)]
   return map(get_nth_subarray, range(num_splits))
 
+def axis_size(mesh_spec, mesh_axis, in_axes, abstract_args):
+  """"""Compute the size of mapped axes, checking for errors.""""""
+  axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
+                if axis is not None}
+  if len(axis_sizes) == 0:
+    msg = ""axis name '{}' not bound to any input axes.""
+    raise ValueError(msg.format(axis_name))
+  elif len(axis_sizes) > 1:
+    msg = ""axis name '{}' bound to multiple axes with different sizes: {}.""
+    raise ValueError(msg.format(axis_name, axis_sizes))
+  else:
+    axis_size = axis_sizes.pop()
+    if axis_size % mesh_spec[mesh_axis]:
+      msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
+             ""axis index {} with size {}, which does not evenly divide {}."")
+      raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
+                                  mesh_spec[mesh_axis], axis_size))
+  return axis_size
 
 ### xla_pcall
 
@@ -218,26 +236,10 @@ def xla_pcall_impl(fun, *args, **params):
 @lu.memoize
 def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
                           *abstract_args):
-  axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
-                if axis is not None}
-  if len(axis_sizes) == 0:
-    msg = ""axis name '{}' not bound to any input axes.""
-    raise ValueError(msg.format(axis_name))
-  elif len(axis_sizes) > 1:
-    msg = ""axis name '{}' bound to multiple axes with different sizes: {}.""
-    raise ValueError(msg.format(axis_name, axis_sizes))
-  else:
-    axis_size = axis_sizes.pop()
-    if axis_size % mesh_spec[mesh_axis]:
-      msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
-             ""axis index {} with size {}, which does not evenly divide {}."")
-      raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
-                                  mesh_spec[mesh_axis], axis_size))
-
-  chunksize = axis_size // mesh_spec[mesh_axis]
+  ax_size = axis_size(mesh_spec, mesh_axis, in_axes, abstract_args)
+  chunksize = ax_size // mesh_spec[mesh_axis]
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
   device_groups = replica_groups(mesh_spec, mesh_axis)
-
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
@@ -252,6 +254,7 @@ def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
   input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
   if leaf_out:
+    # TODO device persistence
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)
   else:",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,80b9bdc449c4f18bfb4f3e78793430ed28ceaabc,9c8018a4560d439b34ab12ddec5e3944b4f10d2e,"cleanup, fix chunksize bug","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 5665d7615..f0bc71812 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -50,20 +50,19 @@ mesh_spec = None
 ### util
 
 
-def chunk_transform(fun, name, in_axes, out_axes_dst):
+def chunk_transform(fun, chunksize, name, in_axes, out_axes_dst):
   """"""Rewrite SPMD operations to act first on local chunks then cross-replica.""""""
   temp_name = object()  # TODO gensym
   fun = parallel.axisvar_split(fun, name, (temp_name, name))
   fun, out_axes_src = parallel.pmap_transform(fun, temp_name, in_axes)
-  fun = move_output_axis_transform(fun, out_axes_src, out_axes_dst)
+  fun = move_output_axis_transform(fun, chunksize, out_axes_src, out_axes_dst)
   return fun
 
 @lu.transformation
-def move_output_axis_transform(src, dst, *args):
+def move_output_axis_transform(chunksize, src, dst, *args):
   """"""Function transformation that moves output axes from src to dst.""""""
   ans = yield args
-  # TODO singleton or chunksize? i think chunksize...
-  yield moveaxis(1, dst(), src(), ans)  # inserts singleton if src is None
+  yield moveaxis(chunksize, dst(), src(), ans)
 
 def chunk_aval(chunksize, aval, axis):
   """"""Transform an abstract value's shape to have chunksize extent along axis.""""""
@@ -143,8 +142,8 @@ def split_array(x, num_splits, axis):
     return x[tuple(idx)]
   return map(get_nth_subarray, range(num_splits))
 
-def axis_size(mesh_spec, mesh_axis, in_axes, abstract_args):
-  """"""Compute the size of mapped axes, checking for errors.""""""
+def chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args):
+  """"""Compute the chunk size for mapped axes, checking for errors.""""""
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -160,7 +159,8 @@ def axis_size(mesh_spec, mesh_axis, in_axes, abstract_args):
              ""axis index {} with size {}, which does not evenly divide {}."")
       raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
                                   mesh_spec[mesh_axis], axis_size))
-  return axis_size
+
+  return axis_size // mesh_spec[mesh_axis]
 
 ### xla_pcall
 
@@ -219,12 +219,14 @@ def xla_pcall_impl(fun, *args, **params):
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
+  abstract_args = map(abstractify, flat_args)
   in_axes = canonicalize_in_axis_spec(in_trees, in_axes)
+  chunksize = chunk_size(mesh_spec_, mesh_axis, in_axes, abstract_args)
   out_axes_thunk = lambda: canonicalize_out_axis_spec(out_tree(), out_axes)
-  fun = chunk_transform(fun, axis_name, in_axes, out_axes_thunk)
+  fun = chunk_transform(fun, chunksize, axis_name, in_axes, out_axes_thunk)
 
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec_, *map(abstractify, flat_args))
+                                       mesh_spec_, *abstract_args)
   leaf_out = out_tree() is leaf
   flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
 
@@ -236,8 +238,7 @@ def xla_pcall_impl(fun, *args, **params):
 @lu.memoize
 def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
                           *abstract_args):
-  ax_size = axis_size(mesh_spec, mesh_axis, in_axes, abstract_args)
-  chunksize = ax_size // mesh_spec[mesh_axis]
+  chunksize = chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args)
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
   device_groups = replica_groups(mesh_spec, mesh_axis)
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 5665d7615..f0bc71812 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -50,20 +50,19 @@ mesh_spec = None
 ### util
 
 
-def chunk_transform(fun, name, in_axes, out_axes_dst):
+def chunk_transform(fun, chunksize, name, in_axes, out_axes_dst):
   """"""Rewrite SPMD operations to act first on local chunks then cross-replica.""""""
   temp_name = object()  # TODO gensym
   fun = parallel.axisvar_split(fun, name, (temp_name, name))
   fun, out_axes_src = parallel.pmap_transform(fun, temp_name, in_axes)
-  fun = move_output_axis_transform(fun, out_axes_src, out_axes_dst)
+  fun = move_output_axis_transform(fun, chunksize, out_axes_src, out_axes_dst)
   return fun
 
 @lu.transformation
-def move_output_axis_transform(src, dst, *args):
+def move_output_axis_transform(chunksize, src, dst, *args):
   """"""Function transformation that moves output axes from src to dst.""""""
   ans = yield args
-  # TODO singleton or chunksize? i think chunksize...
-  yield moveaxis(1, dst(), src(), ans)  # inserts singleton if src is None
+  yield moveaxis(chunksize, dst(), src(), ans)
 
 def chunk_aval(chunksize, aval, axis):
   """"""Transform an abstract value's shape to have chunksize extent along axis.""""""
@@ -143,8 +142,8 @@ def split_array(x, num_splits, axis):
     return x[tuple(idx)]
   return map(get_nth_subarray, range(num_splits))
 
-def axis_size(mesh_spec, mesh_axis, in_axes, abstract_args):
-  """"""Compute the size of mapped axes, checking for errors.""""""
+def chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args):
+  """"""Compute the chunk size for mapped axes, checking for errors.""""""
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -160,7 +159,8 @@ def axis_size(mesh_spec, mesh_axis, in_axes, abstract_args):
              ""axis index {} with size {}, which does not evenly divide {}."")
       raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
                                   mesh_spec[mesh_axis], axis_size))
-  return axis_size
+
+  return axis_size // mesh_spec[mesh_axis]
 
 ### xla_pcall
 
@@ -219,12 +219,14 @@ def xla_pcall_impl(fun, *args, **params):
   flat_args = concatenate(flat_args)
   fun, out_tree = flatten_fun(fun, in_trees)
 
+  abstract_args = map(abstractify, flat_args)
   in_axes = canonicalize_in_axis_spec(in_trees, in_axes)
+  chunksize = chunk_size(mesh_spec_, mesh_axis, in_axes, abstract_args)
   out_axes_thunk = lambda: canonicalize_out_axis_spec(out_tree(), out_axes)
-  fun = chunk_transform(fun, axis_name, in_axes, out_axes_thunk)
+  fun = chunk_transform(fun, chunksize, axis_name, in_axes, out_axes_thunk)
 
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec_, *map(abstractify, flat_args))
+                                       mesh_spec_, *abstract_args)
   leaf_out = out_tree() is leaf
   flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
 
@@ -236,8 +238,7 @@ def xla_pcall_impl(fun, *args, **params):
 @lu.memoize
 def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
                           *abstract_args):
-  ax_size = axis_size(mesh_spec, mesh_axis, in_axes, abstract_args)
-  chunksize = ax_size // mesh_spec[mesh_axis]
+  chunksize = chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args)
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
   device_groups = replica_groups(mesh_spec, mesh_axis)
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,a5661ce1b276ddd1de35364c291214365c524297,80b9bdc449c4f18bfb4f3e78793430ed28ceaabc,hmmm leak bug somewhere,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 87232eb8b..552d4767f 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -58,7 +58,7 @@ def pmap_transform(name, axes, *vals):
     ans = yield in_tracers
     out_tracer = trace.full_raise(ans)
     out_val, out_axis = out_tracer.val, out_tracer.axis
-    del master
+    del master, out_tracer
   yield out_val, out_axis
 
 @lu.transformation_with_aux
@@ -213,7 +213,7 @@ def axisvar_split(name, new_names, *args):
     ans = yield in_tracers
     out_tracer = trace.full_raise(ans)
     out_val = out_tracer.val
-    del master
+    del master, out_tracer
   yield out_val
 
 class SplitTracer(Tracer):
@@ -270,6 +270,7 @@ class SplitTrace(Trace):
         return SplitTracer(self, name, new_names, val_out)
 
   def process_call(self, call_primitive, f, tracers, params):
+    import ipdb; ipdb.set_trace()
     names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
     if all(name is None for name in names_in):
       return call_primitive.bind(f, *vals, **params)
@@ -281,6 +282,7 @@ class SplitTrace(Trace):
       return SplitTracer(self, name, new_names, val_out)
 
   def post_process_call(self, _, out_tracer):
+    import ipdb; ipdb.set_trace()
     name, new_names, val = out_tracer.name, out_tracer.new_names, out_tracer.val
     master = self.master
     def todo(x):
@@ -329,7 +331,7 @@ def papply_transform(name, args, axes):
     out_tracer = yield in_tracers
     out_tracer = trace.full_raise(out_tracer)
     out_val = out_tracer.val
-    del master
+    del master, out_tracer
   yield out_val
 
 class PapplyTracer(Tracer):","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 87232eb8b..552d4767f 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -58,7 +58,7 @@ def pmap_transform(name, axes, *vals):
     ans = yield in_tracers
     out_tracer = trace.full_raise(ans)
     out_val, out_axis = out_tracer.val, out_tracer.axis
-    del master
+    del master, out_tracer
   yield out_val, out_axis
 
 @lu.transformation_with_aux
@@ -213,7 +213,7 @@ def axisvar_split(name, new_names, *args):
     ans = yield in_tracers
     out_tracer = trace.full_raise(ans)
     out_val = out_tracer.val
-    del master
+    del master, out_tracer
   yield out_val
 
 class SplitTracer(Tracer):
@@ -270,6 +270,7 @@ class SplitTrace(Trace):
         return SplitTracer(self, name, new_names, val_out)
 
   def process_call(self, call_primitive, f, tracers, params):
+    import ipdb; ipdb.set_trace()
     names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
     if all(name is None for name in names_in):
       return call_primitive.bind(f, *vals, **params)
@@ -281,6 +282,7 @@ class SplitTrace(Trace):
       return SplitTracer(self, name, new_names, val_out)
 
   def post_process_call(self, _, out_tracer):
+    import ipdb; ipdb.set_trace()
     name, new_names, val = out_tracer.name, out_tracer.new_names, out_tracer.val
     master = self.master
     def todo(x):
@@ -329,7 +331,7 @@ def papply_transform(name, args, axes):
     out_tracer = yield in_tracers
     out_tracer = trace.full_raise(out_tracer)
     out_val = out_tracer.val
-    del master
+    del master, out_tracer
   yield out_val
 
 class PapplyTracer(Tracer):",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,a5661ce1b276ddd1de35364c291214365c524297,80b9bdc449c4f18bfb4f3e78793430ed28ceaabc,hmmm leak bug somewhere,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index f0bc71812..14982be26 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -244,6 +244,7 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
+    import ipdb; ipdb.set_trace()
     assert not env
     compiled, result_shape = compile_replicated(jaxpr, device_groups,
                                                 consts, *abstract_args)","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index f0bc71812..14982be26 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -244,6 +244,7 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
+    import ipdb; ipdb.set_trace()
     assert not env
     compiled, result_shape = compile_replicated(jaxpr, device_groups,
                                                 consts, *abstract_args)",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,a5661ce1b276ddd1de35364c291214365c524297,add leak_bug.py for repro,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 552d4767f..6ec908e93 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -270,7 +270,6 @@ class SplitTrace(Trace):
         return SplitTracer(self, name, new_names, val_out)
 
   def process_call(self, call_primitive, f, tracers, params):
-    import ipdb; ipdb.set_trace()
     names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
     if all(name is None for name in names_in):
       return call_primitive.bind(f, *vals, **params)
@@ -282,7 +281,6 @@ class SplitTrace(Trace):
       return SplitTracer(self, name, new_names, val_out)
 
   def post_process_call(self, _, out_tracer):
-    import ipdb; ipdb.set_trace()
     name, new_names, val = out_tracer.name, out_tracer.new_names, out_tracer.val
     master = self.master
     def todo(x):","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 552d4767f..6ec908e93 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -270,7 +270,6 @@ class SplitTrace(Trace):
         return SplitTracer(self, name, new_names, val_out)
 
   def process_call(self, call_primitive, f, tracers, params):
-    import ipdb; ipdb.set_trace()
     names_in, vals_in = unzip2((t.name, t.val) for t in tracers)
     if all(name is None for name in names_in):
       return call_primitive.bind(f, *vals, **params)
@@ -282,7 +281,6 @@ class SplitTrace(Trace):
       return SplitTracer(self, name, new_names, val_out)
 
   def post_process_call(self, _, out_tracer):
-    import ipdb; ipdb.set_trace()
     name, new_names, val = out_tracer.name, out_tracer.new_names, out_tracer.val
     master = self.master
     def todo(x):",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,a5661ce1b276ddd1de35364c291214365c524297,add leak_bug.py for repro,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 14982be26..f0bc71812 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -244,7 +244,6 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
-    import ipdb; ipdb.set_trace()
     assert not env
     compiled, result_shape = compile_replicated(jaxpr, device_groups,
                                                 consts, *abstract_args)","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 14982be26..f0bc71812 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -244,7 +244,6 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
   with core.new_master(JaxprTrace, True) as master:
     jaxpr, (pval, consts, env) = trace_to_subjaxpr(fun, master).call_wrapped(pvals)
-    import ipdb; ipdb.set_trace()
     assert not env
     compiled, result_shape = compile_replicated(jaxpr, device_groups,
                                                 consts, *abstract_args)",No
jax/experimental/stax.py,jax/experimental/stax.py,274a63e5183e00d05de3526927ac97725f65ae5f,57fc3a4cca22a855d983277e78e47b7bac1d0b2a,Fix implementation of average pooling to align the window element counts with the spatial dimensions.,"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 4a2e4abb7..373f9f1b7 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -178,9 +178,9 @@ SumPool = _pooling_layer(lax.add, 0.)
 
 def _normalize_by_window_size(dims, strides, padding):
   def rescale(outputs, inputs):
-    one = np.ones(inputs.shape[1:3], dtype=inputs.dtype)
+    one = np.ones(inputs.shape[1:-1], dtype=inputs.dtype)
     window_sizes = lax.reduce_window(one, 0., lax.add, dims, strides, padding)
-    return outputs / window_sizes
+    return outputs / window_sizes[..., np.newaxis]
   return rescale
 AvgPool = _pooling_layer(lax.add, 0., _normalize_by_window_size)
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 4a2e4abb7..373f9f1b7 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -178,9 +178,9 @@ SumPool = _pooling_layer(lax.add, 0.)
 
 def _normalize_by_window_size(dims, strides, padding):
   def rescale(outputs, inputs):
-    one = np.ones(inputs.shape[1:3], dtype=inputs.dtype)
+    one = np.ones(inputs.shape[1:-1], dtype=inputs.dtype)
     window_sizes = lax.reduce_window(one, 0., lax.add, dims, strides, padding)
-    return outputs / window_sizes
+    return outputs / window_sizes[..., np.newaxis]
   return rescale
 AvgPool = _pooling_layer(lax.add, 0., _normalize_by_window_size)
 ",No
tests/stax_test.py,tests/stax_test.py,274a63e5183e00d05de3526927ac97725f65ae5f,57fc3a4cca22a855d983277e78e47b7bac1d0b2a,Fix implementation of average pooling to align the window element counts with the spatial dimensions.,"diff --git a/tests/stax_test.py b/tests/stax_test.py
index 245c9b24e..e55e06dc3 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -100,16 +100,20 @@ class StaxTest(jtu.JaxTestCase):
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
-                        .format(window_shape, padding, strides, input_shape),
+                        ""_maxpool={}""
+                        .format(window_shape, padding, strides, input_shape,
+                                max_pool),
        ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
-       ""input_shape"": input_shape}
+       ""input_shape"": input_shape, ""max_pool"": max_pool}
       for window_shape in [(1, 1), (2, 3)]
       for padding in [""VALID""]
       for strides in [None, (2, 1)]
-      for input_shape in [(2, 5, 6, 1)]))
-  def testPoolingShape(self, window_shape, padding, strides, input_shape):
-    init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
-                                       strides=strides)
+      for input_shape in [(2, 5, 6, 1)]
+      for max_pool in [False, True]))
+  def testPoolingShape(self, window_shape, padding, strides, input_shape,
+                       max_pool):
+    layer = stax.MaxPool if max_pool else stax.AvgPool
+    init_fun, apply_fun = layer(window_shape, padding=padding, strides=strides)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
   @parameterized.named_parameters(jtu.cases_from_list(","diff --git a/tests/stax_test.py b/tests/stax_test.py
index 245c9b24e..e55e06dc3 100644
--- a/tests/stax_test.py
+++ b/tests/stax_test.py
@@ -100,16 +100,20 @@ class StaxTest(jtu.JaxTestCase):
 
   @parameterized.named_parameters(jtu.cases_from_list(
       {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
-                        .format(window_shape, padding, strides, input_shape),
+                        ""_maxpool={}""
+                        .format(window_shape, padding, strides, input_shape,
+                                max_pool),
        ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
-       ""input_shape"": input_shape}
+       ""input_shape"": input_shape, ""max_pool"": max_pool}
       for window_shape in [(1, 1), (2, 3)]
       for padding in [""VALID""]
       for strides in [None, (2, 1)]
-      for input_shape in [(2, 5, 6, 1)]))
-  def testPoolingShape(self, window_shape, padding, strides, input_shape):
-    init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
-                                       strides=strides)
+      for input_shape in [(2, 5, 6, 1)]
+      for max_pool in [False, True]))
+  def testPoolingShape(self, window_shape, padding, strides, input_shape,
+                       max_pool):
+    layer = stax.MaxPool if max_pool else stax.AvgPool
+    init_fun, apply_fun = layer(window_shape, padding=padding, strides=strides)
     _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
 
   @parameterized.named_parameters(jtu.cases_from_list(",No
jax/api.py,jax/api.py,780106f892406590cf6c974e9c19caf59558e01c,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,"moving pxla flattening/chunking to api.py, wip","diff --git a/jax/api.py b/jax/api.py
index ddec1d539..70c2ae275 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -32,8 +32,8 @@ import numpy as onp
 from . import core
 from . import linear_util as lu
 from .core import pack, eval_jaxpr
-from .api_util import (pytree_fun_to_jaxtupletree_fun, apply_jaxtree_fun,
-                       pytree_to_jaxtupletree, wraps)
+from .api_util import (pytree_fun_to_jaxtupletree_fun, pytree_to_jaxtupletree,
+                       pytree_fun_to_flatjaxtuple_fun, apply_jaxtree_fun, wraps)
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef,
                         tree_map, tree_flatten, tree_unflatten, tree_structure,
                         tree_transpose)
@@ -258,19 +258,20 @@ def vmap(fun, in_axes=0, out_axes=0):
   return batched_fun
 
 
-def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0, static_argnums=()):
+def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0):
   """"""Set up SPMD function for JIT compilation and parallel execution with XLA.""""""
   @wraps(fun)
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
-    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
-    f, dyn_args = argnums_partial(f, dyn_argnums, args)
-    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
-    check_args(args_flat)
-    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    out_flat = pxla.xla_pcall(jaxtree_fun, *args_flat,
-                              axis_name=axis_name, in_axes=in_axes,
-                              out_axes=out_axes, mesh_axis=mesh_axis)
+    args_flat, in_tree = tree_flatten(args)
+    f, out_tree = pytree_fun_to_flatjaxtuple_fun(f, in_tree)
+    in_axes_ = pxla.canonicalize_in_axis_spec(in_tree, in_axes)
+    out_axes_ = lambda: pxla.canonicalize_out_axis_spec(out_tree(), out_axes)
+    chunksize = pxla.chunk_size(mesh_axis, in_axes_, args_flat)
+    f = pxla.chunk_transform(f, chunksize, axis_name, in_axes_, out_axes_)
+    out_flat = pxla.xla_pcall(f, *args_flat, axis_name=axis_name,
+                              in_axes=in_axes_, out_axes=out_axes_,
+                              mesh_axis=mesh_axis)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)","diff --git a/jax/api.py b/jax/api.py
index ddec1d539..70c2ae275 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -32,8 +32,8 @@ import numpy as onp
 from . import core
 from . import linear_util as lu
 from .core import pack, eval_jaxpr
-from .api_util import (pytree_fun_to_jaxtupletree_fun, apply_jaxtree_fun,
-                       pytree_to_jaxtupletree, wraps)
+from .api_util import (pytree_fun_to_jaxtupletree_fun, pytree_to_jaxtupletree,
+                       pytree_fun_to_flatjaxtuple_fun, apply_jaxtree_fun, wraps)
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef,
                         tree_map, tree_flatten, tree_unflatten, tree_structure,
                         tree_transpose)
@@ -258,19 +258,20 @@ def vmap(fun, in_axes=0, out_axes=0):
   return batched_fun
 
 
-def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0, static_argnums=()):
+def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0):
   """"""Set up SPMD function for JIT compilation and parallel execution with XLA.""""""
   @wraps(fun)
   def f_jitted(*args, **kwargs):
     f = lu.wrap_init(fun, kwargs)
-    dyn_argnums = [i for i in range(len(args)) if i not in static_argnums]
-    f, dyn_args = argnums_partial(f, dyn_argnums, args)
-    args_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, dyn_args))
-    check_args(args_flat)
-    jaxtree_fun, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    out_flat = pxla.xla_pcall(jaxtree_fun, *args_flat,
-                              axis_name=axis_name, in_axes=in_axes,
-                              out_axes=out_axes, mesh_axis=mesh_axis)
+    args_flat, in_tree = tree_flatten(args)
+    f, out_tree = pytree_fun_to_flatjaxtuple_fun(f, in_tree)
+    in_axes_ = pxla.canonicalize_in_axis_spec(in_tree, in_axes)
+    out_axes_ = lambda: pxla.canonicalize_out_axis_spec(out_tree(), out_axes)
+    chunksize = pxla.chunk_size(mesh_axis, in_axes_, args_flat)
+    f = pxla.chunk_transform(f, chunksize, axis_name, in_axes_, out_axes_)
+    out_flat = pxla.xla_pcall(f, *args_flat, axis_name=axis_name,
+                              in_axes=in_axes_, out_axes=out_axes_,
+                              mesh_axis=mesh_axis)
     return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)",No
jax/api_util.py,jax/api_util.py,780106f892406590cf6c974e9c19caf59558e01c,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,"moving pxla flattening/chunking to api.py, wip","diff --git a/jax/api_util.py b/jax/api_util.py
index f2d7dc99d..5134011dd 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -17,7 +17,8 @@ from __future__ import division
 from __future__ import print_function
 
 from .core import pack
-from .tree_util import build_tree, process_pytree
+from .tree_util import (build_tree, process_pytree, tree_flatten,
+                        tree_unflatten, leaf)
 from .linear_util import transformation_with_aux
 from .util import safe_map, unzip2, partial, curry
 
@@ -39,10 +40,10 @@ def get_doc(fun): return getattr(fun, ""__doc__"", """")
 
 
 @transformation_with_aux
-def pytree_fun_to_jaxtupletree_fun(in_trees, *args, **kwargs):
+def pytree_fun_to_jaxtupletree_fun(in_trees, *args):
   py_args = map(build_tree, in_trees, args)
   ans = yield py_args
-  yield process_pytree(pack, ans)
+  yield pytree_to_jaxtupletree(ans)
 
 def apply_jaxtree_fun(fun, io_tree, *py_args):
   in_trees_expected, out_tree = io_tree
@@ -55,3 +56,14 @@ def apply_jaxtree_fun(fun, io_tree, *py_args):
   return build_tree(out_tree, ans)
 
 pytree_to_jaxtupletree = partial(process_pytree, pack)
+
+
+@transformation_with_aux
+def pytree_fun_to_flatjaxtuple_fun(in_tree, *args):
+  py_args = tuple(tree_unflatten(in_tree, args))
+  ans = yield py_args
+  flat_ans, out_tree = tree_flatten(ans)
+  if out_tree is leaf:
+    yield flat_ans[0], out_tree
+  else:
+    yield pack(flat_ans), out_tree","diff --git a/jax/api_util.py b/jax/api_util.py
index f2d7dc99d..5134011dd 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -17,7 +17,8 @@ from __future__ import division
 from __future__ import print_function
 
 from .core import pack
-from .tree_util import build_tree, process_pytree
+from .tree_util import (build_tree, process_pytree, tree_flatten,
+                        tree_unflatten, leaf)
 from .linear_util import transformation_with_aux
 from .util import safe_map, unzip2, partial, curry
 
@@ -39,10 +40,10 @@ def get_doc(fun): return getattr(fun, ""__doc__"", """")
 
 
 @transformation_with_aux
-def pytree_fun_to_jaxtupletree_fun(in_trees, *args, **kwargs):
+def pytree_fun_to_jaxtupletree_fun(in_trees, *args):
   py_args = map(build_tree, in_trees, args)
   ans = yield py_args
-  yield process_pytree(pack, ans)
+  yield pytree_to_jaxtupletree(ans)
 
 def apply_jaxtree_fun(fun, io_tree, *py_args):
   in_trees_expected, out_tree = io_tree
@@ -55,3 +56,14 @@ def apply_jaxtree_fun(fun, io_tree, *py_args):
   return build_tree(out_tree, ans)
 
 pytree_to_jaxtupletree = partial(process_pytree, pack)
+
+
+@transformation_with_aux
+def pytree_fun_to_flatjaxtuple_fun(in_tree, *args):
+  py_args = tuple(tree_unflatten(in_tree, args))
+  ans = yield py_args
+  flat_ans, out_tree = tree_flatten(ans)
+  if out_tree is leaf:
+    yield flat_ans[0], out_tree
+  else:
+    yield pack(flat_ans), out_tree",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,780106f892406590cf6c974e9c19caf59558e01c,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,"moving pxla flattening/chunking to api.py, wip","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 6ec908e93..1b536888b 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -155,7 +155,7 @@ class PmapTrace(Trace):
     return val, todo
 
   def pack(self, tracers):
-    vals = pack([t.val for t in tracers])
+    vals = core.pack([t.val for t in tracers])
     axis = tuple(t.axis for t in tracers)
     name = next(t.name for t in tracers if t.name)
     return PmapTracer(self, name, vals, axis)
@@ -290,7 +290,7 @@ class SplitTrace(Trace):
     return val, todo
 
   def pack(self, tracers):
-    vals = pack([t.val for t in tracers])
+    vals = core.pack([t.val for t in tracers])
     name = next(t.name for t in tracers if t.name is not None)
     new_names = next(t.new_names for t in tracers if t.name is not None)
     return SplitTracer(self, name, new_names, vals)","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 6ec908e93..1b536888b 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -155,7 +155,7 @@ class PmapTrace(Trace):
     return val, todo
 
   def pack(self, tracers):
-    vals = pack([t.val for t in tracers])
+    vals = core.pack([t.val for t in tracers])
     axis = tuple(t.axis for t in tracers)
     name = next(t.name for t in tracers if t.name)
     return PmapTracer(self, name, vals, axis)
@@ -290,7 +290,7 @@ class SplitTrace(Trace):
     return val, todo
 
   def pack(self, tracers):
-    vals = pack([t.val for t in tracers])
+    vals = core.pack([t.val for t in tracers])
     name = next(t.name for t in tracers if t.name is not None)
     new_names = next(t.new_names for t in tracers if t.name is not None)
     return SplitTracer(self, name, new_names, vals)",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,780106f892406590cf6c974e9c19caf59558e01c,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,"moving pxla flattening/chunking to api.py, wip","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index f0bc71812..4159ebf69 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -32,10 +32,11 @@ from .. import tree_util
 from .. import linear_util as lu
 from ..abstract_arrays import ShapedArray
 from ..util import partial, unzip2, concatenate, safe_map, prod
+from ..tree_util import leaf
 from ..lib import xla_bridge as xb
 from .xla import (flatten_fun, tree_flatten, build_tree, xla_shape,
                   xla_destructure, translation_rule, abstractify,
-                  xla_shape_to_result_shape, leaf, JTupleTreeDef)
+                  xla_shape_to_result_shape)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 from .parallel import parallel_translation_rules
 from .batching import moveaxis
@@ -73,35 +74,32 @@ def chunk_aval(chunksize, aval, axis):
     shape[axis] = chunksize
     return ShapedArray(tuple(shape), aval.dtype)
 
-def canonicalize_in_axis_spec(in_trees, spec):
+def canonicalize_in_axis_spec(in_tree, spec_tree_prefix):
   """"""Given argument list in_trees, canonicalize and flatten an in_axes spec.""""""
-  spec = (spec,) * len(in_trees) if type(spec) is int else spec
-  spec = map(build_axis_spec_tree, spec, in_trees)
-  return tuple(tree_util.tree_flatten(spec)[0])
+  spec_tree = build_axis_spec_tree(spec_tree_prefix, in_tree)
+  return tuple(tree_util.tree_flatten(spec_tree)[0])
 
-def canonicalize_out_axis_spec(out_tree, spec):
+def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
   """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
   if out_tree is leaf:
-    return spec
+    return spec_tree_prefix
   else:
-    spec = build_axis_spec_tree(spec, out_tree)
+    spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)
     return tuple(tree_util.tree_flatten(spec)[0])
 
-def build_axis_spec_tree(spec, tree):
-  """"""Given a JaxTuple treedef, canonicalize an axis spec for that tree.""""""
-  if tree is leaf:
+def build_axis_spec_tree(spec, treedef):
+  """"""Given a JaxTuple treedef, canonicalize an axis spec for that treedef.""""""
+  if treedef is leaf:
     assert type(spec) is int
     return spec
-  elif type(tree) is JTupleTreeDef:
+  else:
     spec_type = type(spec)
     if spec_type is int:
-      return tuple(map(partial(build_axis_spec_tree, spec), tree.child_specs))
+      return tuple(map(partial(build_axis_spec_tree, spec), treedef.children))
     elif spec_type is tuple:
-      return tuple(map(build_axis_spec_tree, spec, tree.child_specs))
+      return tuple(map(build_axis_spec_tree, spec, treedef.children))
     else:
       raise TypeError(spec_type)
-  else:
-    raise TypeError(type(tree))
 
 def shard_arg(mesh_spec, mesh_axis, axis, arg):
   """"""Shard and device_put an input array argument along a logical axis.""""""
@@ -142,8 +140,10 @@ def split_array(x, num_splits, axis):
     return x[tuple(idx)]
   return map(get_nth_subarray, range(num_splits))
 
-def chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args):
+def chunk_size(mesh_axis, in_axes, abstract_args):
   """"""Compute the chunk size for mapped axes, checking for errors.""""""
+  global mesh_spec
+  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -154,13 +154,13 @@ def chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args):
     raise ValueError(msg.format(axis_name, axis_sizes))
   else:
     axis_size = axis_sizes.pop()
-    if axis_size % mesh_spec[mesh_axis]:
+    if axis_size % mesh_spec_[mesh_axis]:
       msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
              ""axis index {} with size {}, which does not evenly divide {}."")
       raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
-                                  mesh_spec[mesh_axis], axis_size))
+                                  mesh_spec_[mesh_axis], axis_size))
 
-  return axis_size // mesh_spec[mesh_axis]
+  return axis_size // mesh_spec_[mesh_axis]
 
 ### xla_pcall
 
@@ -215,30 +215,14 @@ def xla_pcall_impl(fun, *args, **params):
   global mesh_spec
   mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
 
-  flat_args, in_trees = unzip2(map(tree_flatten, args))
-  flat_args = concatenate(flat_args)
-  fun, out_tree = flatten_fun(fun, in_trees)
-
-  abstract_args = map(abstractify, flat_args)
-  in_axes = canonicalize_in_axis_spec(in_trees, in_axes)
-  chunksize = chunk_size(mesh_spec_, mesh_axis, in_axes, abstract_args)
-  out_axes_thunk = lambda: canonicalize_out_axis_spec(out_tree(), out_axes)
-  fun = chunk_transform(fun, chunksize, axis_name, in_axes, out_axes_thunk)
-
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec_, *abstract_args)
-  leaf_out = out_tree() is leaf
-  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
-
-  if leaf_out:
-    return flat_ans
-  else:
-    return build_tree(iter(flat_ans), out_tree())
+                                       mesh_spec_, *map(abstractify, args))
+  return compiled_fun(out_axes(), *args)
 
 @lu.memoize
 def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
                           *abstract_args):
-  chunksize = chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args)
+  chunksize = chunk_size(mesh_axis, in_axes, abstract_args)
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
   device_groups = replica_groups(mesh_spec, mesh_axis)
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
@@ -251,10 +235,10 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
   return partial(execute_replicated, in_axes, mesh_axis, mesh_spec, compiled, pval)
 
 def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
-                       leaf_out, out_axes, *args):
+                       out_axes, *args):
   input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
-  if leaf_out:
+  if True:  # TODO
     # TODO device persistence
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index f0bc71812..4159ebf69 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -32,10 +32,11 @@ from .. import tree_util
 from .. import linear_util as lu
 from ..abstract_arrays import ShapedArray
 from ..util import partial, unzip2, concatenate, safe_map, prod
+from ..tree_util import leaf
 from ..lib import xla_bridge as xb
 from .xla import (flatten_fun, tree_flatten, build_tree, xla_shape,
                   xla_destructure, translation_rule, abstractify,
-                  xla_shape_to_result_shape, leaf, JTupleTreeDef)
+                  xla_shape_to_result_shape)
 from .partial_eval import trace_to_subjaxpr, merge_pvals, JaxprTrace, PartialVal
 from .parallel import parallel_translation_rules
 from .batching import moveaxis
@@ -73,35 +74,32 @@ def chunk_aval(chunksize, aval, axis):
     shape[axis] = chunksize
     return ShapedArray(tuple(shape), aval.dtype)
 
-def canonicalize_in_axis_spec(in_trees, spec):
+def canonicalize_in_axis_spec(in_tree, spec_tree_prefix):
   """"""Given argument list in_trees, canonicalize and flatten an in_axes spec.""""""
-  spec = (spec,) * len(in_trees) if type(spec) is int else spec
-  spec = map(build_axis_spec_tree, spec, in_trees)
-  return tuple(tree_util.tree_flatten(spec)[0])
+  spec_tree = build_axis_spec_tree(spec_tree_prefix, in_tree)
+  return tuple(tree_util.tree_flatten(spec_tree)[0])
 
-def canonicalize_out_axis_spec(out_tree, spec):
+def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
   """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
   if out_tree is leaf:
-    return spec
+    return spec_tree_prefix
   else:
-    spec = build_axis_spec_tree(spec, out_tree)
+    spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)
     return tuple(tree_util.tree_flatten(spec)[0])
 
-def build_axis_spec_tree(spec, tree):
-  """"""Given a JaxTuple treedef, canonicalize an axis spec for that tree.""""""
-  if tree is leaf:
+def build_axis_spec_tree(spec, treedef):
+  """"""Given a JaxTuple treedef, canonicalize an axis spec for that treedef.""""""
+  if treedef is leaf:
     assert type(spec) is int
     return spec
-  elif type(tree) is JTupleTreeDef:
+  else:
     spec_type = type(spec)
     if spec_type is int:
-      return tuple(map(partial(build_axis_spec_tree, spec), tree.child_specs))
+      return tuple(map(partial(build_axis_spec_tree, spec), treedef.children))
     elif spec_type is tuple:
-      return tuple(map(build_axis_spec_tree, spec, tree.child_specs))
+      return tuple(map(build_axis_spec_tree, spec, treedef.children))
     else:
       raise TypeError(spec_type)
-  else:
-    raise TypeError(type(tree))
 
 def shard_arg(mesh_spec, mesh_axis, axis, arg):
   """"""Shard and device_put an input array argument along a logical axis.""""""
@@ -142,8 +140,10 @@ def split_array(x, num_splits, axis):
     return x[tuple(idx)]
   return map(get_nth_subarray, range(num_splits))
 
-def chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args):
+def chunk_size(mesh_axis, in_axes, abstract_args):
   """"""Compute the chunk size for mapped axes, checking for errors.""""""
+  global mesh_spec
+  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -154,13 +154,13 @@ def chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args):
     raise ValueError(msg.format(axis_name, axis_sizes))
   else:
     axis_size = axis_sizes.pop()
-    if axis_size % mesh_spec[mesh_axis]:
+    if axis_size % mesh_spec_[mesh_axis]:
       msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
              ""axis index {} with size {}, which does not evenly divide {}."")
       raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
-                                  mesh_spec[mesh_axis], axis_size))
+                                  mesh_spec_[mesh_axis], axis_size))
 
-  return axis_size // mesh_spec[mesh_axis]
+  return axis_size // mesh_spec_[mesh_axis]
 
 ### xla_pcall
 
@@ -215,30 +215,14 @@ def xla_pcall_impl(fun, *args, **params):
   global mesh_spec
   mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
 
-  flat_args, in_trees = unzip2(map(tree_flatten, args))
-  flat_args = concatenate(flat_args)
-  fun, out_tree = flatten_fun(fun, in_trees)
-
-  abstract_args = map(abstractify, flat_args)
-  in_axes = canonicalize_in_axis_spec(in_trees, in_axes)
-  chunksize = chunk_size(mesh_spec_, mesh_axis, in_axes, abstract_args)
-  out_axes_thunk = lambda: canonicalize_out_axis_spec(out_tree(), out_axes)
-  fun = chunk_transform(fun, chunksize, axis_name, in_axes, out_axes_thunk)
-
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec_, *abstract_args)
-  leaf_out = out_tree() is leaf
-  flat_ans = compiled_fun(leaf_out, out_axes_thunk(), *args)
-
-  if leaf_out:
-    return flat_ans
-  else:
-    return build_tree(iter(flat_ans), out_tree())
+                                       mesh_spec_, *map(abstractify, args))
+  return compiled_fun(out_axes(), *args)
 
 @lu.memoize
 def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
                           *abstract_args):
-  chunksize = chunk_size(mesh_spec, mesh_axis, in_axes, abstract_args)
+  chunksize = chunk_size(mesh_axis, in_axes, abstract_args)
   abstract_args = map(partial(chunk_aval, chunksize), abstract_args, in_axes)
   device_groups = replica_groups(mesh_spec, mesh_axis)
   pvals = [PartialVal((aval, core.unit)) for aval in abstract_args]
@@ -251,10 +235,10 @@ def xla_parallel_callable(fun, axis_name, in_axes, mesh_axis, mesh_spec,
   return partial(execute_replicated, in_axes, mesh_axis, mesh_spec, compiled, pval)
 
 def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
-                       leaf_out, out_axes, *args):
+                       out_axes, *args):
   input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
-  if leaf_out:
+  if True:  # TODO
     # TODO device persistence
     out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
     return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)",No
jax/interpreters/xla.py,jax/interpreters/xla.py,780106f892406590cf6c974e9c19caf59558e01c,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,"moving pxla flattening/chunking to api.py, wip","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 2a5b9f569..89696752b 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -368,6 +368,7 @@ def xla_shape(x):
 # instead, for values returned to the user, always destructure tuples.
 # The code here is similar to that in tree_util, but is meant to flatten
 # JaxTuple trees only.
+# TODO(mattjj): since pjit does flattening in api.py, can move/de-duplicate this
 
 @transformation_with_aux
 def flatten_fun(in_trees, *flat_args):","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 2a5b9f569..89696752b 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -368,6 +368,7 @@ def xla_shape(x):
 # instead, for values returned to the user, always destructure tuples.
 # The code here is similar to that in tree_util, but is meant to flatten
 # JaxTuple trees only.
+# TODO(mattjj): since pjit does flattening in api.py, can move/de-duplicate this
 
 @transformation_with_aux
 def flatten_fun(in_trees, *flat_args):",No
leak_bug.py,leak_bug.py,780106f892406590cf6c974e9c19caf59558e01c,afc2b9feb4dad75d5c3d5b73a85bd3aab0ce3e8a,"moving pxla flattening/chunking to api.py, wip","diff --git a/leak_bug.py b/leak_bug.py
index a59bc9261..7f90b33b6 100644
--- a/leak_bug.py
+++ b/leak_bug.py
@@ -8,8 +8,9 @@ import numpy as onp
 pxla.mesh_spec = (1,)
 
 def f(x):
-  return x - np.log(psum(np.exp(x), 'i'))
+  return x - psum(x, 'i')
 
-x = onp.ones((4, 2), onp.float32)
+x = onp.arange(8., dtype=onp.float32).reshape(4, 2)
 f = pjit(f, axis_name='i', in_axes=0, out_axes=0, mesh_axis=0)
 print f(x)
+print x - x.sum(0)","diff --git a/leak_bug.py b/leak_bug.py
index a59bc9261..7f90b33b6 100644
--- a/leak_bug.py
+++ b/leak_bug.py
@@ -8,8 +8,9 @@ import numpy as onp
 pxla.mesh_spec = (1,)
 
 def f(x):
-  return x - np.log(psum(np.exp(x), 'i'))
+  return x - psum(x, 'i')
 
-x = onp.ones((4, 2), onp.float32)
+x = onp.arange(8., dtype=onp.float32).reshape(4, 2)
 f = pjit(f, axis_name='i', in_axes=0, out_axes=0, mesh_axis=0)
 print f(x)
+print x - x.sum(0)",No
jax/api.py,jax/api.py,945fa34e7e1700106c81715dad9116f10c1d0c0b,780106f892406590cf6c974e9c19caf59558e01c,tweaks,"diff --git a/jax/api.py b/jax/api.py
index 70c2ae275..5e5be7e13 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -272,7 +272,10 @@ def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0):
     out_flat = pxla.xla_pcall(f, *args_flat, axis_name=axis_name,
                               in_axes=in_axes_, out_axes=out_axes_,
                               mesh_axis=mesh_axis)
-    return build_tree(out_tree(), out_flat)
+    if out_tree() is None:
+      return out_flat
+    else:
+      return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)
   return f_jitted","diff --git a/jax/api.py b/jax/api.py
index 70c2ae275..5e5be7e13 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -272,7 +272,10 @@ def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0):
     out_flat = pxla.xla_pcall(f, *args_flat, axis_name=axis_name,
                               in_axes=in_axes_, out_axes=out_axes_,
                               mesh_axis=mesh_axis)
-    return build_tree(out_tree(), out_flat)
+    if out_tree() is None:
+      return out_flat
+    else:
+      return build_tree(out_tree(), out_flat)
 
   f_jitted.__name__ = ""pjit({})"".format(f_jitted.__name__)
   return f_jitted",No
jax/api_util.py,jax/api_util.py,945fa34e7e1700106c81715dad9116f10c1d0c0b,780106f892406590cf6c974e9c19caf59558e01c,tweaks,"diff --git a/jax/api_util.py b/jax/api_util.py
index 5134011dd..ac8554a56 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -62,8 +62,11 @@ pytree_to_jaxtupletree = partial(process_pytree, pack)
 def pytree_fun_to_flatjaxtuple_fun(in_tree, *args):
   py_args = tuple(tree_unflatten(in_tree, args))
   ans = yield py_args
-  flat_ans, out_tree = tree_flatten(ans)
+  yield maybe_pytree_to_maybe_jaxtuple(ans)
+
+def maybe_pytree_to_maybe_jaxtuple(maybe_tree):
+  flat_ans, out_tree = tree_flatten(maybe_tree)
   if out_tree is leaf:
-    yield flat_ans[0], out_tree
+    return maybe_tree, None
   else:
-    yield pack(flat_ans), out_tree
+    return pack(flat_ans), out_tree","diff --git a/jax/api_util.py b/jax/api_util.py
index 5134011dd..ac8554a56 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -62,8 +62,11 @@ pytree_to_jaxtupletree = partial(process_pytree, pack)
 def pytree_fun_to_flatjaxtuple_fun(in_tree, *args):
   py_args = tuple(tree_unflatten(in_tree, args))
   ans = yield py_args
-  flat_ans, out_tree = tree_flatten(ans)
+  yield maybe_pytree_to_maybe_jaxtuple(ans)
+
+def maybe_pytree_to_maybe_jaxtuple(maybe_tree):
+  flat_ans, out_tree = tree_flatten(maybe_tree)
   if out_tree is leaf:
-    yield flat_ans[0], out_tree
+    return maybe_tree, None
   else:
-    yield pack(flat_ans), out_tree
+    return pack(flat_ans), out_tree",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,945fa34e7e1700106c81715dad9116f10c1d0c0b,780106f892406590cf6c974e9c19caf59558e01c,tweaks,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 4159ebf69..d6285302d 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 from collections import namedtuple, defaultdict
 from distutils.util import strtobool
+from contextlib import contextmanager
 import itertools as it
 import operator as op
 import os
@@ -81,7 +82,7 @@ def canonicalize_in_axis_spec(in_tree, spec_tree_prefix):
 
 def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
   """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
-  if out_tree is leaf:
+  if out_tree is None:
     return spec_tree_prefix
   else:
     spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)
@@ -143,7 +144,6 @@ def split_array(x, num_splits, axis):
 def chunk_size(mesh_axis, in_axes, abstract_args):
   """"""Compute the chunk size for mapped axes, checking for errors.""""""
   global mesh_spec
-  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -154,13 +154,27 @@ def chunk_size(mesh_axis, in_axes, abstract_args):
     raise ValueError(msg.format(axis_name, axis_sizes))
   else:
     axis_size = axis_sizes.pop()
-    if axis_size % mesh_spec_[mesh_axis]:
+    if axis_size % mesh_spec()[mesh_axis]:
       msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
              ""axis index {} with size {}, which does not evenly divide {}."")
       raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
-                                  mesh_spec_[mesh_axis], axis_size))
+                                  mesh_spec()[mesh_axis], axis_size))
+
+  return axis_size // mesh_spec()[mesh_axis]
+
+
+def mesh_spec():
+  global _mesh_spec
+  return _mesh_spec or (xb.get_replica_count(),)
+_mesh_spec = None
+
+@contextmanager
+def device_mesh(spec):
+  global _mesh_spec
+  _mesh_spec, prev_spec = spec, _mesh_spec
+  yield
+  _mesh_spec = prev_spec
 
-  return axis_size // mesh_spec_[mesh_axis]
 
 ### xla_pcall
 
@@ -212,11 +226,8 @@ def xla_pcall_impl(fun, *args, **params):
   mesh_axis = params.pop('mesh_axis')  # e.g. 0 or 1
   assert not params
 
-  global mesh_spec
-  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
-
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec_, *map(abstractify, args))
+                                       mesh_spec(), *map(abstractify, args))
   return compiled_fun(out_axes(), *args)
 
 @lu.memoize","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 4159ebf69..d6285302d 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -18,6 +18,7 @@ from __future__ import print_function
 
 from collections import namedtuple, defaultdict
 from distutils.util import strtobool
+from contextlib import contextmanager
 import itertools as it
 import operator as op
 import os
@@ -81,7 +82,7 @@ def canonicalize_in_axis_spec(in_tree, spec_tree_prefix):
 
 def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
   """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
-  if out_tree is leaf:
+  if out_tree is None:
     return spec_tree_prefix
   else:
     spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)
@@ -143,7 +144,6 @@ def split_array(x, num_splits, axis):
 def chunk_size(mesh_axis, in_axes, abstract_args):
   """"""Compute the chunk size for mapped axes, checking for errors.""""""
   global mesh_spec
-  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
   axis_sizes = {arg.shape[axis] for arg, axis in zip(abstract_args, in_axes)
                 if axis is not None}
   if len(axis_sizes) == 0:
@@ -154,13 +154,27 @@ def chunk_size(mesh_axis, in_axes, abstract_args):
     raise ValueError(msg.format(axis_name, axis_sizes))
   else:
     axis_size = axis_sizes.pop()
-    if axis_size % mesh_spec_[mesh_axis]:
+    if axis_size % mesh_spec()[mesh_axis]:
       msg = (""axis name '{}' bound to input axis of size {} mapped to mesh ""
              ""axis index {} with size {}, which does not evenly divide {}."")
       raise ValueError(msg.format(axis_name, axis_size, mesh_axis,
-                                  mesh_spec_[mesh_axis], axis_size))
+                                  mesh_spec()[mesh_axis], axis_size))
+
+  return axis_size // mesh_spec()[mesh_axis]
+
+
+def mesh_spec():
+  global _mesh_spec
+  return _mesh_spec or (xb.get_replica_count(),)
+_mesh_spec = None
+
+@contextmanager
+def device_mesh(spec):
+  global _mesh_spec
+  _mesh_spec, prev_spec = spec, _mesh_spec
+  yield
+  _mesh_spec = prev_spec
 
-  return axis_size // mesh_spec_[mesh_axis]
 
 ### xla_pcall
 
@@ -212,11 +226,8 @@ def xla_pcall_impl(fun, *args, **params):
   mesh_axis = params.pop('mesh_axis')  # e.g. 0 or 1
   assert not params
 
-  global mesh_spec
-  mesh_spec_ = mesh_spec or (xb.get_replica_count(),)
-
   compiled_fun = xla_parallel_callable(fun, axis_name, in_axes, mesh_axis,
-                                       mesh_spec_, *map(abstractify, args))
+                                       mesh_spec(), *map(abstractify, args))
   return compiled_fun(out_axes(), *args)
 
 @lu.memoize",No
leak_bug.py,leak_bug.py,945fa34e7e1700106c81715dad9116f10c1d0c0b,780106f892406590cf6c974e9c19caf59558e01c,tweaks,"diff --git a/leak_bug.py b/leak_bug.py
index 7f90b33b6..fd1fb14fb 100644
--- a/leak_bug.py
+++ b/leak_bug.py
@@ -5,12 +5,12 @@ from jax.interpreters.parallel import psum
 from jax import pjit
 
 import numpy as onp
-pxla.mesh_spec = (1,)
 
 def f(x):
   return x - psum(x, 'i')
 
 x = onp.arange(8., dtype=onp.float32).reshape(4, 2)
 f = pjit(f, axis_name='i', in_axes=0, out_axes=0, mesh_axis=0)
+
 print f(x)
 print x - x.sum(0)","diff --git a/leak_bug.py b/leak_bug.py
index 7f90b33b6..fd1fb14fb 100644
--- a/leak_bug.py
+++ b/leak_bug.py
@@ -5,12 +5,12 @@ from jax.interpreters.parallel import psum
 from jax import pjit
 
 import numpy as onp
-pxla.mesh_spec = (1,)
 
 def f(x):
   return x - psum(x, 'i')
 
 x = onp.arange(8., dtype=onp.float32).reshape(4, 2)
 f = pjit(f, axis_name='i', in_axes=0, out_axes=0, mesh_axis=0)
+
 print f(x)
 print x - x.sum(0)",No
jax/api.py,jax/api.py,da2d1854445286c459bb24d0b31d698419eb48d9,945fa34e7e1700106c81715dad9116f10c1d0c0b,tweak,"diff --git a/jax/api.py b/jax/api.py
index 5e5be7e13..5d93cb272 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -36,7 +36,7 @@ from .api_util import (pytree_fun_to_jaxtupletree_fun, pytree_to_jaxtupletree,
                        pytree_fun_to_flatjaxtuple_fun, apply_jaxtree_fun, wraps)
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef,
                         tree_map, tree_flatten, tree_unflatten, tree_structure,
-                        tree_transpose)
+                        tree_transpose, leaf)
 from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,
                    WrapHashably, prod)
 from .lib.xla_bridge import canonicalize_dtype
@@ -272,7 +272,7 @@ def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0):
     out_flat = pxla.xla_pcall(f, *args_flat, axis_name=axis_name,
                               in_axes=in_axes_, out_axes=out_axes_,
                               mesh_axis=mesh_axis)
-    if out_tree() is None:
+    if out_tree() is leaf:
       return out_flat
     else:
       return build_tree(out_tree(), out_flat)","diff --git a/jax/api.py b/jax/api.py
index 5e5be7e13..5d93cb272 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -36,7 +36,7 @@ from .api_util import (pytree_fun_to_jaxtupletree_fun, pytree_to_jaxtupletree,
                        pytree_fun_to_flatjaxtuple_fun, apply_jaxtree_fun, wraps)
 from .tree_util import (process_pytree, node_types, build_tree, PyTreeDef,
                         tree_map, tree_flatten, tree_unflatten, tree_structure,
-                        tree_transpose)
+                        tree_transpose, leaf)
 from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,
                    WrapHashably, prod)
 from .lib.xla_bridge import canonicalize_dtype
@@ -272,7 +272,7 @@ def pjit(fun, axis_name, in_axes=0, out_axes=0, mesh_axis=0):
     out_flat = pxla.xla_pcall(f, *args_flat, axis_name=axis_name,
                               in_axes=in_axes_, out_axes=out_axes_,
                               mesh_axis=mesh_axis)
-    if out_tree() is None:
+    if out_tree() is leaf:
       return out_flat
     else:
       return build_tree(out_tree(), out_flat)",No
jax/api_util.py,jax/api_util.py,da2d1854445286c459bb24d0b31d698419eb48d9,945fa34e7e1700106c81715dad9116f10c1d0c0b,tweak,"diff --git a/jax/api_util.py b/jax/api_util.py
index ac8554a56..ab9361885 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -67,6 +67,6 @@ def pytree_fun_to_flatjaxtuple_fun(in_tree, *args):
 def maybe_pytree_to_maybe_jaxtuple(maybe_tree):
   flat_ans, out_tree = tree_flatten(maybe_tree)
   if out_tree is leaf:
-    return maybe_tree, None
+    return maybe_tree, leaf
   else:
     return pack(flat_ans), out_tree","diff --git a/jax/api_util.py b/jax/api_util.py
index ac8554a56..ab9361885 100644
--- a/jax/api_util.py
+++ b/jax/api_util.py
@@ -67,6 +67,6 @@ def pytree_fun_to_flatjaxtuple_fun(in_tree, *args):
 def maybe_pytree_to_maybe_jaxtuple(maybe_tree):
   flat_ans, out_tree = tree_flatten(maybe_tree)
   if out_tree is leaf:
-    return maybe_tree, None
+    return maybe_tree, leaf
   else:
     return pack(flat_ans), out_tree",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,da2d1854445286c459bb24d0b31d698419eb48d9,945fa34e7e1700106c81715dad9116f10c1d0c0b,tweak,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index d6285302d..843b24c8f 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -82,7 +82,7 @@ def canonicalize_in_axis_spec(in_tree, spec_tree_prefix):
 
 def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
   """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
-  if out_tree is None:
+  if out_tree is leaf:
     return spec_tree_prefix
   else:
     spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index d6285302d..843b24c8f 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -82,7 +82,7 @@ def canonicalize_in_axis_spec(in_tree, spec_tree_prefix):
 
 def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
   """"""Given output out_tree, canonicalize and flatten an out_axes spec.""""""
-  if out_tree is None:
+  if out_tree is leaf:
     return spec_tree_prefix
   else:
     spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,2487eb2d04725d1d5bcc5d6c39a27e1b270520c1,da2d1854445286c459bb24d0b31d698419eb48d9,woo tuple output works,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 843b24c8f..f78a41fe0 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -86,7 +86,7 @@ def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
     return spec_tree_prefix
   else:
     spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)
-    return tuple(tree_util.tree_flatten(spec)[0])
+    return tuple(tree_util.tree_flatten(spec_tree)[0])
 
 def build_axis_spec_tree(spec, treedef):
   """"""Given a JaxTuple treedef, canonicalize an axis spec for that treedef.""""""
@@ -249,12 +249,12 @@ def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
                        out_axes, *args):
   input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
-  if True:  # TODO
-    # TODO device persistence
-    out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
+  out_shards = [merge_pvals(buf.to_py(), pval) for buf in out_bufs]  # TODO
+  if type(out_axes) is int:
     return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)
   else:
-    raise NotImplementedError  # TODO zip*
+    return map(partial(unshard_output, mesh_spec, mesh_axis), out_axes,
+               zip(*out_shards))
 
 
 xla_pcall_p = core.Primitive('xla_pcall')","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index 843b24c8f..f78a41fe0 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -86,7 +86,7 @@ def canonicalize_out_axis_spec(out_tree, spec_tree_prefix):
     return spec_tree_prefix
   else:
     spec_tree = build_axis_spec_tree(spec_tree_prefix, out_tree)
-    return tuple(tree_util.tree_flatten(spec)[0])
+    return tuple(tree_util.tree_flatten(spec_tree)[0])
 
 def build_axis_spec_tree(spec, treedef):
   """"""Given a JaxTuple treedef, canonicalize an axis spec for that treedef.""""""
@@ -249,12 +249,12 @@ def execute_replicated(in_axes, mesh_axis, mesh_spec, compiled, pval,
                        out_axes, *args):
   input_bufs = map(partial(shard_arg, mesh_spec, mesh_axis), in_axes, args)
   out_bufs = compiled.ExecutePerReplica(zip(*input_bufs))
-  if True:  # TODO
-    # TODO device persistence
-    out_shards = [merge_pvals(out_buf.to_py(), pval) for out_buf in out_bufs]
+  out_shards = [merge_pvals(buf.to_py(), pval) for buf in out_bufs]  # TODO
+  if type(out_axes) is int:
     return unshard_output(mesh_spec, mesh_axis, out_axes, out_shards)
   else:
-    raise NotImplementedError  # TODO zip*
+    return map(partial(unshard_output, mesh_spec, mesh_axis), out_axes,
+               zip(*out_shards))
 
 
 xla_pcall_p = core.Primitive('xla_pcall')",No
leak_bug.py,leak_bug.py,2487eb2d04725d1d5bcc5d6c39a27e1b270520c1,da2d1854445286c459bb24d0b31d698419eb48d9,woo tuple output works,"diff --git a/leak_bug.py b/leak_bug.py
index fd1fb14fb..a1ea3ab50 100644
--- a/leak_bug.py
+++ b/leak_bug.py
@@ -7,10 +7,10 @@ from jax import pjit
 import numpy as onp
 
 def f(x):
-  return x - psum(x, 'i')
+  return (x - psum(x, 'i'),)
 
 x = onp.arange(8., dtype=onp.float32).reshape(4, 2)
 f = pjit(f, axis_name='i', in_axes=0, out_axes=0, mesh_axis=0)
 
 print f(x)
-print x - x.sum(0)
+print (x - x.sum(0),)","diff --git a/leak_bug.py b/leak_bug.py
index fd1fb14fb..a1ea3ab50 100644
--- a/leak_bug.py
+++ b/leak_bug.py
@@ -7,10 +7,10 @@ from jax import pjit
 import numpy as onp
 
 def f(x):
-  return x - psum(x, 'i')
+  return (x - psum(x, 'i'),)
 
 x = onp.arange(8., dtype=onp.float32).reshape(4, 2)
 f = pjit(f, axis_name='i', in_axes=0, out_axes=0, mesh_axis=0)
 
 print f(x)
-print x - x.sum(0)
+print (x - x.sum(0),)",No
jax/api.py,jax/api.py,32cda396892ae4b7f13507eb2e7d33ea76803a29,2487eb2d04725d1d5bcc5d6c39a27e1b270520c1,remove chunk transform from api.py for now,"diff --git a/jax/api.py b/jax/api.py
index 5d93cb272..69ff5e450 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -306,24 +306,6 @@ def axisvar_split(fun, name, new_names):
   return split_fun
 
 
-def chunk(fun, name, chunksize, in_axes=0, out_axes=0):
-  """"""Stage SPMD primitives to first operate on chunks, then use collectives.""""""
-  temp_name = object()
-
-  def chunked_fun(*args, **kwargs):
-    f = lu.wrap_init(fun, kwargs)
-    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
-    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
-    f, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    f = parallel.axisvar_split(f, name, (temp_name, name))
-    reshape = partial(parallel.reshape_axis, chunksize)
-    reshaped_args = map(reshape, in_axes_, args)
-    out_flat = parallel.pmap(f, temp_name, reshaped_args, in_axes_, out_axes)
-    return build_tree(out_tree(), out_flat)
-
-  return chunked_fun
-
-
 def papply(fun, in_axes=0):
   """"""Apply a function using parallel computation by sharding inputs.""""""
   axis_name = parallel.newvar()","diff --git a/jax/api.py b/jax/api.py
index 5d93cb272..69ff5e450 100644
--- a/jax/api.py
+++ b/jax/api.py
@@ -306,24 +306,6 @@ def axisvar_split(fun, name, new_names):
   return split_fun
 
 
-def chunk(fun, name, chunksize, in_axes=0, out_axes=0):
-  """"""Stage SPMD primitives to first operate on chunks, then use collectives.""""""
-  temp_name = object()
-
-  def chunked_fun(*args, **kwargs):
-    f = lu.wrap_init(fun, kwargs)
-    in_axes_ = in_axes if isinstance(in_axes, (list, tuple)) else (in_axes,) * len(args)
-    in_flat, in_trees = unzip2(map(pytree_to_jaxtupletree, args))
-    f, out_tree = pytree_fun_to_jaxtupletree_fun(f, in_trees)
-    f = parallel.axisvar_split(f, name, (temp_name, name))
-    reshape = partial(parallel.reshape_axis, chunksize)
-    reshaped_args = map(reshape, in_axes_, args)
-    out_flat = parallel.pmap(f, temp_name, reshaped_args, in_axes_, out_axes)
-    return build_tree(out_tree(), out_flat)
-
-  return chunked_fun
-
-
 def papply(fun, in_axes=0):
   """"""Apply a function using parallel computation by sharding inputs.""""""
   axis_name = parallel.newvar()",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,32cda396892ae4b7f13507eb2e7d33ea76803a29,2487eb2d04725d1d5bcc5d6c39a27e1b270520c1,remove chunk transform from api.py for now,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 1b536888b..3a640da1b 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -184,7 +184,7 @@ def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
 def psum_parallel_translation_rule(c, val, device_groups):
-  # return c.CrossReplicaSum(val, device_grp)  # TODO
+  # return c.CrossReplicaSum(val, device_grp)  # TODO needs updated jaxlib
   return c.CrossReplicaSum(val)
 
 psum_p = PmapPrimitive('psum')
@@ -228,7 +228,11 @@ class SplitTracer(Tracer):
     return core.get_aval(self.val)
 
   def unpack(self):
-    raise NotImplementedError  # TODO(mattjj)
+    if self.name is None:
+      return self.full_lower()
+    else:
+      elt_tracer = partial(SplitTracer, self.trace, self.name, self.new_names)
+      return map(elt_tracer, self.val)
 
   def full_lower(self):
     if self.name is None:
@@ -344,7 +348,7 @@ class PapplyTracer(Tracer):
     return batching.get_aval(self.val)
 
   def unpack(self):
-    raise NotImplementedError  # TODO
+    raise NotImplementedError  # TODO(mattjj,frostig)
 
   def full_lower(self):
     if self.axis is None:
@@ -373,10 +377,10 @@ class PapplyTrace(Trace):
       return PapplyTracer(self, name, val_out, axis_out)
 
   def process_call(self, call_primitive, f, tracers, params):
-    raise NotImplementedError  # TODO(mattjj)
+    raise NotImplementedError  # TODO(mattjj,frostig)
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj)
+    raise NotImplementedError  # TODO(mattjj,frostig)
 
   def pack(self, tracers):
     vals = core.pack([t.val for t in tracers])","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 1b536888b..3a640da1b 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -184,7 +184,7 @@ def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
 def psum_parallel_translation_rule(c, val, device_groups):
-  # return c.CrossReplicaSum(val, device_grp)  # TODO
+  # return c.CrossReplicaSum(val, device_grp)  # TODO needs updated jaxlib
   return c.CrossReplicaSum(val)
 
 psum_p = PmapPrimitive('psum')
@@ -228,7 +228,11 @@ class SplitTracer(Tracer):
     return core.get_aval(self.val)
 
   def unpack(self):
-    raise NotImplementedError  # TODO(mattjj)
+    if self.name is None:
+      return self.full_lower()
+    else:
+      elt_tracer = partial(SplitTracer, self.trace, self.name, self.new_names)
+      return map(elt_tracer, self.val)
 
   def full_lower(self):
     if self.name is None:
@@ -344,7 +348,7 @@ class PapplyTracer(Tracer):
     return batching.get_aval(self.val)
 
   def unpack(self):
-    raise NotImplementedError  # TODO
+    raise NotImplementedError  # TODO(mattjj,frostig)
 
   def full_lower(self):
     if self.axis is None:
@@ -373,10 +377,10 @@ class PapplyTrace(Trace):
       return PapplyTracer(self, name, val_out, axis_out)
 
   def process_call(self, call_primitive, f, tracers, params):
-    raise NotImplementedError  # TODO(mattjj)
+    raise NotImplementedError  # TODO(mattjj,frostig)
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj)
+    raise NotImplementedError  # TODO(mattjj,frostig)
 
   def pack(self, tracers):
     vals = core.pack([t.val for t in tracers])",No
tests/parallel_test.py,tests/parallel_test.py,32cda396892ae4b7f13507eb2e7d33ea76803a29,2487eb2d04725d1d5bcc5d6c39a27e1b270520c1,remove chunk transform from api.py for now,"diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 0293f9042..5a1de3b72 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax import lax
-from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split, chunk
+from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split
 from jax.linear_util import wrap_init
 from jax.interpreters.parallel import psum, scatter_like
 
@@ -134,14 +134,6 @@ class SplitTest(jtu.JaxTestCase):
     expected = onp.sum(onp.sin(x))
     self.assertAllClose(ans, expected, check_dtypes=False)
 
-  def testChunkingSum(self):
-    f = lambda x: x - psum(x, 'i')
-    x = onp.ones((4, 2))
-    fchunked = chunk(f, 'i', 2)
-    ans = pmap(fchunked, 'i')(x)
-    expected = pmap(f, 'i')(x)
-    self.assertAllClose(ans, expected, check_dtypes=False)
-
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/parallel_test.py b/tests/parallel_test.py
index 0293f9042..5a1de3b72 100644
--- a/tests/parallel_test.py
+++ b/tests/parallel_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax import lax
-from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split, chunk
+from jax.api import pmap, papply, jit, make_jaxpr, axisvar_split
 from jax.linear_util import wrap_init
 from jax.interpreters.parallel import psum, scatter_like
 
@@ -134,14 +134,6 @@ class SplitTest(jtu.JaxTestCase):
     expected = onp.sum(onp.sin(x))
     self.assertAllClose(ans, expected, check_dtypes=False)
 
-  def testChunkingSum(self):
-    f = lambda x: x - psum(x, 'i')
-    x = onp.ones((4, 2))
-    fchunked = chunk(f, 'i', 2)
-    ans = pmap(fchunked, 'i')(x)
-    expected = pmap(f, 'i')(x)
-    self.assertAllClose(ans, expected, check_dtypes=False)
-
 
 if __name__ == '__main__':
   absltest.main()",No
jax/interpreters/batching.py,jax/interpreters/batching.py,1a494d0752c527def252f8c2e0132fed7692a35c,32cda396892ae4b7f13507eb2e7d33ea76803a29,add post_process_call for vmap (untested),"diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 842a9122a..17cd1c7db 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -131,7 +131,13 @@ class BatchTrace(Trace):
       return BatchTracer(self, val_out, dim_out())
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
+    val, dim = out_tracer.val, out_tracer.batch_dim
+    master = self.master
+    def todo(x):
+      trace = BatchTrace(master, core.cur_sublevel())
+      return BatchTracer(trace, val, dim)
+
+    return val, todo
 
   def pack(self, tracers):
     vals = pack([t.val for t in tracers])","diff --git a/jax/interpreters/batching.py b/jax/interpreters/batching.py
index 842a9122a..17cd1c7db 100644
--- a/jax/interpreters/batching.py
+++ b/jax/interpreters/batching.py
@@ -131,7 +131,13 @@ class BatchTrace(Trace):
       return BatchTracer(self, val_out, dim_out())
 
   def post_process_call(self, _, out_tracer):
-    raise NotImplementedError  # TODO(mattjj,dougalm)
+    val, dim = out_tracer.val, out_tracer.batch_dim
+    master = self.master
+    def todo(x):
+      trace = BatchTrace(master, core.cur_sublevel())
+      return BatchTracer(trace, val, dim)
+
+    return val, todo
 
   def pack(self, tracers):
     vals = pack([t.val for t in tracers])",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,1a494d0752c527def252f8c2e0132fed7692a35c,32cda396892ae4b7f13507eb2e7d33ea76803a29,add post_process_call for vmap (untested),"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 3a640da1b..38fd47b1d 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -216,6 +216,14 @@ def axisvar_split(name, new_names, *args):
     del master, out_tracer
   yield out_val
 
+@lu.transformation
+def axisvar_split_subtrace(master, name, new_names, *vals):
+  trace = SplitTrace(master, core.cur_sublevel())
+  ans = yield map(partial(SplitTracer, trace, name, new_names), vals)
+  out_tracer = trace.full_raise(ans)
+  out_val = out_tracer.val
+  yield out_val
+
 class SplitTracer(Tracer):
   def __init__(self, trace, name, new_names, val):
     self.trace = trace","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 3a640da1b..38fd47b1d 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -216,6 +216,14 @@ def axisvar_split(name, new_names, *args):
     del master, out_tracer
   yield out_val
 
+@lu.transformation
+def axisvar_split_subtrace(master, name, new_names, *vals):
+  trace = SplitTrace(master, core.cur_sublevel())
+  ans = yield map(partial(SplitTracer, trace, name, new_names), vals)
+  out_tracer = trace.full_raise(ans)
+  out_val = out_tracer.val
+  yield out_val
+
 class SplitTracer(Tracer):
   def __init__(self, trace, name, new_names, val):
     self.trace = trace",No
jax/lax.py,jax/lax.py,f76134e4608c3f0e5fbf3d39796422ae834a7fe6,fa63c5a3abb8933cb648b67d86705a252298b335,"Implement transpose rule for select_and_gather_add (issue #274).

There are a couple of caveats that mean that we shouldn't close the issue yet:
a) we need a jaxlib update to generalize the ReduceWindow support in the XLA/CPU backend.
b) jax_enable_x64 must be set, otherwise 64-bit types aren't available and bad things may happen. We should probably removed type-squashing from the JaxComputationBuilder class.","diff --git a/jax/lax.py b/jax/lax.py
index 471067889..2849a8d6f 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -395,8 +395,13 @@ def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
 
 def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                            window_strides, padding):
+  pair_dtype = _select_and_gather_add_pair_dtype(operand.dtype)
+  pair_select = partial(_select_and_gather_add_pair_reducer,
+                        dtype=operand.dtype, select_prim=select_prim)
+  jaxpr, consts = _reduction_jaxpr(pair_select, pair_dtype(0))
   return select_and_gather_add_p.bind(
       tangents, operand, select_prim=select_prim,
+      pair_select_jaxpr=jaxpr, pair_select_consts=consts,
       window_dimensions=tuple(window_dimensions),
       window_strides=tuple(window_strides), padding=padding)
 
@@ -2383,8 +2388,9 @@ ad.primitive_transposes[select_and_scatter_add_p] = \
     select_and_scatter_add_transpose
 
 
-def select_and_gather_add_shape_rule(
-    tangents, operand, select_prim, window_dimensions, window_strides, padding):
+def _select_and_gather_add_shape_rule(
+    tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
+    window_dimensions, window_strides, padding):
   if tangents.shape != operand.shape:
     msg = (""select_and_gather_add tangents and operand shapes must match, ""
            ""got {} and {}."")
@@ -2392,24 +2398,72 @@ def select_and_gather_add_shape_rule(
   return common_reduce_window_shape_rule(operand, window_dimensions,
                                          window_strides, padding)
 
-def select_and_gather_add_translation(
-    c, tangents, operand, select_prim, window_dimensions, window_strides,
-    padding):
-  raise NotImplementedError(""No efficient translation."")
 
-def select_and_gather_add_transpose(
-    t, tangents, operand, select_prim, window_dimensions, window_strides,
-    padding):
+_UINT_DTYPES = {
+  16: onp.uint16,
+  32: onp.uint32,
+  64: onp.uint64,
+}
+
+def _select_and_gather_add_pair_dtype(dtype):
+  bits = onp.finfo(dtype).bits
+  return _UINT_DTYPES[bits * 2]
+
+def _select_and_gather_add_pair_reducer(x, y, dtype=None, select_prim=None):
+  bits = _dtype(x).type(onp.finfo(dtype).bits)
+  uint_dtype = _UINT_DTYPES[bits]
+  sx = convert_element_type(shift_right_logical(x, bits), uint_dtype)
+  sx = bitcast_convert_type(sx, dtype)
+  sy = convert_element_type(shift_right_logical(y, bits), uint_dtype)
+  sy = bitcast_convert_type(sy, dtype)
+  return select(select_prim.bind(sx, sy), x, y)
+
+def _select_and_gather_add_translation(
+    c, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
+    window_dimensions, window_strides, padding):
+  # XLA doesn't yet implement ReduceWindow on tuples (Google bug b/73062247), so
+  # we implement a pair-wise ReduceWindow by packing two k-bit values into
+  # 2k-bit unsigned integer using bit tricks. This will only work for 32-bit
+  # inputs, and furthermore it also requires jax_enable_x64 to be set.
+  dtype = c.GetShape(operand).numpy_dtype()
+  etype = xla_bridge.dtype_to_etype(dtype)
+  bits = onp.finfo(dtype).bits
+  uint_etype = xla_bridge.dtype_to_etype(_UINT_DTYPES[bits])
+  pair_uint_dtype = _select_and_gather_add_pair_dtype(dtype)
+  pair_uint_etype = xla_bridge.dtype_to_etype(pair_uint_dtype)
+  operand = c.BitcastConvertType(operand, uint_etype)
+  tangents = c.BitcastConvertType(tangents, uint_etype)
+  operand = c.ConvertElementType(operand, pair_uint_etype)
+  tangents = c.ConvertElementType(tangents, pair_uint_etype)
+  operand = c.ShiftLeft(operand, c.Constant(pair_uint_dtype(bits)))
+
+  assert select_prim is ge_p or select_prim is le_p
+  init = -onp.inf if select_prim is ge_p else onp.inf
+  init = c.BitcastConvertType(c.Constant(dtype.type(init)), uint_etype)
+  init = c.ConvertElementType(init, pair_uint_etype)
+  init = c.ShiftLeft(init, c.Constant(pair_uint_dtype(bits)))
+
+  xla_computation = _reduction_computation(
+      c, pair_select_jaxpr, pair_select_consts, init)
+  out = c.ReduceWindow(c.Or(operand, tangents), init,
+                       xla_computation, window_dimensions, window_strides,
+                       padding)
+  out = c.ConvertElementType(out, uint_etype)
+  return c.BitcastConvertType(out, etype)
+
+def _select_and_gather_add_transpose(
+    t, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
+    window_dimensions, window_strides, padding):
   assert tangents is None and operand is not None
   result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                    window_strides, padding)
   return [result, None]
 
 select_and_gather_add_p = standard_primitive(
-    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
-    select_and_gather_add_translation)
+    _select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
+    _select_and_gather_add_translation)
 ad.primitive_transposes[select_and_gather_add_p] = \
-    select_and_gather_add_transpose
+    _select_and_gather_add_transpose
 
 
 sort_shape = lambda operand, dimension: operand.shape","diff --git a/jax/lax.py b/jax/lax.py
index 471067889..2849a8d6f 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -395,8 +395,13 @@ def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
 
 def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                            window_strides, padding):
+  pair_dtype = _select_and_gather_add_pair_dtype(operand.dtype)
+  pair_select = partial(_select_and_gather_add_pair_reducer,
+                        dtype=operand.dtype, select_prim=select_prim)
+  jaxpr, consts = _reduction_jaxpr(pair_select, pair_dtype(0))
   return select_and_gather_add_p.bind(
       tangents, operand, select_prim=select_prim,
+      pair_select_jaxpr=jaxpr, pair_select_consts=consts,
       window_dimensions=tuple(window_dimensions),
       window_strides=tuple(window_strides), padding=padding)
 
@@ -2383,8 +2388,9 @@ ad.primitive_transposes[select_and_scatter_add_p] = \
     select_and_scatter_add_transpose
 
 
-def select_and_gather_add_shape_rule(
-    tangents, operand, select_prim, window_dimensions, window_strides, padding):
+def _select_and_gather_add_shape_rule(
+    tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
+    window_dimensions, window_strides, padding):
   if tangents.shape != operand.shape:
     msg = (""select_and_gather_add tangents and operand shapes must match, ""
            ""got {} and {}."")
@@ -2392,24 +2398,72 @@ def select_and_gather_add_shape_rule(
   return common_reduce_window_shape_rule(operand, window_dimensions,
                                          window_strides, padding)
 
-def select_and_gather_add_translation(
-    c, tangents, operand, select_prim, window_dimensions, window_strides,
-    padding):
-  raise NotImplementedError(""No efficient translation."")
 
-def select_and_gather_add_transpose(
-    t, tangents, operand, select_prim, window_dimensions, window_strides,
-    padding):
+_UINT_DTYPES = {
+  16: onp.uint16,
+  32: onp.uint32,
+  64: onp.uint64,
+}
+
+def _select_and_gather_add_pair_dtype(dtype):
+  bits = onp.finfo(dtype).bits
+  return _UINT_DTYPES[bits * 2]
+
+def _select_and_gather_add_pair_reducer(x, y, dtype=None, select_prim=None):
+  bits = _dtype(x).type(onp.finfo(dtype).bits)
+  uint_dtype = _UINT_DTYPES[bits]
+  sx = convert_element_type(shift_right_logical(x, bits), uint_dtype)
+  sx = bitcast_convert_type(sx, dtype)
+  sy = convert_element_type(shift_right_logical(y, bits), uint_dtype)
+  sy = bitcast_convert_type(sy, dtype)
+  return select(select_prim.bind(sx, sy), x, y)
+
+def _select_and_gather_add_translation(
+    c, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
+    window_dimensions, window_strides, padding):
+  # XLA doesn't yet implement ReduceWindow on tuples (Google bug b/73062247), so
+  # we implement a pair-wise ReduceWindow by packing two k-bit values into
+  # 2k-bit unsigned integer using bit tricks. This will only work for 32-bit
+  # inputs, and furthermore it also requires jax_enable_x64 to be set.
+  dtype = c.GetShape(operand).numpy_dtype()
+  etype = xla_bridge.dtype_to_etype(dtype)
+  bits = onp.finfo(dtype).bits
+  uint_etype = xla_bridge.dtype_to_etype(_UINT_DTYPES[bits])
+  pair_uint_dtype = _select_and_gather_add_pair_dtype(dtype)
+  pair_uint_etype = xla_bridge.dtype_to_etype(pair_uint_dtype)
+  operand = c.BitcastConvertType(operand, uint_etype)
+  tangents = c.BitcastConvertType(tangents, uint_etype)
+  operand = c.ConvertElementType(operand, pair_uint_etype)
+  tangents = c.ConvertElementType(tangents, pair_uint_etype)
+  operand = c.ShiftLeft(operand, c.Constant(pair_uint_dtype(bits)))
+
+  assert select_prim is ge_p or select_prim is le_p
+  init = -onp.inf if select_prim is ge_p else onp.inf
+  init = c.BitcastConvertType(c.Constant(dtype.type(init)), uint_etype)
+  init = c.ConvertElementType(init, pair_uint_etype)
+  init = c.ShiftLeft(init, c.Constant(pair_uint_dtype(bits)))
+
+  xla_computation = _reduction_computation(
+      c, pair_select_jaxpr, pair_select_consts, init)
+  out = c.ReduceWindow(c.Or(operand, tangents), init,
+                       xla_computation, window_dimensions, window_strides,
+                       padding)
+  out = c.ConvertElementType(out, uint_etype)
+  return c.BitcastConvertType(out, etype)
+
+def _select_and_gather_add_transpose(
+    t, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
+    window_dimensions, window_strides, padding):
   assert tangents is None and operand is not None
   result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                    window_strides, padding)
   return [result, None]
 
 select_and_gather_add_p = standard_primitive(
-    select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
-    select_and_gather_add_translation)
+    _select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add',
+    _select_and_gather_add_translation)
 ad.primitive_transposes[select_and_gather_add_p] = \
-    select_and_gather_add_transpose
+    _select_and_gather_add_transpose
 
 
 sort_shape = lambda operand, dimension: operand.shape",No
tests/lax_test.py,tests/lax_test.py,f76134e4608c3f0e5fbf3d39796422ae834a7fe6,fa63c5a3abb8933cb648b67d86705a252298b335,"Implement transpose rule for select_and_gather_add (issue #274).

There are a couple of caveats that mean that we shouldn't close the issue yet:
a) we need a jaxlib update to generalize the ReduceWindow support in the XLA/CPU backend.
b) jax_enable_x64 must be set, otherwise 64-bit types aren't available and bad things may happen. We should probably removed type-squashing from the JaxComputationBuilder class.","diff --git a/tests/lax_test.py b/tests/lax_test.py
index e8550c442..18e9e1274 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -2016,14 +2016,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     # pylint: disable=cell-var-from-loop
     for shape, dims, strides in all_configs:
       operand = rng(shape, dtype)
-      if op is lax.add:
-        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
-      else:
+      if op is not lax.add:
         # this test can fail if there are duplicates in operand
         self.assertEqual(onp.unique(operand).size, operand.size,
                          msg=""test requires operand elements to be unique."")
-        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
-                            1e-2, 1e-2, 1e-2)
+      jtu.check_vjp(fun, partial(api.vjp, fun), (operand,), 1e-2, 1e-2, 1e-2)
+
+      # TODO(phawkins): enable both gradients after a jaxlib update.
+      # check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
     # pylint: enable=cell-var-from-loop
 
   # TODO(b/205052657): enable more tests when supported","diff --git a/tests/lax_test.py b/tests/lax_test.py
index e8550c442..18e9e1274 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -2016,14 +2016,14 @@ class LaxAutodiffTest(jtu.JaxTestCase):
     # pylint: disable=cell-var-from-loop
     for shape, dims, strides in all_configs:
       operand = rng(shape, dtype)
-      if op is lax.add:
-        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
-      else:
+      if op is not lax.add:
         # this test can fail if there are duplicates in operand
         self.assertEqual(onp.unique(operand).size, operand.size,
                          msg=""test requires operand elements to be unique."")
-        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
-                            1e-2, 1e-2, 1e-2)
+      jtu.check_vjp(fun, partial(api.vjp, fun), (operand,), 1e-2, 1e-2, 1e-2)
+
+      # TODO(phawkins): enable both gradients after a jaxlib update.
+      # check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
     # pylint: enable=cell-var-from-loop
 
   # TODO(b/205052657): enable more tests when supported",No
jax/lax.py,jax/lax.py,9f84455fb2371bae06407e057ed2709c8a2392e0,f76134e4608c3f0e5fbf3d39796422ae834a7fe6,Check for jax_enable_x64 in select_and_gather_add translation rule.,"diff --git a/jax/lax.py b/jax/lax.py
index 2849a8d6f..e1dd53dde 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -32,6 +32,7 @@ from .util import partial, prod
 from . import core
 from . import ad_util
 from . import linear_util as lu
+from .config import flags
 from .core import Primitive
 from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                               array_types, make_shaped_array)
@@ -44,6 +45,8 @@ from .util import curry, safe_zip, unzip2, prod
 from .tree_util import build_tree
 from .lib import xla_bridge
 
+FLAGS = flags.FLAGS
+
 _max = builtins.max
 _min = builtins.max
 _reduce = six.moves.reduce
@@ -2424,10 +2427,16 @@ def _select_and_gather_add_translation(
   # XLA doesn't yet implement ReduceWindow on tuples (Google bug b/73062247), so
   # we implement a pair-wise ReduceWindow by packing two k-bit values into
   # 2k-bit unsigned integer using bit tricks. This will only work for 32-bit
-  # inputs, and furthermore it also requires jax_enable_x64 to be set.
+  # inputs (since we don't have 128-bit integer types).
+  # TODO(phawkins): unless jax_enable_x64 is set, we won't have correct 64-bit
+  # types.
   dtype = c.GetShape(operand).numpy_dtype()
-  etype = xla_bridge.dtype_to_etype(dtype)
   bits = onp.finfo(dtype).bits
+  if not FLAGS.jax_enable_x64 and bits >= 32:
+    raise NotImplementedError(
+        ""Translation of select_and_gather_add requires flag --jax_enable_x64 ""
+        ""to be set"")
+  etype = xla_bridge.dtype_to_etype(dtype)
   uint_etype = xla_bridge.dtype_to_etype(_UINT_DTYPES[bits])
   pair_uint_dtype = _select_and_gather_add_pair_dtype(dtype)
   pair_uint_etype = xla_bridge.dtype_to_etype(pair_uint_dtype)","diff --git a/jax/lax.py b/jax/lax.py
index 2849a8d6f..e1dd53dde 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -32,6 +32,7 @@ from .util import partial, prod
 from . import core
 from . import ad_util
 from . import linear_util as lu
+from .config import flags
 from .core import Primitive
 from .abstract_arrays import (UnshapedArray, ShapedArray, ConcreteArray,
                               array_types, make_shaped_array)
@@ -44,6 +45,8 @@ from .util import curry, safe_zip, unzip2, prod
 from .tree_util import build_tree
 from .lib import xla_bridge
 
+FLAGS = flags.FLAGS
+
 _max = builtins.max
 _min = builtins.max
 _reduce = six.moves.reduce
@@ -2424,10 +2427,16 @@ def _select_and_gather_add_translation(
   # XLA doesn't yet implement ReduceWindow on tuples (Google bug b/73062247), so
   # we implement a pair-wise ReduceWindow by packing two k-bit values into
   # 2k-bit unsigned integer using bit tricks. This will only work for 32-bit
-  # inputs, and furthermore it also requires jax_enable_x64 to be set.
+  # inputs (since we don't have 128-bit integer types).
+  # TODO(phawkins): unless jax_enable_x64 is set, we won't have correct 64-bit
+  # types.
   dtype = c.GetShape(operand).numpy_dtype()
-  etype = xla_bridge.dtype_to_etype(dtype)
   bits = onp.finfo(dtype).bits
+  if not FLAGS.jax_enable_x64 and bits >= 32:
+    raise NotImplementedError(
+        ""Translation of select_and_gather_add requires flag --jax_enable_x64 ""
+        ""to be set"")
+  etype = xla_bridge.dtype_to_etype(dtype)
   uint_etype = xla_bridge.dtype_to_etype(_UINT_DTYPES[bits])
   pair_uint_dtype = _select_and_gather_add_pair_dtype(dtype)
   pair_uint_etype = xla_bridge.dtype_to_etype(pair_uint_dtype)",No
jax/test_util.py,jax/test_util.py,5f5baaa4cdea64545d3e546cbf595a3c835e6f4c,c092f0cafc100b107bbbc7695d3ed037270b247b,tweak tests for internal purposes,"diff --git a/jax/test_util.py b/jax/test_util.py
index c953c8ee4..9306f6d5f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -63,7 +63,7 @@ def numpy_eq(x, y):
   testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
   testing_x32 = not FLAGS.jax_enable_x64
   if testing_tpu or testing_x32:
-    return onp.allclose(x, y, 1e-3, 1e-3)
+    return onp.allclose(x, y, 1e-3, 1e-3, equal_nan=testing_tpu)
   else:
     return onp.allclose(x, y)
 
@@ -76,7 +76,7 @@ def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
     rtol = max(rtol, 1e-1)
   assert a.shape == b.shape
   return onp.allclose(a, b, atol=atol * a.size, rtol=rtol * b.size,
-                      equal_nan=equal_nan)
+                      equal_nan=equal_nan or testing_tpu)
 
 
 def check_eq(xs, ys):","diff --git a/jax/test_util.py b/jax/test_util.py
index c953c8ee4..9306f6d5f 100644
--- a/jax/test_util.py
+++ b/jax/test_util.py
@@ -63,7 +63,7 @@ def numpy_eq(x, y):
   testing_tpu = FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
   testing_x32 = not FLAGS.jax_enable_x64
   if testing_tpu or testing_x32:
-    return onp.allclose(x, y, 1e-3, 1e-3)
+    return onp.allclose(x, y, 1e-3, 1e-3, equal_nan=testing_tpu)
   else:
     return onp.allclose(x, y)
 
@@ -76,7 +76,7 @@ def numpy_close(a, b, atol=ATOL, rtol=RTOL, equal_nan=False):
     rtol = max(rtol, 1e-1)
   assert a.shape == b.shape
   return onp.allclose(a, b, atol=atol * a.size, rtol=rtol * b.size,
-                      equal_nan=equal_nan)
+                      equal_nan=equal_nan or testing_tpu)
 
 
 def check_eq(xs, ys):",No
tests/batching_test.py,tests/batching_test.py,5f5baaa4cdea64545d3e546cbf595a3c835e6f4c,c092f0cafc100b107bbbc7695d3ed037270b247b,tweak tests for internal purposes,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 9bf6cfa30..db438eb00 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -293,6 +293,7 @@ class BatchingTest(jtu.JaxTestCase):
     expected = np.array([True, False])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  @jtu.skip_on_devices(""tpu"")
   def testHessian(self):
     # test based on code from sindhwani@google
     def fun(x, t):","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 9bf6cfa30..db438eb00 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -293,6 +293,7 @@ class BatchingTest(jtu.JaxTestCase):
     expected = np.array([True, False])
     self.assertAllClose(ans, expected, check_dtypes=True)
 
+  @jtu.skip_on_devices(""tpu"")
   def testHessian(self):
     # test based on code from sindhwani@google
     def fun(x, t):",No
tests/lax_test.py,tests/lax_test.py,5f5baaa4cdea64545d3e546cbf595a3c835e6f4c,c092f0cafc100b107bbbc7695d3ed037270b247b,tweak tests for internal purposes,"diff --git a/tests/lax_test.py b/tests/lax_test.py
index e8550c442..d77b841e6 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1730,6 +1730,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
       for dtype in float_dtypes))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
+  @jtu.skip_on_devices(""tpu"")
   def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
     tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
     lhs = rng(lhs_shape, dtype)","diff --git a/tests/lax_test.py b/tests/lax_test.py
index e8550c442..d77b841e6 100644
--- a/tests/lax_test.py
+++ b/tests/lax_test.py
@@ -1730,6 +1730,7 @@ class LaxAutodiffTest(jtu.JaxTestCase):
       for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
       for dtype in float_dtypes))
   @jtu.skip_on_flag(""jax_xla_backend"", ""xrt"")
+  @jtu.skip_on_devices(""tpu"")
   def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
     tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
     lhs = rng(lhs_shape, dtype)",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,ae84919a56b87411064da00ec8026eb6cb67ccfe,af69d341a7236f76b49077f8f9e530e918eaeafe,Fix some TODOs that were previously blocked on a Jaxlib release.,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index a9c376b62..ad5c92ff4 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -149,8 +149,7 @@ JAX_REDUCER_RECORDS = [
     op_record(""prod"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, number_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""var"", 1, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    # TODO(phawkins): test inexact_dtypes for std after a jaxlib release.
-    op_record(""std"", 1, float_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""std"", 1, inexact_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index a9c376b62..ad5c92ff4 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -149,8 +149,7 @@ JAX_REDUCER_RECORDS = [
     op_record(""prod"", 1, number_dtypes, all_shapes, jtu.rand_small_positive(), []),
     op_record(""sum"", 1, number_dtypes, all_shapes, jtu.rand_default(), []),
     op_record(""var"", 1, number_dtypes, nonempty_shapes, jtu.rand_default(), []),
-    # TODO(phawkins): test inexact_dtypes for std after a jaxlib release.
-    op_record(""std"", 1, float_dtypes, nonempty_shapes, jtu.rand_default(), []),
+    op_record(""std"", 1, inexact_dtypes, nonempty_shapes, jtu.rand_default(), []),
 ]
 
 JAX_REDUCER_NO_DTYPE_RECORDS = [",No
tests/linalg_test.py,tests/linalg_test.py,ae84919a56b87411064da00ec8026eb6cb67ccfe,af69d341a7236f76b49077f8f9e530e918eaeafe,Fix some TODOs that were previously blocked on a Jaxlib release.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 9494a5ed6..ea2a5ac6d 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -45,10 +45,8 @@ def float_types():
              for dtype in [onp.float32, onp.float64])
 
 def complex_types():
-  return {onp.complex64}
-  # TODO(phawkins): change to the following after another jaxlib release.
-  #return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
-  #           for dtype in [onp.complex64, onp.complex128])
+  return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+             for dtype in [onp.complex64, onp.complex128])
 
 
 class NumpyLinalgTest(jtu.JaxTestCase):
@@ -304,9 +302,6 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLu(self, shape, dtype, rng):
-    # TODO(phawkins): remove this after a jaxlib release.
-    if not hasattr(lapack, ""jax_getrf""):
-      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng(shape, dtype)]
 
     self._CheckAgainstNumpy(jsp.linalg.lu, osp.linalg.lu, args_maker,
@@ -323,9 +318,6 @@ class ScipyLinalgTest(jtu.JaxTestCase):
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuGrad(self, shape, dtype, rng):
-    # TODO(phawkins): remove this after a jaxlib release.
-    if not hasattr(lapack, ""jax_getrf""):
-      self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
     jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-1)
@@ -341,9 +333,6 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuFactor(self, n, dtype, rng):
-    # TODO(phawkins): remove this after a jaxlib release.
-    if not hasattr(lapack, ""jax_getrf""):
-      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
     self._CheckAgainstNumpy(jsp.linalg.lu_factor, osp.linalg.lu_factor,","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index 9494a5ed6..ea2a5ac6d 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -45,10 +45,8 @@ def float_types():
              for dtype in [onp.float32, onp.float64])
 
 def complex_types():
-  return {onp.complex64}
-  # TODO(phawkins): change to the following after another jaxlib release.
-  #return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
-  #           for dtype in [onp.complex64, onp.complex128])
+  return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+             for dtype in [onp.complex64, onp.complex128])
 
 
 class NumpyLinalgTest(jtu.JaxTestCase):
@@ -304,9 +302,6 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLu(self, shape, dtype, rng):
-    # TODO(phawkins): remove this after a jaxlib release.
-    if not hasattr(lapack, ""jax_getrf""):
-      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng(shape, dtype)]
 
     self._CheckAgainstNumpy(jsp.linalg.lu, osp.linalg.lu, args_maker,
@@ -323,9 +318,6 @@ class ScipyLinalgTest(jtu.JaxTestCase):
   # TODO(phawkins): enable when there is an LU implementation for GPU/TPU.
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuGrad(self, shape, dtype, rng):
-    # TODO(phawkins): remove this after a jaxlib release.
-    if not hasattr(lapack, ""jax_getrf""):
-      self.skipTest(""No LU implementation available"")
     a = rng(shape, dtype)
 
     jtu.check_grads(jsp.linalg.lu, (a,), 2, rtol=1e-1)
@@ -341,9 +333,6 @@ class ScipyLinalgTest(jtu.JaxTestCase):
       for rng in [jtu.rand_default()]))
   @jtu.skip_on_devices(""gpu"", ""tpu"")
   def testLuFactor(self, n, dtype, rng):
-    # TODO(phawkins): remove this after a jaxlib release.
-    if not hasattr(lapack, ""jax_getrf""):
-      self.skipTest(""No LU implementation available"")
     args_maker = lambda: [rng((n, n), dtype)]
 
     self._CheckAgainstNumpy(jsp.linalg.lu_factor, osp.linalg.lu_factor,",No
jax/interpreters/parallel.py,jax/interpreters/parallel.py,d968e1e5729bee9d6d21a087eb5989e1110a65f6,bde6d08a7e37f00f22dd50d2e853db767685e2f4,fix replica_groups logic,"diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 38fd47b1d..3b73867d9 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -184,8 +184,10 @@ def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
 def psum_parallel_translation_rule(c, val, device_groups):
-  # return c.CrossReplicaSum(val, device_grp)  # TODO needs updated jaxlib
-  return c.CrossReplicaSum(val)
+  if len(device_groups) > 1:
+    return c.CrossReplicaSum(val, device_groups)
+  else:
+    return c.CrossReplicaSum(val)
 
 psum_p = PmapPrimitive('psum')
 pmap_primitive_rules[psum_p] = psum_pmap_rule","diff --git a/jax/interpreters/parallel.py b/jax/interpreters/parallel.py
index 38fd47b1d..3b73867d9 100644
--- a/jax/interpreters/parallel.py
+++ b/jax/interpreters/parallel.py
@@ -184,8 +184,10 @@ def psum_pmap_rule(val, axis):
   return val.sum(axis), None
 
 def psum_parallel_translation_rule(c, val, device_groups):
-  # return c.CrossReplicaSum(val, device_grp)  # TODO needs updated jaxlib
-  return c.CrossReplicaSum(val)
+  if len(device_groups) > 1:
+    return c.CrossReplicaSum(val, device_groups)
+  else:
+    return c.CrossReplicaSum(val)
 
 psum_p = PmapPrimitive('psum')
 pmap_primitive_rules[psum_p] = psum_pmap_rule",No
jax/interpreters/pxla.py,jax/interpreters/pxla.py,d968e1e5729bee9d6d21a087eb5989e1110a65f6,bde6d08a7e37f00f22dd50d2e853db767685e2f4,fix replica_groups logic,"diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index f78a41fe0..715765c9d 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -129,7 +129,8 @@ def replica_groups(mesh_spec, mesh_axis):
   """"""Given a mesh axis along which to operate, compute XLA replica_groups.""""""
   groups = onp.split(onp.arange(prod(mesh_spec)).reshape(mesh_spec),
                      mesh_spec[mesh_axis], axis=mesh_axis)
-  return tuple(tuple(group.ravel()) for group in groups)
+  groups = map(onp.ravel, groups)
+  return tuple(tuple(group) for group in zip(*groups))
 
 def split_array(x, num_splits, axis):
   """"""A special-case of numpy.split implemented in terms of indexing.""""""","diff --git a/jax/interpreters/pxla.py b/jax/interpreters/pxla.py
index f78a41fe0..715765c9d 100644
--- a/jax/interpreters/pxla.py
+++ b/jax/interpreters/pxla.py
@@ -129,7 +129,8 @@ def replica_groups(mesh_spec, mesh_axis):
   """"""Given a mesh axis along which to operate, compute XLA replica_groups.""""""
   groups = onp.split(onp.arange(prod(mesh_spec)).reshape(mesh_spec),
                      mesh_spec[mesh_axis], axis=mesh_axis)
-  return tuple(tuple(group.ravel()) for group in groups)
+  groups = map(onp.ravel, groups)
+  return tuple(tuple(group) for group in zip(*groups))
 
 def split_array(x, num_splits, axis):
   """"""A special-case of numpy.split implemented in terms of indexing.""""""",No
tests/linalg_test.py,tests/linalg_test.py,3cbcff37d356a72150e04a84991f0c32053350e8,ae84919a56b87411064da00ec8026eb6cb67ccfe,Use a set comprehension in linalg test.,"diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index ea2a5ac6d..2ecba5000 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -41,12 +41,12 @@ T = lambda x: onp.swapaxes(x, -1, -2)
 
 
 def float_types():
-  return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
-             for dtype in [onp.float32, onp.float64])
+  return {onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+          for dtype in [onp.float32, onp.float64]}
 
 def complex_types():
-  return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
-             for dtype in [onp.complex64, onp.complex128])
+  return {onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+          for dtype in [onp.complex64, onp.complex128]}
 
 
 class NumpyLinalgTest(jtu.JaxTestCase):","diff --git a/tests/linalg_test.py b/tests/linalg_test.py
index ea2a5ac6d..2ecba5000 100644
--- a/tests/linalg_test.py
+++ b/tests/linalg_test.py
@@ -41,12 +41,12 @@ T = lambda x: onp.swapaxes(x, -1, -2)
 
 
 def float_types():
-  return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
-             for dtype in [onp.float32, onp.float64])
+  return {onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+          for dtype in [onp.float32, onp.float64]}
 
 def complex_types():
-  return set(onp.dtype(xla_bridge.canonicalize_dtype(dtype))
-             for dtype in [onp.complex64, onp.complex128])
+  return {onp.dtype(xla_bridge.canonicalize_dtype(dtype))
+          for dtype in [onp.complex64, onp.complex128]}
 
 
 class NumpyLinalgTest(jtu.JaxTestCase):",No
jax/lax.py,jax/lax.py,a15bad401f33ff01e9e80df609f70a3bc11b50a0,d968e1e5729bee9d6d21a087eb5989e1110a65f6,"Added batching rules for convolutions + pooling.

Added batching rules:
conv_general_dilated_batch_rule
select_and_scatter_add_batch_rule
reduce_window_max_batch_rule
reduce_window_sum_batch_rule","diff --git a/jax/lax.py b/jax/lax.py
index f7ef847b1..59ddacb31 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1128,13 +1128,61 @@ def conv_general_dilated_translation_rule(
   return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                               rhs_dilation, dimension_numbers)
 
+def conv_general_dilated_batch_rule(
+    batched_args, batch_dims, window_strides, padding,
+    lhs_dilation, rhs_dilation, dimension_numbers, **unused_kwargs):
+  lhs, rhs = batched_args
+  lhs_bdim, rhs_bdim = batch_dims
+  lhs_dim, rhs_dim, out_dim = dimension_numbers
+
+  if lhs_bdim is not None and rhs_bdim is not None:
+    lhs = batching.move_dim_to_front(lhs, lhs_bdim)
+    rhs = batching.move_dim_to_front(rhs, rhs_bdim)
+
+    outputs = [
+        conv_general_dilated(l, r, window_strides, padding,
+                             lhs_dilation, rhs_dilation, dimension_numbers)
+        for l, r in zip(lhs, rhs)]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+
+  elif lhs_bdim is not None:
+    # Currently we don't handle cases where the batch dimension of the
+    # convolution isn't the first dimension.
+    if lhs_dim[0] != 0 or out_dim[0] != 0:
+      raise NotImplementedError
+    lhs = batching.move_dim_to_front(lhs, lhs_dim[0])
+    lhs = batching.move_dim_to_front(lhs, lhs_bdim)
+
+    batched_size = lhs.shape[0]
+    n_size = lhs.shape[1]
+
+    lhs = reshape(lhs, (batched_size * n_size,) + lhs.shape[2:])
+    outputs = conv_general_dilated(
+        lhs, rhs, window_strides, padding,
+        lhs_dilation, rhs_dilation, dimension_numbers)
+    outputs = reshape(outputs, (batched_size, n_size,) + outputs.shape[1:])
+
+    return outputs, 0
+  elif rhs_bdim is not None:
+    # TODO(schsam): Consider a loop instead of unrolling.
+    rhs = batching.move_dim_to_front(rhs, rhs_bdim)
+    outputs = [
+        conv_general_dilated(lhs, x, window_strides, padding,
+                                 lhs_dilation, rhs_dilation, dimension_numbers)
+        for x in rhs]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
 conv_general_dilated_p = standard_primitive(
     conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
     'conv_general_dilated', conv_general_dilated_translation_rule)
 ad.defbilinear(conv_general_dilated_p,
                conv_general_dilated_transpose_lhs,
                conv_general_dilated_transpose_rhs)
-
+batching.primitive_batchers[
+    conv_general_dilated_p] = conv_general_dilated_batch_rule
 
 def dot_shape_rule(lhs, rhs):
   if lhs.ndim == 0 or rhs.ndim == 0:
@@ -2291,12 +2339,26 @@ def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                               xla_bridge.get_xla_client().PaddingType.VALID)
   assert result.shape == input_shape
   return [result]
+def reduce_window_sum_batch_rule(
+    batched_args, bdims, window_dimensions, window_strides, padding, **kwargs):
+  operand, = batched_args
+  bdim, = bdims
+
+  if bdim is not None:
+    window_dimensions = \
+        window_dimensions[:bdim] + (1,) + window_dimensions[bdim:]
+    window_strides = window_strides[:bdim] + (1,) + window_strides[bdim:]
+
+  oprand = _reduce_window_sum(
+      operand, window_dimensions, window_strides, padding)
+
+  return oprand, 0
 
 reduce_window_sum_p = standard_primitive(
     reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
     reduce_window_sum_translation_rule)
 ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)
-
+batching.primitive_batchers[reduce_window_sum_p] = reduce_window_sum_batch_rule
 
 def reduce_window_chooser_translation_rule(
     prim, identity, c, operand, window_dimensions, window_strides, padding):
@@ -2338,6 +2400,20 @@ def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
       onp.subtract(operand_padded, window_dimensions), window_strides) + 1
   return tuple(t)
 
+def reduce_window_max_batch_rule(
+    batched_args, bdims, window_dimensions, window_strides, padding, **kwargs):
+  operand, = batched_args
+  bdim, = bdims
+
+  if bdim is not None:
+    window_dimensions = \
+        window_dimensions[:bdim] + (1,) + window_dimensions[bdim:]
+    window_strides = window_strides[:bdim] + (1,) + window_strides[bdim:]
+
+  operand = _reduce_window_max(
+      operand, window_dimensions, window_strides, padding)
+
+  return operand, 0
 
 reduce_window_max_translation_rule = partial(
     reduce_window_chooser_translation_rule, max_p, _get_max_identity)
@@ -2345,7 +2421,7 @@ reduce_window_max_p = standard_primitive(
     common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
     reduce_window_max_translation_rule)
 ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))
-
+batching.primitive_batchers[reduce_window_max_p] = reduce_window_max_batch_rule
 
 reduce_window_min_translation_rule = partial(
     reduce_window_chooser_translation_rule, min_p, _get_min_identity)
@@ -2402,12 +2478,40 @@ def select_and_scatter_add_transpose(
                                   window_strides, padding)
   return [result, None]
 
+def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
+  source, operand = batched_args
+  s_bdims, o_bdims = batch_dims
+
+  if s_bdims is not None and o_bdims is not None:
+    source = batching.move_dim_to_front(source, s_bdims)
+    operand = batching.move_dim_to_front(operand, o_bdims)
+    outputs = [
+        _select_and_scatter_add(s, o, **kwargs) for s, o in zip(source, operand)]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+  elif s_bdims is not None:
+    source = batching.move_dim_to_front(source, s_bdims)
+    outputs = [
+        _select_and_scatter_add(s, operand, **kwargs) for s in source]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+  elif o_bdims is not None:
+    operand = batching.move_dim_to_front(operand, o_bdims)
+    outputs = [
+        _select_and_scatter_add(source, o, **kwargs) for o in operand]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+
 select_and_scatter_add_p = standard_primitive(
     select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
     select_and_scatter_add_translation)
 ad.primitive_transposes[select_and_scatter_add_p] = \
     select_and_scatter_add_transpose
-
+batching.primitive_batchers[select_and_scatter_add_p] = \
+    select_and_scatter_add_batch_rule
 
 def _select_and_gather_add_shape_rule(
     tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,","diff --git a/jax/lax.py b/jax/lax.py
index f7ef847b1..59ddacb31 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1128,13 +1128,61 @@ def conv_general_dilated_translation_rule(
   return c.ConvGeneralDilated(lhs, rhs, window_strides, padding, lhs_dilation,
                               rhs_dilation, dimension_numbers)
 
+def conv_general_dilated_batch_rule(
+    batched_args, batch_dims, window_strides, padding,
+    lhs_dilation, rhs_dilation, dimension_numbers, **unused_kwargs):
+  lhs, rhs = batched_args
+  lhs_bdim, rhs_bdim = batch_dims
+  lhs_dim, rhs_dim, out_dim = dimension_numbers
+
+  if lhs_bdim is not None and rhs_bdim is not None:
+    lhs = batching.move_dim_to_front(lhs, lhs_bdim)
+    rhs = batching.move_dim_to_front(rhs, rhs_bdim)
+
+    outputs = [
+        conv_general_dilated(l, r, window_strides, padding,
+                             lhs_dilation, rhs_dilation, dimension_numbers)
+        for l, r in zip(lhs, rhs)]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+
+  elif lhs_bdim is not None:
+    # Currently we don't handle cases where the batch dimension of the
+    # convolution isn't the first dimension.
+    if lhs_dim[0] != 0 or out_dim[0] != 0:
+      raise NotImplementedError
+    lhs = batching.move_dim_to_front(lhs, lhs_dim[0])
+    lhs = batching.move_dim_to_front(lhs, lhs_bdim)
+
+    batched_size = lhs.shape[0]
+    n_size = lhs.shape[1]
+
+    lhs = reshape(lhs, (batched_size * n_size,) + lhs.shape[2:])
+    outputs = conv_general_dilated(
+        lhs, rhs, window_strides, padding,
+        lhs_dilation, rhs_dilation, dimension_numbers)
+    outputs = reshape(outputs, (batched_size, n_size,) + outputs.shape[1:])
+
+    return outputs, 0
+  elif rhs_bdim is not None:
+    # TODO(schsam): Consider a loop instead of unrolling.
+    rhs = batching.move_dim_to_front(rhs, rhs_bdim)
+    outputs = [
+        conv_general_dilated(lhs, x, window_strides, padding,
+                                 lhs_dilation, rhs_dilation, dimension_numbers)
+        for x in rhs]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
 conv_general_dilated_p = standard_primitive(
     conv_general_dilated_shape_rule, conv_general_dilated_dtype_rule,
     'conv_general_dilated', conv_general_dilated_translation_rule)
 ad.defbilinear(conv_general_dilated_p,
                conv_general_dilated_transpose_lhs,
                conv_general_dilated_transpose_rhs)
-
+batching.primitive_batchers[
+    conv_general_dilated_p] = conv_general_dilated_batch_rule
 
 def dot_shape_rule(lhs, rhs):
   if lhs.ndim == 0 or rhs.ndim == 0:
@@ -2291,12 +2339,26 @@ def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                               xla_bridge.get_xla_client().PaddingType.VALID)
   assert result.shape == input_shape
   return [result]
+def reduce_window_sum_batch_rule(
+    batched_args, bdims, window_dimensions, window_strides, padding, **kwargs):
+  operand, = batched_args
+  bdim, = bdims
+
+  if bdim is not None:
+    window_dimensions = \
+        window_dimensions[:bdim] + (1,) + window_dimensions[bdim:]
+    window_strides = window_strides[:bdim] + (1,) + window_strides[bdim:]
+
+  oprand = _reduce_window_sum(
+      operand, window_dimensions, window_strides, padding)
+
+  return oprand, 0
 
 reduce_window_sum_p = standard_primitive(
     reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum',
     reduce_window_sum_translation_rule)
 ad.deflinear(reduce_window_sum_p, reduce_window_sum_transpose_rule)
-
+batching.primitive_batchers[reduce_window_sum_p] = reduce_window_sum_batch_rule
 
 def reduce_window_chooser_translation_rule(
     prim, identity, c, operand, window_dimensions, window_strides, padding):
@@ -2338,6 +2400,20 @@ def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,
       onp.subtract(operand_padded, window_dimensions), window_strides) + 1
   return tuple(t)
 
+def reduce_window_max_batch_rule(
+    batched_args, bdims, window_dimensions, window_strides, padding, **kwargs):
+  operand, = batched_args
+  bdim, = bdims
+
+  if bdim is not None:
+    window_dimensions = \
+        window_dimensions[:bdim] + (1,) + window_dimensions[bdim:]
+    window_strides = window_strides[:bdim] + (1,) + window_strides[bdim:]
+
+  operand = _reduce_window_max(
+      operand, window_dimensions, window_strides, padding)
+
+  return operand, 0
 
 reduce_window_max_translation_rule = partial(
     reduce_window_chooser_translation_rule, max_p, _get_max_identity)
@@ -2345,7 +2421,7 @@ reduce_window_max_p = standard_primitive(
     common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max',
     reduce_window_max_translation_rule)
 ad.defjvp(reduce_window_max_p, partial(reduce_window_chooser_jvp_rule, max_p))
-
+batching.primitive_batchers[reduce_window_max_p] = reduce_window_max_batch_rule
 
 reduce_window_min_translation_rule = partial(
     reduce_window_chooser_translation_rule, min_p, _get_min_identity)
@@ -2402,12 +2478,40 @@ def select_and_scatter_add_transpose(
                                   window_strides, padding)
   return [result, None]
 
+def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
+  source, operand = batched_args
+  s_bdims, o_bdims = batch_dims
+
+  if s_bdims is not None and o_bdims is not None:
+    source = batching.move_dim_to_front(source, s_bdims)
+    operand = batching.move_dim_to_front(operand, o_bdims)
+    outputs = [
+        _select_and_scatter_add(s, o, **kwargs) for s, o in zip(source, operand)]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+  elif s_bdims is not None:
+    source = batching.move_dim_to_front(source, s_bdims)
+    outputs = [
+        _select_and_scatter_add(s, operand, **kwargs) for s in source]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+  elif o_bdims is not None:
+    operand = batching.move_dim_to_front(operand, o_bdims)
+    outputs = [
+        _select_and_scatter_add(source, o, **kwargs) for o in operand]
+    outputs = [reshape(out, (1,) + out.shape) for out in outputs]
+    outputs = concatenate(outputs, 0)
+    return outputs, 0
+
 select_and_scatter_add_p = standard_primitive(
     select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add',
     select_and_scatter_add_translation)
 ad.primitive_transposes[select_and_scatter_add_p] = \
     select_and_scatter_add_transpose
-
+batching.primitive_batchers[select_and_scatter_add_p] = \
+    select_and_scatter_add_batch_rule
 
 def _select_and_gather_add_shape_rule(
     tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,",No
tests/batching_test.py,tests/batching_test.py,dfced4fcc56670832341fbf911ec1646ad033a6d,a15bad401f33ff01e9e80df609f70a3bc11b50a0,Added tests for conv + pooling batching rules.,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index db438eb00..4b6b36b48 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -366,7 +366,97 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
     self.assertAllClose(sv, onp.broadcast_to(v[0, ::-1], (3, 4)),
                         check_dtypes=True)
-
+  
+  def testConvGeneralDilated(self):
+    W = onp.random.randn(3, 3, 1, 5)
+    X = onp.random.randn(10, 5, 5, 1)
+
+    def f(params, x):
+      one = (1, 1)
+      dimension_numbers = ('NHWC', 'HWIO', 'NHWC')
+      y = lax.conv_general_dilated(
+          x, params, one, 'SAME', one, one, dimension_numbers)
+      return y
+    grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
+
+    # NOTE(schsam): We have to do this reshape thing because the current
+    # implementation of conv general expects a 4d-matrix.
+
+    # Test forward prop.
+    per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example = np.reshape(per_example, (10, 5, 5, 5))
+    per_example_direct = f(W, X)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+    # Test gradients.
+    per_example = vmap(partial(grad_loss, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example_direct = []
+    for i in range(10):
+      g = grad_loss(W, np.reshape(X[i], (1, 5, 5, 1)))
+      per_example_direct += [
+          np.reshape(g, (1,) + g.shape)]
+    per_example_direct = np.concatenate(per_example_direct, axis=0)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+  def testMaxPool(self):
+    W = onp.random.randn(3, 3, 1, 5)
+    X = onp.random.randn(10, 5, 5, 1)
+
+    def f(params, x):
+      one = (1, 1)
+      dimension_numbers = ('NHWC', 'HWIO', 'NHWC')
+      y = lax.conv_general_dilated(
+          x, params, one, 'SAME', one, one, dimension_numbers)
+      y = lax.reduce_window(
+          y, -np.inf, lax.max, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')
+      return y
+    grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
+
+    # Test forward prop.
+    per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example = np.reshape(per_example, (10, 5, 5, 5))
+    per_example_direct = f(W, X)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+    # Test gradients.
+    per_example = vmap(partial(grad_loss, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example_direct = []
+    for i in range(10):
+      g = grad_loss(W, np.reshape(X[i], (1, 5, 5, 1)))
+      per_example_direct += [
+          np.reshape(g, (1,) + g.shape)]
+    per_example_direct = np.concatenate(per_example_direct, axis=0)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+  def testSumPool(self):
+    W = onp.random.randn(3, 3, 1, 5)
+    X = onp.random.randn(10, 5, 5, 1)
+
+    def f(params, x):
+      one = (1, 1)
+      dimension_numbers = ('NHWC', 'HWIO', 'NHWC')
+      y = lax.conv_general_dilated(
+          x, params, one, 'SAME', one, one, dimension_numbers)
+      y = lax.reduce_window(
+          y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')
+      return y
+    grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
+
+    # Test forward prop.
+    per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example = np.reshape(per_example, (10, 5, 5, 5))
+    per_example_direct = f(W, X)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+    # Test gradients.
+    per_example = vmap(partial(grad_loss, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example_direct = []
+    for i in range(10):
+      g = grad_loss(W, np.reshape(X[i], (1, 5, 5, 1)))
+      per_example_direct += [
+          np.reshape(g, (1,) + g.shape)]
+    per_example_direct = np.concatenate(per_example_direct, axis=0)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index db438eb00..4b6b36b48 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -366,7 +366,97 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(sk, k[:, ::-1], check_dtypes=True)
     self.assertAllClose(sv, onp.broadcast_to(v[0, ::-1], (3, 4)),
                         check_dtypes=True)
+  
+  def testConvGeneralDilated(self):
+    W = onp.random.randn(3, 3, 1, 5)
+    X = onp.random.randn(10, 5, 5, 1)
 
+    def f(params, x):
+      one = (1, 1)
+      dimension_numbers = ('NHWC', 'HWIO', 'NHWC')
+      y = lax.conv_general_dilated(
+          x, params, one, 'SAME', one, one, dimension_numbers)
+      return y
+    grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
+
+    # NOTE(schsam): We have to do this reshape thing because the current
+    # implementation of conv general expects a 4d-matrix.
+
+    # Test forward prop.
+    per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example = np.reshape(per_example, (10, 5, 5, 5))
+    per_example_direct = f(W, X)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+    # Test gradients.
+    per_example = vmap(partial(grad_loss, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example_direct = []
+    for i in range(10):
+      g = grad_loss(W, np.reshape(X[i], (1, 5, 5, 1)))
+      per_example_direct += [
+          np.reshape(g, (1,) + g.shape)]
+    per_example_direct = np.concatenate(per_example_direct, axis=0)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+  def testMaxPool(self):
+    W = onp.random.randn(3, 3, 1, 5)
+    X = onp.random.randn(10, 5, 5, 1)
+
+    def f(params, x):
+      one = (1, 1)
+      dimension_numbers = ('NHWC', 'HWIO', 'NHWC')
+      y = lax.conv_general_dilated(
+          x, params, one, 'SAME', one, one, dimension_numbers)
+      y = lax.reduce_window(
+          y, -np.inf, lax.max, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')
+      return y
+    grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
+
+    # Test forward prop.
+    per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example = np.reshape(per_example, (10, 5, 5, 5))
+    per_example_direct = f(W, X)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+    # Test gradients.
+    per_example = vmap(partial(grad_loss, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example_direct = []
+    for i in range(10):
+      g = grad_loss(W, np.reshape(X[i], (1, 5, 5, 1)))
+      per_example_direct += [
+          np.reshape(g, (1,) + g.shape)]
+    per_example_direct = np.concatenate(per_example_direct, axis=0)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+  def testSumPool(self):
+    W = onp.random.randn(3, 3, 1, 5)
+    X = onp.random.randn(10, 5, 5, 1)
+
+    def f(params, x):
+      one = (1, 1)
+      dimension_numbers = ('NHWC', 'HWIO', 'NHWC')
+      y = lax.conv_general_dilated(
+          x, params, one, 'SAME', one, one, dimension_numbers)
+      y = lax.reduce_window(
+          y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')
+      return y
+    grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
+
+    # Test forward prop.
+    per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example = np.reshape(per_example, (10, 5, 5, 5))
+    per_example_direct = f(W, X)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
+
+    # Test gradients.
+    per_example = vmap(partial(grad_loss, W))(np.reshape(X, (10, 1, 5, 5, 1)))
+    per_example_direct = []
+    for i in range(10):
+      g = grad_loss(W, np.reshape(X[i], (1, 5, 5, 1)))
+      per_example_direct += [
+          np.reshape(g, (1,) + g.shape)]
+    per_example_direct = np.concatenate(per_example_direct, axis=0)
+    self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
 
 if __name__ == '__main__':
   absltest.main()",Yes
tests/batching_test.py,tests/batching_test.py,6181560ef026597a0eefd5a1af6e27e40ea3a673,dfced4fcc56670832341fbf911ec1646ad033a6d,"Changed tests to always use float32, for conv compatibility.","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 4b6b36b48..26ebfb20a 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -368,8 +368,8 @@ class BatchingTest(jtu.JaxTestCase):
                         check_dtypes=True)
   
   def testConvGeneralDilated(self):
-    W = onp.random.randn(3, 3, 1, 5)
-    X = onp.random.randn(10, 5, 5, 1)
+    W = np.array(onp.random.randn(3, 3, 1, 5), dtype=onp.float32)
+    X = np.array(onp.random.randn(10, 5, 5, 1), dtype=onp.float32)
 
     def f(params, x):
       one = (1, 1)
@@ -399,8 +399,8 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
 
   def testMaxPool(self):
-    W = onp.random.randn(3, 3, 1, 5)
-    X = onp.random.randn(10, 5, 5, 1)
+    W = np.array(onp.random.randn(3, 3, 1, 5), dtype=onp.float32)
+    X = np.array(onp.random.randn(10, 5, 5, 1), dtype=onp.float32)
 
     def f(params, x):
       one = (1, 1)
@@ -429,8 +429,8 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
 
   def testSumPool(self):
-    W = onp.random.randn(3, 3, 1, 5)
-    X = onp.random.randn(10, 5, 5, 1)
+    W = np.array(onp.random.randn(3, 3, 1, 5), dtype=onp.float32)
+    X = np.array(onp.random.randn(10, 5, 5, 1), dtype=onp.float32)
 
     def f(params, x):
       one = (1, 1)","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 4b6b36b48..26ebfb20a 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -368,8 +368,8 @@ class BatchingTest(jtu.JaxTestCase):
                         check_dtypes=True)
   
   def testConvGeneralDilated(self):
-    W = onp.random.randn(3, 3, 1, 5)
-    X = onp.random.randn(10, 5, 5, 1)
+    W = np.array(onp.random.randn(3, 3, 1, 5), dtype=onp.float32)
+    X = np.array(onp.random.randn(10, 5, 5, 1), dtype=onp.float32)
 
     def f(params, x):
       one = (1, 1)
@@ -399,8 +399,8 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
 
   def testMaxPool(self):
-    W = onp.random.randn(3, 3, 1, 5)
-    X = onp.random.randn(10, 5, 5, 1)
+    W = np.array(onp.random.randn(3, 3, 1, 5), dtype=onp.float32)
+    X = np.array(onp.random.randn(10, 5, 5, 1), dtype=onp.float32)
 
     def f(params, x):
       one = (1, 1)
@@ -429,8 +429,8 @@ class BatchingTest(jtu.JaxTestCase):
     self.assertAllClose(per_example, per_example_direct, check_dtypes=True)
 
   def testSumPool(self):
-    W = onp.random.randn(3, 3, 1, 5)
-    X = onp.random.randn(10, 5, 5, 1)
+    W = np.array(onp.random.randn(3, 3, 1, 5), dtype=onp.float32)
+    X = np.array(onp.random.randn(10, 5, 5, 1), dtype=onp.float32)
 
     def f(params, x):
       one = (1, 1)",No
jax/interpreters/xla.py,jax/interpreters/xla.py,5dc15868ee0ef3f41ef82b63667798c031cde79d,5a52a9dc1b53edf79fd26fc5d34bfb9071f7806c,"Make translation rule for select_and_gather_add work even when --jax_enable_x64 is disabled.

Add support for constants whose types are not canonicalized by passing an optional flag to constant factories.

I am not entirely happy with the type canonicalization approach, but it seems good enough for this specific use case.","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 89696752b..478b4bfb2 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -156,7 +156,7 @@ def xla_destructure(c, ans):
   num_elements = len(c.GetShape(ans).tuple_shapes())
   return [c.GetTupleElement(ans, i) for i in range(num_elements)]
 
-def unit_constant(c, val):
+def unit_constant(c, val, canonicalize_types=True):
   assert not val  # must be unit
   return c.Tuple()
 xb.register_constant_handler(JaxTuple, unit_constant)
@@ -323,8 +323,10 @@ class DeviceArray(DeviceValue):
 core.pytype_aval_mappings[DeviceArray] = ConcreteArray
 pytype_aval_mappings[DeviceArray] = make_shaped_array
 canonicalize_dtype_handlers[DeviceArray] = identity
-xb.register_constant_handler(DeviceArray,
-                             lambda c, val: c.Constant(onp.asarray(val)))
+
+def _device_array_constant_handler(c, val, canonicalize_types=True):
+  return c.Constant(onp.asarray(val), canonicalize_types=canonicalize_types)
+xb.register_constant_handler(DeviceArray, _device_array_constant_handler)
 
 pytype_aval_mappings[ConcreteArray] = make_shaped_array
 pytype_aval_mappings[ShapedArray] = lambda x: x
@@ -332,7 +334,7 @@ pytype_aval_mappings[ShapedArray] = lambda x: x
 
 class DeviceConstant(DeviceArray):
   @staticmethod
-  def constant_handler(c, constant_instance):
+  def constant_handler(c, constant_instance, canonicalize_types=True):
     assert False
 
 def instantiate_device_constant(const, cutoff=1e6):","diff --git a/jax/interpreters/xla.py b/jax/interpreters/xla.py
index 89696752b..478b4bfb2 100644
--- a/jax/interpreters/xla.py
+++ b/jax/interpreters/xla.py
@@ -156,7 +156,7 @@ def xla_destructure(c, ans):
   num_elements = len(c.GetShape(ans).tuple_shapes())
   return [c.GetTupleElement(ans, i) for i in range(num_elements)]
 
-def unit_constant(c, val):
+def unit_constant(c, val, canonicalize_types=True):
   assert not val  # must be unit
   return c.Tuple()
 xb.register_constant_handler(JaxTuple, unit_constant)
@@ -323,8 +323,10 @@ class DeviceArray(DeviceValue):
 core.pytype_aval_mappings[DeviceArray] = ConcreteArray
 pytype_aval_mappings[DeviceArray] = make_shaped_array
 canonicalize_dtype_handlers[DeviceArray] = identity
-xb.register_constant_handler(DeviceArray,
-                             lambda c, val: c.Constant(onp.asarray(val)))
+
+def _device_array_constant_handler(c, val, canonicalize_types=True):
+  return c.Constant(onp.asarray(val), canonicalize_types=canonicalize_types)
+xb.register_constant_handler(DeviceArray, _device_array_constant_handler)
 
 pytype_aval_mappings[ConcreteArray] = make_shaped_array
 pytype_aval_mappings[ShapedArray] = lambda x: x
@@ -332,7 +334,7 @@ pytype_aval_mappings[ShapedArray] = lambda x: x
 
 class DeviceConstant(DeviceArray):
   @staticmethod
-  def constant_handler(c, constant_instance):
+  def constant_handler(c, constant_instance, canonicalize_types=True):
     assert False
 
 def instantiate_device_constant(const, cutoff=1e6):",No
jax/lax.py,jax/lax.py,5dc15868ee0ef3f41ef82b63667798c031cde79d,5a52a9dc1b53edf79fd26fc5d34bfb9071f7806c,"Make translation rule for select_and_gather_add work even when --jax_enable_x64 is disabled.

Add support for constants whose types are not canonicalized by passing an optional flag to constant factories.

I am not entirely happy with the type canonicalization approach, but it seems good enough for this specific use case.","diff --git a/jax/lax.py b/jax/lax.py
index f7ef847b1..a6c631332 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -399,13 +399,8 @@ def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
 
 def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                            window_strides, padding):
-  pair_dtype = _select_and_gather_add_pair_dtype(operand.dtype)
-  pair_select = partial(_select_and_gather_add_pair_reducer,
-                        dtype=operand.dtype, select_prim=select_prim)
-  jaxpr, consts = _reduction_jaxpr(pair_select, pair_dtype(0))
   return select_and_gather_add_p.bind(
       tangents, operand, select_prim=select_prim,
-      pair_select_jaxpr=jaxpr, pair_select_consts=consts,
       window_dimensions=tuple(window_dimensions),
       window_strides=tuple(window_strides), padding=padding)
 
@@ -1038,7 +1033,7 @@ def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
   return new_dtype
 
 def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
-  new_etype = xla_bridge.dtype_to_etype(new_dtype)
+  new_etype = xla_bridge.dtype_to_etype_exact(new_dtype)
   return c.ConvertElementType(operand, new_element_type=new_etype)
 
 convert_element_type_p = standard_primitive(
@@ -2410,8 +2405,7 @@ ad.primitive_transposes[select_and_scatter_add_p] = \
 
 
 def _select_and_gather_add_shape_rule(
-    tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
-    window_dimensions, window_strides, padding):
+    tangents, operand, select_prim, window_dimensions, window_strides, padding):
   if tangents.shape != operand.shape:
     msg = (""select_and_gather_add tangents and operand shapes must match, ""
            ""got {} and {}."")
@@ -2426,52 +2420,61 @@ _UINT_DTYPES = {
   64: onp.uint64,
 }
 
-def _select_and_gather_add_pair_dtype(dtype):
+def _select_and_gather_add_pair_reducer(dtype, select_prim):
   bits = onp.finfo(dtype).bits
-  return _UINT_DTYPES[bits * 2]
+  pair_uint_dtype = _UINT_DTYPES[bits * 2]
+  uint_etype = xla_bridge.dtype_to_etype_exact(_UINT_DTYPES[bits])
+  etype = xla_bridge.dtype_to_etype_exact(dtype)
+
+  c = xla_bridge.make_computation_builder(""select_and_gather_pair_reducer"")
+  x = c.ParameterWithShape(xla_bridge.Shape.array_shape(pair_uint_dtype, ()))
+  y = c.ParameterWithShape(xla_bridge.Shape.array_shape(pair_uint_dtype, ()))
+
+  bits_const = c.Constant(pair_uint_dtype(bits), canonicalize_types=False)
 
-def _select_and_gather_add_pair_reducer(x, y, dtype=None, select_prim=None):
-  bits = _dtype(x).type(onp.finfo(dtype).bits)
-  uint_dtype = _UINT_DTYPES[bits]
-  sx = convert_element_type(shift_right_logical(x, bits), uint_dtype)
-  sx = bitcast_convert_type(sx, dtype)
-  sy = convert_element_type(shift_right_logical(y, bits), uint_dtype)
-  sy = bitcast_convert_type(sy, dtype)
-  return select(select_prim.bind(sx, sy), x, y)
+  def fst(t):
+    st = c.ConvertElementType(c.ShiftRightLogical(t, bits_const), uint_etype)
+    return c.BitcastConvertType(st, etype)
+  sx = fst(x)
+  sy = fst(y)
+
+  assert select_prim is ge_p or select_prim is le_p
+  which = c.Ge(sx, sy) if select_prim is ge_p else c.Le(sx, sy)
+  c.Select(which, x, y)
+  return c.Build()
 
 def _select_and_gather_add_translation(
-    c, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
-    window_dimensions, window_strides, padding):
+    c, tangents, operand, select_prim, window_dimensions, window_strides,
+    padding):
   # XLA doesn't yet implement ReduceWindow on tuples (Google bug b/73062247), so
   # we implement a pair-wise ReduceWindow by packing two k-bit values into
-  # 2k-bit unsigned integer using bit tricks. This will only work for 32-bit
+  # 2k-bit unsigned integer using bit tricks. This will only work for <= 32-bit
   # inputs (since we don't have 128-bit integer types).
-  # TODO(phawkins): unless jax_enable_x64 is set, we won't have correct 64-bit
-  # types.
   dtype = c.GetShape(operand).numpy_dtype()
   bits = onp.finfo(dtype).bits
-  if not FLAGS.jax_enable_x64 and bits >= 32:
+  if bits > 32:
     raise NotImplementedError(
-        ""Translation of select_and_gather_add requires flag --jax_enable_x64 ""
-        ""to be set"")
+        ""select_and_gather_add is not implemented for type larger than 32 bits"")
   etype = xla_bridge.dtype_to_etype(dtype)
   uint_etype = xla_bridge.dtype_to_etype(_UINT_DTYPES[bits])
-  pair_uint_dtype = _select_and_gather_add_pair_dtype(dtype)
-  pair_uint_etype = xla_bridge.dtype_to_etype(pair_uint_dtype)
+  pair_uint_dtype = _UINT_DTYPES[bits * 2]
+  pair_uint_etype = xla_bridge.dtype_to_etype_exact(pair_uint_dtype)
+
   operand = c.BitcastConvertType(operand, uint_etype)
   tangents = c.BitcastConvertType(tangents, uint_etype)
   operand = c.ConvertElementType(operand, pair_uint_etype)
   tangents = c.ConvertElementType(tangents, pair_uint_etype)
-  operand = c.ShiftLeft(operand, c.Constant(pair_uint_dtype(bits)))
+  operand = c.ShiftLeft(
+    operand, c.Constant(pair_uint_dtype(bits), canonicalize_types=False))
 
   assert select_prim is ge_p or select_prim is le_p
   init = -onp.inf if select_prim is ge_p else onp.inf
   init = c.BitcastConvertType(c.Constant(dtype.type(init)), uint_etype)
   init = c.ConvertElementType(init, pair_uint_etype)
-  init = c.ShiftLeft(init, c.Constant(pair_uint_dtype(bits)))
+  init = c.ShiftLeft(
+    init, c.Constant(pair_uint_dtype(bits), canonicalize_types=False))
 
-  xla_computation = _reduction_computation(
-      c, pair_select_jaxpr, pair_select_consts, init)
+  xla_computation = _select_and_gather_add_pair_reducer(dtype, select_prim)
   out = c.ReduceWindow(c.Or(operand, tangents), init,
                        xla_computation, window_dimensions, window_strides,
                        padding)
@@ -2479,8 +2482,8 @@ def _select_and_gather_add_translation(
   return c.BitcastConvertType(out, etype)
 
 def _select_and_gather_add_transpose(
-    t, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
-    window_dimensions, window_strides, padding):
+    t, tangents, operand, select_prim, window_dimensions, window_strides,
+    padding):
   assert tangents is None and operand is not None
   result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                    window_strides, padding)
@@ -2637,9 +2640,10 @@ class FilledConstant(xla.DeviceConstant):
     return onp.full(self.shape, self.fill_value)
 
   @staticmethod
-  def constant_handler(c, filled_const):
-    return c.Broadcast(c.NumpyArrayConstant(filled_const.fill_value),
-                       filled_const.shape)
+  def constant_handler(c, filled_const, canonicalize_types=True):
+    return c.Broadcast(
+      c.NumpyArrayConstant(filled_const.fill_value, canonicalize_types),
+      filled_const.shape)
 
 
 class IotaConstant(xla.DeviceConstant):
@@ -2664,9 +2668,11 @@ class IotaConstant(xla.DeviceConstant):
     return self._npy_value
 
   @staticmethod
-  def constant_handler(c, iota_constant):
-    return c.BroadcastedIota(iota_constant.dtype, iota_constant.shape,
-                             iota_constant.axis)
+  def constant_handler(c, iota_constant, canonicalize_types=True):
+    dtype = iota_constant.dtype
+    if canonicalize_types:
+      dtype = xla_bridge.canonicalize_dtype(dtype)
+    return c.BroadcastedIota(dtype, iota_constant.shape, iota_constant.axis)
 
 
 class EyeConstant(xla.DeviceConstant):
@@ -2693,7 +2699,11 @@ class EyeConstant(xla.DeviceConstant):
     return self._npy_value
 
   @staticmethod
-  def constant_handler(c, diag_const):
+  def constant_handler(c, diag_const, canonicalize_types=True):
+    if canonicalize_types:
+      etype = xla_bridge.dtype_to_etype(diag_const.dtype)
+    else:
+      etype = xla_bridge.dtype_to_etype_exact(diag_const.dtype)
     etype = xla_bridge.dtype_to_etype(diag_const.dtype)
     iotas = [c.BroadcastedIota(onp.bool_, diag_const.shape, axis)
              for axis in diag_const.axes]","diff --git a/jax/lax.py b/jax/lax.py
index f7ef847b1..a6c631332 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -399,13 +399,8 @@ def _select_and_scatter_add(source, operand, select_prim, window_dimensions,
 
 def _select_and_gather_add(tangents, operand, select_prim, window_dimensions,
                            window_strides, padding):
-  pair_dtype = _select_and_gather_add_pair_dtype(operand.dtype)
-  pair_select = partial(_select_and_gather_add_pair_reducer,
-                        dtype=operand.dtype, select_prim=select_prim)
-  jaxpr, consts = _reduction_jaxpr(pair_select, pair_dtype(0))
   return select_and_gather_add_p.bind(
       tangents, operand, select_prim=select_prim,
-      pair_select_jaxpr=jaxpr, pair_select_consts=consts,
       window_dimensions=tuple(window_dimensions),
       window_strides=tuple(window_strides), padding=padding)
 
@@ -1038,7 +1033,7 @@ def convert_element_type_dtype_rule(operand, new_dtype, old_dtype):
   return new_dtype
 
 def convert_element_type_translation_rule(c, operand, new_dtype, old_dtype):
-  new_etype = xla_bridge.dtype_to_etype(new_dtype)
+  new_etype = xla_bridge.dtype_to_etype_exact(new_dtype)
   return c.ConvertElementType(operand, new_element_type=new_etype)
 
 convert_element_type_p = standard_primitive(
@@ -2410,8 +2405,7 @@ ad.primitive_transposes[select_and_scatter_add_p] = \
 
 
 def _select_and_gather_add_shape_rule(
-    tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
-    window_dimensions, window_strides, padding):
+    tangents, operand, select_prim, window_dimensions, window_strides, padding):
   if tangents.shape != operand.shape:
     msg = (""select_and_gather_add tangents and operand shapes must match, ""
            ""got {} and {}."")
@@ -2426,52 +2420,61 @@ _UINT_DTYPES = {
   64: onp.uint64,
 }
 
-def _select_and_gather_add_pair_dtype(dtype):
+def _select_and_gather_add_pair_reducer(dtype, select_prim):
   bits = onp.finfo(dtype).bits
-  return _UINT_DTYPES[bits * 2]
+  pair_uint_dtype = _UINT_DTYPES[bits * 2]
+  uint_etype = xla_bridge.dtype_to_etype_exact(_UINT_DTYPES[bits])
+  etype = xla_bridge.dtype_to_etype_exact(dtype)
 
-def _select_and_gather_add_pair_reducer(x, y, dtype=None, select_prim=None):
-  bits = _dtype(x).type(onp.finfo(dtype).bits)
-  uint_dtype = _UINT_DTYPES[bits]
-  sx = convert_element_type(shift_right_logical(x, bits), uint_dtype)
-  sx = bitcast_convert_type(sx, dtype)
-  sy = convert_element_type(shift_right_logical(y, bits), uint_dtype)
-  sy = bitcast_convert_type(sy, dtype)
-  return select(select_prim.bind(sx, sy), x, y)
+  c = xla_bridge.make_computation_builder(""select_and_gather_pair_reducer"")
+  x = c.ParameterWithShape(xla_bridge.Shape.array_shape(pair_uint_dtype, ()))
+  y = c.ParameterWithShape(xla_bridge.Shape.array_shape(pair_uint_dtype, ()))
+
+  bits_const = c.Constant(pair_uint_dtype(bits), canonicalize_types=False)
+
+  def fst(t):
+    st = c.ConvertElementType(c.ShiftRightLogical(t, bits_const), uint_etype)
+    return c.BitcastConvertType(st, etype)
+  sx = fst(x)
+  sy = fst(y)
+
+  assert select_prim is ge_p or select_prim is le_p
+  which = c.Ge(sx, sy) if select_prim is ge_p else c.Le(sx, sy)
+  c.Select(which, x, y)
+  return c.Build()
 
 def _select_and_gather_add_translation(
-    c, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
-    window_dimensions, window_strides, padding):
+    c, tangents, operand, select_prim, window_dimensions, window_strides,
+    padding):
   # XLA doesn't yet implement ReduceWindow on tuples (Google bug b/73062247), so
   # we implement a pair-wise ReduceWindow by packing two k-bit values into
-  # 2k-bit unsigned integer using bit tricks. This will only work for 32-bit
+  # 2k-bit unsigned integer using bit tricks. This will only work for <= 32-bit
   # inputs (since we don't have 128-bit integer types).
-  # TODO(phawkins): unless jax_enable_x64 is set, we won't have correct 64-bit
-  # types.
   dtype = c.GetShape(operand).numpy_dtype()
   bits = onp.finfo(dtype).bits
-  if not FLAGS.jax_enable_x64 and bits >= 32:
+  if bits > 32:
     raise NotImplementedError(
-        ""Translation of select_and_gather_add requires flag --jax_enable_x64 ""
-        ""to be set"")
+        ""select_and_gather_add is not implemented for type larger than 32 bits"")
   etype = xla_bridge.dtype_to_etype(dtype)
   uint_etype = xla_bridge.dtype_to_etype(_UINT_DTYPES[bits])
-  pair_uint_dtype = _select_and_gather_add_pair_dtype(dtype)
-  pair_uint_etype = xla_bridge.dtype_to_etype(pair_uint_dtype)
+  pair_uint_dtype = _UINT_DTYPES[bits * 2]
+  pair_uint_etype = xla_bridge.dtype_to_etype_exact(pair_uint_dtype)
+
   operand = c.BitcastConvertType(operand, uint_etype)
   tangents = c.BitcastConvertType(tangents, uint_etype)
   operand = c.ConvertElementType(operand, pair_uint_etype)
   tangents = c.ConvertElementType(tangents, pair_uint_etype)
-  operand = c.ShiftLeft(operand, c.Constant(pair_uint_dtype(bits)))
+  operand = c.ShiftLeft(
+    operand, c.Constant(pair_uint_dtype(bits), canonicalize_types=False))
 
   assert select_prim is ge_p or select_prim is le_p
   init = -onp.inf if select_prim is ge_p else onp.inf
   init = c.BitcastConvertType(c.Constant(dtype.type(init)), uint_etype)
   init = c.ConvertElementType(init, pair_uint_etype)
-  init = c.ShiftLeft(init, c.Constant(pair_uint_dtype(bits)))
+  init = c.ShiftLeft(
+    init, c.Constant(pair_uint_dtype(bits), canonicalize_types=False))
 
-  xla_computation = _reduction_computation(
-      c, pair_select_jaxpr, pair_select_consts, init)
+  xla_computation = _select_and_gather_add_pair_reducer(dtype, select_prim)
   out = c.ReduceWindow(c.Or(operand, tangents), init,
                        xla_computation, window_dimensions, window_strides,
                        padding)
@@ -2479,8 +2482,8 @@ def _select_and_gather_add_translation(
   return c.BitcastConvertType(out, etype)
 
 def _select_and_gather_add_transpose(
-    t, tangents, operand, select_prim, pair_select_jaxpr, pair_select_consts,
-    window_dimensions, window_strides, padding):
+    t, tangents, operand, select_prim, window_dimensions, window_strides,
+    padding):
   assert tangents is None and operand is not None
   result = _select_and_scatter_add(t, operand, select_prim, window_dimensions,
                                    window_strides, padding)
@@ -2637,9 +2640,10 @@ class FilledConstant(xla.DeviceConstant):
     return onp.full(self.shape, self.fill_value)
 
   @staticmethod
-  def constant_handler(c, filled_const):
-    return c.Broadcast(c.NumpyArrayConstant(filled_const.fill_value),
-                       filled_const.shape)
+  def constant_handler(c, filled_const, canonicalize_types=True):
+    return c.Broadcast(
+      c.NumpyArrayConstant(filled_const.fill_value, canonicalize_types),
+      filled_const.shape)
 
 
 class IotaConstant(xla.DeviceConstant):
@@ -2664,9 +2668,11 @@ class IotaConstant(xla.DeviceConstant):
     return self._npy_value
 
   @staticmethod
-  def constant_handler(c, iota_constant):
-    return c.BroadcastedIota(iota_constant.dtype, iota_constant.shape,
-                             iota_constant.axis)
+  def constant_handler(c, iota_constant, canonicalize_types=True):
+    dtype = iota_constant.dtype
+    if canonicalize_types:
+      dtype = xla_bridge.canonicalize_dtype(dtype)
+    return c.BroadcastedIota(dtype, iota_constant.shape, iota_constant.axis)
 
 
 class EyeConstant(xla.DeviceConstant):
@@ -2693,7 +2699,11 @@ class EyeConstant(xla.DeviceConstant):
     return self._npy_value
 
   @staticmethod
-  def constant_handler(c, diag_const):
+  def constant_handler(c, diag_const, canonicalize_types=True):
+    if canonicalize_types:
+      etype = xla_bridge.dtype_to_etype(diag_const.dtype)
+    else:
+      etype = xla_bridge.dtype_to_etype_exact(diag_const.dtype)
     etype = xla_bridge.dtype_to_etype(diag_const.dtype)
     iotas = [c.BroadcastedIota(onp.bool_, diag_const.shape, axis)
              for axis in diag_const.axes]",Yes
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,5dc15868ee0ef3f41ef82b63667798c031cde79d,5a52a9dc1b53edf79fd26fc5d34bfb9071f7806c,"Make translation rule for select_and_gather_add work even when --jax_enable_x64 is disabled.

Add support for constants whose types are not canonicalized by passing an optional flag to constant factories.

I am not entirely happy with the type canonicalization approach, but it seems good enough for this specific use case.","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 3dd4ea1de..c6454aeb1 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -206,6 +206,11 @@ def dtype_to_etype(dtype):
   """"""Convert from dtype to canonical etype (reading FLAGS.jax_enable_x64).""""""
   return _dtype_to_etype[canonicalize_dtype(dtype)]
 
+@memoize
+def dtype_to_etype_exact(dtype):
+  """"""Convert from dtype to exact etype (ignoring FLAGS.jax_enable_x64).""""""
+  return _dtype_to_etype[str(onp.dtype(dtype))]
+
 
 _dtype_to_32bit_dtype = {
     str(onp.dtype('int64')): onp.dtype('int32'),
@@ -301,15 +306,16 @@ class _JaxComputationBuilderBase(object):
     return super(_JaxComputationBuilderBase, self).ParameterWithShape(
         shape_of(value), name=name, parameter_num=parameter_num)
 
-  def NumpyArrayConstant(self, value):
-    normalized_value = normalize_to_xla_dtypes(value)
-    return super(_JaxComputationBuilderBase, self).Constant(normalized_value)
+  def NumpyArrayConstant(self, value, canonicalize_types=True):
+    if canonicalize_types:
+      value = normalize_to_xla_dtypes(value)
+    return super(_JaxComputationBuilderBase, self).Constant(value)
 
-  def ConstantLike(self, example_value, value):
+  def ConstantLike(self, example_value, value, canonicalize_types=True):
     example_value = onp.asarray(example_value)
     return self.Constant(onp.array(value).astype(example_value.dtype))
 
-  def Constant(self, py_val):
+  def Constant(self, py_val, canonicalize_types=True):
     """"""Translate constant `py_val` to a constant for this ComputationBuilder.
 
     Args:
@@ -320,7 +326,7 @@ class _JaxComputationBuilderBase(object):
     """"""
     py_type = type(py_val)
     if py_type in _constant_handlers:
-      return _constant_handlers[py_type](self, py_val)
+      return _constant_handlers[py_type](self, py_val, canonicalize_types)
     else:
       raise TypeError(""No constant handler for type: {}"".format(py_type))
 
@@ -341,7 +347,7 @@ def register_constant_handler(type_, handler_fun):
 _constant_handlers = {}
 
 
-def _ndarray_constant_handler(c, val):
+def _ndarray_constant_handler(c, val, canonicalize_types=True):
   """"""Constant handler for ndarray literals, handling zero-size strides.
 
   This function essentially calls c.NumpyArrayConstant(val) except it has
@@ -365,18 +371,21 @@ def _ndarray_constant_handler(c, val):
     other_axes, = onp.where(onp.not_equal(0, val.strides))
     collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None)
                               for ax in range(val.ndim))]
-    xla_val = c.Broadcast(c.NumpyArrayConstant(collapsed_val),
-                          onp.take(val.shape, zero_stride_axes))
+    xla_val = c.Broadcast(
+        c.NumpyArrayConstant(collapsed_val, canonicalize_types),
+        onp.take(val.shape, zero_stride_axes))
     permutation = onp.argsort(tuple(zero_stride_axes) + tuple(other_axes))
     return c.Transpose(xla_val, permutation)
   else:
-    return c.NumpyArrayConstant(val)
+    return c.NumpyArrayConstant(val, canonicalize_types)
 register_constant_handler(onp.ndarray, _ndarray_constant_handler)
 
 
+def _scalar_constant_handler(c, val, canonicalize_types=True):
+  return c.NumpyArrayConstant(val, canonicalize_types)
+
 for scalar_type in [onp.int8, onp.int16, onp.int32, onp.int64,
                     onp.uint8, onp.uint16, onp.uint32, onp.uint64,
                     onp.float16, onp.float32, onp.float64, onp.float128,
                     float, int, bool, onp.bool_]:
-  register_constant_handler(scalar_type,
-                            lambda c, val: c.NumpyArrayConstant(val))
+  register_constant_handler(scalar_type, _scalar_constant_handler)","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 3dd4ea1de..c6454aeb1 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -206,6 +206,11 @@ def dtype_to_etype(dtype):
   """"""Convert from dtype to canonical etype (reading FLAGS.jax_enable_x64).""""""
   return _dtype_to_etype[canonicalize_dtype(dtype)]
 
+@memoize
+def dtype_to_etype_exact(dtype):
+  """"""Convert from dtype to exact etype (ignoring FLAGS.jax_enable_x64).""""""
+  return _dtype_to_etype[str(onp.dtype(dtype))]
+
 
 _dtype_to_32bit_dtype = {
     str(onp.dtype('int64')): onp.dtype('int32'),
@@ -301,15 +306,16 @@ class _JaxComputationBuilderBase(object):
     return super(_JaxComputationBuilderBase, self).ParameterWithShape(
         shape_of(value), name=name, parameter_num=parameter_num)
 
-  def NumpyArrayConstant(self, value):
-    normalized_value = normalize_to_xla_dtypes(value)
-    return super(_JaxComputationBuilderBase, self).Constant(normalized_value)
+  def NumpyArrayConstant(self, value, canonicalize_types=True):
+    if canonicalize_types:
+      value = normalize_to_xla_dtypes(value)
+    return super(_JaxComputationBuilderBase, self).Constant(value)
 
-  def ConstantLike(self, example_value, value):
+  def ConstantLike(self, example_value, value, canonicalize_types=True):
     example_value = onp.asarray(example_value)
     return self.Constant(onp.array(value).astype(example_value.dtype))
 
-  def Constant(self, py_val):
+  def Constant(self, py_val, canonicalize_types=True):
     """"""Translate constant `py_val` to a constant for this ComputationBuilder.
 
     Args:
@@ -320,7 +326,7 @@ class _JaxComputationBuilderBase(object):
     """"""
     py_type = type(py_val)
     if py_type in _constant_handlers:
-      return _constant_handlers[py_type](self, py_val)
+      return _constant_handlers[py_type](self, py_val, canonicalize_types)
     else:
       raise TypeError(""No constant handler for type: {}"".format(py_type))
 
@@ -341,7 +347,7 @@ def register_constant_handler(type_, handler_fun):
 _constant_handlers = {}
 
 
-def _ndarray_constant_handler(c, val):
+def _ndarray_constant_handler(c, val, canonicalize_types=True):
   """"""Constant handler for ndarray literals, handling zero-size strides.
 
   This function essentially calls c.NumpyArrayConstant(val) except it has
@@ -365,18 +371,21 @@ def _ndarray_constant_handler(c, val):
     other_axes, = onp.where(onp.not_equal(0, val.strides))
     collapsed_val = val[tuple(0 if ax in zero_stride_axes else slice(None)
                               for ax in range(val.ndim))]
-    xla_val = c.Broadcast(c.NumpyArrayConstant(collapsed_val),
-                          onp.take(val.shape, zero_stride_axes))
+    xla_val = c.Broadcast(
+        c.NumpyArrayConstant(collapsed_val, canonicalize_types),
+        onp.take(val.shape, zero_stride_axes))
     permutation = onp.argsort(tuple(zero_stride_axes) + tuple(other_axes))
     return c.Transpose(xla_val, permutation)
   else:
-    return c.NumpyArrayConstant(val)
+    return c.NumpyArrayConstant(val, canonicalize_types)
 register_constant_handler(onp.ndarray, _ndarray_constant_handler)
 
 
+def _scalar_constant_handler(c, val, canonicalize_types=True):
+  return c.NumpyArrayConstant(val, canonicalize_types)
+
 for scalar_type in [onp.int8, onp.int16, onp.int32, onp.int64,
                     onp.uint8, onp.uint16, onp.uint32, onp.uint64,
                     onp.float16, onp.float32, onp.float64, onp.float128,
                     float, int, bool, onp.bool_]:
-  register_constant_handler(scalar_type,
-                            lambda c, val: c.NumpyArrayConstant(val))
+  register_constant_handler(scalar_type, _scalar_constant_handler)",No
WORKSPACE,WORKSPACE,86f5d189cf563b027c3cd00eea38072c003905c8,5a52a9dc1b53edf79fd26fc5d34bfb9071f7806c,Update XLA version to include fix for XLA reduce-window on CPU (https://github.com/tensorflow/tensorflow/commit/33bf4d37ff71950cdcffdfeda36d8e0995cc67b2),"diff --git a/WORKSPACE b/WORKSPACE
index 859bbff0e..307fe23ff 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""7cb4e7a3eea5c61c5d4674d20be642e49c77d7cda76113cb3c3bccd805de6c7f"",
-   strip_prefix = ""tensorflow-7a283b835b80808a9cdc6043998f29c8188342a5"",
+   sha256 = ""b6d83f1a9f0af38884e81c7284a1671a2e136d0c506bbed21468c049db9ed6c3"",
+   strip_prefix = ""tensorflow-33bf4d37ff71950cdcffdfeda36d8e0995cc67b2"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/7a283b835b80808a9cdc6043998f29c8188342a5.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/33bf4d37ff71950cdcffdfeda36d8e0995cc67b2.tar.gz"",
    ],
 )
 ","diff --git a/WORKSPACE b/WORKSPACE
index 859bbff0e..307fe23ff 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -17,10 +17,10 @@ http_archive(
 #    and update the sha256 with the result.
 http_archive(
    name = ""org_tensorflow"",
-   sha256 = ""7cb4e7a3eea5c61c5d4674d20be642e49c77d7cda76113cb3c3bccd805de6c7f"",
-   strip_prefix = ""tensorflow-7a283b835b80808a9cdc6043998f29c8188342a5"",
+   sha256 = ""b6d83f1a9f0af38884e81c7284a1671a2e136d0c506bbed21468c049db9ed6c3"",
+   strip_prefix = ""tensorflow-33bf4d37ff71950cdcffdfeda36d8e0995cc67b2"",
    urls = [
-       ""https://github.com/tensorflow/tensorflow/archive/7a283b835b80808a9cdc6043998f29c8188342a5.tar.gz"",
+       ""https://github.com/tensorflow/tensorflow/archive/33bf4d37ff71950cdcffdfeda36d8e0995cc67b2.tar.gz"",
    ],
 )
 ",No
jax/lax.py,jax/lax.py,3b8d43cefae3e37668c82ea4bd4d9ec61ccabff7,6181560ef026597a0eefd5a1af6e27e40ea3a673,Misc. small fixes.,"diff --git a/jax/lax.py b/jax/lax.py
index 59ddacb31..08b92754c 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1136,9 +1136,9 @@ def conv_general_dilated_batch_rule(
   lhs_dim, rhs_dim, out_dim = dimension_numbers
 
   if lhs_bdim is not None and rhs_bdim is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     lhs = batching.move_dim_to_front(lhs, lhs_bdim)
     rhs = batching.move_dim_to_front(rhs, rhs_bdim)
-
     outputs = [
         conv_general_dilated(l, r, window_strides, padding,
                              lhs_dilation, rhs_dilation, dimension_numbers)
@@ -1152,12 +1152,10 @@ def conv_general_dilated_batch_rule(
     # convolution isn't the first dimension.
     if lhs_dim[0] != 0 or out_dim[0] != 0:
       raise NotImplementedError
-    lhs = batching.move_dim_to_front(lhs, lhs_dim[0])
+      
     lhs = batching.move_dim_to_front(lhs, lhs_bdim)
-
     batched_size = lhs.shape[0]
     n_size = lhs.shape[1]
-
     lhs = reshape(lhs, (batched_size * n_size,) + lhs.shape[2:])
     outputs = conv_general_dilated(
         lhs, rhs, window_strides, padding,
@@ -1166,7 +1164,7 @@ def conv_general_dilated_batch_rule(
 
     return outputs, 0
   elif rhs_bdim is not None:
-    # TODO(schsam): Consider a loop instead of unrolling.
+    #TODO(#212): use a map construct instead of unrolling.
     rhs = batching.move_dim_to_front(rhs, rhs_bdim)
     outputs = [
         conv_general_dilated(lhs, x, window_strides, padding,
@@ -2339,6 +2337,7 @@ def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                               xla_bridge.get_xla_client().PaddingType.VALID)
   assert result.shape == input_shape
   return [result]
+
 def reduce_window_sum_batch_rule(
     batched_args, bdims, window_dimensions, window_strides, padding, **kwargs):
   operand, = batched_args
@@ -2483,6 +2482,7 @@ def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
   s_bdims, o_bdims = batch_dims
 
   if s_bdims is not None and o_bdims is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     source = batching.move_dim_to_front(source, s_bdims)
     operand = batching.move_dim_to_front(operand, o_bdims)
     outputs = [
@@ -2491,6 +2491,7 @@ def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
     outputs = concatenate(outputs, 0)
     return outputs, 0
   elif s_bdims is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     source = batching.move_dim_to_front(source, s_bdims)
     outputs = [
         _select_and_scatter_add(s, operand, **kwargs) for s in source]
@@ -2498,6 +2499,7 @@ def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
     outputs = concatenate(outputs, 0)
     return outputs, 0
   elif o_bdims is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     operand = batching.move_dim_to_front(operand, o_bdims)
     outputs = [
         _select_and_scatter_add(source, o, **kwargs) for o in operand]","diff --git a/jax/lax.py b/jax/lax.py
index 59ddacb31..08b92754c 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1136,9 +1136,9 @@ def conv_general_dilated_batch_rule(
   lhs_dim, rhs_dim, out_dim = dimension_numbers
 
   if lhs_bdim is not None and rhs_bdim is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     lhs = batching.move_dim_to_front(lhs, lhs_bdim)
     rhs = batching.move_dim_to_front(rhs, rhs_bdim)
-
     outputs = [
         conv_general_dilated(l, r, window_strides, padding,
                              lhs_dilation, rhs_dilation, dimension_numbers)
@@ -1152,12 +1152,10 @@ def conv_general_dilated_batch_rule(
     # convolution isn't the first dimension.
     if lhs_dim[0] != 0 or out_dim[0] != 0:
       raise NotImplementedError
-    lhs = batching.move_dim_to_front(lhs, lhs_dim[0])
+      
     lhs = batching.move_dim_to_front(lhs, lhs_bdim)
-
     batched_size = lhs.shape[0]
     n_size = lhs.shape[1]
-
     lhs = reshape(lhs, (batched_size * n_size,) + lhs.shape[2:])
     outputs = conv_general_dilated(
         lhs, rhs, window_strides, padding,
@@ -1166,7 +1164,7 @@ def conv_general_dilated_batch_rule(
 
     return outputs, 0
   elif rhs_bdim is not None:
-    # TODO(schsam): Consider a loop instead of unrolling.
+    #TODO(#212): use a map construct instead of unrolling.
     rhs = batching.move_dim_to_front(rhs, rhs_bdim)
     outputs = [
         conv_general_dilated(lhs, x, window_strides, padding,
@@ -2339,6 +2337,7 @@ def reduce_window_sum_transpose_rule(cotangent, window_dimensions,
                               xla_bridge.get_xla_client().PaddingType.VALID)
   assert result.shape == input_shape
   return [result]
+
 def reduce_window_sum_batch_rule(
     batched_args, bdims, window_dimensions, window_strides, padding, **kwargs):
   operand, = batched_args
@@ -2483,6 +2482,7 @@ def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
   s_bdims, o_bdims = batch_dims
 
   if s_bdims is not None and o_bdims is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     source = batching.move_dim_to_front(source, s_bdims)
     operand = batching.move_dim_to_front(operand, o_bdims)
     outputs = [
@@ -2491,6 +2491,7 @@ def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
     outputs = concatenate(outputs, 0)
     return outputs, 0
   elif s_bdims is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     source = batching.move_dim_to_front(source, s_bdims)
     outputs = [
         _select_and_scatter_add(s, operand, **kwargs) for s in source]
@@ -2498,6 +2499,7 @@ def select_and_scatter_add_batch_rule(batched_args, batch_dims, **kwargs):
     outputs = concatenate(outputs, 0)
     return outputs, 0
   elif o_bdims is not None:
+    #TODO(#212): use a map construct instead of unrolling.
     operand = batching.move_dim_to_front(operand, o_bdims)
     outputs = [
         _select_and_scatter_add(source, o, **kwargs) for o in operand]",No
tests/batching_test.py,tests/batching_test.py,e538eb1a53700068523b2c5377e868c62e1722d9,3b8d43cefae3e37668c82ea4bd4d9ec61ccabff7,Removed comment from test.,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 26ebfb20a..cae4ae84e 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -379,9 +379,6 @@ class BatchingTest(jtu.JaxTestCase):
       return y
     grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
 
-    # NOTE(schsam): We have to do this reshape thing because the current
-    # implementation of conv general expects a 4d-matrix.
-
     # Test forward prop.
     per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
     per_example = np.reshape(per_example, (10, 5, 5, 5))","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 26ebfb20a..cae4ae84e 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -379,9 +379,6 @@ class BatchingTest(jtu.JaxTestCase):
       return y
     grad_loss = grad(lambda params, x: np.mean(f(params, x) ** 2))
 
-    # NOTE(schsam): We have to do this reshape thing because the current
-    # implementation of conv general expects a 4d-matrix.
-
     # Test forward prop.
     per_example = vmap(partial(f, W))(np.reshape(X, (10, 1, 5, 5, 1)))
     per_example = np.reshape(per_example, (10, 5, 5, 5))",No
