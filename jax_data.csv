old_filepath,new_filepath,commitSHA,parentcommitSHA,commit_message,diff_myers1,diff_hist2,Discrepancy
jax/lib/xla_bridge.py,jax/lib/xla_bridge.py,1d902436ce39b8caf1e8e5e3dc2f27990b9fda0b,a30e858e59d7184b9e54dc3f3955238221d70439,change xla_data_pb2 import,"diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 4dc7f2bb8..6ec5e8fb0 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -29,7 +29,7 @@ import warnings
 from absl import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
-from tensorflow.compiler.xla import xla_data_pb2
+from . import xla_data_pb2
 from . import xla_client
 
 ","diff --git a/jax/lib/xla_bridge.py b/jax/lib/xla_bridge.py
index 4dc7f2bb8..6ec5e8fb0 100644
--- a/jax/lib/xla_bridge.py
+++ b/jax/lib/xla_bridge.py
@@ -29,7 +29,7 @@ import warnings
 from absl import flags
 import numpy as onp  # 'onp' rather than 'np' to distinguish from autograd.numpy
 
-from tensorflow.compiler.xla import xla_data_pb2
+from . import xla_data_pb2
 from . import xla_client
 
 ",No
WORKSPACE,build/WORKSPACE,fd8d83dca27fd8e8b642743e669bda6285f66e9e,f78c05195d4edab4baf9664acc3f4aab9f09281f,tweak build file,"diff --git a/build/WORKSPACE b/build/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/build/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/build/WORKSPACE b/build/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/build/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
build/build_jax.sh,build/build_jax.sh,fd8d83dca27fd8e8b642743e669bda6285f66e9e,f78c05195d4edab4baf9664acc3f4aab9f09281f,tweak build file,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index dcdec3484..708f78e16 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,15 @@
 #!/bin/bash
 set -exv
 
+init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
+if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
+then
+  (>&2 echo ""must be executed from jax repo root"")
+  exit 1
+fi
+
+cp build/WORKSPACE .
+
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -75,5 +84,6 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
+rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}","diff --git a/build/build_jax.sh b/build/build_jax.sh
index dcdec3484..708f78e16 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,15 @@
 #!/bin/bash
 set -exv
 
+init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
+if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
+then
+  (>&2 echo ""must be executed from jax repo root"")
+  exit 1
+fi
+
+cp build/WORKSPACE .
+
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -75,5 +84,6 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
+rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}",No
jax/examples/__init__.py,examples/__init__.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/__init__.py b/examples/__init__.py
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/examples/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.","diff --git a/examples/__init__.py b/examples/__init__.py
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/examples/__init__.py
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.",No
jax/examples/datasets.py,examples/datasets.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/datasets.py b/examples/datasets.py
new file mode 100644
index 000000000..4178017fa
--- /dev/null
+++ b/examples/datasets.py
@@ -0,0 +1,96 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Datasets used in examples.""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import array
+import gzip
+import os
+from os import path
+import struct
+import urllib2
+
+import numpy as np
+
+
+_DATA = ""/tmp/jax_example_data/""
+
+
+def _download(url, filename):
+  """"""Download a url to a file in the JAX data temp directory.""""""
+  if not path.exists(_DATA):
+    os.makedirs(_DATA)
+  out_file = path.join(_DATA, filename)
+  if not path.isfile(out_file):
+    with open(out_file, ""wb"") as f:
+      f.write(urllib2.urlopen(url, out_file).read())
+      print(""downloaded {} to {}"".format(url, _DATA))
+
+
+def _partial_flatten(x):
+  """"""Flatten all but the first dimension of an ndarray.""""""
+  return np.reshape(x, (x.shape[0], -1))
+
+
+def _one_hot(x, k, dtype=np.float32):
+  """"""Create a one-hot encoding of x of size k.""""""
+  return np.array(x[:, None] == np.arange(k), dtype)
+
+
+def mnist_raw():
+  """"""Download and parse the raw MNIST dataset.""""""
+  base_url = ""http://yann.lecun.com/exdb/mnist/""
+
+  def parse_labels(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _ = struct.unpack("">II"", fh.read(8))
+      return np.array(array.array(""B"", fh.read()), dtype=np.uint8)
+
+  def parse_images(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _, num_data, rows, cols = struct.unpack("">IIII"", fh.read(16))
+      return np.array(array.array(""B"", fh.read()),
+                      dtype=np.uint8).reshape(num_data, rows, cols)
+
+  for filename in [""train-images-idx3-ubyte.gz"", ""train-labels-idx1-ubyte.gz"",
+                   ""t10k-images-idx3-ubyte.gz"", ""t10k-labels-idx1-ubyte.gz""]:
+    _download(base_url + filename, filename)
+
+  train_images = parse_images(path.join(_DATA, ""train-images-idx3-ubyte.gz""))
+  train_labels = parse_labels(path.join(_DATA, ""train-labels-idx1-ubyte.gz""))
+  test_images = parse_images(path.join(_DATA, ""t10k-images-idx3-ubyte.gz""))
+  test_labels = parse_labels(path.join(_DATA, ""t10k-labels-idx1-ubyte.gz""))
+
+  return train_images, train_labels, test_images, test_labels
+
+
+def mnist(permute_train=False):
+  """"""Download, parse and process MNIST data to unit scale and one-hot labels.""""""
+  train_images, train_labels, test_images, test_labels = mnist_raw()
+
+  train_images = _partial_flatten(train_images) / np.float32(255.)
+  test_images = _partial_flatten(test_images) / np.float32(255.)
+  train_labels = _one_hot(train_labels, 10)
+  test_labels = _one_hot(test_labels, 10)
+
+  if permute_train:
+    perm = np.random.RandomState(0).permutation(train_images.shape[0])
+    train_images = train_images[perm]
+    train_labels = train_labels[perm]
+
+  return train_images, train_labels, test_images, test_labels","diff --git a/examples/datasets.py b/examples/datasets.py
new file mode 100644
index 000000000..4178017fa
--- /dev/null
+++ b/examples/datasets.py
@@ -0,0 +1,96 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Datasets used in examples.""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import array
+import gzip
+import os
+from os import path
+import struct
+import urllib2
+
+import numpy as np
+
+
+_DATA = ""/tmp/jax_example_data/""
+
+
+def _download(url, filename):
+  """"""Download a url to a file in the JAX data temp directory.""""""
+  if not path.exists(_DATA):
+    os.makedirs(_DATA)
+  out_file = path.join(_DATA, filename)
+  if not path.isfile(out_file):
+    with open(out_file, ""wb"") as f:
+      f.write(urllib2.urlopen(url, out_file).read())
+      print(""downloaded {} to {}"".format(url, _DATA))
+
+
+def _partial_flatten(x):
+  """"""Flatten all but the first dimension of an ndarray.""""""
+  return np.reshape(x, (x.shape[0], -1))
+
+
+def _one_hot(x, k, dtype=np.float32):
+  """"""Create a one-hot encoding of x of size k.""""""
+  return np.array(x[:, None] == np.arange(k), dtype)
+
+
+def mnist_raw():
+  """"""Download and parse the raw MNIST dataset.""""""
+  base_url = ""http://yann.lecun.com/exdb/mnist/""
+
+  def parse_labels(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _ = struct.unpack("">II"", fh.read(8))
+      return np.array(array.array(""B"", fh.read()), dtype=np.uint8)
+
+  def parse_images(filename):
+    with gzip.open(filename, ""rb"") as fh:
+      _, num_data, rows, cols = struct.unpack("">IIII"", fh.read(16))
+      return np.array(array.array(""B"", fh.read()),
+                      dtype=np.uint8).reshape(num_data, rows, cols)
+
+  for filename in [""train-images-idx3-ubyte.gz"", ""train-labels-idx1-ubyte.gz"",
+                   ""t10k-images-idx3-ubyte.gz"", ""t10k-labels-idx1-ubyte.gz""]:
+    _download(base_url + filename, filename)
+
+  train_images = parse_images(path.join(_DATA, ""train-images-idx3-ubyte.gz""))
+  train_labels = parse_labels(path.join(_DATA, ""train-labels-idx1-ubyte.gz""))
+  test_images = parse_images(path.join(_DATA, ""t10k-images-idx3-ubyte.gz""))
+  test_labels = parse_labels(path.join(_DATA, ""t10k-labels-idx1-ubyte.gz""))
+
+  return train_images, train_labels, test_images, test_labels
+
+
+def mnist(permute_train=False):
+  """"""Download, parse and process MNIST data to unit scale and one-hot labels.""""""
+  train_images, train_labels, test_images, test_labels = mnist_raw()
+
+  train_images = _partial_flatten(train_images) / np.float32(255.)
+  test_images = _partial_flatten(test_images) / np.float32(255.)
+  train_labels = _one_hot(train_labels, 10)
+  test_labels = _one_hot(test_labels, 10)
+
+  if permute_train:
+    perm = np.random.RandomState(0).permutation(train_images.shape[0])
+    train_images = train_images[perm]
+    train_labels = train_labels[perm]
+
+  return train_images, train_labels, test_images, test_labels",No
jax/examples/interactive.py,examples/interactive.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/interactive.py b/examples/interactive.py
new file mode 100644
index 000000000..e0c061aed
--- /dev/null
+++ b/examples/interactive.py
@@ -0,0 +1,32 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+
+from absl import app
+
+import IPython
+import numpy as onp
+
+from jax import lax
+from jax import numpy as np
+from jax import jit, grad, vmap
+
+
+def main(unused_argv):
+  IPython.embed(user_ns=dict(globals(), **locals()))
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/interactive.py b/examples/interactive.py
new file mode 100644
index 000000000..e0c061aed
--- /dev/null
+++ b/examples/interactive.py
@@ -0,0 +1,32 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+
+from absl import app
+
+import IPython
+import numpy as onp
+
+from jax import lax
+from jax import numpy as np
+from jax import jit, grad, vmap
+
+
+def main(unused_argv):
+  IPython.embed(user_ns=dict(globals(), **locals()))
+
+if __name__ == ""__main__"":
+  app.run(main)",No
jax/examples/mnist_classifier.py,examples/mnist_classifier.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
new file mode 100644
index 000000000..58b660024
--- /dev/null
+++ b/examples/mnist_classifier.py
@@ -0,0 +1,99 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic MNIST example using Numpy and JAX.
+
+The primary aim here is simplicity and minimal dependencies.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import time
+
+from absl import app
+import numpy.random as npr
+
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.scipy.misc import logsumexp
+import jax.numpy as np
+
+
+def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
+  return [(scale * rng.randn(m, n), scale * rng.randn(n))
+          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
+
+def predict(params, inputs):
+  for w, b in params:
+    outputs = np.dot(inputs, w) + b
+    inputs = np.tanh(outputs)
+  return outputs - logsumexp(outputs, axis=1, keepdims=True)
+
+def loss(params, batch):
+  inputs, targets = batch
+  preds = predict(params, inputs)
+  return -np.mean(preds * targets)
+
+def accuracy(params, batch):
+  inputs, targets = batch
+  target_class = np.argmax(targets, axis=1)
+  predicted_class = np.argmax(predict(params, inputs), axis=1)
+  return np.mean(predicted_class == target_class)
+
+
+def main(unused_argv):
+  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
+  param_scale = 0.1
+  step_size = 0.001
+  num_epochs = 10
+  batch_size = 32
+
+  train_images, train_labels, test_images, test_labels = datasets.mnist()
+  num_train = train_images.shape[0]
+  num_complete_batches, leftover = divmod(num_train, batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  def data_stream():
+    rng = npr.RandomState(0)
+    while True:
+      perm = rng.permutation(num_train)
+      for i in range(num_batches):
+        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
+        yield train_images[batch_idx], train_labels[batch_idx]
+  batches = data_stream()
+
+  @jit
+  def update(params, batch):
+    grads = grad(loss)(params, batch)
+    return [(w - step_size * dw, b - step_size * db)
+            for (w, b), (dw, db) in zip(params, grads)]
+
+  params = init_random_params(param_scale, layer_sizes)
+  for epoch in range(num_epochs):
+    start_time = time.time()
+    for _ in range(num_batches):
+      params = update(params, next(batches))
+    epoch_time = time.time() - start_time
+
+    train_acc = accuracy(params, (train_images, train_labels))
+    test_acc = accuracy(params, (test_images, test_labels))
+    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
+    print(""Training set accuracy {}"".format(train_acc))
+    print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
new file mode 100644
index 000000000..58b660024
--- /dev/null
+++ b/examples/mnist_classifier.py
@@ -0,0 +1,99 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic MNIST example using Numpy and JAX.
+
+The primary aim here is simplicity and minimal dependencies.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import time
+
+from absl import app
+import numpy.random as npr
+
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.scipy.misc import logsumexp
+import jax.numpy as np
+
+
+def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
+  return [(scale * rng.randn(m, n), scale * rng.randn(n))
+          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
+
+def predict(params, inputs):
+  for w, b in params:
+    outputs = np.dot(inputs, w) + b
+    inputs = np.tanh(outputs)
+  return outputs - logsumexp(outputs, axis=1, keepdims=True)
+
+def loss(params, batch):
+  inputs, targets = batch
+  preds = predict(params, inputs)
+  return -np.mean(preds * targets)
+
+def accuracy(params, batch):
+  inputs, targets = batch
+  target_class = np.argmax(targets, axis=1)
+  predicted_class = np.argmax(predict(params, inputs), axis=1)
+  return np.mean(predicted_class == target_class)
+
+
+def main(unused_argv):
+  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
+  param_scale = 0.1
+  step_size = 0.001
+  num_epochs = 10
+  batch_size = 32
+
+  train_images, train_labels, test_images, test_labels = datasets.mnist()
+  num_train = train_images.shape[0]
+  num_complete_batches, leftover = divmod(num_train, batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  def data_stream():
+    rng = npr.RandomState(0)
+    while True:
+      perm = rng.permutation(num_train)
+      for i in range(num_batches):
+        batch_idx = perm[i * batch_size:(i + 1) * batch_size]
+        yield train_images[batch_idx], train_labels[batch_idx]
+  batches = data_stream()
+
+  @jit
+  def update(params, batch):
+    grads = grad(loss)(params, batch)
+    return [(w - step_size * dw, b - step_size * db)
+            for (w, b), (dw, db) in zip(params, grads)]
+
+  params = init_random_params(param_scale, layer_sizes)
+  for epoch in range(num_epochs):
+    start_time = time.time()
+    for _ in range(num_batches):
+      params = update(params, next(batches))
+    epoch_time = time.time() - start_time
+
+    train_acc = accuracy(params, (train_images, train_labels))
+    test_acc = accuracy(params, (test_images, test_labels))
+    print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
+    print(""Training set accuracy {}"".format(train_acc))
+    print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
jax/examples/mnist_vae.py,examples/mnist_vae.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
new file mode 100644
index 000000000..163e59b38
--- /dev/null
+++ b/examples/mnist_vae.py
@@ -0,0 +1,145 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic variational autoencoder (VAE) on binarized MNIST using Numpy and JAX.
+
+This file uses the stax network definition library and the minmax optimization
+library.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import time
+
+from absl import app
+import matplotlib.pyplot as plt
+
+from jax import lax, random
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.experimental import minmax
+from jax.experimental import stax
+from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import jax.numpy as np
+
+
+def gaussian_kl(mu, sigmasq):
+  """"""KL divergence from a diagonal Gaussian to the standard Gaussian.""""""
+  return -0.5 * np.sum(1. + np.log(sigmasq) - mu**2. - sigmasq)
+
+def gaussian_sample(rng, mu, sigmasq):
+  """"""Sample a diagonal Gaussian.""""""
+  return mu + np.sqrt(sigmasq) * random.normal(rng, mu.shape)
+
+def bernoulli_logpdf(logits, x):
+  """"""Bernoulli log pdf of data x given logits.""""""
+  return -np.sum(np.logaddexp(0., np.where(x, -1., 1.) * logits))
+
+def elbo(rng, params, images):
+  """"""Monte Carlo estimate of the negative evidence lower bound.""""""
+  enc_params, dec_params = params
+  mu_z, sigmasq_z = encode(enc_params, images)
+  logits_x = decode(dec_params, gaussian_sample(rng, mu_z, sigmasq_z))
+  return bernoulli_logpdf(logits_x, images) - gaussian_kl(mu_z, sigmasq_z)
+
+def image_sample(rng, params, nrow, ncol):
+  """"""Sample images from the generative model.""""""
+  _, dec_params = params
+  code_rng, img_rng = random.split(rng)
+  logits = decode(dec_params, random.normal(code_rng, (nrow * ncol, 10)))
+  sampled_images = random.bernoulli(img_rng, np.logaddexp(0., logits))
+  return image_grid(nrow, ncol, sampled_images, (28, 28))
+
+def image_grid(nrow, ncol, imagevecs, imshape):
+  """"""Reshape a stack of image vectors into an image grid for plotting.""""""
+  images = iter(imagevecs.reshape((-1,) + imshape))
+  return np.vstack([np.hstack([next(images).T for _ in range(ncol)][::-1])
+                    for _ in range(nrow)]).T
+
+
+encoder_init, encode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    FanOut(2),
+    stax.parallel(Dense(10), stax.serial(Dense(10), Softplus)),
+)
+
+decoder_init, decode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    Dense(28 * 28),
+)
+
+
+def main(unused_argv):
+  step_size = 0.001
+  num_epochs = 100
+  batch_size = 32
+  nrow, ncol = 10, 10  # sampled image grid size
+  rng = random.PRNGKey(0)
+
+  test_rng = random.PRNGKey(1)  # fixed prng key for evaluation
+  imfile = os.path.join(os.getenv(""TMPDIR"", ""/tmp/""), ""mnist_vae_{:03d}.png"")
+
+  train_images, _, test_images, _ = datasets.mnist(permute_train=True)
+  num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  # TODO(mattjj): automatically keep large closed-over consts device-persistent
+  train_images = jit(lambda x: x)(train_images)  # dataset on device
+
+  _, init_encoder_params = encoder_init((batch_size, 28 * 28))
+  _, init_decoder_params = decoder_init((batch_size, 10))
+  init_params = init_encoder_params, init_decoder_params
+
+  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)
+
+  def binarize_batch(rng, i, images):
+    i = i % num_batches
+    batch = lax.dynamic_slice_in_dim(images, i * batch_size, batch_size)
+    return random.bernoulli(rng, batch)
+
+  @jit
+  def run_epoch(rng, opt_state, images):
+    def body_fun(i, (rng, opt_state, images)):
+      rng, elbo_rng, data_rng = random.split(rng, 3)
+      batch = binarize_batch(data_rng, i, images)
+      loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
+      g = grad(loss)(minmax.get_params(opt_state))
+      return rng, opt_update(i, g, opt_state), images
+    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))
+
+  @jit
+  def evaluate(opt_state, images):
+    params = minmax.get_params(opt_state)
+    elbo_rng, data_rng, image_rng = random.split(test_rng, 3)
+    binarized_test = random.bernoulli(data_rng, images)
+    test_elbo = elbo(elbo_rng, params, binarized_test) / images.shape[0]
+    sampled_images = image_sample(image_rng, params, nrow, ncol)
+    return test_elbo, sampled_images
+
+  opt_state = opt_init(init_params)
+  for epoch in range(num_epochs):
+    tic = time.time()
+    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
+    test_elbo, images = evaluate(opt_state, test_images)
+    print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
+    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
new file mode 100644
index 000000000..163e59b38
--- /dev/null
+++ b/examples/mnist_vae.py
@@ -0,0 +1,145 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""A basic variational autoencoder (VAE) on binarized MNIST using Numpy and JAX.
+
+This file uses the stax network definition library and the minmax optimization
+library.
+""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import time
+
+from absl import app
+import matplotlib.pyplot as plt
+
+from jax import lax, random
+from jax.api import jit, grad
+from jax.examples import datasets
+from jax.experimental import minmax
+from jax.experimental import stax
+from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import jax.numpy as np
+
+
+def gaussian_kl(mu, sigmasq):
+  """"""KL divergence from a diagonal Gaussian to the standard Gaussian.""""""
+  return -0.5 * np.sum(1. + np.log(sigmasq) - mu**2. - sigmasq)
+
+def gaussian_sample(rng, mu, sigmasq):
+  """"""Sample a diagonal Gaussian.""""""
+  return mu + np.sqrt(sigmasq) * random.normal(rng, mu.shape)
+
+def bernoulli_logpdf(logits, x):
+  """"""Bernoulli log pdf of data x given logits.""""""
+  return -np.sum(np.logaddexp(0., np.where(x, -1., 1.) * logits))
+
+def elbo(rng, params, images):
+  """"""Monte Carlo estimate of the negative evidence lower bound.""""""
+  enc_params, dec_params = params
+  mu_z, sigmasq_z = encode(enc_params, images)
+  logits_x = decode(dec_params, gaussian_sample(rng, mu_z, sigmasq_z))
+  return bernoulli_logpdf(logits_x, images) - gaussian_kl(mu_z, sigmasq_z)
+
+def image_sample(rng, params, nrow, ncol):
+  """"""Sample images from the generative model.""""""
+  _, dec_params = params
+  code_rng, img_rng = random.split(rng)
+  logits = decode(dec_params, random.normal(code_rng, (nrow * ncol, 10)))
+  sampled_images = random.bernoulli(img_rng, np.logaddexp(0., logits))
+  return image_grid(nrow, ncol, sampled_images, (28, 28))
+
+def image_grid(nrow, ncol, imagevecs, imshape):
+  """"""Reshape a stack of image vectors into an image grid for plotting.""""""
+  images = iter(imagevecs.reshape((-1,) + imshape))
+  return np.vstack([np.hstack([next(images).T for _ in range(ncol)][::-1])
+                    for _ in range(nrow)]).T
+
+
+encoder_init, encode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    FanOut(2),
+    stax.parallel(Dense(10), stax.serial(Dense(10), Softplus)),
+)
+
+decoder_init, decode = stax.serial(
+    Dense(512), Relu,
+    Dense(512), Relu,
+    Dense(28 * 28),
+)
+
+
+def main(unused_argv):
+  step_size = 0.001
+  num_epochs = 100
+  batch_size = 32
+  nrow, ncol = 10, 10  # sampled image grid size
+  rng = random.PRNGKey(0)
+
+  test_rng = random.PRNGKey(1)  # fixed prng key for evaluation
+  imfile = os.path.join(os.getenv(""TMPDIR"", ""/tmp/""), ""mnist_vae_{:03d}.png"")
+
+  train_images, _, test_images, _ = datasets.mnist(permute_train=True)
+  num_complete_batches, leftover = divmod(train_images.shape[0], batch_size)
+  num_batches = num_complete_batches + bool(leftover)
+
+  # TODO(mattjj): automatically keep large closed-over consts device-persistent
+  train_images = jit(lambda x: x)(train_images)  # dataset on device
+
+  _, init_encoder_params = encoder_init((batch_size, 28 * 28))
+  _, init_decoder_params = decoder_init((batch_size, 10))
+  init_params = init_encoder_params, init_decoder_params
+
+  opt_init, opt_update = minmax.momentum(step_size, mass=0.9)
+
+  def binarize_batch(rng, i, images):
+    i = i % num_batches
+    batch = lax.dynamic_slice_in_dim(images, i * batch_size, batch_size)
+    return random.bernoulli(rng, batch)
+
+  @jit
+  def run_epoch(rng, opt_state, images):
+    def body_fun(i, (rng, opt_state, images)):
+      rng, elbo_rng, data_rng = random.split(rng, 3)
+      batch = binarize_batch(data_rng, i, images)
+      loss = lambda params: -elbo(elbo_rng, params, batch) / batch_size
+      g = grad(loss)(minmax.get_params(opt_state))
+      return rng, opt_update(i, g, opt_state), images
+    return lax.fori_loop(0, num_batches, body_fun, (rng, opt_state, images))
+
+  @jit
+  def evaluate(opt_state, images):
+    params = minmax.get_params(opt_state)
+    elbo_rng, data_rng, image_rng = random.split(test_rng, 3)
+    binarized_test = random.bernoulli(data_rng, images)
+    test_elbo = elbo(elbo_rng, params, binarized_test) / images.shape[0]
+    sampled_images = image_sample(image_rng, params, nrow, ncol)
+    return test_elbo, sampled_images
+
+  opt_state = opt_init(init_params)
+  for epoch in range(num_epochs):
+    tic = time.time()
+    rng, opt_state, _ = run_epoch(rng, opt_state, train_images)
+    test_elbo, images = evaluate(opt_state, test_images)
+    print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
+    plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
jax/tests/api_test.py,tests/api_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/api_test.py b/tests/api_test.py
new file mode 100644
index 000000000..00adfbae2
--- /dev/null
+++ b/tests/api_test.py
@@ -0,0 +1,213 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as onp
+from absl.testing import absltest
+from jax import test_util as jtu
+
+import jax.numpy as np
+from jax import jit, grad
+from jax.core import Primitive
+from jax.interpreters.partial_eval import def_abstract_eval
+from jax.interpreters.ad import defjvp
+from jax.abstract_arrays import concretization_err_msg
+
+class APITest(jtu.JaxTestCase):
+
+  def test_grad_argnums(self):
+    def f(x, y, z, flag=False):
+      assert flag
+      return 1.0 * x + 2.0 * y + 3.0 * z
+
+    assert grad(f)(1.0, 1.0, 1.0, flag=True) == 1.0
+    assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
+    assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
+
+
+  def test_jit_static_args(self):
+    side = []
+
+    def f(x, y, z, flag=False, flag2=False):
+      assert flag
+      side.append(None)
+      return 100*x + 10*y + z
+
+    f1 = jit(f)
+    assert f1(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True) == 213
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True, flag2=True) == 213
+    assert len(side) == 2
+
+    side[:] = []
+    f2 = jit(f, static_argnums=[0,2])
+    assert f2(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f2(1, 3, 3, flag=True) == 133
+    assert len(side) == 1
+    assert f2(2, 2, 3, flag=True) == 223
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True) == 243
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True, flag2=True) == 243
+    assert len(side) == 3
+    assert f2(2, 5, 3, flag=True, flag2=True) == 253
+    assert len(side) == 3
+
+  def test_grad_of_jit(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    assert grad(f)(1.0) == 2.0
+    assert len(side) == 1
+    assert grad(f)(2.0) == 4.0
+    assert len(side) == 1
+
+  def test_jit_of_grad(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    g = jit(grad(f))
+    assert g(1.0) == 2.0
+    assert len(side) == 1
+    assert g(2.0) == 4.0
+    assert len(side) == 1
+
+
+  def test_bad_input(self):
+    def f(x):
+      return x
+
+    jtu.check_raises(lambda: grad(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+    jtu.check_raises(lambda: jit(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+  # TODO(dougalm): enable when we remove 'None' from pytree nodes
+  # def test_bad_output(self):
+  #   def f(x):
+  #     pass
+
+  #   grad(f)(onp.zeros(3))
+  #   jit(f)(onp.zeros(3))
+  #   assert False
+
+  def test_grad_tuple_output(self):
+    jtu.check_raises(lambda: grad(lambda x: (x,x))(1.0), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: (1.0, 1.0)"")
+
+  def test_grad_unit_output(self):
+    jtu.check_raises(lambda: grad(lambda x: ())(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: ()"")
+
+  def test_grad_nonscalar_output(self):
+    jtu.check_raises(lambda: grad(lambda x: x)(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: [ 0.  0.  0.]"")
+
+  def test_unwrapped_numpy(self):
+    def f(x):
+      return onp.exp(x)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3)), Exception,
+                     ""Tracer can't be used with raw numpy functions. ""
+                     ""You might have\n  import numpy as np\ninstead of\n""
+                     ""  import jax.numpy as np"")
+
+  def test_binop_mismatch(self):
+    def f(x, y):
+      return x + y
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     ValueError,
+                     ""Incompatible shapes for broadcasting: ((3,), (4,))"")
+
+  def test_dot_mismatch(self):
+    def f(x, y):
+      return np.dot(x, y)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     TypeError,
+                     ""Incompatible shapes for dot: got (3,) and (4,)."")
+
+  def test_switch_value_jit(self):
+    def f(x):
+      y = x > 0
+      if y:
+        return x
+      else:
+        return -x
+
+    assert grad(f)(1.0) == 1.0
+    assert grad(f)(-1.0) == -1.0
+    jtu.check_raises(lambda: jit(f)(1), TypeError, concretization_err_msg(bool))
+
+  def test_range_err(self):
+    def f(x, n):
+      for i in range(n):
+        x = x + i
+      return x
+
+    assert jit(f, static_argnums=(1,))(0, 5) == 10
+    jtu.check_raises(lambda: jit(f)(0, 5), TypeError, concretization_err_msg(int))
+
+  def test_casts(self):
+    for castfun in [float, int, long, complex, hex, oct]:
+      f = lambda x: castfun(x)
+      jtu.check_raises(lambda: jit(f)(0), TypeError, concretization_err_msg(castfun))
+
+  def test_unimplemented_interpreter_rules(self):
+    foo_p = Primitive('foo')
+    def foo(x):
+      return foo_p.bind(x)
+
+    jtu.check_raises(lambda: foo(1.0), NotImplementedError,
+                     ""Evaluation rule for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""Abstract evaluation for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Forward-mode differentiation rule for 'foo' not implemented"")
+
+    def_abstract_eval(foo_p, lambda x: x)
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""XLA translation rule for 'foo' not implemented"")
+
+    foo_p.def_impl(lambda x: x)
+    defjvp(foo_p, lambda g, x: foo(g))
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Reverse-mode differentiation rule for 'foo' not implemented"")
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/api_test.py b/tests/api_test.py
new file mode 100644
index 000000000..00adfbae2
--- /dev/null
+++ b/tests/api_test.py
@@ -0,0 +1,213 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as onp
+from absl.testing import absltest
+from jax import test_util as jtu
+
+import jax.numpy as np
+from jax import jit, grad
+from jax.core import Primitive
+from jax.interpreters.partial_eval import def_abstract_eval
+from jax.interpreters.ad import defjvp
+from jax.abstract_arrays import concretization_err_msg
+
+class APITest(jtu.JaxTestCase):
+
+  def test_grad_argnums(self):
+    def f(x, y, z, flag=False):
+      assert flag
+      return 1.0 * x + 2.0 * y + 3.0 * z
+
+    assert grad(f)(1.0, 1.0, 1.0, flag=True) == 1.0
+    assert grad(f, argnums=1)(1.0, 1.0, 1.0, flag=True) == 2.0
+    assert grad(f, argnums=(2, 0))(1.0, 1.0, 1.0, flag=True) == (3.0, 1.0)
+
+
+  def test_jit_static_args(self):
+    side = []
+
+    def f(x, y, z, flag=False, flag2=False):
+      assert flag
+      side.append(None)
+      return 100*x + 10*y + z
+
+    f1 = jit(f)
+    assert f1(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True) == 213
+    assert len(side) == 1
+    assert f1(2, 1, 3, flag=True, flag2=True) == 213
+    assert len(side) == 2
+
+    side[:] = []
+    f2 = jit(f, static_argnums=[0,2])
+    assert f2(1, 2, 3, flag=True) == 123
+    assert len(side) == 1
+    assert f2(1, 3, 3, flag=True) == 133
+    assert len(side) == 1
+    assert f2(2, 2, 3, flag=True) == 223
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True) == 243
+    assert len(side) == 2
+    assert f2(2, 4, 3, flag=True, flag2=True) == 243
+    assert len(side) == 3
+    assert f2(2, 5, 3, flag=True, flag2=True) == 253
+    assert len(side) == 3
+
+  def test_grad_of_jit(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    assert grad(f)(1.0) == 2.0
+    assert len(side) == 1
+    assert grad(f)(2.0) == 4.0
+    assert len(side) == 1
+
+  def test_jit_of_grad(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x * x
+
+    g = jit(grad(f))
+    assert g(1.0) == 2.0
+    assert len(side) == 1
+    assert g(2.0) == 4.0
+    assert len(side) == 1
+
+
+  def test_bad_input(self):
+    def f(x):
+      return x
+
+    jtu.check_raises(lambda: grad(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+    jtu.check_raises(lambda: jit(f)(""foo""), TypeError,
+                     ""Argument 'foo' of type <type 'str'> is not a valid JAX type"")
+
+  # TODO(dougalm): enable when we remove 'None' from pytree nodes
+  # def test_bad_output(self):
+  #   def f(x):
+  #     pass
+
+  #   grad(f)(onp.zeros(3))
+  #   jit(f)(onp.zeros(3))
+  #   assert False
+
+  def test_grad_tuple_output(self):
+    jtu.check_raises(lambda: grad(lambda x: (x,x))(1.0), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: (1.0, 1.0)"")
+
+  def test_grad_unit_output(self):
+    jtu.check_raises(lambda: grad(lambda x: ())(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: ()"")
+
+  def test_grad_nonscalar_output(self):
+    jtu.check_raises(lambda: grad(lambda x: x)(onp.zeros(3)), TypeError,
+                     ""Gradient only defined for scalar-output functions. ""
+                     ""Output was: [ 0.  0.  0.]"")
+
+  def test_unwrapped_numpy(self):
+    def f(x):
+      return onp.exp(x)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3)), Exception,
+                     ""Tracer can't be used with raw numpy functions. ""
+                     ""You might have\n  import numpy as np\ninstead of\n""
+                     ""  import jax.numpy as np"")
+
+  def test_binop_mismatch(self):
+    def f(x, y):
+      return x + y
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     ValueError,
+                     ""Incompatible shapes for broadcasting: ((3,), (4,))"")
+
+  def test_dot_mismatch(self):
+    def f(x, y):
+      return np.dot(x, y)
+
+    jtu.check_raises(lambda: grad(f)(onp.zeros(3), onp.zeros(4)),
+                     TypeError,
+                     ""Incompatible shapes for dot: got (3,) and (4,)."")
+
+  def test_switch_value_jit(self):
+    def f(x):
+      y = x > 0
+      if y:
+        return x
+      else:
+        return -x
+
+    assert grad(f)(1.0) == 1.0
+    assert grad(f)(-1.0) == -1.0
+    jtu.check_raises(lambda: jit(f)(1), TypeError, concretization_err_msg(bool))
+
+  def test_range_err(self):
+    def f(x, n):
+      for i in range(n):
+        x = x + i
+      return x
+
+    assert jit(f, static_argnums=(1,))(0, 5) == 10
+    jtu.check_raises(lambda: jit(f)(0, 5), TypeError, concretization_err_msg(int))
+
+  def test_casts(self):
+    for castfun in [float, int, long, complex, hex, oct]:
+      f = lambda x: castfun(x)
+      jtu.check_raises(lambda: jit(f)(0), TypeError, concretization_err_msg(castfun))
+
+  def test_unimplemented_interpreter_rules(self):
+    foo_p = Primitive('foo')
+    def foo(x):
+      return foo_p.bind(x)
+
+    jtu.check_raises(lambda: foo(1.0), NotImplementedError,
+                     ""Evaluation rule for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""Abstract evaluation for 'foo' not implemented"")
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Forward-mode differentiation rule for 'foo' not implemented"")
+
+    def_abstract_eval(foo_p, lambda x: x)
+
+    jtu.check_raises(lambda: jit(foo)(1.0), NotImplementedError,
+                     ""XLA translation rule for 'foo' not implemented"")
+
+    foo_p.def_impl(lambda x: x)
+    defjvp(foo_p, lambda g, x: foo(g))
+
+    jtu.check_raises(lambda: grad(foo)(1.0), NotImplementedError,
+                     ""Reverse-mode differentiation rule for 'foo' not implemented"")
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/batching_test.py,tests/batching_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/batching_test.py b/tests/batching_test.py
new file mode 100644
index 000000000..6020079ef
--- /dev/null
+++ b/tests/batching_test.py
@@ -0,0 +1,140 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import jax.numpy as np
+from jax import test_util as jtu
+from jax.abstract_arrays import ShapedArray
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
+from jax.api import vmap
+from jax.core import unit
+from jax.interpreters import partial_eval as pe
+from jax.util import partial
+
+
+class BatchingTest(jtu.JaxTestCase):
+
+  def testConstantFunction(self):
+    ans = vmap(lambda x: 3, onp.ones(4))
+    expected = 3 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testNestedBatchingMatMat(self):
+    def matvec(A, b):
+      return vmap(np.vdot, A, b, in_bdims=(0, None))
+
+    def matmat(A, B):
+      return vmap(matvec, A, B, in_bdims=(None, 1), out_bdim=1)
+
+    R = onp.random.RandomState(0).randn
+    A = R(4, 3)
+    B = R(3, 2)
+
+    ans = matmat(A, B)
+    expected = onp.dot(A, B)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+    # this is a crude check that we only call a single dot
+    def pv_like(x):
+      aval = ShapedArray(onp.shape(x), onp.result_type(x))
+      return pe.PartialVal((aval, unit))
+
+    def make_jaxpr(fun, example_args):
+      jaxpr, _, _, _ = trace_to_jaxpr(fun, map(pv_like, example_args))
+      return jaxpr
+
+    jaxpr = make_jaxpr(matmat, (A, B))
+    self.assertEqual(len(jaxpr.eqns), 1)
+
+  def testPerExampleGradients(self):
+    def predict(params, inputs):
+      for W, b in params:
+        outputs = np.dot(W, inputs) + b
+        inputs = np.tanh(outputs)
+      return outputs
+
+    def loss(params, data):
+      inputs, targets = data
+      predictions = predict(params, inputs)
+      return np.sum((predictions - targets)**2)
+
+    batch_size = 5
+    layer_sizes = [3, 2, 4]
+
+    R = onp.random.RandomState(0).randn
+    params = [(R(m, n), R(m))
+              for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]
+
+    input_vec = R(3)
+    target_vec = R(4)
+    datum = (input_vec, target_vec)
+
+    input_batch = R(5, 3)
+    target_batch = R(5, 4)
+    batch = (input_batch, target_batch)
+
+    ans = vmap(partial(grad(loss), params), batch)
+
+    for ans_pair, param_pair in zip(ans, params):
+      dW, db = ans_pair
+      W, b = param_pair
+
+      self.assertEqual(dW.shape, (batch_size,) + W.shape)
+      self.assertEqual(db.shape, (batch_size,) + b.shape)
+
+  def testJacobians(self):
+    def jacbwd(f, x):
+      y, pullback = vjp(f, x)
+      std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
+      jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    def jacfwd(f, x):
+      pushfwd = lambda v: jvp(f, (x,), (v,))
+      std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
+      y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    R = onp.random.RandomState(0).randn
+
+    A = R(4, 3)
+    b = R(4)
+    f = lambda x: np.tanh(np.dot(A, x) + b)
+
+    x = R(3)
+    self.assertAllClose(jacfwd(f, x), jacbwd(f, x), check_dtypes=False)
+
+  def testBatchOfCompile(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x + x
+
+    g = jit(lambda x: vmap(f, x))
+    self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
+    self.assertEqual(len(side), 1)
+    self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
+                        check_dtypes=False)
+    self.assertEqual(len(side), 1)
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
new file mode 100644
index 000000000..6020079ef
--- /dev/null
+++ b/tests/batching_test.py
@@ -0,0 +1,140 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import jax.numpy as np
+from jax import test_util as jtu
+from jax.abstract_arrays import ShapedArray
+from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
+from jax.api import vmap
+from jax.core import unit
+from jax.interpreters import partial_eval as pe
+from jax.util import partial
+
+
+class BatchingTest(jtu.JaxTestCase):
+
+  def testConstantFunction(self):
+    ans = vmap(lambda x: 3, onp.ones(4))
+    expected = 3 * onp.ones(4)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+  def testNestedBatchingMatMat(self):
+    def matvec(A, b):
+      return vmap(np.vdot, A, b, in_bdims=(0, None))
+
+    def matmat(A, B):
+      return vmap(matvec, A, B, in_bdims=(None, 1), out_bdim=1)
+
+    R = onp.random.RandomState(0).randn
+    A = R(4, 3)
+    B = R(3, 2)
+
+    ans = matmat(A, B)
+    expected = onp.dot(A, B)
+    self.assertAllClose(ans, expected, check_dtypes=False)
+
+    # this is a crude check that we only call a single dot
+    def pv_like(x):
+      aval = ShapedArray(onp.shape(x), onp.result_type(x))
+      return pe.PartialVal((aval, unit))
+
+    def make_jaxpr(fun, example_args):
+      jaxpr, _, _, _ = trace_to_jaxpr(fun, map(pv_like, example_args))
+      return jaxpr
+
+    jaxpr = make_jaxpr(matmat, (A, B))
+    self.assertEqual(len(jaxpr.eqns), 1)
+
+  def testPerExampleGradients(self):
+    def predict(params, inputs):
+      for W, b in params:
+        outputs = np.dot(W, inputs) + b
+        inputs = np.tanh(outputs)
+      return outputs
+
+    def loss(params, data):
+      inputs, targets = data
+      predictions = predict(params, inputs)
+      return np.sum((predictions - targets)**2)
+
+    batch_size = 5
+    layer_sizes = [3, 2, 4]
+
+    R = onp.random.RandomState(0).randn
+    params = [(R(m, n), R(m))
+              for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]
+
+    input_vec = R(3)
+    target_vec = R(4)
+    datum = (input_vec, target_vec)
+
+    input_batch = R(5, 3)
+    target_batch = R(5, 4)
+    batch = (input_batch, target_batch)
+
+    ans = vmap(partial(grad(loss), params), batch)
+
+    for ans_pair, param_pair in zip(ans, params):
+      dW, db = ans_pair
+      W, b = param_pair
+
+      self.assertEqual(dW.shape, (batch_size,) + W.shape)
+      self.assertEqual(db.shape, (batch_size,) + b.shape)
+
+  def testJacobians(self):
+    def jacbwd(f, x):
+      y, pullback = vjp(f, x)
+      std_basis = onp.eye(onp.size(y)).reshape((-1,) + onp.shape(y))
+      jac_flat, = vmap(pullback, std_basis, out_bdim=onp.ndim(y))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    def jacfwd(f, x):
+      pushfwd = lambda v: jvp(f, (x,), (v,))
+      std_basis = onp.eye(onp.size(x)).reshape((-1,) + onp.shape(x))
+      y, jac_flat = vmap(pushfwd, std_basis, out_bdim=(None, 0))
+      return jac_flat.reshape(onp.shape(y) + onp.shape(x))
+
+    R = onp.random.RandomState(0).randn
+
+    A = R(4, 3)
+    b = R(4)
+    f = lambda x: np.tanh(np.dot(A, x) + b)
+
+    x = R(3)
+    self.assertAllClose(jacfwd(f, x), jacbwd(f, x), check_dtypes=False)
+
+  def testBatchOfCompile(self):
+    side = []
+
+    @jit
+    def f(x):
+      side.append(None)
+      return x + x
+
+    g = jit(lambda x: vmap(f, x))
+    self.assertAllClose(g(onp.ones(2)), 2 * onp.ones(2), check_dtypes=False)
+    self.assertEqual(len(side), 1)
+    self.assertAllClose(g(2 * onp.ones(2)), 4 * onp.ones(2),
+                        check_dtypes=False)
+    self.assertEqual(len(side), 1)
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/core_test.py,tests/core_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/core_test.py b/tests/core_test.py
new file mode 100644
index 000000000..a69bd4bd3
--- /dev/null
+++ b/tests/core_test.py
@@ -0,0 +1,325 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import namedtuple
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import api
+from jax import core
+from jax import numpy as np
+from jax import test_util as jtu
+from jax.api import jvp, linearize, vjp, jit
+from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
+from jax.tree_util import tree_flatten, tree_multimap
+from jax.util import partial
+from jax.interpreters import partial_eval as pe
+from jax.interpreters import xla
+
+_ = pe.PartialVal((UnshapedArray(onp.float32), core.unit))
+__ = pe.PartialVal((ShapedArray((), onp.float32), core.unit))
+
+def call(f, *args):
+  return jit(f)(*args)
+
+def simple_fun(x, y):
+  return np.sin(x * y)
+
+def simple_fun_fanout(x, y):
+  return np.sin(x * y) * x
+
+def fun_with_call(x):
+  return call(np.sin, x)
+
+def fun_with_nested_calls(x):
+  def f(y):
+    y2 = np.sin(y) + 1.0 + (2.0 * x)
+
+    @jit
+    def g(z):
+      return y2 * z * x + (x * y)
+
+    return call(g, y)
+
+  return call(f, x)
+
+def error(*args):
+  def f(*args):
+    assert False
+  return f
+
+def fun_with_nested_calls_2(x):
+  def bar(y):
+    def baz(w):
+      q = call(lambda x: y, x)
+      q = q + call(lambda: y)
+      q = q + call(lambda y: w + y, y)
+      return call(lambda w: call(np.sin, x) * y, 1.0) + q
+      # return call(lambda w: call(error(np.sin), x) * y, 1.0) + q
+    p, t = jvp(baz, (x + 1.0,), (y,))
+    return t + (x * p)
+  return call(bar, x)
+
+def fun_call_jitted(x):
+  @jit
+  def g(z):
+    return x * z
+
+  return call(g, x)
+
+def fun_with_two_calls(x):
+  return call(np.sin, x) + call(np.cos, x)
+
+def fun_with_call_closure(x):
+  def foo(y, z):
+    return (x * x) * np.sin(y) * z
+
+  return call(foo, x, np.cos(x)) + x
+
+def product_io_fun(x, y):
+  xa = x['a']
+  xb = x['b']
+  y1, (y2, y3) = y
+  return np.sin(xa + y2), [xb, (y1, y3)]
+
+
+R = onp.random.randn
+TestSpec = namedtuple('TestSpec', ['fun', 'args'])
+test_specs_base = [
+    TestSpec(simple_fun, (R(3, 2), R(3, 2))),
+    TestSpec(simple_fun_fanout, (R(3, 2), R(3, 2))),
+    TestSpec(product_io_fun, ({'a': R(2, 2), 'b': R(2, 2)},
+                              (R(2, 2), (R(2, 2), R(2, 2))))),
+    TestSpec(fun_with_call, (R(3, 2),)),
+    TestSpec(fun_with_two_calls, (R(3, 2),)),
+    TestSpec(fun_with_call_closure, (R(3, 2),)),
+    TestSpec(fun_call_jitted, (R(1,),)),
+    TestSpec(fun_with_nested_calls, (R(),)),
+    TestSpec(fun_with_nested_calls, (R(3, 2),)),
+    TestSpec(fun_with_nested_calls_2, (R(1, 2),)),
+]
+
+def jvp_unlinearized(f, primals, tangents):
+  out, jvp = linearize(f, *primals)
+  return out, jvp(tangents)
+
+test_specs = []
+for ts in test_specs_base:
+  test_specs.append(ts)
+  test_specs.append(TestSpec(partial(jvp, ts.fun), (ts.args, ts.args)))
+  test_specs.append(TestSpec(jit(ts.fun), ts.args))
+  test_specs.append(TestSpec(jit(jit(ts.fun)), ts.args))
+  test_specs.append(TestSpec(partial(jvp_unlinearized, ts.fun),
+                             (ts.args, ts.args)))
+
+
+def fwd_deriv(f):
+  def df(x):
+    return jvp(f, (x,), (1.0,))[1]
+
+  return df
+
+def check_trace_eval(f, pvals, vals, expected_out_pval):
+  jaxpr, consts, out_pval, _ = api.trace_to_jaxpr(f, pvals)
+  assert expected_out_pval == out_pval, (expected_out_pval, out_pval)
+  output_traced = core.eval_jaxpr(jaxpr, consts, (), *vals)
+  output_traced = pe.merge_pvals(output_traced, out_pval)
+  output_eval = f(*vals)
+  assert onp.allclose(output_traced, output_eval), \
+      '\neval:         {}\ntrace + eval: {}'.format(output_eval, output_traced)
+
+
+class CoreTest(jtu.JaxTestCase):
+
+  def DISABLED_test_pack_unpack(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    y = onp.array(1.0)
+    def foo(x):
+      x1, y1 = core.pack((x, y))
+      assert y1 is y, (y1, y)
+      return x1
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def DISABLED_test_tup_add(self):
+    # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
+    y = onp.array(1.0)
+    def foo(x):
+      return np.tup_add(core.pack((x, y)))
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def test_tree_multimap(self):
+    xs = ({'a': 1}, [2, 3])
+    ys = ({'a': 10}, [20, 30])
+    ys_bad = ({'a': 10, 'b': 10}, [20, 30])
+    zs = ({'a': 11}, [22, 33])
+
+    f = lambda x, y: x + y
+    assert tree_multimap(f, xs, ys) == zs
+    try:
+      tree_multimap(f, xs, ys_bad)
+      assert False
+    except TypeError:
+      pass
+
+  def DIABLED_test_print_jaxpr_compound(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
+    print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
+
+  def test_tree_flatten(self):
+    flat, _ = tree_flatten(({'a': 1}, [2, 3], 4))
+    assert flat == [1, 2, 3, 4]
+
+  @parameterized.parameters(test_specs)
+  def test_jit(self, f, args):
+    jtu.check_eq(jit(f)(*args), f(*args))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp, f), args)
+
+  def test_jvp_zeros(self):
+    def foo(x):
+      def bar(y):
+        x1, y1 = core.pack((x, y))
+        return np.sin(x1 * y1)
+      return jvp(bar, (3 * x,), (2 * x,))
+
+    jtu.check_eq(jit(foo)(0.5), foo(0.5))
+
+  def test_dynamic_subfun_context(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(np.sin(x), y)
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  def DISABLED_test_nested_grad(self):
+    def foo(x):
+      print(type(x), x)
+      def bar(y):
+        return np.cos(y) * x
+      print(x * x)
+      return call(bar, x*x)
+
+    print(api.trace_to_jaxpr(api.grad(foo), (__,)))
+
+  def test_nested(self):
+    def foo(x):
+      def bar(y):
+        def baz(w):
+          q = call(lambda x: y, x) + call(lambda: y)
+          return call(lambda w: call(np.sin, x) * y, 1.0) + q
+        p, t = jvp(baz, (x + 1.0,), (y,))
+        return t + (x * p)
+
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp_linearized(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp_unlinearized, f), args)
+
+  @parameterized.parameters(test_specs)
+  def test_vjp(self, f, args):
+    print(f)
+    jtu.check_vjp(f, partial(vjp, f), args)
+
+  def test_jvp_closure(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(x, y)
+      return jvp(bar, (3.0,), (1.0,))[1]
+    ans = jvp(foo, (1.0,), (2.0,))
+    assert ans == (1.0, 2.0), ans
+
+  def test_jit_closure(self):
+    def foo(x):
+      @jit
+      def bar(y):
+        return x + y
+      return bar(0.0)
+    assert jvp(foo, (1.0,), (2.0,)) == (1.0, 2.0)
+
+  def test_simple_trace(self):
+    def foo(x):
+      return np.sin(x) + np.cos(x)
+    pval = pe.PartialVal((ShapedArray((3, 2), onp.float32), core.unit))
+    check_trace_eval(foo, (pval,), (onp.random.randn(3, 2),), pval)
+
+  def test_nullary_trace(self):
+    def foo():
+      return 1.2
+    check_trace_eval(foo, (), (), (None, 1.2))
+
+  def test_simple_jit(self):
+    def foo(x):
+      if x.shape == ():
+        return x + 1.
+      else:
+        return x + 2.
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    x1, y1 = onp.array(1.0), onp.array(2.0)
+    assert foo(x1) == y1
+    assert foo2(x1) == y1
+    assert foo3(x1) == y1
+
+    x2, y2 = onp.array([1.0, 2.0]), onp.array([3.0, 4.0])
+    assert onp.all(foo(x2) == y2)
+    assert onp.all(foo2(x2) == y2)
+    assert onp.all(foo3(x2) == y2)
+
+  def test_product_jit(self):
+    def foo(x, tup):
+      y, z = tup
+      w = x + z
+      return (w, {'x': y}), z
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    args = (1.0, (2.0, 3.0))
+    expected_output = ((4.0, {'x': 2.0}), 3.0)
+
+    assert foo(*args) == expected_output
+    assert foo2(*args) == expected_output
+    assert foo3(*args) == foo(*args)
+
+  def test_jvp_2(self):
+    d_sin = fwd_deriv(np.sin)
+    d2_sin = fwd_deriv(d_sin)
+    d3_sin = fwd_deriv(d2_sin)
+
+    assert d_sin(0.0) == 1.0
+    assert d2_sin(0.0) == 0.0
+    assert d3_sin(0.0) == -1.0
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/core_test.py b/tests/core_test.py
new file mode 100644
index 000000000..a69bd4bd3
--- /dev/null
+++ b/tests/core_test.py
@@ -0,0 +1,325 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import namedtuple
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import api
+from jax import core
+from jax import numpy as np
+from jax import test_util as jtu
+from jax.api import jvp, linearize, vjp, jit
+from jax.lax import UnshapedArray, ShapedArray, ConcreteArray
+from jax.tree_util import tree_flatten, tree_multimap
+from jax.util import partial
+from jax.interpreters import partial_eval as pe
+from jax.interpreters import xla
+
+_ = pe.PartialVal((UnshapedArray(onp.float32), core.unit))
+__ = pe.PartialVal((ShapedArray((), onp.float32), core.unit))
+
+def call(f, *args):
+  return jit(f)(*args)
+
+def simple_fun(x, y):
+  return np.sin(x * y)
+
+def simple_fun_fanout(x, y):
+  return np.sin(x * y) * x
+
+def fun_with_call(x):
+  return call(np.sin, x)
+
+def fun_with_nested_calls(x):
+  def f(y):
+    y2 = np.sin(y) + 1.0 + (2.0 * x)
+
+    @jit
+    def g(z):
+      return y2 * z * x + (x * y)
+
+    return call(g, y)
+
+  return call(f, x)
+
+def error(*args):
+  def f(*args):
+    assert False
+  return f
+
+def fun_with_nested_calls_2(x):
+  def bar(y):
+    def baz(w):
+      q = call(lambda x: y, x)
+      q = q + call(lambda: y)
+      q = q + call(lambda y: w + y, y)
+      return call(lambda w: call(np.sin, x) * y, 1.0) + q
+      # return call(lambda w: call(error(np.sin), x) * y, 1.0) + q
+    p, t = jvp(baz, (x + 1.0,), (y,))
+    return t + (x * p)
+  return call(bar, x)
+
+def fun_call_jitted(x):
+  @jit
+  def g(z):
+    return x * z
+
+  return call(g, x)
+
+def fun_with_two_calls(x):
+  return call(np.sin, x) + call(np.cos, x)
+
+def fun_with_call_closure(x):
+  def foo(y, z):
+    return (x * x) * np.sin(y) * z
+
+  return call(foo, x, np.cos(x)) + x
+
+def product_io_fun(x, y):
+  xa = x['a']
+  xb = x['b']
+  y1, (y2, y3) = y
+  return np.sin(xa + y2), [xb, (y1, y3)]
+
+
+R = onp.random.randn
+TestSpec = namedtuple('TestSpec', ['fun', 'args'])
+test_specs_base = [
+    TestSpec(simple_fun, (R(3, 2), R(3, 2))),
+    TestSpec(simple_fun_fanout, (R(3, 2), R(3, 2))),
+    TestSpec(product_io_fun, ({'a': R(2, 2), 'b': R(2, 2)},
+                              (R(2, 2), (R(2, 2), R(2, 2))))),
+    TestSpec(fun_with_call, (R(3, 2),)),
+    TestSpec(fun_with_two_calls, (R(3, 2),)),
+    TestSpec(fun_with_call_closure, (R(3, 2),)),
+    TestSpec(fun_call_jitted, (R(1,),)),
+    TestSpec(fun_with_nested_calls, (R(),)),
+    TestSpec(fun_with_nested_calls, (R(3, 2),)),
+    TestSpec(fun_with_nested_calls_2, (R(1, 2),)),
+]
+
+def jvp_unlinearized(f, primals, tangents):
+  out, jvp = linearize(f, *primals)
+  return out, jvp(tangents)
+
+test_specs = []
+for ts in test_specs_base:
+  test_specs.append(ts)
+  test_specs.append(TestSpec(partial(jvp, ts.fun), (ts.args, ts.args)))
+  test_specs.append(TestSpec(jit(ts.fun), ts.args))
+  test_specs.append(TestSpec(jit(jit(ts.fun)), ts.args))
+  test_specs.append(TestSpec(partial(jvp_unlinearized, ts.fun),
+                             (ts.args, ts.args)))
+
+
+def fwd_deriv(f):
+  def df(x):
+    return jvp(f, (x,), (1.0,))[1]
+
+  return df
+
+def check_trace_eval(f, pvals, vals, expected_out_pval):
+  jaxpr, consts, out_pval, _ = api.trace_to_jaxpr(f, pvals)
+  assert expected_out_pval == out_pval, (expected_out_pval, out_pval)
+  output_traced = core.eval_jaxpr(jaxpr, consts, (), *vals)
+  output_traced = pe.merge_pvals(output_traced, out_pval)
+  output_eval = f(*vals)
+  assert onp.allclose(output_traced, output_eval), \
+      '\neval:         {}\ntrace + eval: {}'.format(output_eval, output_traced)
+
+
+class CoreTest(jtu.JaxTestCase):
+
+  def DISABLED_test_pack_unpack(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    y = onp.array(1.0)
+    def foo(x):
+      x1, y1 = core.pack((x, y))
+      assert y1 is y, (y1, y)
+      return x1
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def DISABLED_test_tup_add(self):
+    # TODO(mattjj,dougalm): put tup_add somewhere (was in array_type.py)
+    y = onp.array(1.0)
+    def foo(x):
+      return np.tup_add(core.pack((x, y)))
+
+    pe.trace_to_jaxpr(foo, (_,))
+
+  def test_tree_multimap(self):
+    xs = ({'a': 1}, [2, 3])
+    ys = ({'a': 10}, [20, 30])
+    ys_bad = ({'a': 10, 'b': 10}, [20, 30])
+    zs = ({'a': 11}, [22, 33])
+
+    f = lambda x, y: x + y
+    assert tree_multimap(f, xs, ys) == zs
+    try:
+      tree_multimap(f, xs, ys_bad)
+      assert False
+    except TypeError:
+      pass
+
+  def DIABLED_test_print_jaxpr_compound(self):
+    # TODO(dougalm): figure out what jaxpr-tracing api to expose and re-enable
+    pv = pe.PartialVal((ShapedArray((2, 3), onp.float32), core.unit))
+    print(pe.trace_to_jaxpr(fun_with_call_closure, (pv,))[0])
+
+  def test_tree_flatten(self):
+    flat, _ = tree_flatten(({'a': 1}, [2, 3], 4))
+    assert flat == [1, 2, 3, 4]
+
+  @parameterized.parameters(test_specs)
+  def test_jit(self, f, args):
+    jtu.check_eq(jit(f)(*args), f(*args))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp, f), args)
+
+  def test_jvp_zeros(self):
+    def foo(x):
+      def bar(y):
+        x1, y1 = core.pack((x, y))
+        return np.sin(x1 * y1)
+      return jvp(bar, (3 * x,), (2 * x,))
+
+    jtu.check_eq(jit(foo)(0.5), foo(0.5))
+
+  def test_dynamic_subfun_context(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(np.sin(x), y)
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  def DISABLED_test_nested_grad(self):
+    def foo(x):
+      print(type(x), x)
+      def bar(y):
+        return np.cos(y) * x
+      print(x * x)
+      return call(bar, x*x)
+
+    print(api.trace_to_jaxpr(api.grad(foo), (__,)))
+
+  def test_nested(self):
+    def foo(x):
+      def bar(y):
+        def baz(w):
+          q = call(lambda x: y, x) + call(lambda: y)
+          return call(lambda w: call(np.sin, x) * y, 1.0) + q
+        p, t = jvp(baz, (x + 1.0,), (y,))
+        return t + (x * p)
+
+      return call(bar, x)
+
+    print(api.trace_to_jaxpr(foo, (__,)))
+
+  @parameterized.parameters(test_specs)
+  def test_jvp_linearized(self, f, args):
+    print(f)
+    jtu.check_jvp(f, partial(jvp_unlinearized, f), args)
+
+  @parameterized.parameters(test_specs)
+  def test_vjp(self, f, args):
+    print(f)
+    jtu.check_vjp(f, partial(vjp, f), args)
+
+  def test_jvp_closure(self):
+    def foo(x):
+      def bar(y):
+        return np.multiply(x, y)
+      return jvp(bar, (3.0,), (1.0,))[1]
+    ans = jvp(foo, (1.0,), (2.0,))
+    assert ans == (1.0, 2.0), ans
+
+  def test_jit_closure(self):
+    def foo(x):
+      @jit
+      def bar(y):
+        return x + y
+      return bar(0.0)
+    assert jvp(foo, (1.0,), (2.0,)) == (1.0, 2.0)
+
+  def test_simple_trace(self):
+    def foo(x):
+      return np.sin(x) + np.cos(x)
+    pval = pe.PartialVal((ShapedArray((3, 2), onp.float32), core.unit))
+    check_trace_eval(foo, (pval,), (onp.random.randn(3, 2),), pval)
+
+  def test_nullary_trace(self):
+    def foo():
+      return 1.2
+    check_trace_eval(foo, (), (), (None, 1.2))
+
+  def test_simple_jit(self):
+    def foo(x):
+      if x.shape == ():
+        return x + 1.
+      else:
+        return x + 2.
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    x1, y1 = onp.array(1.0), onp.array(2.0)
+    assert foo(x1) == y1
+    assert foo2(x1) == y1
+    assert foo3(x1) == y1
+
+    x2, y2 = onp.array([1.0, 2.0]), onp.array([3.0, 4.0])
+    assert onp.all(foo(x2) == y2)
+    assert onp.all(foo2(x2) == y2)
+    assert onp.all(foo3(x2) == y2)
+
+  def test_product_jit(self):
+    def foo(x, tup):
+      y, z = tup
+      w = x + z
+      return (w, {'x': y}), z
+
+    foo2 = jit(foo)
+    foo3 = jit(foo2)
+
+    args = (1.0, (2.0, 3.0))
+    expected_output = ((4.0, {'x': 2.0}), 3.0)
+
+    assert foo(*args) == expected_output
+    assert foo2(*args) == expected_output
+    assert foo3(*args) == foo(*args)
+
+  def test_jvp_2(self):
+    d_sin = fwd_deriv(np.sin)
+    d2_sin = fwd_deriv(d_sin)
+    d3_sin = fwd_deriv(d2_sin)
+
+    assert d_sin(0.0) == 1.0
+    assert d2_sin(0.0) == 0.0
+    assert d3_sin(0.0) == -1.0
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/lapax_test.py,tests/lapax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lapax_test.py b/tests/lapax_test.py
new file mode 100644
index 000000000..07ef0ad20
--- /dev/null
+++ b/tests/lapax_test.py
@@ -0,0 +1,205 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the LAPAX linear algebra module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import itertools
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import jit
+from jax import test_util as jtu
+from jax.experimental import lapax
+
+
+class LapaxTest(jtu.JaxTestCase):
+
+  def testSolveLowerTriangularVec(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3))
+    rhs = npr.randn(3, 1)
+    rhs2 = npr.randn(3, 1)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularMat(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(4, 4))
+    lhs2 = onp.tril(npr.randn(4, 4))
+    rhs = npr.randn(4, 3)
+    rhs2 = npr.randn(4, 3)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularBroadcasting(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3, 3))
+    rhs = npr.randn(3, 3, 2)
+    rhs2 = npr.randn(3, 3, 2)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(4, 4))
+    arr2 = square(npr.randn(4, 4))
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(lapax.cholesky)
+    check(fun, arr)
+    check(fun, arr2)
+
+  def testBlockedCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(11, 11))
+    arr2 = square(npr.randn(11, 11))
+
+    chol = lambda x: lapax.cholesky(x, block_size=3)
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(chol)
+    check(fun, arr)
+    check(fun, arr2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 4, 4), (2, 4, 6) if left else (2, 6, 4)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangular(self, lower, left_side, transpose_a, lhs_shape,
+                          rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 8, 8), (2, 8, 10) if left else (2, 10, 8)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangularBlocked(self, lower, left_side, transpose_a, lhs_shape,
+                                 rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A).astype(A.dtype)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a,
+        block_size=3)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lapax_test.py b/tests/lapax_test.py
new file mode 100644
index 000000000..07ef0ad20
--- /dev/null
+++ b/tests/lapax_test.py
@@ -0,0 +1,205 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the LAPAX linear algebra module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import itertools
+
+import numpy as onp
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from jax import jit
+from jax import test_util as jtu
+from jax.experimental import lapax
+
+
+class LapaxTest(jtu.JaxTestCase):
+
+  def testSolveLowerTriangularVec(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3))
+    rhs = npr.randn(3, 1)
+    rhs2 = npr.randn(3, 1)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularMat(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(4, 4))
+    lhs2 = onp.tril(npr.randn(4, 4))
+    rhs = npr.randn(4, 3)
+    rhs2 = npr.randn(4, 3)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testSolveLowerTriangularBroadcasting(self):
+    npr = onp.random.RandomState(1)
+    lhs = onp.tril(npr.randn(3, 3, 3))
+    lhs2 = onp.tril(npr.randn(3, 3, 3))
+    rhs = npr.randn(3, 3, 2)
+    rhs2 = npr.randn(3, 3, 2)
+
+    def check(fun, lhs, rhs):
+      a1 = onp.linalg.solve(lhs, rhs)
+      a2 = fun(lhs, rhs)
+      a3 = fun(lhs, rhs)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    solve_triangular = lambda a, b: lapax.solve_triangular(
+        a, b, left_side=True, lower=True, trans_a=False)
+
+    fun = jit(solve_triangular)
+    check(fun, lhs, rhs)
+    check(fun, lhs2, rhs2)
+
+  def testCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(4, 4))
+    arr2 = square(npr.randn(4, 4))
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(lapax.cholesky)
+    check(fun, arr)
+    check(fun, arr2)
+
+  def testBlockedCholeskyMat(self):
+    npr = onp.random.RandomState(0)
+    square = lambda rhs: onp.dot(rhs, rhs.T)
+    arr = square(npr.randn(11, 11))
+    arr2 = square(npr.randn(11, 11))
+
+    chol = lambda x: lapax.cholesky(x, block_size=3)
+
+    def check(fun, arr):
+      a1 = onp.linalg.cholesky(arr)
+      a2 = fun(arr)
+      a3 = fun(arr)
+      self.assertArraysAllClose(a1, a2, check_dtypes=True)
+      self.assertArraysAllClose(a2, a3, check_dtypes=True)
+
+    fun = jit(chol)
+    check(fun, arr)
+    check(fun, arr2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 4, 4), (2, 4, 6) if left else (2, 6, 4)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangular(self, lower, left_side, transpose_a, lhs_shape,
+                          rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs={}_rhs={}_lower={}_leftside={}_transposea={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           lower, left, transpose_a),
+       ""lower"": lower, ""left_side"": left, ""transpose_a"": transpose_a,
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lower, left, transpose_a in itertools.product([False, True], repeat=3)
+      for lhs_shape, rhs_shape in [
+          ((2, 8, 8), (2, 8, 10) if left else (2, 10, 8)),
+      ]
+      for dtype in [onp.float32, onp.float64]
+      for rng in [jtu.rand_default()])
+  def testSolveTriangularBlocked(self, lower, left_side, transpose_a, lhs_shape,
+                                 rhs_shape, dtype, rng):
+    # pylint: disable=invalid-name
+    T = lambda X: onp.swapaxes(X, -1, -2)
+    K = rng(lhs_shape, dtype)
+    L = onp.linalg.cholesky(onp.matmul(K, T(K))
+                            + lhs_shape[-1] * onp.eye(lhs_shape[-1]))
+    L = L.astype(K.dtype)
+    B = rng(rhs_shape, dtype)
+
+    A = L if lower else T(L)
+    inv = onp.linalg.inv(T(A) if transpose_a else A).astype(A.dtype)
+    np_ans = onp.matmul(inv, B) if left_side else onp.matmul(B, inv)
+
+    lapax_ans = lapax.solve_triangular(
+        L if lower else T(L), B, left_side, lower, transpose_a,
+        block_size=3)
+
+    self.assertAllClose(np_ans, lapax_ans, check_dtypes=False)
+    # pylint: enable=invalid-name
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_numpy_indexing_test.py,tests/lax_numpy_indexing_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
new file mode 100644
index 000000000..eb3329dd9
--- /dev/null
+++ b/tests/lax_numpy_indexing_test.py
@@ -0,0 +1,586 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import collections
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import lax
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+# We disable the whitespace continuation check in this file because otherwise it
+# makes the test name formatting unwieldy.
+# pylint: disable=bad-continuation
+
+
+float_dtypes = [onp.float32, onp.float64]
+int_dtypes = [onp.int32, onp.int64]
+bool_types = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + int_dtypes + bool_types
+
+IndexSpec = collections.namedtuple(""IndexTest"", [""shape"", ""indexer""])
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+class IndexingTest(jtu.JaxTestCase):
+  """"""Tests for Numpy indexing translation rules.""""""
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(
+              name, jtu.format_shape_dtype_string( shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(10,), indexer=slice(1, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(-3, None)),
+      ]),
+      (""OneSliceIndexNegativeStride"", [
+          IndexSpec(shape=(10,), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(1, 8, -1)),  # empty result
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(0, 8, -1)),  # empty result
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -1)),
+      ]),
+      (""OneSliceIndexNonUnitStride"", [
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 2)),
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, 3, 2)),
+          IndexSpec(shape=(10,), indexer=slice(1, None, 2)),
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 8, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, 2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -2)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(10, 8), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(
+              shape=(10, 8), indexer=(slice(None, None, -1), slice(None, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(10, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(10, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      (""EmptyIndex"", [
+          IndexSpec(shape=(), indexer=()),
+          IndexSpec(shape=(3,), indexer=()),
+          IndexSpec(shape=(3, 4), indexer=()),
+      ]),
+  ] for shape, indexer in index_specs for dtype in all_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: x[indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(name,
+                                            jtu.format_shape_dtype_string(
+                                                shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(5,), indexer=slice(1, 3)),
+          IndexSpec(shape=(5,), indexer=slice(1, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(None, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(-3, None)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      # TODO(mattjj): these fail for uninteresting dtype reasons
+      # (""EmptyIndex"",
+      #  [IndexSpec(shape=(), indexer=()),
+      #   IndexSpec(shape=(3,), indexer=()),
+      #   IndexSpec(shape=(3, 4), indexer=()),
+      #   ]),
+  ] for shape, indexer in index_specs for dtype in float_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  def _ReplaceSlicesWithTuples(self, idx):
+    """"""Helper method to replace slices with tuples for dynamic indexing args.""""""
+    if isinstance(idx, slice):
+      triple = idx.start, idx.stop, idx.step
+      isnone = [i for i, elt in enumerate(triple) if elt is None]
+      zeros = itertools.repeat(0)
+      nones = itertools.repeat(None)
+      out = lax.subvals(triple, zip(isnone, zeros))
+      return out, lambda out: slice(*lax.subvals(out, zip(isnone, nones)))
+    elif isinstance(idx, (tuple, list)) and idx:
+      t = type(idx)
+      elts, packs = zip(*map(self._ReplaceSlicesWithTuples, idx))
+      return elts, lambda elts: t((pack(i) for pack, i in zip(packs, elts)))
+    else:
+      return idx, lambda x: x
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneSliceIndex"",
+           [IndexSpec(shape=(5,), indexer=slice(1, 3)),
+            IndexSpec(shape=(5, 4), indexer=slice(1, 3))]),
+          (""TwoSliceIndices"",
+           [IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+            IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2)))]),
+          (""NonUnitStrides"", [
+              IndexSpec(shape=(3,), indexer=slice(None, None, -1)),
+              IndexSpec(shape=(3, 3), indexer=slice(0, 3, -2)),
+              IndexSpec(shape=(3, 4, 5), indexer=slice(0, 4, 2))
+          ]),
+          (""OnlyStartOrStopDynamic"", [
+              IndexSpec(shape=(5, 4), indexer=(slice(None, 3), slice(0, 2))),
+              IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None)))
+          ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self.assertRaises(IndexError, lambda: fun(*args_maker()))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2)]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2))]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2),
+            ]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+            ]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
+    # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(unpacked_indexer, x):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    arr = rng(shape, dtype)
+    check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype), indexer]
+    fun = lambda x, idx: x[idx]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""SlicesAndOneIntArrayIndex"",
+           [IndexSpec(shape=(2, 3), indexer=(onp.array([0, 1]), slice(1, 2))),
+            IndexSpec(shape=(2, 3), indexer=(slice(0, 2),
+                                             onp.array([0, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                slice(None))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([[0, 2], [1, 1]]),
+                                                slice(None))),
+            ]),
+          (""SlicesAndTwoIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                Ellipsis,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                Ellipsis)),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                slice(1, 3))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                slice(1, 3),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2, -2]),
+                                                slice(None, None, 2),
+                                                onp.array([-1, 2, -1]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([[0, 2], [2, 0]]),
+                                                Ellipsis,
+                                                onp.array([[1, 0], [1, 0]]))),
+            ]),
+          (""NonesAndIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2]),
+                                                None,
+                                                onp.array([-1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
+                            for e in indexer]
+    substitutes = [(i, e) for i, e in enumerate(indexer)
+                   if not isinstance(e, onp.ndarray)]
+    args_maker = lambda: [rng(shape, dtype), indexer_with_dummies]
+
+    def fun(x, indexer_with_dummies):
+      idx = type(indexer)(lax.subvals(indexer_with_dummies, substitutes))
+      return x[idx]
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  def testAdvancedIndexingManually(self):
+    x = onp.random.RandomState(0).randn(3, 4, 5)
+    index_array = onp.array([0, 2, -1, 0])
+
+    op = lambda x, index_array: x[..., index_array, :]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[..., index_array, :, index_array, None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[index_array, ..., index_array[:, None], None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+  def testUnpacking(self):
+
+    def foo(x):
+      a, b, c = x
+      return a + b + c
+
+    cfoo = api.jit(foo)
+
+    a1 = foo(onp.arange(3))
+    a2 = cfoo(onp.arange(3))
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py
new file mode 100644
index 000000000..eb3329dd9
--- /dev/null
+++ b/tests/lax_numpy_indexing_test.py
@@ -0,0 +1,586 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import collections
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import lax
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+# We disable the whitespace continuation check in this file because otherwise it
+# makes the test name formatting unwieldy.
+# pylint: disable=bad-continuation
+
+
+float_dtypes = [onp.float32, onp.float64]
+int_dtypes = [onp.int32, onp.int64]
+bool_types = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + int_dtypes + bool_types
+
+IndexSpec = collections.namedtuple(""IndexTest"", [""shape"", ""indexer""])
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+class IndexingTest(jtu.JaxTestCase):
+  """"""Tests for Numpy indexing translation rules.""""""
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(
+              name, jtu.format_shape_dtype_string( shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(10,), indexer=slice(1, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, -1)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, None)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(-3, None)),
+      ]),
+      (""OneSliceIndexNegativeStride"", [
+          IndexSpec(shape=(10,), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10,), indexer=slice(1, 8, -1)),  # empty result
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10,), indexer=slice(None, None, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(3, 1, -1)),
+          IndexSpec(shape=(10, 8), indexer=slice(0, 8, -1)),  # empty result
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -1)),
+      ]),
+      (""OneSliceIndexNonUnitStride"", [
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 2)),
+          IndexSpec(shape=(10,), indexer=slice(0, 8, 3)),
+          IndexSpec(shape=(10,), indexer=slice(1, 3, 2)),
+          IndexSpec(shape=(10,), indexer=slice(1, None, 2)),
+          IndexSpec(shape=(10,), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(1, 8, 3)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, 2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, 1, -2)),
+          IndexSpec(shape=(10, 8), indexer=slice(None, None, -2)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(10, 8), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(
+              shape=(10, 8), indexer=(slice(None, None, -1), slice(None, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(10, 8, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(10, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(10, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      (""EmptyIndex"", [
+          IndexSpec(shape=(), indexer=()),
+          IndexSpec(shape=(3,), indexer=()),
+          IndexSpec(shape=(3, 4), indexer=()),
+      ]),
+  ] for shape, indexer in index_specs for dtype in all_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: x[indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters({
+      ""testcase_name"":
+          ""{}_inshape={}_indexer={}"".format(name,
+                                            jtu.format_shape_dtype_string(
+                                                shape, dtype), indexer),
+      ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer
+  } for name, index_specs in [
+      (""OneIntIndex"", [
+          IndexSpec(shape=(3,), indexer=1),
+          IndexSpec(shape=(3, 3), indexer=0),
+          IndexSpec(shape=(3, 4, 5), indexer=2),
+          IndexSpec(shape=(3,), indexer=-1),
+          IndexSpec(shape=(3,), indexer=-2),
+      ]),
+      (""TwoIntIndices"", [
+          IndexSpec(shape=(3, 3), indexer=(2, 1)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+          IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+      ]),
+      (""ThreeIntIndices"", [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      (""OneSliceIndex"", [
+          IndexSpec(shape=(5,), indexer=slice(1, 3)),
+          IndexSpec(shape=(5,), indexer=slice(1, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, -1)),
+          IndexSpec(shape=(5,), indexer=slice(None, None, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(1, None)),
+          IndexSpec(shape=(5, 4), indexer=slice(None, 3)),
+          IndexSpec(shape=(5, 4), indexer=slice(-3, None)),
+      ]),
+      (""TwoSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(slice(1, None), slice(None, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None))),
+          IndexSpec(shape=(5, 4, 3), indexer=(slice(1, None), slice(0, 2))),
+      ]),
+      (""OneColonIndex"", [
+          IndexSpec(shape=(3,), indexer=slice(None)),
+          IndexSpec(shape=(3, 4), indexer=slice(None)),
+      ]),
+      (""MultipleColonIndices"", [
+          IndexSpec(shape=(3, 4), indexer=(slice(None), slice(None))),
+          IndexSpec(shape=(3, 4, 5), indexer=(slice(None), slice(None))),
+      ]),
+      (""MixedSliceIndices"", [
+          IndexSpec(shape=(5, 4), indexer=(slice(None), slice(0, 2))),
+          IndexSpec(shape=(5, 4), indexer=(1, slice(None))),
+      ]),
+      (""EllipsisIndex"", [
+          IndexSpec(shape=(3,), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4), indexer=Ellipsis),
+          IndexSpec(shape=(3, 4, 5), indexer=(0, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis, 2, 3)),
+      ]),
+      (""NoneIndex"", [
+          IndexSpec(shape=(), indexer=None),
+          IndexSpec(shape=(), indexer=(None, None)),
+          IndexSpec(shape=(), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3,), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=None),
+          IndexSpec(shape=(3, 4), indexer=(Ellipsis, None)),
+          IndexSpec(shape=(3, 4), indexer=(0, None, Ellipsis)),
+          IndexSpec(shape=(3, 4, 5), indexer=(1, None, Ellipsis)),
+      ]),
+      # TODO(mattjj): these fail for uninteresting dtype reasons
+      # (""EmptyIndex"",
+      #  [IndexSpec(shape=(), indexer=()),
+      #   IndexSpec(shape=(3,), indexer=()),
+      #   IndexSpec(shape=(3, 4), indexer=()),
+      #   ]),
+  ] for shape, indexer in index_specs for dtype in float_dtypes
+                                  for rng in [jtu.rand_default()])
+  @jtu.skip_on_devices(""tpu"")
+  def testStaticIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  def _ReplaceSlicesWithTuples(self, idx):
+    """"""Helper method to replace slices with tuples for dynamic indexing args.""""""
+    if isinstance(idx, slice):
+      triple = idx.start, idx.stop, idx.step
+      isnone = [i for i, elt in enumerate(triple) if elt is None]
+      zeros = itertools.repeat(0)
+      nones = itertools.repeat(None)
+      out = lax.subvals(triple, zip(isnone, zeros))
+      return out, lambda out: slice(*lax.subvals(out, zip(isnone, nones)))
+    elif isinstance(idx, (tuple, list)) and idx:
+      t = type(idx)
+      elts, packs = zip(*map(self._ReplaceSlicesWithTuples, idx))
+      return elts, lambda elts: t((pack(i) for pack, i in zip(packs, elts)))
+    else:
+      return idx, lambda x: x
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneSliceIndex"",
+           [IndexSpec(shape=(5,), indexer=slice(1, 3)),
+            IndexSpec(shape=(5, 4), indexer=slice(1, 3))]),
+          (""TwoSliceIndices"",
+           [IndexSpec(shape=(5, 4), indexer=(slice(1, 3), slice(0, 2))),
+            IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, 2)))]),
+          (""NonUnitStrides"", [
+              IndexSpec(shape=(3,), indexer=slice(None, None, -1)),
+              IndexSpec(shape=(3, 3), indexer=slice(0, 3, -2)),
+              IndexSpec(shape=(3, 4, 5), indexer=slice(0, 4, 2))
+          ]),
+          (""OnlyStartOrStopDynamic"", [
+              IndexSpec(shape=(5, 4), indexer=(slice(None, 3), slice(0, 2))),
+              IndexSpec(shape=(5, 4, 3), indexer=(slice(1, 3), slice(0, None)))
+          ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithSlicesErrors(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self.assertRaises(IndexError, lambda: fun(*args_maker()))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2)]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2))]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicIndexingWithIntegers(self, shape, dtype, rng, indexer):
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    def fun(x, unpacked_indexer):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    args_maker = lambda: [rng(shape, dtype), unpacked_indexer]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""OneIntIndex"",
+           [IndexSpec(shape=(3,), indexer=1),
+            IndexSpec(shape=(3, 3), indexer=0),
+            IndexSpec(shape=(3, 4, 5), indexer=2),
+            IndexSpec(shape=(3,), indexer=-1),
+            IndexSpec(shape=(3,), indexer=-2),
+            ]),
+          (""TwoIntIndices"",
+           [IndexSpec(shape=(3, 3), indexer=(2, 1)),
+            IndexSpec(shape=(3, 4, 5), indexer=(1, 2)),
+            IndexSpec(shape=(3, 4, 5), indexer=(-1, 2)),
+            ]),
+          (""ThreeIntIndices"",
+           [IndexSpec((3, 4, 5), indexer=(1, 2, 3))]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def DISABLED_testDynamicIndexingWithIntegersGrads(self, shape, dtype, rng, indexer):
+    # TODO(mattjj): re-enable (test works but for grad-of-compile, in flux)
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    unpacked_indexer, pack_indexer = self._ReplaceSlicesWithTuples(indexer)
+
+    @api.jit
+    def fun(unpacked_indexer, x):
+      indexer = pack_indexer(unpacked_indexer)
+      return x[indexer]
+
+    arr = rng(shape, dtype)
+    check_grads(partial(fun, unpacked_indexer), (arr,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    args_maker = lambda: [rng(shape, dtype), indexer]
+    fun = lambda x, idx: x[idx]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""One1DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([0, 1])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([1, 2, 1])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([0, 2, 0, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-1, 1])),
+            IndexSpec(shape=(3,), indexer=onp.array([-2, -1])),
+            ]),
+          (""One2DIntArrayIndex"",
+           [IndexSpec(shape=(3,), indexer=onp.array([[0, 0]])),
+            IndexSpec(shape=(3, 3), indexer=onp.array([[1, 2, 1],
+                                                       [0, 1, -1]])),
+            IndexSpec(shape=(3, 4, 5), indexer=onp.array([[0, 2, 0, 1],
+                                                          [-1, -2, 1, 0]])),
+            ]),
+          (""Two1DIntArrayIndicesNoBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([0, 1]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2, 0, 1]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""Two1DIntArrayIndicesWithBroadcasting"",
+           [IndexSpec(shape=(3, 3), indexer=[onp.array([[0, 1]]),
+                                             onp.array([1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[onp.array([[0, 2, 0, 1]]),
+                                                onp.array([-1, 0, -1, 2])]),
+            ]),
+          (""ListOfPythonInts"",
+           [IndexSpec(shape=(3,), indexer=[0, 1, 0]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, -1]),
+            ]),
+          (""ListOfListsOfPythonInts"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1]]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]], [[2, 3, 0, 3]]]),
+            ]),
+          (""ListOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[0, onp.array([0, 1])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[0, 1,
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+          (""ListOfListsOfPythonIntsAndIntArrays"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[[0, 1], onp.array([0])]),
+            IndexSpec(shape=(3, 4, 5), indexer=[[[0], [-1]],
+                                                onp.array([[2, 3, 0, 3]])]),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testAdvancedIntegerIndexingGrads(self, shape, dtype, rng, indexer):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    arg = rng(shape, dtype)
+    fun = lambda x: x[indexer]**2
+    check_grads(fun, (arg,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_indexer={}""
+       .format(name, jtu.format_shape_dtype_string(shape, dtype), indexer),
+       ""shape"": shape, ""dtype"": dtype, ""rng"": rng, ""indexer"": indexer}
+      for name, index_specs in [
+          (""SlicesAndOneIntArrayIndex"",
+           [IndexSpec(shape=(2, 3), indexer=(onp.array([0, 1]), slice(1, 2))),
+            IndexSpec(shape=(2, 3), indexer=(slice(0, 2),
+                                             onp.array([0, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                slice(None))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([[0, 2], [1, 1]]),
+                                                slice(None))),
+            ]),
+          (""SlicesAndTwoIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                Ellipsis,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                Ellipsis)),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                onp.array([-1, 2]),
+                                                slice(1, 3))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                slice(1, 3),
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2, -2]),
+                                                slice(None, None, 2),
+                                                onp.array([-1, 2, -1]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([[0, 2], [2, 0]]),
+                                                Ellipsis,
+                                                onp.array([[1, 0], [1, 0]]))),
+            ]),
+          (""NonesAndIntArrayIndices"",
+           [IndexSpec(shape=(3, 4, 5), indexer=[onp.array([0, 2]),
+                                                None,
+                                                onp.array([-1, 2])]),
+            IndexSpec(shape=(3, 4, 5), indexer=(onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            IndexSpec(shape=(3, 4, 5), indexer=(Ellipsis,
+                                                onp.array([0, 2]),
+                                                None,
+                                                None,
+                                                onp.array([-1, 2]))),
+            ]),
+      ]
+      for shape, indexer in index_specs
+      for dtype in all_dtypes
+      for rng in [jtu.rand_default()])
+  def testMixedAdvancedIntegerIndexing(self, shape, dtype, rng, indexer):
+    indexer_with_dummies = [e if isinstance(e, onp.ndarray) else ()
+                            for e in indexer]
+    substitutes = [(i, e) for i, e in enumerate(indexer)
+                   if not isinstance(e, onp.ndarray)]
+    args_maker = lambda: [rng(shape, dtype), indexer_with_dummies]
+
+    def fun(x, indexer_with_dummies):
+      idx = type(indexer)(lax.subvals(indexer_with_dummies, substitutes))
+      return x[idx]
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  def testAdvancedIndexingManually(self):
+    x = onp.random.RandomState(0).randn(3, 4, 5)
+    index_array = onp.array([0, 2, -1, 0])
+
+    op = lambda x, index_array: x[..., index_array, :]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[..., index_array, :, index_array, None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+    op = lambda x, index_array: x[index_array, ..., index_array[:, None], None]
+    cop = api.jit(op)
+
+    a1 = op(x, index_array)
+    a2 = cop(x, index_array)
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+  def testUnpacking(self):
+
+    def foo(x):
+      a, b, c = x
+      return a + b + c
+
+    cfoo = api.jit(foo)
+
+    a1 = foo(onp.arange(3))
+    a2 = cfoo(onp.arange(3))
+
+    self.assertAllClose(a1, a2, check_dtypes=True)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_numpy_test.py,tests/lax_numpy_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
new file mode 100644
index 000000000..7b0c918fd
--- /dev/null
+++ b/tests/lax_numpy_test.py
@@ -0,0 +1,528 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_ONE_TO_ONE_OP_RECORDS = [
+    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
+    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+]
+
+JAX_COMPOUND_OP_RECORDS = [
+    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""expm1_large""),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""log1p_large""),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+]
+
+JAX_REDUCER_RECORDS = [
+    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+]
+
+JAX_ARGMINMAX_RECORDS = [
+    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedNumpyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Numpy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
+                                 JAX_COMPOUND_OP_RECORDS)
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis, ""keepdims"": keepdims}
+      for rec in JAX_REDUCER_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
+    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
+    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis}
+      for rec in JAX_ARGMINMAX_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape)))
+  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
+
+    def onp_fun(array_to_reduce):
+      return onp_op(array_to_reduce, axis)
+
+    def lnp_fun(array_to_reduce):
+      return lnp_op(array_to_reduce, axis)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""matrix-scalar"", (3, 3), ()),
+          (""scalar-matrix"", (), (3, 3)),
+          (""matrix-vector"", (4, 5), (5,)),
+          (""vector-matrix"", (6,), (6, 4)),
+          (""matrix-matrix"", (3, 4), (4, 5)),
+          (""tensor-vector"", (4, 3, 2), (2,)),
+          (""vector-tensor"", (2,), (3, 2, 4)),
+          (""tensor-matrix"", (4, 3, 2), (2, 5)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""vector-vector"", (3,), (3,)),
+          (""matrix-vector"", (3, 3), (3,)),
+          (""vector-matrix"", (3,), (3, 3)),
+          (""matrix-matrix"", (3, 3), (3, 3)),
+          (""vector-tensor"", (3,), (5, 3, 2)),
+          (""tensor-vector"", (5, 3, 2), (2,)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-matrix"", (5, 2, 3), (3, 2)),
+          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
+          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
+                            check_dtypes=True)
+    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
+       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
+  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
+    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
+    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_decimals={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), decimals),
+       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for decimals in [0, 1, -2])
+  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
+    onp_fun = lambda x: onp.round(x, decimals=decimals)
+    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
+          axis, "","".join(str(d) for d in base_shape),
+          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
+       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
+       ""rng"": jtu.rand_default()}
+      for num_arrs in [3]
+      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for axis in range(-len(base_shape)+1, len(base_shape)))
+  def testConcatenate(self, axis, base_shape, dtypes, rng):
+    wrapped_axis = axis % len(base_shape)
+    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
+    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
+    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)
+
+    def args_maker():
+      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
+          ""_"".join(str(d) for d in shape),
+          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
+       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
+      for shape in all_shapes
+      for fill_value_dtype in default_dtypes
+      for out_dtype in default_dtypes)
+  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
+    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
+    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
+    args_maker = lambda: [rng((), fill_value_dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
+       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
+       ""dtype"": dtype, ""rng"": jtu.rand_default()}
+      for shape, axis, num_sections in [
+          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
+          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
+      for dtype in default_dtypes)
+  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
+    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
+    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          ((3, 4), 12),
+          ((3, 4), (12,)),
+          ((3, 4), -1),
+          ((2, 1, 4), (-1,)),
+          ((2, 2, 4), (2, 8))
+      ])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    onp_fun = lambda x: onp.reshape(x, out_shape)
+    lnp_fun = lambda x: lnp.reshape(x, out_shape)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
+       ""rng"": jtu.rand_default()}
+      for arg_shape in [(), (3,), (3, 4)]
+      for dtype in default_dtypes
+      for dim in range(-len(arg_shape)+1, len(arg_shape)))
+  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
+    onp_fun = lambda x: onp.expand_dims(x, dim)
+    lnp_fun = lambda x: lnp.expand_dims(x, dim)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax1, ax2 in [
+          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
+          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
+      for dtype in default_dtypes)
+  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
+    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
+    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax in [
+          ((3, 1), None),
+          ((3, 1), 1),
+          ((1, 3, 1), (0, 2)),
+          ((1, 4, 1), (0,))]
+      for dtype in default_dtypes)
+  def testSqueeze(self, arg_shape, dtype, ax, rng):
+    onp_fun = lambda x: onp.squeeze(x, ax)
+    lnp_fun = lambda x: lnp.squeeze(x, ax)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
+      for i, arg in enumerate([
+          [1, 2, 3], [1., 2., 3.],
+          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
+          [[3, onp.array(2), 1], onp.arange(3.)],
+      ]))
+  def testArray(self, arg):
+    args_maker = lambda: [arg]
+    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)
+
+  def testAllClose(self):
+    rng = onp.random.RandomState(0)
+    x = rng.randn(2, 2)
+    y = rng.randn(2)
+
+    def same(list1, list2):
+      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
+      elements_close = list(map(allclose, list1, list2))
+      return lnp.all(lnp.array(elements_close))
+
+    csame = api.jit(same)
+
+    a1 = same((x, y), (x, y))
+    a2 = csame((x, y), (x, y))
+    a3 = csame((x, y), (x, 2 * y))
+
+    self.assertTrue(a1)
+    self.assertTrue(a2)
+    self.assertFalse(a3)
+
+  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
+  def DISABLED_testOnesBroadcastingConstantHandler(self):
+    # TODO(mattjj): update this test for jax3
+
+    def fun(x):
+      ones = lnp.ones((3, 4))
+      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)
+
+      # To check that the constant handler generates a Broadcast for stride-zero
+      # arrays, we monkey-patch the client instance.
+      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
+      # we can check the HLO more directly.
+      c = x._node.c
+      Broadcast = c.Broadcast  # pylint: disable=invalid-name
+      was_called = []
+      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
+      out = x + ones  # the ndarray constant handler should call Broadcast here
+      assert was_called, ""Broadcast was not called.""
+
+      return out
+
+    fun = api.jit(fun)
+    out_val = fun(lnp.ones(4))
+    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)
+
+  def testZeroStridesConstantHandler(self):
+    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
+    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))
+
+    def fun(x):
+      return x * const
+
+    fun = api.jit(fun)
+    out_val = fun(3.)
+    self.assertAllClose(out_val, 3. * const, check_dtypes=False)
+
+  def testIsInstanceNdarrayDuringTracing(self):
+    arr = onp.ones(3)
+
+    @api.jit
+    def f(x):
+      self.assertIsInstance(x, lnp.ndarray)
+      return lnp.sum(x)
+
+    f(arr)
+
+
+  def testNonArrayErrorMessage(self):
+    x = [1., 2.]
+    y = onp.array([3., 4.])
+
+    def g(x, y):
+      return lnp.add(x, y)
+
+    def f(x, y):
+      return lnp.dot(x, y)
+
+    self.assertRaises(TypeError, lambda: g(x, y))
+    self.assertRaises(TypeError, lambda: f(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))
+
+  def testAbstractionErrorMessage(self):
+
+    @api.jit
+    def f(x, n):
+      for _ in range(n):
+        x = x * x
+      return x
+
+    self.assertRaises(TypeError, lambda: f(3., 3))
+
+    @api.jit
+    def g(x):
+      if x > 0.:
+        return x * 2
+      else:
+        return x + 2
+
+    self.assertRaises(TypeError, lambda: g(3.))
+
+  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
+    # TODO(mattjj): update this for jax3
+    foo = lnp._not_implemented(lambda x: x)
+
+    # No error if there's no tracing.
+    foo(onp.arange(3))
+
+    cfoo = api.jit(foo)
+    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))
+
+  # TODO(mattjj): test infix operator overrides
+
+  def DISABLED_testRavel(self):
+    # TODO(mattjj): support this method-based syntax?
+    rng = onp.random.RandomState(0)
+    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
+    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test other ndarray-like method overrides
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
new file mode 100644
index 000000000..7b0c918fd
--- /dev/null
+++ b/tests/lax_numpy_test.py
@@ -0,0 +1,528 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import api
+from jax import numpy as lnp
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_ONE_TO_ONE_OP_RECORDS = [
+    op_record(""abs"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""add"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""bitwise_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""bitwise_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""ceil"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""conj"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""conjugate"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""exp"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""floor"", 1, float_dtypes, jtu.rand_default(), []),
+    op_record(""greater"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""greater_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""less_equal"", 2, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""log"", 1, numeric_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""logical_and"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_not"", 1, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_or"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""logical_xor"", 2, default_dtypes, jtu.rand_bool(), []),
+    op_record(""maximum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""minimum"", 2, default_dtypes, jtu.rand_some_inf(), []),
+    op_record(""multiply"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""negative"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""not_equal"", 2, default_dtypes, jtu.rand_some_equal(), [""rev""]),
+    op_record(""power"", 2, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""subtract"", 2, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""tanh"", 1, numeric_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sin"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""cos"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+]
+
+JAX_COMPOUND_OP_RECORDS = [
+    op_record(""cosh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""expm1_large""),
+    op_record(""expm1"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""floor_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""isclose"", 2, float_dtypes, jtu.rand_small_positive(), []),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_positive(), [],
+              test_name=""log1p_large""),
+    op_record(""log1p"", 1, numeric_dtypes, jtu.rand_small_positive(), []),
+    op_record(""logaddexp"", 2, float_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""ravel"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""remainder"", 2, default_dtypes, jtu.rand_nonzero(), []),
+    op_record(""sinh"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""sqrt"", 1, default_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""transpose"", 1, default_dtypes, jtu.rand_default(), [""rev""]),
+    op_record(""true_divide"", 2, default_dtypes, jtu.rand_nonzero(), [""rev""]),
+    op_record(""where"", 3, (onp.float32, onp.int64), jtu.rand_some_zero(), []),
+]
+
+JAX_REDUCER_RECORDS = [
+    op_record(""all"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""any"", 1, bool_dtypes, jtu.rand_default(), []),
+    op_record(""max"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""mean"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""min"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""prod"", 1, default_dtypes, jtu.rand_small_positive(), []),
+    op_record(""sum"", 1, default_dtypes, jtu.rand_default(), []),
+    op_record(""var"", 1, default_dtypes, jtu.rand_default(), []),
+]
+
+JAX_ARGMINMAX_RECORDS = [
+    op_record(""argmin"", 1, default_dtypes, jtu.rand_some_equal(), []),
+    op_record(""argmax"", 1, default_dtypes, jtu.rand_some_equal(), []),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedNumpyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Numpy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(rec.test_name, shapes,
+                                                    dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name)}
+      for rec in itertools.chain(JAX_ONE_TO_ONE_OP_RECORDS,
+                                 JAX_COMPOUND_OP_RECORDS)
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testOp(self, onp_op, lnp_op, rng, shapes, dtypes):
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    self._CheckAgainstNumpy(onp_op, lnp_op, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}_keepdims={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis, ""keepdims"": keepdims}
+      for rec in JAX_REDUCER_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testReducer(self, onp_op, lnp_op, rng, shape, dtype, axis, keepdims):
+    onp_fun = lambda x: onp_op(x, axis, keepdims=keepdims)
+    lnp_fun = lambda x: lnp_op(x, axis, keepdims=keepdims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""{}_inshape={}_axis={}"".format(
+          rec.test_name.capitalize(),
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rec.rng, ""shape"": shape, ""dtype"": dtype,
+       ""onp_op"": getattr(onp, rec.name), ""lnp_op"": getattr(lnp, rec.name),
+       ""axis"": axis}
+      for rec in JAX_ARGMINMAX_RECORDS
+      for shape in all_shapes for dtype in rec.dtypes
+      for axis in range(-len(shape), len(shape)))
+  def testArgMinMax(self, onp_op, lnp_op, rng, shape, dtype, axis):
+
+    def onp_fun(array_to_reduce):
+      return onp_op(array_to_reduce, axis)
+
+    def lnp_fun(array_to_reduce):
+      return lnp_op(array_to_reduce, axis)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""matrix-scalar"", (3, 3), ()),
+          (""scalar-matrix"", (), (3, 3)),
+          (""matrix-vector"", (4, 5), (5,)),
+          (""vector-matrix"", (6,), (6, 4)),
+          (""matrix-matrix"", (3, 4), (4, 5)),
+          (""tensor-vector"", (4, 3, 2), (2,)),
+          (""vector-tensor"", (2,), (3, 2, 4)),
+          (""tensor-matrix"", (4, 3, 2), (2, 5)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-tensor"", (2, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testDot(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.dot, lnp.dot, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_{}_{}"".format(
+          name,
+          jtu.format_shape_dtype_string(lhs_shape, lhs_dtype),
+          jtu.format_shape_dtype_string(rhs_shape, rhs_dtype)),
+       ""lhs_shape"": lhs_shape, ""lhs_dtype"": lhs_dtype,
+       ""rhs_shape"": rhs_shape, ""rhs_dtype"": rhs_dtype,
+       ""rng"": rng}
+      for rng in [jtu.rand_default()]
+      for name, lhs_shape, rhs_shape in [
+          (""vector-vector"", (3,), (3,)),
+          (""matrix-vector"", (3, 3), (3,)),
+          (""vector-matrix"", (3,), (3, 3)),
+          (""matrix-matrix"", (3, 3), (3, 3)),
+          (""vector-tensor"", (3,), (5, 3, 2)),
+          (""tensor-vector"", (5, 3, 2), (2,)),
+          (""matrix-tensor"", (5, 2), (3, 2, 4)),
+          (""tensor-matrix"", (5, 2, 3), (3, 2)),
+          (""tensor-tensor"", (5, 3, 4), (5, 4, 1)),
+          (""tensor-tensor-broadcast"", (3, 1, 3, 4), (5, 4, 1))]
+      for lhs_dtype, rhs_dtype in CombosWithReplacement(float_dtypes, 2))
+  def testMatmul(self, lhs_shape, lhs_dtype, rhs_shape, rhs_dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, lhs_dtype), rng(rhs_shape, rhs_dtype)]
+    self._CheckAgainstNumpy(onp.matmul, lnp.matmul, args_maker,
+                            check_dtypes=True)
+    self._CompileAndCheck(lnp.matmul, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_amin={}_amax={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), a_min, a_max),
+       ""shape"": shape, ""dtype"": dtype, ""a_min"": a_min, ""a_max"": a_max,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for a_min, a_max in [(-1, None), (None, 1), (-1, 1)])
+  def testClipStaticBounds(self, shape, dtype, a_min, a_max, rng):
+    onp_fun = lambda x: onp.clip(x, a_min=a_min, a_max=a_max)
+    lnp_fun = lambda x: lnp.clip(x, a_min=a_min, a_max=a_max)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_decimals={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), decimals),
+       ""shape"": shape, ""dtype"": dtype, ""decimals"": decimals,
+       ""rng"": jtu.rand_default()}
+      for shape in all_shapes for dtype in float_dtypes
+      for decimals in [0, 1, -2])
+  def testRoundStaticDecimals(self, shape, dtype, decimals, rng):
+    onp_fun = lambda x: onp.round(x, decimals=decimals)
+    lnp_fun = lambda x: lnp.round(x, decimals=decimals)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_axis={}_baseshape=[{}]_dtypes=[{}]"".format(
+          axis, "","".join(str(d) for d in base_shape),
+          "","".join(onp.dtype(dtype).name for dtype in dtypes)),
+       ""axis"": axis, ""base_shape"": base_shape, ""dtypes"": dtypes,
+       ""rng"": jtu.rand_default()}
+      for num_arrs in [3]
+      for dtypes in CombosWithReplacement(default_dtypes, num_arrs)
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for axis in range(-len(base_shape)+1, len(base_shape)))
+  def testConcatenate(self, axis, base_shape, dtypes, rng):
+    wrapped_axis = axis % len(base_shape)
+    shapes = [base_shape[:wrapped_axis] + (size,) + base_shape[wrapped_axis+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), dtypes)]
+    onp_fun = lambda *args: onp.concatenate(args, axis=axis)
+    lnp_fun = lambda *args: lnp.concatenate(args, axis=axis)
+
+    def args_maker():
+      return [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
+          ""_"".join(str(d) for d in shape),
+          onp.dtype(fill_value_dtype).name, onp.dtype(out_dtype).name),
+       ""shape"": shape, ""fill_value_dtype"": fill_value_dtype,
+       ""out_dtype"": out_dtype, ""rng"": jtu.rand_default()}
+      for shape in all_shapes
+      for fill_value_dtype in default_dtypes
+      for out_dtype in default_dtypes)
+  def testFull(self, shape, fill_value_dtype, out_dtype, rng):
+    onp_fun = lambda fill_value: onp.full(shape, fill_value, dtype=out_dtype)
+    lnp_fun = lambda fill_value: lnp.full(shape, fill_value, dtype=out_dtype)
+    args_maker = lambda: [rng((), fill_value_dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}_axis={}_{}sections"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, num_sections),
+       ""shape"": shape, ""num_sections"": num_sections, ""axis"": axis,
+       ""dtype"": dtype, ""rng"": jtu.rand_default()}
+      for shape, axis, num_sections in [
+          ((3,), 0, 3), ((12,), 0, 3), ((12, 4), 0, 4), ((12, 4), 1, 2),
+          ((2, 3, 4), -1, 2), ((2, 3, 4), -2, 3)]
+      for dtype in default_dtypes)
+  def testSplitStaticInt(self, shape, num_sections, axis, dtype, rng):
+    onp_fun = lambda x: onp.split(x, num_sections, axis=axis)
+    lnp_fun = lambda x: lnp.split(x, num_sections, axis=axis)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          ((3, 4), 12),
+          ((3, 4), (12,)),
+          ((3, 4), -1),
+          ((2, 1, 4), (-1,)),
+          ((2, 2, 4), (2, 8))
+      ])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    onp_fun = lambda x: onp.reshape(x, out_shape)
+    lnp_fun = lambda x: lnp.reshape(x, out_shape)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_expanddim={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), dim),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""dim"": dim,
+       ""rng"": jtu.rand_default()}
+      for arg_shape in [(), (3,), (3, 4)]
+      for dtype in default_dtypes
+      for dim in range(-len(arg_shape)+1, len(arg_shape)))
+  def testExpandDimsStaticDim(self, arg_shape, dtype, dim, rng):
+    onp_fun = lambda x: onp.expand_dims(x, dim)
+    lnp_fun = lambda x: lnp.expand_dims(x, dim)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axes=({},{})"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax1, ax2),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax1"": ax1, ""ax2"": ax2,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax1, ax2 in [
+          ((3, 4), 0, 1), ((3, 4), 1, 0), ((3, 4, 5), 1, 2),
+          ((3, 4, 5), -1, -2), ((3, 4, 5), 0, 1)]
+      for dtype in default_dtypes)
+  def testSwapAxesStaticAxes(self, arg_shape, dtype, ax1, ax2, rng):
+    onp_fun = lambda x: onp.swapaxes(x, ax1, ax2)
+    lnp_fun = lambda x: lnp.swapaxes(x, ax1, ax2)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype), ax),
+       ""arg_shape"": arg_shape, ""dtype"": dtype, ""ax"": ax,
+       ""rng"": jtu.rand_default()}
+      for arg_shape, ax in [
+          ((3, 1), None),
+          ((3, 1), 1),
+          ((1, 3, 1), (0, 2)),
+          ((1, 4, 1), (0,))]
+      for dtype in default_dtypes)
+  def testSqueeze(self, arg_shape, dtype, ax, rng):
+    onp_fun = lambda x: onp.squeeze(x, ax)
+    lnp_fun = lambda x: lnp.squeeze(x, ax)
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_arg{}"".format(i), ""arg"": arg}
+      for i, arg in enumerate([
+          [1, 2, 3], [1., 2., 3.],
+          [[1, 2], [3, 4], [5, 6]], [[1, 2.], [3, 4], [5, 6]],
+          [[3, onp.array(2), 1], onp.arange(3.)],
+      ]))
+  def testArray(self, arg):
+    args_maker = lambda: [arg]
+    self._CheckAgainstNumpy(onp.array, lnp.array, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lnp.array, args_maker, check_dtypes=True)
+
+  def testAllClose(self):
+    rng = onp.random.RandomState(0)
+    x = rng.randn(2, 2)
+    y = rng.randn(2)
+
+    def same(list1, list2):
+      allclose = functools.partial(lnp.allclose, atol=1e-3, rtol=1e-3)
+      elements_close = list(map(allclose, list1, list2))
+      return lnp.all(lnp.array(elements_close))
+
+    csame = api.jit(same)
+
+    a1 = same((x, y), (x, y))
+    a2 = csame((x, y), (x, y))
+    a3 = csame((x, y), (x, 2 * y))
+
+    self.assertTrue(a1)
+    self.assertTrue(a2)
+    self.assertFalse(a3)
+
+  @jtu.skip_on_devices(""tpu"")  # TODO(mattjj): investigate this failure
+  def DISABLED_testOnesBroadcastingConstantHandler(self):
+    # TODO(mattjj): update this test for jax3
+
+    def fun(x):
+      ones = lnp.ones((3, 4))
+      assert isinstance(ones, onp.ndarray) and ones.strides == (0, 0)
+
+      # To check that the constant handler generates a Broadcast for stride-zero
+      # arrays, we monkey-patch the client instance.
+      # TODO(mattjj): once we have better HLO dumping and inspecting facilities,
+      # we can check the HLO more directly.
+      c = x._node.c
+      Broadcast = c.Broadcast  # pylint: disable=invalid-name
+      was_called = []
+      c.Broadcast = lambda *args: was_called.append(True) or Broadcast(*args)
+      out = x + ones  # the ndarray constant handler should call Broadcast here
+      assert was_called, ""Broadcast was not called.""
+
+      return out
+
+    fun = api.jit(fun)
+    out_val = fun(lnp.ones(4))
+    self.assertAllClose(out_val, onp.full((3, 4), 2.), check_dtypes=False)
+
+  def testZeroStridesConstantHandler(self):
+    raw_const = onp.random.RandomState(0).randn(1, 2, 1, 1, 5, 1)
+    const = onp.broadcast_to(raw_const, (3, 2, 3, 4, 5, 6))
+
+    def fun(x):
+      return x * const
+
+    fun = api.jit(fun)
+    out_val = fun(3.)
+    self.assertAllClose(out_val, 3. * const, check_dtypes=False)
+
+  def testIsInstanceNdarrayDuringTracing(self):
+    arr = onp.ones(3)
+
+    @api.jit
+    def f(x):
+      self.assertIsInstance(x, lnp.ndarray)
+      return lnp.sum(x)
+
+    f(arr)
+
+
+  def testNonArrayErrorMessage(self):
+    x = [1., 2.]
+    y = onp.array([3., 4.])
+
+    def g(x, y):
+      return lnp.add(x, y)
+
+    def f(x, y):
+      return lnp.dot(x, y)
+
+    self.assertRaises(TypeError, lambda: g(x, y))
+    self.assertRaises(TypeError, lambda: f(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(g)(x, y))
+    self.assertRaises(TypeError, lambda: api.jit(f)(x, y))
+
+  def testAbstractionErrorMessage(self):
+
+    @api.jit
+    def f(x, n):
+      for _ in range(n):
+        x = x * x
+      return x
+
+    self.assertRaises(TypeError, lambda: f(3., 3))
+
+    @api.jit
+    def g(x):
+      if x > 0.:
+        return x * 2
+      else:
+        return x + 2
+
+    self.assertRaises(TypeError, lambda: g(3.))
+
+  def DISABLED_testTracingPrimitiveWithNoTranslationErrorMessage(self):
+    # TODO(mattjj): update this for jax3
+    foo = lnp._not_implemented(lambda x: x)
+
+    # No error if there's no tracing.
+    foo(onp.arange(3))
+
+    cfoo = api.jit(foo)
+    self.assertRaises(NotImplementedError, lambda: cfoo(onp.arange(3)))
+
+  # TODO(mattjj): test infix operator overrides
+
+  def DISABLED_testRavel(self):
+    # TODO(mattjj): support this method-based syntax?
+    rng = onp.random.RandomState(0)
+    args_maker = lambda: [rng.randn(3, 4).astype(""float32"")]
+    self._CompileAndCheck(lambda x: x.ravel(), args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test other ndarray-like method overrides
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_scipy_test.py,tests/lax_scipy_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
new file mode 100644
index 000000000..21a5ac4b3
--- /dev/null
+++ b/tests/lax_scipy_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.misc as osp_misc
+import scipy.special as osp_special
+
+from jax import api
+from jax import test_util as jtu
+from jax.scipy import misc as lsp_misc
+from jax.scipy import special as lsp_special
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_SPECIAL_FUNCTION_RECORDS = [
+    op_record(""gammaln"", 1, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""digamma"", 1, float_dtypes, jtu.rand_positive(), []),
+    op_record(""erf"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfc"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfinv"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedScipyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Scipy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}_keepdims={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": jtu.rand_default(), ""shape"": shape, ""dtype"": dtype,
+       ""axis"": axis, ""keepdims"": keepdims}
+      for shape in all_shapes for dtype in float_dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    def scipy_fun(array_to_reduce):
+      return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    def lax_fun(array_to_reduce):
+      return lsp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.test_name, shapes, dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""modes"": rec.diff_modes,
+       ""scipy_op"": getattr(osp_special, rec.name),
+       ""lax_op"": getattr(lsp_special, rec.name)}
+      for rec in JAX_SPECIAL_FUNCTION_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
+    # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
+        and not shapes[0]):
+      return absltest.unittest.skip(""real() on scalar not supported on tpu"")
+
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    args = args_maker()
+    self.assertAllClose(scipy_op(*args), lax_op(*args), atol=1e-3, rtol=1e-3,
+                        check_dtypes=False)
+    self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
new file mode 100644
index 000000000..21a5ac4b3
--- /dev/null
+++ b/tests/lax_scipy_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.misc as osp_misc
+import scipy.special as osp_special
+
+from jax import api
+from jax import test_util as jtu
+from jax.scipy import misc as lsp_misc
+from jax.scipy import special as lsp_special
+
+FLAGS = flags.FLAGS
+
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+numeric_dtypes = float_dtypes + complex_dtypes + int_dtypes
+
+
+OpRecord = collections.namedtuple(""OpRecord"", [""name"", ""nargs"", ""dtypes"", ""rng"",
+                                               ""diff_modes"", ""test_name""])
+
+
+def op_record(name, nargs, dtypes, rng, diff_modes, test_name=None):
+  test_name = test_name or name
+  return OpRecord(name, nargs, dtypes, rng, diff_modes, test_name)
+
+JAX_SPECIAL_FUNCTION_RECORDS = [
+    op_record(""gammaln"", 1, float_dtypes, jtu.rand_positive(), [""rev""]),
+    op_record(""digamma"", 1, float_dtypes, jtu.rand_positive(), []),
+    op_record(""erf"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfc"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+    op_record(""erfinv"", 1, float_dtypes, jtu.rand_small_positive(), [""rev""]),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxBackedScipyTests(jtu.JaxTestCase):
+  """"""Tests for LAX-backed Scipy implementation.""""""
+
+  def _GetArgsMaker(self, rng, shapes, dtypes):
+    return lambda: [rng(shape, dtype) for shape, dtype in zip(shapes, dtypes)]
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_axis={}_keepdims={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis, keepdims),
+       ""rng"": jtu.rand_default(), ""shape"": shape, ""dtype"": dtype,
+       ""axis"": axis, ""keepdims"": keepdims}
+      for shape in all_shapes for dtype in float_dtypes
+      for axis in range(-len(shape), len(shape))
+      for keepdims in [False, True])
+  def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    def scipy_fun(array_to_reduce):
+      return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    def lax_fun(array_to_reduce):
+      return lsp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.test_name, shapes, dtypes),
+       ""rng"": rec.rng, ""shapes"": shapes, ""dtypes"": dtypes,
+       ""modes"": rec.diff_modes,
+       ""scipy_op"": getattr(osp_special, rec.name),
+       ""lax_op"": getattr(lsp_special, rec.name)}
+      for rec in JAX_SPECIAL_FUNCTION_RECORDS
+      for shapes in CombosWithReplacement(all_shapes, rec.nargs)
+      for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
+  def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
+    # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
+        and not shapes[0]):
+      return absltest.unittest.skip(""real() on scalar not supported on tpu"")
+
+    args_maker = self._GetArgsMaker(rng, shapes, dtypes)
+    args = args_maker()
+    self.assertAllClose(scipy_op(*args), lax_op(*args), atol=1e-3, rtol=1e-3,
+                        check_dtypes=False)
+    self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/lax_test.py,tests/lax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/lax_test.py b/tests/lax_test.py
new file mode 100644
index 000000000..b8c78524c
--- /dev/null
+++ b/tests/lax_test.py
@@ -0,0 +1,1981 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import numpy.random as npr
+
+from jax import api
+from jax import core
+from jax import lax
+from jax import test_util as jtu
+from jax import lax_reference
+from jax.interpreters import xla
+from jax.lib import xla_bridge
+
+FLAGS = flags.FLAGS
+
+
+def num_float_bits(dtype):
+  return onp.finfo(xla_bridge.canonicalize_dtype(dtype)).bits
+
+
+### lax tests
+
+# For standard unops and binops, we can generate a large number of tests on
+# arguments of appropriate shapes and dtypes using the following table.
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + complex_dtypes + int_dtypes + bool_dtypes
+
+compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]
+
+OpRecord = collections.namedtuple(""OpRecord"",
+                                  [""op"", ""nargs"", ""dtypes"", ""rng"", ""tol""])
+
+
+def op_record(op, nargs, dtypes, rng, tol=1e-5):
+  return OpRecord(op, nargs, dtypes, rng, tol)
+
+LAX_OPS = [
+    op_record(lax.neg, 1, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sign, 1, default_dtypes, jtu.rand_small()),
+    op_record(lax.floor, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.ceil, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.round, 1, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.is_finite, 1, float_dtypes, jtu.rand_small()),
+
+    op_record(lax.exp, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.expm1, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.log, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.log1p, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tanh, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sin, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cos, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.atan2, 2, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.sqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.rsqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.square, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.reciprocal, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tan, 1, float_dtypes, jtu.rand_default()),
+    op_record(lax.asin, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.acos, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.atan, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.sinh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cosh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.asinh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.acosh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.lgamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.digamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.erf, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erfc, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erf_inv, 1, float_dtypes, jtu.rand_small(), tol=1e-2),
+
+    op_record(lax.real, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.imag, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.complex, 2, [onp.float32], jtu.rand_default()),
+    op_record(lax.conj, 1, [onp.float32] + complex_dtypes, jtu.rand_default()),
+    op_record(lax.abs, 1, default_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.pow, 2, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.bitwise_and, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_not, 1, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_or, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_xor, 2, bool_dtypes, jtu.rand_small()),
+
+    op_record(lax.add, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sub, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.mul, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.div, 2, default_dtypes + complex_dtypes, jtu.rand_nonzero()),
+    op_record(lax.rem, 2, default_dtypes, jtu.rand_nonzero()),
+
+    op_record(lax.max, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.min, 2, default_dtypes, jtu.rand_small()),
+
+    op_record(lax.eq, 2, all_dtypes, jtu.rand_some_equal()),
+    op_record(lax.ne, 2, all_dtypes, jtu.rand_small()),
+    op_record(lax.ge, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.gt, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.le, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.lt, 2, default_dtypes, jtu.rand_small()),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxTest(jtu.JaxTestCase):
+  """"""Numerical tests for LAX operations.""""""
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOp(self, op, rng, shapes, dtype):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""tol"": rec.tol}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    numpy_op = getattr(lax_reference, op.__name__)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker, tol=tol)
+
+  # TODO test shift_left, shift_right_arithmetic, shift_right_logical
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
+                            rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    numpy_op = lambda *args: lax_reference.concatenate(args, dim)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv(lhs, rhs, strides, padding)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
+                           rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)
+    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dilation, rhs_dilation, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
+      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
+      rhs_dilation, rng):
+    # TODO(mattjj): make this test pass
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    def numpy_fun(lhs, rhs):
+      return lax_reference.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CheckAgainstNumpy(fun, numpy_fun, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}""
+       ""_dims={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation,
+           "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""dimension_numbers"": dim_nums,
+       ""perms"": perms, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (2, 1)]
+      for padding in [((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2)], repeat=2)
+      for rng in [jtu.rand_small()]
+      for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
+                              ([0, 1, 2, 3], [0, 1, 2, 3])),
+                             ((""NHWC"", ""HWIO"", ""NHWC""),
+                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
+                             padding, lhs_dilation, rhs_dilation,
+                             dimension_numbers, perms, rng):
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+
+    def args_maker():
+      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),
+              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]
+
+    def fun(lhs, rhs):
+      return lax.conv_general_dilated(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,
+          dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test conv_general_dilated against numpy
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDot(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               lhs_contracting, rhs_contracting),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""lhs_contracting"": lhs_contracting, ""rhs_contracting"": rhs_contracting,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [
+          # these all fail with ""RuntimeError: Unimplemented: Dot with
+          # non-standard contracting dimensions not implemented.""
+          # [(3, 5), (2, 5), [1], [1]],
+          # [(5, 3), (5, 2), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],
+          # [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],
+          [(3, 2), (2, 4), [1], [0]],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
+                                 lhs_contracting, rhs_contracting, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
+                                     dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
+                                 dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)
+    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
+                                     dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    numpy_op = lambda x: lax_reference.reshape(x, out_shape)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPad(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadAgainstNumpy(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
+    numpy_op = lambda x: lax_reference.pad(x, onp.array(0, dtype), pads)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testReverse(self):
+    rev = api.jit(lambda operand: lax.rev(operand, dimensions))
+
+    dimensions = [0]
+    self.assertAllClose(onp.array([3, 2, 1]), rev(onp.array([1, 2, 3])),
+                        check_dtypes=False)
+
+    dimensions = [0, 1]
+    self.assertAllClose(onp.array([[6, 5, 4], [3, 2, 1]]),
+                        rev(onp.array([[1, 2, 3], [4, 5, 6]])),
+                        check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSlice(self, shape, dtype, starts, limits, strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
+                            strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                   size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)
+    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
+                          check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CheckAgainstNumpy(lax.dynamic_update_slice,
+                            lax_reference.dynamic_update_slice, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTranspose(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    numpy_op = lambda x: lax_reference.transpose(x, perm)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, default_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.iinfo(onp.int32).min, lax.max, [onp.int32]),
+          (onp.iinfo(onp.int64).min, lax.max, [onp.int64]),
+          (onp.iinfo(onp.uint32).min, lax.max, [onp.uint32]),
+          (onp.iinfo(onp.uint64).min, lax.max, [onp.uint64]),
+          (onp.inf, lax.min, float_dtypes),
+          (onp.iinfo(onp.int32).max, lax.min, [onp.int32]),
+          (onp.iinfo(onp.int64).max, lax.min, [onp.int64]),
+          (onp.iinfo(onp.uint32).max, lax.min, [onp.uint32]),
+          (onp.iinfo(onp.uint64).max, lax.min, [onp.uint64]),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduce(self, op, init_val, shape, dtype, dims, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype), init_val]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    fun = lambda operand: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, [onp.float32]),
+          (-onp.inf, lax.max, [onp.float32]),
+          (onp.inf, lax.min, [onp.float32]),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testReduceWindow(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((4, 6), (2, 1), (1, 2))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],
+              [(2, 1), (1, 2)],
+              [(1, 1), (2, 1), (1, 2)]),
+          itertools.product(
+              [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],
+              [(1, 2, 2, 1), (1, 1, 1, 1)]))
+
+    def fun(operand, init_val):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype), init_val]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype)]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSort(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: lax.sort(x, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortAgainstNumpy(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.sort(x, axis)
+    numpy_op = lambda x: lax_reference.sort(x, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)
+    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testWhileWithTuple(self):
+    limit = 10
+
+    def loop_cond(state):
+      pos, _ = state
+      return lax.lt(pos, limit)
+
+    def loop_body(state):
+      pos, count = state
+      return (lax.add(pos, 1), lax.add(count, 1))
+
+    def loop(init):
+      result = lax._while_loop(loop_cond, loop_body, (init, 0))
+      _, count = result
+      return count
+
+    cloop = api.jit(loop)
+
+    self.assertEqual(loop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(3), limit - 3)
+
+  def testNestedWhile(self):
+
+    def outer_loop(num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        num, i, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        num, i, count = state
+        return (num, lax.add(i, 1), inner_loop(i, count))
+
+      init_val = (num, 0, 0)
+      _, i, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return (i, count)
+
+    def inner_loop(i, count):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        i, j, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, count = state
+        return (i, lax.add(j, 1), lax.add(count, 1))
+
+      init_val = (i, 0, count)
+      _, _, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return count
+
+    cloop = api.jit(outer_loop)
+
+    self.assertEqual(outer_loop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(2), (2, 3))
+    self.assertEqual(cloop(4), (4, 10))
+
+  def testNestedWhileWithDynamicUpdateSlice(self):
+    num = 5
+
+    def update_entry(arr, val, i, j):
+      val = lax.reshape(val, [1, 1])
+      return lax.dynamic_update_slice(arr, val, (i, j))
+
+    def outer_loop(arr):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, num, _, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        i, num, arr, out = state
+        return (lax.add(i, 1), num, arr, inner_loop(i, arr, out))
+
+      out = onp.zeros(arr.shape, dtype=arr.dtype)
+      init_val = (0, num, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    def inner_loop(i, arr, out):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, j, _, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, arr, out = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        arr_i_j = lax.dynamic_index_in_dim(arr_i, j, 0, False)
+        out = update_entry(out, arr_i_j, i, j)
+        return (i, lax.add(j, 1), arr, out)
+
+      init_val = (i, 0, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    cloop = api.jit(outer_loop)
+    arr = npr.RandomState(0).randn(5, 5)
+    self.assertAllClose(outer_loop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+
+  def testLoopWithConjunctionCondition(self):
+    def sum_first_n(arr, num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        arr, num, i, _ = state
+        return lax.bitwise_and(lax.lt(i, num), lax.lt(i, arr.shape[0]))
+
+      def body_fun(state):
+        arr, num, i, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, num, lax.add(i, 1), lax.add(total, arr_i))
+
+      init_val = (arr, num, 0, 0.)
+      _, _, _, total = lax._while_loop(cond_fun, body_fun, init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopBasic(self):
+    def count(num):
+      def body_fun(i, tot):
+        return lax.add(tot, i)
+      return lax.fori_loop(0, num, body_fun, 0)
+
+    cfun = api.jit(count)
+
+    self.assertEqual(count(2), 1)
+    self.assertEqual(count(2), cfun(2))
+    self.assertEqual(count(3), 3)
+    self.assertEqual(count(3), cfun(3))
+    self.assertEqual(count(4), 6)
+    self.assertEqual(count(4), cfun(4))
+
+  def testForiLoopTupleState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i))
+
+      init_val = (arr, 0.)
+      _, total = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun,
+                               init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopDictState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state['arr'], state['total']
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return {'arr': arr, 'total': lax.add(total, arr_i)}
+
+      init_val = {'arr': arr, 'total': 0.}
+      out_val = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return out_val['total']
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopEmptyTupleInState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total, _ = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i), ())
+
+      init_val = (arr, 0., ())
+      _, tot, _ = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return tot
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),
+                                   ((5, 3, 2), (5, 2, 4)),
+                                   ((1, 2, 2, 3), (1, 2, 3, 1))]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
+    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)
+
+  def testCollapse(self):
+
+    @api.jit
+    def collapse_first_two(x):
+      return lax.collapse(x, 0, 2)
+
+    self.assertEqual((6,), collapse_first_two(onp.zeros((2, 3))).shape)
+    self.assertEqual((6, 4), collapse_first_two(onp.zeros((2, 3, 4))).shape)
+    self.assertEqual((2, 3, 4),
+                     collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in all_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTake(self, shape, dtype, idxs, axes, rng):
+    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
+    fun = lambda src, idxs: lax.index_take(src, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+    ridxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(src_shape, dtype), rng(dst_shape, dtype), ridxs()]
+    fun = lambda src, dst, idxs: lax.index_untake(src, dst, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+
+GradTestSpec = collections.namedtuple(
+    ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])
+
+LAX_GRAD_OPS = [
+    GradTestSpec(lax.neg, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.floor, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.ceil, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.round, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.rem, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.exp, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.expm1, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log1p, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.tanh, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sin, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.cos, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+
+    GradTestSpec(lax.erf, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erfc, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erf_inv, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.lgamma, nargs=1, order=2, rng=jtu.rand_small(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.real, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    GradTestSpec(lax.imag, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    # GradTestSpec(lax.complex, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float32]),  # TODO(mattjj): enable
+    GradTestSpec(lax.conj, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float32, onp.complex64]),
+    GradTestSpec(lax.abs, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.pow, nargs=2, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.add, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sub, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.mul, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.div, nargs=2, order=1, rng=jtu.rand_not_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.max, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.min, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+]
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+def check_grads_bilinear(f, args, order, atol=None, rtol=None):
+  # Can use large eps to make up for numerical inaccuracies since the op is
+  # bilinear (relying on the fact that we only check one arg at a time)
+  lhs, rhs = args
+  check_grads(lambda lhs: f(lhs, rhs), (lhs,), order, atol, rtol, eps=1.)
+  check_grads(lambda rhs: f(lhs, rhs), (rhs,), order, atol, rtol, eps=1.)
+
+
+class LaxAutodiffTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""order"": rec.order}
+      for rec in LAX_GRAD_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes
+  )
+  def testOpGrad(self, op, rng, shapes, dtype, order):
+    if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
+      if dtype is onp.complex64:
+        return absltest.unittest.skip(""complex grads unimplemented on tpu"")
+      if op is lax.pow:
+        return absltest.unittest.skip(""pow grad imprecise on tpu"")
+    tol = 1e-1 if num_float_bits(dtype) == 32 else None
+    args = tuple(rng(shape, dtype) for shape in shapes)
+    check_grads(op, args, order, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.float64], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
+    args = (rng((2, 3), from_dtype),)
+    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
+    check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (), ()],
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [min_shape, operand_shape, max_shape]
+    min, operand, max = (rng(shape, dtype) for shape in shapes)
+    min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
+    check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in float_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    operands = tuple(rng(shape, dtype) for shape in shapes)
+    concatenate = lambda *args: lax.concatenate(args, dim)
+    check_grads(concatenate, operands, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""rng"": rng,}
+       for lhs_shape, rhs_shape, all_strides in itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)])])
+       for strides in all_strides
+       for dtype in [onp.float32]
+       for padding in [""VALID"", ""SAME""]
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv, window_strides=strides, padding=padding)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng}
+       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in
+       itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+             [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+             [(1, 1), (2, 1)], [(1, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)], [((1, 1),), ((0, 0),)],
+             [(1,), (2,)], [(1,), (2,)])])
+       for strides in all_strides
+       for rhs_dil in rhs_dils
+       for lhs_dil in lhs_dils
+       for dtype in [onp.float32]
+       for padding in all_pads
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                     padding, lhs_dil, rhs_dil, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv_with_general_padding, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}_dims={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil, "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
+       ""perms"": perms}
+      for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
+          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+          [(1, 1), (2, 1)], [(1, 1)])
+          for b, i, j in itertools.product([1, 2], repeat=3)]
+      for strides in all_strides
+      for rhs_dil in rhs_dils
+      for lhs_dil in lhs_dils
+      for dtype in [onp.float32]
+      for padding in all_pads
+      for rng in [jtu.rand_default()]
+      for dim_nums, perms in [
+          ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dil, rhs_dil, dimension_numbers,
+                                 perms, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-3
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+    lhs = onp.transpose(rng(lhs_shape, dtype), lhs_perm)
+    rhs = onp.transpose(rng(rhs_shape, dtype), rhs_perm)
+    conv = partial(lax.conv_general_dilated, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil,
+                   dimension_numbers=dimension_numbers)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
+      for dtype in float_dtypes)
+  def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
+    tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": jtu.rand_small()}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 2), (2, 4), (([1], [0]), ([], []))),
+          ((3, 5), (2, 5), (([1], [1]), ([], []))),
+          ((5, 3), (5, 2), (([0], [0]), ([], []))),
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+      ]
+      for dtype in float_dtypes)
+  @jtu.skip_on_devices(""tpu"")
+  def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
+                                          dimension_numbers, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-2
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
+    check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in float_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    args = (rng(shape, dtype),)
+    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
+    check_grads(broadcast, args, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(inshape, dtype)
+    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(arg_shape, dtype)
+    reshape = lambda x: lax.reshape(x, out_shape)
+    check_grads(reshape, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in float_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadGrad(self, shape, dtype, pads, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+
+    operand = rng(shape, dtype)
+    pad = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    check_grads(pad, (operand,), 2, tol, tol, tol)
+
+    operand = rng(shape, dtype)
+    padding_value = onp.array(0., dtype)
+    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)
+    check_grads(pad, (operand, padding_value), 2, tol, tol, tol)
+
+  def testReverseGrad(self):
+    rev = lambda operand: lax.rev(operand, dimensions)
+
+    dimensions = [0]
+    check_grads(rev, (onp.array([3., 2., 1.]),), 2)
+
+    dimensions = [0, 1]
+    check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    pred = rng(pred_shape, onp.bool_)
+    on_true = rng(arg_shape, dtype)
+    on_false = rng(arg_shape, dtype)
+    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
+    check_grads(select, (on_true, on_false), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    slice = lambda x: lax.slice(x, starts, limits, strides)
+    check_grads(slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
+                           rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
+    check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
+                                 update_shape, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    update = rng(update_shape, dtype)
+    start_indices = onp.array(start_indices)
+    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
+    check_grads(dus, (operand, update), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeGrad(self, shape, dtype, perm, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    transpose = lambda x: lax.transpose(x, perm)
+    check_grads(transpose, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, float_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.inf, lax.min, float_dtypes),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)],
+          [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)],
+          [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    init_val = onp.asarray(init_val, dtype=dtype)
+    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
+    check_grads(reduce, (operand,), 1, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes, rng in [
+          (0, lax.add, [onp.float32], jtu.rand_small()),
+          (-onp.inf, lax.max, [onp.float32], jtu.rand_default()),
+          (onp.inf, lax.min, [onp.float32], jtu.rand_default()),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_default()])
+  def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((6, 5, 4, 3), (2, 2, 1, 1), (1, 2, 1, 1))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],  # shapes
+              [(2, 1), (1, 2)],  # window_dimensions
+              [(1, 1), (2, 1), (1, 2)]  # strides
+          ),
+          itertools.product(
+              [(3, 2, 4, 6)],  # shapes
+              [(1, 1, 2, 1), (2, 1, 2, 1)],  # window_dimensions
+              [(1, 2, 2, 1), (1, 1, 1, 1)]),  # strides
+      )
+
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      operand = rng(shape, dtype)
+      if op is lax.add:
+        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
+      else:
+        # this test can fail if there are duplicates in operand
+        self.assertEqual(onp.unique(operand).size, operand.size,
+                         msg=""test requires operand elements to be unique."")
+        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
+                            1e-2, 1e-2, 1e-2)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32]
+      for shape in [(5,), (5, 7)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortGrad(self, shape, dtype, axis, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    sort = lambda x: lax.sort(x, axis)
+    check_grads(sort, (operand,), 2, tol, tol, tol)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32]
+      for val_dtype in [onp.float32]
+      for shape in [(3,), (5, 3)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+    keys, values = args_maker()
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in float_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(shape, dtype)
+    index_take = lambda src: lax.index_take(src, idxs, axes)
+    check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(src_shape, dtype)
+    dst = rng(dst_shape, dtype)
+    index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
+    check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
+
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/lax_test.py b/tests/lax_test.py
new file mode 100644
index 000000000..b8c78524c
--- /dev/null
+++ b/tests/lax_test.py
@@ -0,0 +1,1981 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+from functools import partial
+import itertools
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import numpy.random as npr
+
+from jax import api
+from jax import core
+from jax import lax
+from jax import test_util as jtu
+from jax import lax_reference
+from jax.interpreters import xla
+from jax.lib import xla_bridge
+
+FLAGS = flags.FLAGS
+
+
+def num_float_bits(dtype):
+  return onp.finfo(xla_bridge.canonicalize_dtype(dtype)).bits
+
+
+### lax tests
+
+# For standard unops and binops, we can generate a large number of tests on
+# arguments of appropriate shapes and dtypes using the following table.
+
+float_dtypes = [onp.float32, onp.float64]
+complex_dtypes = [onp.complex64]
+int_dtypes = [onp.int32, onp.int64]
+bool_dtypes = [onp.bool_]
+default_dtypes = float_dtypes + int_dtypes
+all_dtypes = float_dtypes + complex_dtypes + int_dtypes + bool_dtypes
+
+compatible_shapes = [[(3,)], [(3, 4), (3, 1), (1, 4)], [(2, 3, 4), (2, 1, 4)]]
+
+OpRecord = collections.namedtuple(""OpRecord"",
+                                  [""op"", ""nargs"", ""dtypes"", ""rng"", ""tol""])
+
+
+def op_record(op, nargs, dtypes, rng, tol=1e-5):
+  return OpRecord(op, nargs, dtypes, rng, tol)
+
+LAX_OPS = [
+    op_record(lax.neg, 1, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sign, 1, default_dtypes, jtu.rand_small()),
+    op_record(lax.floor, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.ceil, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.round, 1, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.is_finite, 1, float_dtypes, jtu.rand_small()),
+
+    op_record(lax.exp, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.expm1, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.log, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.log1p, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tanh, 1, float_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sin, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cos, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.atan2, 2, float_dtypes, jtu.rand_default()),
+
+    op_record(lax.sqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.rsqrt, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.square, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.reciprocal, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.tan, 1, float_dtypes, jtu.rand_default()),
+    op_record(lax.asin, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.acos, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.atan, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.sinh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.cosh, 1, float_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.asinh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+    op_record(lax.acosh, 1, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.lgamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.digamma, 1, float_dtypes, jtu.rand_positive()),
+    op_record(lax.erf, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erfc, 1, float_dtypes, jtu.rand_small()),
+    op_record(lax.erf_inv, 1, float_dtypes, jtu.rand_small(), tol=1e-2),
+
+    op_record(lax.real, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.imag, 1, complex_dtypes, jtu.rand_default()),
+    op_record(lax.complex, 2, [onp.float32], jtu.rand_default()),
+    op_record(lax.conj, 1, [onp.float32] + complex_dtypes, jtu.rand_default()),
+    op_record(lax.abs, 1, default_dtypes + complex_dtypes, jtu.rand_default()),
+    op_record(lax.pow, 2, float_dtypes + complex_dtypes, jtu.rand_positive()),
+
+    op_record(lax.bitwise_and, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_not, 1, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_or, 2, bool_dtypes, jtu.rand_small()),
+    op_record(lax.bitwise_xor, 2, bool_dtypes, jtu.rand_small()),
+
+    op_record(lax.add, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.sub, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.mul, 2, default_dtypes + complex_dtypes, jtu.rand_small()),
+    op_record(lax.div, 2, default_dtypes + complex_dtypes, jtu.rand_nonzero()),
+    op_record(lax.rem, 2, default_dtypes, jtu.rand_nonzero()),
+
+    op_record(lax.max, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.min, 2, default_dtypes, jtu.rand_small()),
+
+    op_record(lax.eq, 2, all_dtypes, jtu.rand_some_equal()),
+    op_record(lax.ne, 2, all_dtypes, jtu.rand_small()),
+    op_record(lax.ge, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.gt, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.le, 2, default_dtypes, jtu.rand_small()),
+    op_record(lax.lt, 2, default_dtypes, jtu.rand_small()),
+]
+
+CombosWithReplacement = itertools.combinations_with_replacement
+
+
+class LaxTest(jtu.JaxTestCase):
+  """"""Numerical tests for LAX operations.""""""
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOp(self, op, rng, shapes, dtype):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""tol"": rec.tol}
+      for rec in LAX_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes)
+  def testOpAgainstNumpy(self, op, rng, shapes, dtype, tol):
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    numpy_op = getattr(lax_reference, op.__name__)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker, tol=tol)
+
+  # TODO test shift_left, shift_right_arithmetic, shift_right_logical
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.convert_element_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.convert_element_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertType(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}""
+       .format(from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.int32, ""float32"", ""int32""], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testBitcastConvertTypeAgainstNumpy(self, from_dtype, to_dtype, rng):
+    args_maker = lambda: [rng((2, 3), from_dtype)]
+    op = lambda x: lax.bitcast_convert_type(x, to_dtype)
+    numpy_op = lambda x: lax_reference.bitcast_convert_type(x, to_dtype)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClamp(self, min_shape, operand_shape, max_shape, dtype, rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CompileAndCheck(lax.clamp, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), ()],
+          [(), (2, 3), (2, 3)],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampAgainstNumpy(self, min_shape, operand_shape, max_shape, dtype,
+                            rng):
+    shapes = [min_shape, operand_shape, max_shape]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    self._CheckAgainstNumpy(lax.clamp, lax_reference.clamp, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenate(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in default_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateAgainstNumpy(self, dim, base_shape, dtype, num_arrs, rng):
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    args_maker = lambda: [rng(shape, dtype) for shape in shapes]
+    op = lambda *args: lax.concatenate(args, dim)
+    numpy_op = lambda *args: lax_reference.concatenate(args, dim)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConv(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv(lhs, rhs, strides, padding)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype), strides, padding),
+          ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+          ""strides"": strides, ""padding"": padding, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32]
+      for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testConvAgainstNumpy(self, lhs_shape, rhs_shape, dtype, strides, padding,
+                           rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda lhs, rhs: lax.conv(lhs, rhs, strides, padding)
+    numpy_op = lambda lhs, rhs: lax_reference.conv(lhs, rhs, strides, padding)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def testConvWithGeneralPadding(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dilation, rhs_dilation, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([1, 2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (1, 2), (2, 1)]
+      for padding in [((0, 0), (0, 0)), ((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2), (2, 2)], repeat=2)
+      for rng in [jtu.rand_small()])
+  def DISABLED_testConvWithGeneralPaddingAgainstNumpy(
+      self, lhs_shape, rhs_shape, dtype, strides, padding, lhs_dilation,
+      rhs_dilation, rng):
+    # TODO(mattjj): make this test pass
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    def numpy_fun(lhs, rhs):
+      return lax_reference.conv_with_general_padding(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation)
+
+    self._CheckAgainstNumpy(fun, numpy_fun, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       ""_lhs_dilation={}_rhs_dilation={}""
+       ""_dims={}"".format(
+           jtu.format_shape_dtype_string(lhs_shape, dtype),
+           jtu.format_shape_dtype_string(rhs_shape, dtype),
+           strides, padding, lhs_dilation, rhs_dilation,
+           "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dilation"": lhs_dilation,
+       ""rhs_dilation"": rhs_dilation, ""dimension_numbers"": dim_nums,
+       ""perms"": perms, ""rng"": rng}
+      for lhs_shape, rhs_shape in [
+          ((b, i, 9, 10), (j, i, 4, 5))
+          for b, i, j in itertools.product([2, 3], repeat=3)]
+      for dtype in [onp.float32] for strides in [(1, 1), (2, 1)]
+      for padding in [((1, 2), (2, 0))]
+      for lhs_dilation, rhs_dilation in itertools.product(
+          [(1, 1), (1, 2)], repeat=2)
+      for rng in [jtu.rand_small()]
+      for dim_nums, perms in [((""NCHW"", ""OIHW"", ""NCHW""),
+                              ([0, 1, 2, 3], [0, 1, 2, 3])),
+                             ((""NHWC"", ""HWIO"", ""NHWC""),
+                              ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  def testConvGeneralDilated(self, lhs_shape, rhs_shape, dtype, strides,
+                             padding, lhs_dilation, rhs_dilation,
+                             dimension_numbers, perms, rng):
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+
+    def args_maker():
+      return [lax.transpose(rng(lhs_shape, dtype), lhs_perm),
+              lax.transpose(rng(rhs_shape, dtype), rhs_perm)]
+
+    def fun(lhs, rhs):
+      return lax.conv_general_dilated(
+          lhs, rhs, strides, padding, lhs_dilation, rhs_dilation,
+          dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(mattjj): test conv_general_dilated against numpy
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDot(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.dot, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape in [(3,), (4, 3)] for rhs_shape in [(3,), (3, 6)]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDotAgainstNumpy(self, lhs_shape, rhs_shape, dtype, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CheckAgainstNumpy(lax.dot, lax_reference.dot, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_lhs_contracting={}_rhs_contracting={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               lhs_contracting, rhs_contracting),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""lhs_contracting"": lhs_contracting, ""rhs_contracting"": rhs_contracting,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape, lhs_contracting, rhs_contracting in [
+          # these all fail with ""RuntimeError: Unimplemented: Dot with
+          # non-standard contracting dimensions not implemented.""
+          # [(3, 5), (2, 5), [1], [1]],
+          # [(5, 3), (5, 2), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0], [0]],
+          # [(5, 3, 2), (5, 2, 4), [0,2], [0,1]],
+          # [(1, 2, 2, 3), (1, 2, 3, 1), [1], [1]],
+          [(3, 2), (2, 4), [1], [0]],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractOnly(self, lhs_shape, rhs_shape, dtype,
+                                 lhs_contracting, rhs_contracting, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    dimension_numbers = ((lhs_contracting, rhs_contracting), ([], []))
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralContractAndBatch(self, lhs_shape, rhs_shape, dtype,
+                                     dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+
+    def fun(lhs, rhs):
+      return lax.dot_general(lhs, rhs, dimension_numbers)
+
+    self._CompileAndCheck(fun, args_maker, check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": rng}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+          ((3, 4, 2, 4), (3, 4, 3, 2), (([2], [3]), ([0, 1], [0, 1]))),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testDotGeneralAgainstNumpy(self, lhs_shape, rhs_shape, dtype,
+                                 dimension_numbers, rng):
+    args_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    op = lambda x, y: lax.dot_general(x, y, dimension_numbers)
+    numpy_op = lambda x, y: lax_reference.dot_general(x, y, dimension_numbers)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcast(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_broadcast_sizes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in default_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastAgainstNumpy(self, shape, dtype, broadcast_sizes, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.broadcast(x, broadcast_sizes)
+    numpy_op = lambda x: lax_reference.broadcast(x, broadcast_sizes)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDim(self, inshape, dtype, outshape, dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimAgainstNumpy(self, inshape, dtype, outshape,
+                                     dimensions, rng):
+    args_maker = lambda: [rng(inshape, dtype)]
+    op = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    numpy_op = lambda x: lax_reference.broadcast_in_dim(x, outshape, dimensions)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshape(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeAgainstNumpy(self, arg_shape, out_shape, dtype, rng):
+    args_maker = lambda: [rng(arg_shape, dtype)]
+    op = lambda x: lax.reshape(x, out_shape)
+    numpy_op = lambda x: lax_reference.reshape(x, out_shape)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPad(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in default_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadAgainstNumpy(self, shape, dtype, pads, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.pad(x, onp.array(0, dtype), pads)
+    numpy_op = lambda x: lax_reference.pad(x, onp.array(0, dtype), pads)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testReverse(self):
+    rev = api.jit(lambda operand: lax.rev(operand, dimensions))
+
+    dimensions = [0]
+    self.assertAllClose(onp.array([3, 2, 1]), rev(onp.array([1, 2, 3])),
+                        check_dtypes=False)
+
+    dimensions = [0, 1]
+    self.assertAllClose(onp.array([[6, 5, 4], [3, 2, 1]]),
+                        rev(onp.array([[1, 2, 3], [4, 5, 6]])),
+                        check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelect(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CompileAndCheck(lax.select, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, arg_dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""arg_dtype"": arg_dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for arg_dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectAgainstNumpy(self, pred_shape, arg_shape, arg_dtype, rng):
+
+    def args_maker():
+      return [rng(pred_shape, onp.bool_), rng(arg_shape, arg_dtype),
+              rng(arg_shape, arg_dtype)]
+
+    return self._CheckAgainstNumpy(lax.select, lax_reference.select, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSlice(self, shape, dtype, starts, limits, strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceAgainstNumpy(self, shape, dtype, starts, limits,
+                            strides, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.slice(x, starts, limits, strides)
+    numpy_op = lambda x: lax_reference.slice(x, starts, limits, strides)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSlice(self, shape, dtype, start_indices, size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, starts: lax.dynamic_slice(x, starts, size_indices)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceAgainstNumpy(self, shape, dtype, start_indices,
+                                   size_indices, rng):
+    args_maker = lambda: [rng(shape, dtype), onp.array(start_indices)]
+    op = lambda x, s: lax.dynamic_slice(x, s, size_indices)
+    numpy_op = lambda x, s: lax_reference.dynamic_slice(x, s, size_indices)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CompileAndCheck(lax.dynamic_update_slice, args_maker,
+                          check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSlice(self, shape, dtype, start_indices, update_shape,
+                             rng):
+
+    def args_maker():
+      return [rng(shape, dtype), rng(update_shape, dtype),
+              onp.array(start_indices)]
+
+    self._CheckAgainstNumpy(lax.dynamic_update_slice,
+                            lax_reference.dynamic_update_slice, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTranspose(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    self._CompileAndCheck(op, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in default_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeAgainstNumpy(self, shape, dtype, perm, rng):
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.transpose(x, perm)
+    numpy_op = lambda x: lax_reference.transpose(x, perm)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, default_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.iinfo(onp.int32).min, lax.max, [onp.int32]),
+          (onp.iinfo(onp.int64).min, lax.max, [onp.int64]),
+          (onp.iinfo(onp.uint32).min, lax.max, [onp.uint32]),
+          (onp.iinfo(onp.uint64).min, lax.max, [onp.uint64]),
+          (onp.inf, lax.min, float_dtypes),
+          (onp.iinfo(onp.int32).max, lax.min, [onp.int32]),
+          (onp.iinfo(onp.int64).max, lax.min, [onp.int64]),
+          (onp.iinfo(onp.uint32).max, lax.min, [onp.uint32]),
+          (onp.iinfo(onp.uint64).max, lax.min, [onp.uint64]),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduce(self, op, init_val, shape, dtype, dims, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+    fun = lambda operand, init_val: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype), init_val]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    fun = lambda operand: lax.reduce(operand, init_val, op, dims)
+    args_maker = lambda: [rng(shape, dtype)]
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, [onp.float32]),
+          (-onp.inf, lax.max, [onp.float32]),
+          (onp.inf, lax.min, [onp.float32]),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_small()])
+  def testReduceWindow(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((4, 6), (2, 1), (1, 2))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],
+              [(2, 1), (1, 2)],
+              [(1, 1), (2, 1), (1, 2)]),
+          itertools.product(
+              [(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)],
+              [(1, 2, 2, 1), (1, 1, 1, 1)]))
+
+    def fun(operand, init_val):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype), init_val]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+    # we separately test the version that uses a concrete init_val because it
+    # can hit different code paths
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      args_maker = lambda: [rng(shape, dtype)]
+      self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSort(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    fun = lambda x: lax.sort(x, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(5,), (5, 7)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortAgainstNumpy(self, shape, dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort only implemented for R1 on non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    args_maker = lambda: [rng(shape, dtype)]
+    op = lambda x: lax.sort(x, axis)
+    numpy_op = lambda x: lax_reference.sort(x, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyVal(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32, onp.int32, onp.uint32]
+      for val_dtype in [onp.float32, onp.int32, onp.uint32]
+      for shape in [(3,), (5, 3)]
+      for axis in [-1, len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValAgainstNumpy(self, shape, key_dtype, val_dtype, axis, rng):
+    if len(shape) > 1 and not FLAGS.jax_test_dut.startswith(""tpu""):
+      msg = ""sort_key_val only implemented for R1 non-TPU backends""
+      return absltest.unittest.skip(msg)
+
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+
+    op = lambda ks, vs: lax.sort_key_val(ks, vs, axis)
+    numpy_op = lambda ks, vs: lax_reference.sort_key_val(ks, vs, axis)
+    self._CheckAgainstNumpy(op, numpy_op, args_maker)
+
+  def testWhileWithTuple(self):
+    limit = 10
+
+    def loop_cond(state):
+      pos, _ = state
+      return lax.lt(pos, limit)
+
+    def loop_body(state):
+      pos, count = state
+      return (lax.add(pos, 1), lax.add(count, 1))
+
+    def loop(init):
+      result = lax._while_loop(loop_cond, loop_body, (init, 0))
+      _, count = result
+      return count
+
+    cloop = api.jit(loop)
+
+    self.assertEqual(loop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(2), limit - 2)
+    self.assertEqual(cloop(3), limit - 3)
+
+  def testNestedWhile(self):
+
+    def outer_loop(num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        num, i, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        num, i, count = state
+        return (num, lax.add(i, 1), inner_loop(i, count))
+
+      init_val = (num, 0, 0)
+      _, i, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return (i, count)
+
+    def inner_loop(i, count):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        i, j, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, count = state
+        return (i, lax.add(j, 1), lax.add(count, 1))
+
+      init_val = (i, 0, count)
+      _, _, count = lax._while_loop(cond_fun, body_fun, init_val)
+      return count
+
+    cloop = api.jit(outer_loop)
+
+    self.assertEqual(outer_loop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(3), (3, 6))
+    self.assertEqual(cloop(2), (2, 3))
+    self.assertEqual(cloop(4), (4, 10))
+
+  def testNestedWhileWithDynamicUpdateSlice(self):
+    num = 5
+
+    def update_entry(arr, val, i, j):
+      val = lax.reshape(val, [1, 1])
+      return lax.dynamic_update_slice(arr, val, (i, j))
+
+    def outer_loop(arr):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, num, _, _ = state
+        return lax.lt(i, num)
+
+      def body_fun(state):
+        i, num, arr, out = state
+        return (lax.add(i, 1), num, arr, inner_loop(i, arr, out))
+
+      out = onp.zeros(arr.shape, dtype=arr.dtype)
+      init_val = (0, num, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    def inner_loop(i, arr, out):  # pylint: disable=missing-docstring
+
+      def cond_fun(state):
+        i, j, _, _ = state
+        return lax.le(j, i)
+
+      def body_fun(state):
+        i, j, arr, out = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        arr_i_j = lax.dynamic_index_in_dim(arr_i, j, 0, False)
+        out = update_entry(out, arr_i_j, i, j)
+        return (i, lax.add(j, 1), arr, out)
+
+      init_val = (i, 0, arr, out)
+      _, _, _, out = lax._while_loop(cond_fun, body_fun, init_val)
+      return out
+
+    cloop = api.jit(outer_loop)
+    arr = npr.RandomState(0).randn(5, 5)
+    self.assertAllClose(outer_loop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+    self.assertAllClose(cloop(arr), onp.tril(arr), check_dtypes=False)
+
+  def testLoopWithConjunctionCondition(self):
+    def sum_first_n(arr, num):  # pylint: disable=missing-docstring
+      def cond_fun(state):
+        arr, num, i, _ = state
+        return lax.bitwise_and(lax.lt(i, num), lax.lt(i, arr.shape[0]))
+
+      def body_fun(state):
+        arr, num, i, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, num, lax.add(i, 1), lax.add(total, arr_i))
+
+      init_val = (arr, num, 0, 0.)
+      _, _, _, total = lax._while_loop(cond_fun, body_fun, init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopBasic(self):
+    def count(num):
+      def body_fun(i, tot):
+        return lax.add(tot, i)
+      return lax.fori_loop(0, num, body_fun, 0)
+
+    cfun = api.jit(count)
+
+    self.assertEqual(count(2), 1)
+    self.assertEqual(count(2), cfun(2))
+    self.assertEqual(count(3), 3)
+    self.assertEqual(count(3), cfun(3))
+    self.assertEqual(count(4), 6)
+    self.assertEqual(count(4), cfun(4))
+
+  def testForiLoopTupleState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i))
+
+      init_val = (arr, 0.)
+      _, total = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun,
+                               init_val)
+      return total
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopDictState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total = state['arr'], state['total']
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return {'arr': arr, 'total': lax.add(total, arr_i)}
+
+      init_val = {'arr': arr, 'total': 0.}
+      out_val = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return out_val['total']
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  def testForiLoopEmptyTupleInState(self):
+    def sum_first_n(arr, num):
+      def body_fun(i, state):
+        arr, total, _ = state
+        arr_i = lax.dynamic_index_in_dim(arr, i, 0, False)
+        return (arr, lax.add(total, arr_i), ())
+
+      init_val = (arr, 0., ())
+      _, tot, _ = lax.fori_loop(0, lax.min(arr.shape[0], num), body_fun, init_val)
+      return tot
+
+    cfun = api.jit(sum_first_n)
+    x = npr.RandomState(0).randn(10)
+
+    for num in [0, 5, 10, 15]:
+      self.assertAllClose(sum_first_n(x, num), onp.sum(x[:num]),
+                          check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+      self.assertAllClose(cfun(x, num), onp.sum(x[:num]), check_dtypes=False)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for lhs_shape, rhs_shape in [((3, 2), (2, 4)),
+                                   ((5, 3, 2), (5, 2, 4)),
+                                   ((1, 2, 2, 3), (1, 2, 3, 1))]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_small()])
+  def testBatchMatMul(self, lhs_shape, rhs_shape, dtype, rng):
+    arg_maker = lambda: [rng(lhs_shape, dtype), rng(rhs_shape, dtype)]
+    self._CompileAndCheck(lax.batch_matmul, arg_maker, check_dtypes=True)
+
+  def testCollapse(self):
+
+    @api.jit
+    def collapse_first_two(x):
+      return lax.collapse(x, 0, 2)
+
+    self.assertEqual((6,), collapse_first_two(onp.zeros((2, 3))).shape)
+    self.assertEqual((6, 4), collapse_first_two(onp.zeros((2, 3, 4))).shape)
+    self.assertEqual((2, 3, 4),
+                     collapse_first_two(onp.zeros((1, 2, 3, 4))).shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in all_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTake(self, shape, dtype, idxs, axes, rng):
+    rand_idxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(shape, dtype), rand_idxs()]
+    fun = lambda src, idxs: lax.index_take(src, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in default_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntake(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+    ridxs = lambda: tuple(rng(e.shape, e.dtype) for e in idxs)
+    args_maker = lambda: [rng(src_shape, dtype), rng(dst_shape, dtype), ridxs()]
+    fun = lambda src, dst, idxs: lax.index_untake(src, dst, idxs, axes)
+    self._CompileAndCheck(fun, args_maker, check_dtypes=True)
+
+
+GradTestSpec = collections.namedtuple(
+    ""GradTestSpec"", [""op"", ""nargs"", ""order"", ""rng"", ""dtypes""])
+
+LAX_GRAD_OPS = [
+    GradTestSpec(lax.neg, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.floor, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.ceil, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.round, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.rem, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.exp, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.expm1, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.log1p, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.tanh, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sin, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.cos, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64]),
+
+    GradTestSpec(lax.erf, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erfc, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.erf_inv, nargs=1, order=2, rng=jtu.rand_small(),
+                 dtypes=[onp.float64]),
+    # GradTestSpec(lax.lgamma, nargs=1, order=2, rng=jtu.rand_small(),
+    #              dtypes=[onp.float64]),  # TODO(mattjj): enable
+
+    GradTestSpec(lax.real, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    GradTestSpec(lax.imag, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.complex64]),
+    # GradTestSpec(lax.complex, nargs=2, order=2, rng=jtu.rand_default(),
+    #              dtypes=[onp.float32]),  # TODO(mattjj): enable
+    GradTestSpec(lax.conj, nargs=1, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float32, onp.complex64]),
+    GradTestSpec(lax.abs, nargs=1, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.pow, nargs=2, order=2, rng=jtu.rand_positive(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.add, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.sub, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.mul, nargs=2, order=2, rng=jtu.rand_default(),
+                 dtypes=[onp.float64, onp.complex64]),
+    GradTestSpec(lax.div, nargs=2, order=1, rng=jtu.rand_not_small(),
+                 dtypes=[onp.float64, onp.complex64]),
+
+    GradTestSpec(lax.max, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+    GradTestSpec(lax.min, nargs=2, order=2, rng=jtu.rand_some_equal(),
+                 dtypes=[onp.float64]),
+]
+
+
+def check_grads(f, args, order, atol=None, rtol=None, eps=None):
+  # TODO(mattjj,dougalm): add higher-order check
+  default_tol = 1e-6 if FLAGS.jax_enable_x64 else 1e-2
+  atol = atol or default_tol
+  rtol = rtol or default_tol
+  eps = eps or default_tol
+  jtu.check_jvp(f, partial(api.jvp, f), args, atol, rtol, eps)
+  jtu.check_vjp(f, partial(api.vjp, f), args, atol, rtol, eps)
+
+
+def check_grads_bilinear(f, args, order, atol=None, rtol=None):
+  # Can use large eps to make up for numerical inaccuracies since the op is
+  # bilinear (relying on the fact that we only check one arg at a time)
+  lhs, rhs = args
+  check_grads(lambda lhs: f(lhs, rhs), (lhs,), order, atol, rtol, eps=1.)
+  check_grads(lambda rhs: f(lhs, rhs), (rhs,), order, atol, rtol, eps=1.)
+
+
+class LaxAutodiffTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          rec.op.__name__, shapes, itertools.repeat(dtype)),
+       ""op"": rec.op, ""rng"": rec.rng, ""shapes"": shapes, ""dtype"": dtype,
+       ""order"": rec.order}
+      for rec in LAX_GRAD_OPS
+      for shape_group in compatible_shapes
+      for shapes in CombosWithReplacement(shape_group, rec.nargs)
+      for dtype in rec.dtypes
+  )
+  def testOpGrad(self, op, rng, shapes, dtype, order):
+    if FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu""):
+      if dtype is onp.complex64:
+        return absltest.unittest.skip(""complex grads unimplemented on tpu"")
+      if op is lax.pow:
+        return absltest.unittest.skip(""pow grad imprecise on tpu"")
+    tol = 1e-1 if num_float_bits(dtype) == 32 else None
+    args = tuple(rng(shape, dtype) for shape in shapes)
+    check_grads(op, args, order, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_from_dtype={}_to_dtype={}"".format(
+          from_dtype, to_dtype),
+       ""from_dtype"": from_dtype, ""to_dtype"": to_dtype, ""rng"": rng}
+      for from_dtype, to_dtype in itertools.product(
+          [onp.float32, onp.float64], repeat=2)
+      for rng in [jtu.rand_default()])
+  def testConvertElementTypeGrad(self, from_dtype, to_dtype, rng):
+    args = (rng((2, 3), from_dtype),)
+    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)
+    check_grads(convert_element_type, args, 1, 1e-3, 1e-3, 1e-3)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_min_shape={}_operand_shape={}_max_shape={}"".format(
+          jtu.format_shape_dtype_string(min_shape, dtype),
+          jtu.format_shape_dtype_string(operand_shape, dtype),
+          jtu.format_shape_dtype_string(max_shape, dtype)),
+       ""min_shape"": min_shape, ""operand_shape"": operand_shape,
+       ""max_shape"": max_shape, ""dtype"": dtype, ""rng"": rng}
+      for min_shape, operand_shape, max_shape in [
+          [(), (), ()],
+          [(), (2, 3), ()],
+          [(2, 3), (2, 3), (2, 3)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testClampGrad(self, min_shape, operand_shape, max_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [min_shape, operand_shape, max_shape]
+    min, operand, max = (rng(shape, dtype) for shape in shapes)
+    min, max = onp.minimum(min, max), onp.maximum(min, max)  # broadcast
+    check_grads(lax.clamp, (min, operand, max), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dim={}_baseshape=[{}]_dtype={}_narrs={}"".format(
+          dim, "","".join(str(d) for d in base_shape), onp.dtype(dtype).name,
+          num_arrs),
+       ""dim"": dim, ""base_shape"": base_shape, ""dtype"": dtype,
+       ""num_arrs"": num_arrs, ""rng"": rng}
+      for num_arrs in [3]
+      for dtype in float_dtypes
+      for base_shape in [(4,), (3, 4), (2, 3, 4)]
+      for dim in range(len(base_shape))
+      for rng in [jtu.rand_default()])
+  def testConcatenateGrad(self, dim, base_shape, dtype, num_arrs, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    shapes = [base_shape[:dim] + (size,) + base_shape[dim+1:]
+              for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]
+    operands = tuple(rng(shape, dtype) for shape in shapes)
+    concatenate = lambda *args: lax.concatenate(args, dim)
+    check_grads(concatenate, operands, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""rng"": rng,}
+       for lhs_shape, rhs_shape, all_strides in itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)])])
+       for strides in all_strides
+       for dtype in [onp.float32]
+       for padding in [""VALID"", ""SAME""]
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGrad(self, lhs_shape, rhs_shape, dtype, strides, padding, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv, window_strides=strides, padding=padding)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng}
+       for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in
+       itertools.chain(
+           [((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+             [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+             [(1, 1), (2, 1)], [(1, 1)])
+            for b, i, j in itertools.product([2, 3], repeat=3)],
+           [((4, 2, 1), (3, 2, 1), [(1,)], [((1, 1),), ((0, 0),)],
+             [(1,), (2,)], [(1,), (2,)])])
+       for strides in all_strides
+       for rhs_dil in rhs_dils
+       for lhs_dil in lhs_dils
+       for dtype in [onp.float32]
+       for padding in all_pads
+       for rng in [jtu.rand_small()])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvWithGeneralPaddingGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                     padding, lhs_dil, rhs_dil, rng):
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    conv = partial(lax.conv_with_general_padding, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=1e-2, rtol=1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_strides={}_padding={}_lhs_dilation={}_""
+       ""rhs_dilation={}_dims={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               strides, padding, lhs_dil, rhs_dil, "","".join(dim_nums)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""strides"": strides, ""padding"": padding, ""lhs_dil"": lhs_dil,
+       ""rhs_dil"": rhs_dil, ""rng"": rng, ""dimension_numbers"": dim_nums,
+       ""perms"": perms}
+      for lhs_shape, rhs_shape, all_strides, all_pads, lhs_dils, rhs_dils in [
+          ((b, i, 3, 4), (j, i, 1, 2), [(1, 1), (1, 2), (2, 1)],
+          [((0, 0), (0, 0)), ((-1, 0), (0, -1)), ((1, 0), (0, 1))],
+          [(1, 1), (2, 1)], [(1, 1)])
+          for b, i, j in itertools.product([1, 2], repeat=3)]
+      for strides in all_strides
+      for rhs_dil in rhs_dils
+      for lhs_dil in lhs_dils
+      for dtype in [onp.float32]
+      for padding in all_pads
+      for rng in [jtu.rand_default()]
+      for dim_nums, perms in [
+          ((""NCHW"", ""OIHW"", ""NCHW""), ([0, 1, 2, 3], [0, 1, 2, 3])),
+          ((""NHWC"", ""HWIO"", ""NHWC""), ([0, 2, 3, 1], [2, 3, 1, 0]))])
+  @jtu.skip_on_devices(""tpu"")
+  def testConvGeneralDilatedGrad(self, lhs_shape, rhs_shape, dtype, strides,
+                                 padding, lhs_dil, rhs_dil, dimension_numbers,
+                                 perms, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-3
+    lhs_perm, rhs_perm = perms  # permute to compatible shapes
+    lhs = onp.transpose(rng(lhs_shape, dtype), lhs_perm)
+    rhs = onp.transpose(rng(rhs_shape, dtype), rhs_perm)
+    conv = partial(lax.conv_general_dilated, window_strides=strides,
+                   padding=padding, lhs_dilation=lhs_dil, rhs_dilation=rhs_dil,
+                   dimension_numbers=dimension_numbers)
+    check_grads_bilinear(conv, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_lhs_shape={}_rhs_shape={}"".format(
+          jtu.format_shape_dtype_string(lhs_shape, dtype),
+          jtu.format_shape_dtype_string(rhs_shape, dtype)),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""rng"": jtu.rand_default()}
+      for lhs_shape in [(2,), (3, 2)] for rhs_shape in [(2,), (2, 4)]
+      for dtype in float_dtypes)
+  def testDotGrad(self, lhs_shape, rhs_shape, dtype, rng):
+    tol = 1e-1 if num_float_bits(dtype) == 32 else 1e-3
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    check_grads_bilinear(lax.dot, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_lhs_shape={}_rhs_shape={}_dimension_numbers={}""
+       .format(jtu.format_shape_dtype_string(lhs_shape, dtype),
+               jtu.format_shape_dtype_string(rhs_shape, dtype),
+               dimension_numbers),
+       ""lhs_shape"": lhs_shape, ""rhs_shape"": rhs_shape, ""dtype"": dtype,
+       ""dimension_numbers"": dimension_numbers, ""rng"": jtu.rand_small()}
+      for lhs_shape, rhs_shape, dimension_numbers in [
+          ((3, 2), (2, 4), (([1], [0]), ([], []))),
+          ((3, 5), (2, 5), (([1], [1]), ([], []))),
+          ((5, 3), (5, 2), (([0], [0]), ([], []))),
+          ((3, 3, 2), (3, 2, 4), (([2], [1]), ([0], [0]))),
+      ]
+      for dtype in float_dtypes)
+  @jtu.skip_on_devices(""tpu"")
+  def testDotGeneralContractAndBatchGrads(self, lhs_shape, rhs_shape, dtype,
+                                          dimension_numbers, rng):
+    tol = 1e-1 if onp.finfo(dtype).bits == 32 else 1e-2
+    lhs = rng(lhs_shape, dtype)
+    rhs = rng(rhs_shape, dtype)
+    dot_general = partial(lax.dot_general, dimension_numbers=dimension_numbers)
+    check_grads_bilinear(dot_general, (lhs, rhs), order=2, atol=tol, rtol=tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_dtype={}_broadcast_sizes={}"".format(
+          shape, onp.dtype(dtype).name, broadcast_sizes),
+       ""shape"": shape, ""dtype"": dtype, ""broadcast_sizes"": broadcast_sizes,
+       ""rng"": rng}
+      for shape in [(), (2, 3)]
+      for dtype in float_dtypes
+      for broadcast_sizes in [(), (2,), (1, 2)]
+      for rng in [jtu.rand_default()])
+  def testBroadcastGrad(self, shape, dtype, broadcast_sizes, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    args = (rng(shape, dtype),)
+    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)
+    check_grads(broadcast, args, 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}_bcdims={}"".format(
+          jtu.format_shape_dtype_string(inshape, dtype),
+          outshape, broadcast_dimensions),
+       ""inshape"": inshape, ""dtype"": dtype, ""outshape"": outshape,
+       ""dimensions"": broadcast_dimensions, ""rng"": rng}
+      for inshape, outshape, broadcast_dimensions in [
+          ([2], [2, 2], [0]),
+          ([2], [2, 2], [1]),
+          ([2], [2, 3], [0]),
+          ([], [2, 3], []),
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(inshape, dtype)
+    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)
+    check_grads(broadcast_in_dim, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_outshape={}"".format(
+          jtu.format_shape_dtype_string(arg_shape, dtype),
+          jtu.format_shape_dtype_string(out_shape, dtype)),
+       ""arg_shape"": arg_shape, ""out_shape"": out_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for arg_shape, out_shape in [
+          [(3, 4), (12,)], [(2, 1, 4), (8,)], [(2, 2, 4), (2, 8)]
+      ]
+      for rng in [jtu.rand_default()])
+  def testReshapeGrad(self, arg_shape, out_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(arg_shape, dtype)
+    reshape = lambda x: lax.reshape(x, out_shape)
+    check_grads(reshape, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_inshape={}_pads={}""
+       .format(jtu.format_shape_dtype_string(shape, dtype), pads),
+       ""shape"": shape, ""dtype"": dtype, ""pads"": pads, ""rng"": jtu.rand_small()}
+      for shape in [(2, 3)]
+      for dtype in float_dtypes
+      for pads in [[(1, 2, 1), (0, 1, 0)]])
+  def testPadGrad(self, shape, dtype, pads, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+
+    operand = rng(shape, dtype)
+    pad = lambda operand: lax.pad(operand, onp.array(0, dtype), pads)
+    check_grads(pad, (operand,), 2, tol, tol, tol)
+
+    operand = rng(shape, dtype)
+    padding_value = onp.array(0., dtype)
+    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)
+    check_grads(pad, (operand, padding_value), 2, tol, tol, tol)
+
+  def testReverseGrad(self):
+    rev = lambda operand: lax.rev(operand, dimensions)
+
+    dimensions = [0]
+    check_grads(rev, (onp.array([3., 2., 1.]),), 2)
+
+    dimensions = [0, 1]
+    check_grads(rev, (onp.array([[6., 5., 4.], [3., 2., 1.]]),), 2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_predshape={}_argshapes={}"".format(
+          jtu.format_shape_dtype_string(pred_shape, onp.bool_),
+          jtu.format_shape_dtype_string(arg_shape, dtype)),
+       ""pred_shape"": pred_shape, ""arg_shape"": arg_shape, ""dtype"": dtype,
+       ""rng"": rng}
+      for arg_shape in [(), (3,), (2, 3)]
+      for pred_shape in ([(), arg_shape] if arg_shape else [()])
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSelectGrad(self, pred_shape, arg_shape, dtype, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    pred = rng(pred_shape, onp.bool_)
+    on_true = rng(arg_shape, dtype)
+    on_false = rng(arg_shape, dtype)
+    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)
+    check_grads(select, (on_true, on_false), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_shape={}_start_indices={}_limit_indices={}_strides={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, limit_indices, strides),
+       ""shape"": shape, ""dtype"": dtype, ""starts"": start_indices,
+       ""limits"": limit_indices, ""strides"": strides, ""rng"": rng}
+      for shape, start_indices, limit_indices, strides in [
+        [(3,), (1,), (2,), None],
+        [(7,), (4,), (7,), None],
+        [(5,), (1,), (5,), (2,)],
+        [(8,), (1,), (6,), (2,)],
+        [(5, 3), (1, 1), (3, 2), None],
+        [(5, 3), (1, 1), (3, 1), None],
+        [(7, 5, 3), (4, 0, 1), (7, 1, 3), None],
+        [(5, 3), (1, 1), (2, 1), (1, 1)],
+        [(5, 3), (1, 1), (5, 3), (2, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testSliceGrad(self, shape, dtype, starts, limits, strides, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    slice = lambda x: lax.slice(x, starts, limits, strides)
+    check_grads(slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_size_indices={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, size_indices),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""size_indices"": size_indices, ""rng"": rng}
+      for shape, start_indices, size_indices in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices,
+                           rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)
+    check_grads(dynamic_slice, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_start_indices={}_update_shape={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype),
+          start_indices, update_shape),
+       ""shape"": shape, ""dtype"": dtype, ""start_indices"": start_indices,
+       ""update_shape"": update_shape, ""rng"": rng}
+      for shape, start_indices, update_shape in [
+        [(3,), (1,), (1,)],
+        [(5, 3), (1, 1), (3, 1)],
+        [(7, 5, 3), (4, 1, 0), (2, 0, 1)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testDynamicUpdateSliceGrad(self, shape, dtype, start_indices,
+                                 update_shape, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    update = rng(update_shape, dtype)
+    start_indices = onp.array(start_indices)
+    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)
+    check_grads(dus, (operand, update), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_perm={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), perm),
+       ""shape"": shape, ""dtype"": dtype, ""perm"": perm, ""rng"": rng}
+      for shape, perm in [
+        [(3, 4), (1, 0)],
+        [(3, 4), (0, 1)],
+        [(3, 4, 5), (2, 1, 0)],
+        [(3, 4, 5), (1, 0, 2)],
+      ]
+      for dtype in float_dtypes
+      for rng in [jtu.rand_default()])
+  def testTransposeGrad(self, shape, dtype, perm, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    transpose = lambda x: lax.transpose(x, perm)
+    check_grads(transpose, (operand,), 2, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_inshape={}_reducedims={}""
+       .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), dims),
+       ""op"": op, ""init_val"": init_val, ""shape"": shape, ""dtype"": dtype,
+       ""dims"": dims, ""rng"": rng}
+      for init_val, op, dtypes in [
+          (0, lax.add, float_dtypes),
+          (-onp.inf, lax.max, float_dtypes),
+          (onp.inf, lax.min, float_dtypes),
+      ]
+      for dtype in dtypes
+      for shape, dims in [
+          [(3, 4, 5), (0,)],
+          [(3, 4, 5), (1, 2)],
+          [(3, 4, 5), (0, 2)],
+          [(3, 4, 5), (0, 1, 2)]
+      ]
+      for rng in [jtu.rand_small()])
+  def testReduceGrad(self, op, init_val, shape, dtype, dims, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    init_val = onp.asarray(init_val, dtype=dtype)
+    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)
+    check_grads(reduce, (operand,), 1, tol, tol, tol)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_op={}_dtype={}_padding={}""
+       .format(op.__name__, onp.dtype(dtype).name, padding),
+       ""op"": op, ""init_val"": init_val, ""dtype"": dtype, ""padding"": padding,
+       ""rng"": rng}
+      for init_val, op, dtypes, rng in [
+          (0, lax.add, [onp.float32], jtu.rand_small()),
+          (-onp.inf, lax.max, [onp.float32], jtu.rand_default()),
+          (onp.inf, lax.min, [onp.float32], jtu.rand_default()),
+      ]
+      for dtype in dtypes
+      for padding in [""VALID"", ""SAME""]
+      for rng in [jtu.rand_default()])
+  def testReduceWindowGrad(self, op, init_val, dtype, padding, rng):
+    init_val = onp.asarray(init_val, dtype=dtype)
+
+    # We need this conditional and the corresponding loop logic to be in the
+    # test method, rather than at the parameterized test level, because it
+    # depends on FLAGS for the device under test.
+    if FLAGS.jax_test_dut == ""tpu"":
+      all_configs = [((6, 5, 4, 3), (2, 2, 1, 1), (1, 2, 1, 1))]
+    else:
+      all_configs = itertools.chain(
+          itertools.product(
+              [(4, 6)],  # shapes
+              [(2, 1), (1, 2)],  # window_dimensions
+              [(1, 1), (2, 1), (1, 2)]  # strides
+          ),
+          itertools.product(
+              [(3, 2, 4, 6)],  # shapes
+              [(1, 1, 2, 1), (2, 1, 2, 1)],  # window_dimensions
+              [(1, 2, 2, 1), (1, 1, 1, 1)]),  # strides
+      )
+
+    def fun(operand):
+      return lax.reduce_window(operand, init_val, op, dims, strides, padding)
+
+    # pylint: disable=cell-var-from-loop
+    for shape, dims, strides in all_configs:
+      operand = rng(shape, dtype)
+      if op is lax.add:
+        check_grads(fun, (operand,), 1, 1e-2, 1e-2, 1e-2)
+      else:
+        # this test can fail if there are duplicates in operand
+        self.assertEqual(onp.unique(operand).size, operand.size,
+                         msg=""test requires operand elements to be unique."")
+        jtu.check_vjp(fun, partial(api.vjp, fun), (operand,),
+                            1e-2, 1e-2, 1e-2)
+    # pylint: enable=cell-var-from-loop
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), axis),
+       ""rng"": rng, ""shape"": shape, ""dtype"": dtype, ""axis"": axis}
+      for dtype in [onp.float32]
+      for shape in [(5,), (5, 7)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortGrad(self, shape, dtype, axis, rng):
+    tol = 1e-2 if onp.finfo(dtype).bits == 32 else None
+    operand = rng(shape, dtype)
+    sort = lambda x: lax.sort(x, axis)
+    check_grads(sort, (operand,), 2, tol, tol, tol)
+
+  # TODO(b/205052657): enable more tests when supported
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_keyshape={}_valshape={}_axis={}"".format(
+          jtu.format_shape_dtype_string(shape, key_dtype),
+          jtu.format_shape_dtype_string(shape, val_dtype),
+          axis),
+       ""rng"": rng, ""shape"": shape,
+       ""key_dtype"": key_dtype, ""val_dtype"": val_dtype, ""axis"": axis}
+      for key_dtype in [onp.float32]
+      for val_dtype in [onp.float32]
+      for shape in [(3,), (5, 3)]
+      for axis in [len(shape) - 1]
+      for rng in [jtu.rand_default()])
+  def testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, rng):
+    # This test relies on the property that wherever keys are tied, values are
+    # too, since we don't guarantee the same ordering of values with equal keys.
+    # To avoid that case, we generate unique keys (globally in the key array).
+    perm_rng = onp.random.RandomState(0)
+    def args_maker():
+      flat_keys = onp.arange(onp.prod(shape, dtype=int), dtype=key_dtype)
+      keys = perm_rng.permutation(flat_keys).reshape(shape)
+      values = rng(shape, val_dtype)
+      return keys, values
+    keys, values = args_maker()
+
+    fun = lambda keys, values: lax.sort_key_val(keys, values, axis)
+    check_grads(fun, (keys, values), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(shape, dtype), idxs, axes),
+       ""shape"": shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes, ""rng"": rng}
+      for dtype in float_dtypes
+      for shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexTakeGrad(self, shape, dtype, idxs, axes, rng):
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(shape, dtype)
+    index_take = lambda src: lax.index_take(src, idxs, axes)
+    check_grads(index_take, (src,), 2, 1e-2, 1e-2, 1e-2)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_dst_shape={}_idxs={}_axes={}"".format(
+          jtu.format_shape_dtype_string(dst_shape, dtype), idxs, axes),
+       ""dst_shape"": dst_shape, ""dtype"": dtype, ""idxs"": idxs, ""axes"": axes,
+       ""rng"": rng}
+      for dtype in float_dtypes
+      for dst_shape, idxs, axes in [
+          [(3, 4, 5), (onp.array([0, 2, 1]),), (0,)],
+          [(3, 4, 5), (onp.array([-1, -2]),), (0,)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 1)],
+          [(3, 4, 5), (onp.array([0, 2]), onp.array([1, 3])), (0, 2)],
+      ]
+      for rng in [jtu.rand_default()])
+  def testIndexUntakeGrad(self, dst_shape, dtype, idxs, axes, rng):
+    # We call lax.index_take to get the shapes right
+    src_shape = lax.index_take(rng(dst_shape, dtype), idxs, axes).shape
+
+    idxs = tuple(rng(e.shape, e.dtype) for e in idxs)
+    src = rng(src_shape, dtype)
+    dst = rng(dst_shape, dtype)
+    index_untake = lambda src, dst: lax.index_untake(src, dst, idxs, axes)
+    check_grads(index_untake, (src, dst), 2, 1e-2, 1e-2, 1e-2)
+
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/minmax_test.py,tests/minmax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/minmax_test.py b/tests/minmax_test.py
new file mode 100644
index 000000000..63436c6fd
--- /dev/null
+++ b/tests/minmax_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the minmax optimizer module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+from absl.testing import absltest
+
+import jax.test_util as jtu
+import jax.numpy as np
+from jax.api import grad
+from jax.experimental import minmax
+from jax.lib import xla_bridge as xla
+
+
+class OptimizerTests(jtu.JaxTestCase):
+
+  def _CheckOptimizer(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    self._CheckFuns(optimizer, loss, x0, *args)
+    self._CheckRun(optimizer, loss, x0, num_steps, *args, **kwargs)
+
+  def _CheckFuns(self, optimizer, loss, x0, *args):
+    init_fun, update_fun = optimizer(*args)
+    opt_state = init_fun(x0)
+    update_fun(0, grad(loss)(x0, None), opt_state)  # doesn't crash
+
+  @jtu.skip_on_devices('gpu')
+  def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    return # TODO(mattjj): bring back fax!
+    num_repl = xla.get_replica_count()
+    infeeder = fax.make_infeed_from_sequence(
+        [np.ones(1, dtype='float32')] * num_steps * num_repl,
+        with_pyvals=True)
+
+    def op(infeed, x0):
+      opt_init, opt_update = optimizer(*args, **kwargs)
+      return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
+    cop = jit(op)
+
+    a1, _ = op(infeeder(), x0)
+    a2, _ = cop(infeeder(), x0)
+
+    assert loss(a1, None) < 1e-3
+    assert loss(a2, None) < 1e-3
+    self.assertAllClose(a1, a2, check_dtypes=False)
+
+  def testSgdScalar(self):
+    def loss(x, _): return x**2
+    x0 = 1.
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdNestedTuple(self):
+    def loss(xyz, _):
+      x, (y, z) = xyz
+      return sum(np.dot(a, a) for a in [x, y, z])
+    x0 = (np.ones(2), (np.ones(2), np.ones(2)))
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testMomentumVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    mass = 0.
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_size, mass)
+
+  def testRmspropVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_size)
+
+  @jtu.skip_on_devices('cpu')  # TODO(mattjj): investigate numerical failure
+  def testAdamVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_size)
+
+  def testSgdClosure(self):
+    def loss(y, x, _): return y**2 * x**2
+    x0 = 1.
+    y = 1.
+    num_iters = 20
+    step_size = 0.1
+    partial_loss = functools.partial(loss, y)
+    self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
+
+if __name__ == '__main__':
+  absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
new file mode 100644
index 000000000..63436c6fd
--- /dev/null
+++ b/tests/minmax_test.py
@@ -0,0 +1,118 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for the minmax optimizer module.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+from absl.testing import absltest
+
+import jax.test_util as jtu
+import jax.numpy as np
+from jax.api import grad
+from jax.experimental import minmax
+from jax.lib import xla_bridge as xla
+
+
+class OptimizerTests(jtu.JaxTestCase):
+
+  def _CheckOptimizer(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    self._CheckFuns(optimizer, loss, x0, *args)
+    self._CheckRun(optimizer, loss, x0, num_steps, *args, **kwargs)
+
+  def _CheckFuns(self, optimizer, loss, x0, *args):
+    init_fun, update_fun = optimizer(*args)
+    opt_state = init_fun(x0)
+    update_fun(0, grad(loss)(x0, None), opt_state)  # doesn't crash
+
+  @jtu.skip_on_devices('gpu')
+  def _CheckRun(self, optimizer, loss, x0, num_steps, *args, **kwargs):
+    return # TODO(mattjj): bring back fax!
+    num_repl = xla.get_replica_count()
+    infeeder = fax.make_infeed_from_sequence(
+        [np.ones(1, dtype='float32')] * num_steps * num_repl,
+        with_pyvals=True)
+
+    def op(infeed, x0):
+      opt_init, opt_update = optimizer(*args, **kwargs)
+      return minmax.run_optimizer(loss, infeed, opt_update, opt_init(x0))
+    cop = jit(op)
+
+    a1, _ = op(infeeder(), x0)
+    a2, _ = cop(infeeder(), x0)
+
+    assert loss(a1, None) < 1e-3
+    assert loss(a2, None) < 1e-3
+    self.assertAllClose(a1, a2, check_dtypes=False)
+
+  def testSgdScalar(self):
+    def loss(x, _): return x**2
+    x0 = 1.
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testSgdNestedTuple(self):
+    def loss(xyz, _):
+      x, (y, z) = xyz
+      return sum(np.dot(a, a) for a in [x, y, z])
+    x0 = (np.ones(2), (np.ones(2), np.ones(2)))
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_size)
+
+  def testMomentumVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    mass = 0.
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_size, mass)
+
+  def testRmspropVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_size)
+
+  @jtu.skip_on_devices('cpu')  # TODO(mattjj): investigate numerical failure
+  def testAdamVector(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_size)
+
+  def testSgdClosure(self):
+    def loss(y, x, _): return y**2 * x**2
+    x0 = 1.
+    y = 1.
+    num_iters = 20
+    step_size = 0.1
+    partial_loss = functools.partial(loss, y)
+    self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
+
+if __name__ == '__main__':
+  absltest.main()",No
jax/tests/random_test.py,tests/random_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/random_test.py b/tests/random_test.py
new file mode 100644
index 000000000..9d3ae7239
--- /dev/null
+++ b/tests/random_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.special
+import scipy.stats
+
+from jax import api
+from jax import lax
+from jax import random
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+
+class LaxRandomTest(jtu.JaxTestCase):
+
+  def _CheckCollisions(self, samples, nbits):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Chebyshev
+    nitems = len(samples)
+    nbins = 2 ** nbits
+    nexpected = nbins * (1 - ((nbins - 1) / nbins) ** nitems)
+    ncollisions = len(onp.unique(samples))
+    sq_percent_deviation = ((ncollisions - nexpected) / nexpected) ** 2
+    self.assertLess(sq_percent_deviation, 1 / onp.sqrt(nexpected * fail_prob))
+
+  def _CheckKolmogorovSmirnovCDF(self, samples, cdf):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Kolmo CDF
+    statistic = scipy.stats.kstest(samples, cdf).statistic
+    self.assertLess(1. - scipy.special.kolmogorov(statistic), fail_prob)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNumpyAndXLAAgreeOnFloatEndianness(self, dtype):
+    if not FLAGS.jax_enable_x64 and onp.issubdtype(dtype, onp.float64):
+      return absltest.unittest.skip(""can't test float64 agreement"")
+
+    bits_dtype = onp.uint32 if onp.finfo(dtype).bits == 32 else onp.uint64
+    numpy_bits = onp.array(1., dtype).view(bits_dtype)
+    xla_bits = api.jit(
+        lambda: lax.bitcast_convert_type(onp.array(1., dtype), bits_dtype))()
+    self.assertEqual(numpy_bits, xla_bits)
+
+  def testThreefry2x32(self):
+    # We test the hash by comparing to known values provided in the test code of
+    # the original reference implementation of Threefry. For the values, see
+    # https://github.com/DEShawResearch/Random123-Boost/blob/65e3d874b67aa7b3e02d5ad8306462f52d2079c0/libs/random/test/test_threefry.cpp#L30-L32
+    expected = (""0x6b200159L"", ""0x99ba4efeL"")
+    result = random.threefry_2x32(onp.uint32([0, 0]), onp.uint32([0, 0]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0x1cb996fcL"", ""0xbb002be7L"")
+    result = random.threefry_2x32(onp.uint32([-1, -1]), onp.uint32([-1, -1]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0xc4923a9cL"", ""0x483df7a0L"")
+    result = random.threefry_2x32(
+        onp.uint32([0x13198a2e, 0x03707344]),
+        onp.uint32([0x243f6a88, 0x85a308d3]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testRngUniform(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.uniform(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckCollisions(samples, onp.finfo(dtype).nmant)
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.uniform().cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.int32, onp.int64])
+  def testRngRandint(self, dtype):
+    lo = 5
+    hi = 10
+
+    key = random.PRNGKey(0)
+    rand = lambda key: random.randint(key, (10000,), lo, hi, dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self.assertTrue(onp.all(lo <= samples))
+      self.assertTrue(onp.all(samples < hi))
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNormal(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.normal(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
new file mode 100644
index 000000000..9d3ae7239
--- /dev/null
+++ b/tests/random_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl import flags
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+import scipy.special
+import scipy.stats
+
+from jax import api
+from jax import lax
+from jax import random
+from jax import test_util as jtu
+
+FLAGS = flags.FLAGS
+
+
+class LaxRandomTest(jtu.JaxTestCase):
+
+  def _CheckCollisions(self, samples, nbits):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Chebyshev
+    nitems = len(samples)
+    nbins = 2 ** nbits
+    nexpected = nbins * (1 - ((nbins - 1) / nbins) ** nitems)
+    ncollisions = len(onp.unique(samples))
+    sq_percent_deviation = ((ncollisions - nexpected) / nexpected) ** 2
+    self.assertLess(sq_percent_deviation, 1 / onp.sqrt(nexpected * fail_prob))
+
+  def _CheckKolmogorovSmirnovCDF(self, samples, cdf):
+    fail_prob = 0.01  # conservative bound on statistical fail prob by Kolmo CDF
+    statistic = scipy.stats.kstest(samples, cdf).statistic
+    self.assertLess(1. - scipy.special.kolmogorov(statistic), fail_prob)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNumpyAndXLAAgreeOnFloatEndianness(self, dtype):
+    if not FLAGS.jax_enable_x64 and onp.issubdtype(dtype, onp.float64):
+      return absltest.unittest.skip(""can't test float64 agreement"")
+
+    bits_dtype = onp.uint32 if onp.finfo(dtype).bits == 32 else onp.uint64
+    numpy_bits = onp.array(1., dtype).view(bits_dtype)
+    xla_bits = api.jit(
+        lambda: lax.bitcast_convert_type(onp.array(1., dtype), bits_dtype))()
+    self.assertEqual(numpy_bits, xla_bits)
+
+  def testThreefry2x32(self):
+    # We test the hash by comparing to known values provided in the test code of
+    # the original reference implementation of Threefry. For the values, see
+    # https://github.com/DEShawResearch/Random123-Boost/blob/65e3d874b67aa7b3e02d5ad8306462f52d2079c0/libs/random/test/test_threefry.cpp#L30-L32
+    expected = (""0x6b200159L"", ""0x99ba4efeL"")
+    result = random.threefry_2x32(onp.uint32([0, 0]), onp.uint32([0, 0]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0x1cb996fcL"", ""0xbb002be7L"")
+    result = random.threefry_2x32(onp.uint32([-1, -1]), onp.uint32([-1, -1]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+    expected = (""0xc4923a9cL"", ""0x483df7a0L"")
+    result = random.threefry_2x32(
+        onp.uint32([0x13198a2e, 0x03707344]),
+        onp.uint32([0x243f6a88, 0x85a308d3]))
+    self.assertEqual(expected, tuple(map(hex, result)))
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testRngUniform(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.uniform(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckCollisions(samples, onp.finfo(dtype).nmant)
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.uniform().cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.int32, onp.int64])
+  def testRngRandint(self, dtype):
+    lo = 5
+    hi = 10
+
+    key = random.PRNGKey(0)
+    rand = lambda key: random.randint(key, (10000,), lo, hi, dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self.assertTrue(onp.all(lo <= samples))
+      self.assertTrue(onp.all(samples < hi))
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.randint(lo, hi).cdf)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64])
+  def testNormal(self, dtype):
+    key = random.PRNGKey(0)
+    rand = lambda key: random.normal(key, (10000,), dtype)
+    crand = api.jit(rand)
+
+    uncompiled_samples = rand(key)
+    compiled_samples = crand(key)
+
+    for samples in [uncompiled_samples, compiled_samples]:
+      self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
jax/tests/stax_test.py,tests/stax_test.py,489dd1c81a3625d3d862f23374d41aa57ba5d2e6,fd8d83dca27fd8e8b642743e669bda6285f66e9e,move jax tests and examples to repo root,"diff --git a/tests/stax_test.py b/tests/stax_test.py
new file mode 100644
index 000000000..1689301f9
--- /dev/null
+++ b/tests/stax_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for Stax library.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import test_util as jtu
+from jax import random
+from jax.experimental import stax
+
+
+def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
+  result_shape, params = init_fun(input_shape)
+  inputs = onp.random.RandomState(0).randn(*input_shape).astype(""float32"")
+  rng_key = random.PRNGKey(0)
+  result = apply_fun(params, inputs, rng_key)
+  test_case.assertEqual(result.shape, result_shape)
+
+
+class StaxTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (5,)])
+  def testRandnInitShape(self, shape):
+    out = stax.randn()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (2, 3, 4)])
+  def testGlorotInitShape(self, shape):
+    out = stax.glorot()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_channels={}_filter_shape={}_padding={}_strides={}_input_shape={}""
+       .format(channels, filter_shape, padding, strides, input_shape),
+       ""channels"": channels, ""filter_shape"": filter_shape, ""padding"": padding,
+       ""strides"": strides, ""input_shape"": input_shape}
+      for channels in [2, 3]
+      for filter_shape in [(1, 1), (2, 3)]
+      for padding in [""SAME"", ""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 10, 11, 1)])
+  def testConvShape(self, channels, filter_shape, padding, strides,
+                    input_shape):
+    init_fun, apply_fun = stax.Conv(channels, filter_shape, strides=strides,
+                                    padding=padding)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_out_dim={}_input_shape={}""
+                        .format(out_dim, input_shape),
+       ""out_dim"": out_dim, ""input_shape"": input_shape}
+      for out_dim in [3, 4]
+      for input_shape in [(2, 3), (3, 4)])
+  def testDenseShape(self, out_dim, input_shape):
+    init_fun, apply_fun = stax.Dense(out_dim)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testReluShape(self, input_shape):
+    init_fun, apply_fun = stax.Relu
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
+                        .format(window_shape, padding, strides, input_shape),
+       ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
+       ""input_shape"": input_shape}
+      for window_shape in [(1, 1), (2, 3)]
+      for padding in [""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 5, 6, 1)])
+  def testPoolingShape(self, window_shape, padding, strides, input_shape):
+    init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
+                                       strides=strides)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testFlattenShape(self, input_shape):
+    init_fun, apply_fun = stax.Flatten
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}_spec={}"".format(input_shape, i),
+       ""input_shape"": input_shape, ""spec"": spec}
+      for input_shape in [(2, 5, 6, 1)]
+      for i, spec in enumerate([
+          [stax.Conv(3, (2, 2))],
+          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]]))
+  def testSerialComposeLayersShape(self, input_shape, spec):
+    init_fun, apply_fun = stax.serial(*spec)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(3, 4), (2, 5, 6, 1)])
+  def testDropoutShape(self, input_shape):
+    init_fun, apply_fun = stax.Dropout(0.9)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+
+if __name__ == ""__main__"":
+  absltest.main()","diff --git a/tests/stax_test.py b/tests/stax_test.py
new file mode 100644
index 000000000..1689301f9
--- /dev/null
+++ b/tests/stax_test.py
@@ -0,0 +1,132 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Tests for Stax library.""""""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+import numpy as onp
+
+from jax import test_util as jtu
+from jax import random
+from jax.experimental import stax
+
+
+def _CheckShapeAgreement(test_case, init_fun, apply_fun, input_shape):
+  result_shape, params = init_fun(input_shape)
+  inputs = onp.random.RandomState(0).randn(*input_shape).astype(""float32"")
+  rng_key = random.PRNGKey(0)
+  result = apply_fun(params, inputs, rng_key)
+  test_case.assertEqual(result.shape, result_shape)
+
+
+class StaxTest(jtu.JaxTestCase):
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (5,)])
+  def testRandnInitShape(self, shape):
+    out = stax.randn()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(shape), ""shape"": shape}
+      for shape in [(2, 3), (2, 3, 4)])
+  def testGlorotInitShape(self, shape):
+    out = stax.glorot()(shape)
+    self.assertEqual(out.shape, shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"":
+       ""_channels={}_filter_shape={}_padding={}_strides={}_input_shape={}""
+       .format(channels, filter_shape, padding, strides, input_shape),
+       ""channels"": channels, ""filter_shape"": filter_shape, ""padding"": padding,
+       ""strides"": strides, ""input_shape"": input_shape}
+      for channels in [2, 3]
+      for filter_shape in [(1, 1), (2, 3)]
+      for padding in [""SAME"", ""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 10, 11, 1)])
+  def testConvShape(self, channels, filter_shape, padding, strides,
+                    input_shape):
+    init_fun, apply_fun = stax.Conv(channels, filter_shape, strides=strides,
+                                    padding=padding)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_out_dim={}_input_shape={}""
+                        .format(out_dim, input_shape),
+       ""out_dim"": out_dim, ""input_shape"": input_shape}
+      for out_dim in [3, 4]
+      for input_shape in [(2, 3), (3, 4)])
+  def testDenseShape(self, out_dim, input_shape):
+    init_fun, apply_fun = stax.Dense(out_dim)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testReluShape(self, input_shape):
+    init_fun, apply_fun = stax.Relu
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_window_shape={}_padding={}_strides={}_input_shape={}""
+                        .format(window_shape, padding, strides, input_shape),
+       ""window_shape"": window_shape, ""padding"": padding, ""strides"": strides,
+       ""input_shape"": input_shape}
+      for window_shape in [(1, 1), (2, 3)]
+      for padding in [""VALID""]
+      for strides in [None, (2, 1)]
+      for input_shape in [(2, 5, 6, 1)])
+  def testPoolingShape(self, window_shape, padding, strides, input_shape):
+    init_fun, apply_fun = stax.MaxPool(window_shape, padding=padding,
+                                       strides=strides)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(2, 3), (2, 3, 4)])
+  def testFlattenShape(self, input_shape):
+    init_fun, apply_fun = stax.Flatten
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}_spec={}"".format(input_shape, i),
+       ""input_shape"": input_shape, ""spec"": spec}
+      for input_shape in [(2, 5, 6, 1)]
+      for i, spec in enumerate([
+          [stax.Conv(3, (2, 2))],
+          [stax.Conv(3, (2, 2)), stax.Flatten, stax.Dense(4)]]))
+  def testSerialComposeLayersShape(self, input_shape, spec):
+    init_fun, apply_fun = stax.serial(*spec)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_input_shape={}"".format(input_shape),
+       ""input_shape"": input_shape}
+      for input_shape in [(3, 4), (2, 5, 6, 1)])
+  def testDropoutShape(self, input_shape):
+    init_fun, apply_fun = stax.Dropout(0.9)
+    _CheckShapeAgreement(self, init_fun, apply_fun, input_shape)
+
+
+if __name__ == ""__main__"":
+  absltest.main()",No
.gitignore,.gitignore,1b836515c2ce7f529e615b89302186018ad406c8,9584c4f55a5f40b470ed5505ef9b5b8da8ee3a12,update gitignore,"diff --git a/.gitignore b/.gitignore
index a1b5719c3..944358884 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,3 +3,4 @@
 /jax/lib/pywrap_xla.py
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
+jax.egg-info","diff --git a/.gitignore b/.gitignore
index a1b5719c3..944358884 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,3 +3,4 @@
 /jax/lib/pywrap_xla.py
 /jax/lib/xla_client.py
 /jax/lib/xla_data_pb2.py
+jax.egg-info",No
setup.py,setup.py,0dfa736ece742445721d3e031964ffb6695b933a,1b836515c2ce7f529e615b89302186018ad406c8,make setup.py copy over .so files,"diff --git a/setup.py b/setup.py
index 7fa4bdef2..54b9e9c46 100644
--- a/setup.py
+++ b/setup.py
@@ -1,4 +1,5 @@
 from setuptools import setup
+from glob import glob
 
 setup(
     name='jax',
@@ -11,4 +12,5 @@ setup(
     install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
+    package_data={'jax.lib': glob('jax/lib/*.so')},
 )","diff --git a/setup.py b/setup.py
index 7fa4bdef2..54b9e9c46 100644
--- a/setup.py
+++ b/setup.py
@@ -1,4 +1,5 @@
 from setuptools import setup
+from glob import glob
 
 setup(
     name='jax',
@@ -11,4 +12,5 @@ setup(
     install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
+    package_data={'jax.lib': glob('jax/lib/*.so')},
 )",No
jax/LICENSE,LICENSE,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/LICENSE b/LICENSE
new file mode 100644
index 000000000..d64569567
--- /dev/null
+++ b/LICENSE
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      ""License"" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      ""Licensor"" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      ""Legal Entity"" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      ""control"" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      ""You"" (or ""Your"") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      ""Source"" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      ""Object"" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      ""Work"" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      ""Derivative Works"" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      ""Contribution"" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, ""submitted""
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as ""Not a Contribution.""
+
+      ""Contributor"" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a ""NOTICE"" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an ""AS IS"" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets ""[]""
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same ""printed page"" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the ""License"");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an ""AS IS"" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.","diff --git a/LICENSE b/LICENSE
new file mode 100644
index 000000000..d64569567
--- /dev/null
+++ b/LICENSE
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      ""License"" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      ""Licensor"" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      ""Legal Entity"" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      ""control"" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      ""You"" (or ""Your"") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      ""Source"" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      ""Object"" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      ""Work"" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      ""Derivative Works"" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      ""Contribution"" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, ""submitted""
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as ""Not a Contribution.""
+
+      ""Contributor"" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a ""NOTICE"" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an ""AS IS"" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets ""[]""
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same ""printed page"" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the ""License"");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an ""AS IS"" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.",No
jax/LICENSE_SHORT,LICENSE_SHORT,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/LICENSE_SHORT b/LICENSE_SHORT
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/LICENSE_SHORT
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.","diff --git a/LICENSE_SHORT b/LICENSE_SHORT
new file mode 100644
index 000000000..b0c7da3d7
--- /dev/null
+++ b/LICENSE_SHORT
@@ -0,0 +1,13 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.",No
jax/BUILD,jax/BUILD,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/jax/BUILD b/jax/BUILD
index 0b3567534..c5155e6e7 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,14 +1,4 @@
 # JAX is Autograd and XLA
-package(default_visibility = [""//visibility:public""])
-
-licenses([""notice""])  # Apache 2
-
-exports_files([""LICENSE""])
-
-load("":build_defs.bzl"", ""jax_test"")
-
-package_group(name = ""jax"")
-
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -29,128 +19,20 @@ py_library(
     ],
 )
 
-jax_test(
-    name = ""core_test"",
-    srcs = [""tests/core_test.py""],
-    shard_count = {
-        ""cpu"": 5,
-    },
-)
-
-jax_test(
-    name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""random_test"",
-    srcs = [""tests/random_test.py""],
-)
-
-jax_test(
-    name = ""api_test"",
-    srcs = [""tests/api_test.py""],
-)
-
-jax_test(
-    name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
-)
-
-py_binary(
-    name = ""interactive"",
-    srcs = [""examples/interactive.py""],
-    deps = ["":libjax""],
-)
-
 py_library(
     name = ""stax"",
     srcs = [""experimental/stax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
-)
-
 py_library(
     name = ""minmax"",
     srcs = [""experimental/minmax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
-)
-
 py_library(
     name = ""lapax"",
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
-
-jax_test(
-    name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
-)
-
-py_library(
-    name = ""datasets"",
-    srcs = [""examples/datasets.py""],
-)
-
-py_binary(
-    name = ""mnist_classifier"",
-    srcs = [""examples/mnist_classifier.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-    ],
-)
-
-py_binary(
-    name = ""mnist_vae"",
-    srcs = [""examples/mnist_vae.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-        "":minmax"",
-        "":stax"",
-    ],
-)","diff --git a/jax/BUILD b/jax/BUILD
index 0b3567534..c5155e6e7 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,14 +1,4 @@
 # JAX is Autograd and XLA
-package(default_visibility = [""//visibility:public""])
-
-licenses([""notice""])  # Apache 2
-
-exports_files([""LICENSE""])
-
-load("":build_defs.bzl"", ""jax_test"")
-
-package_group(name = ""jax"")
-
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -29,128 +19,20 @@ py_library(
     ],
 )
 
-jax_test(
-    name = ""core_test"",
-    srcs = [""tests/core_test.py""],
-    shard_count = {
-        ""cpu"": 5,
-    },
-)
-
-jax_test(
-    name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
-    shard_count = {
-        ""cpu"": 40,
-        ""gpu"": 20,
-    },
-)
-
-jax_test(
-    name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
-    shard_count = {
-        ""cpu"": 10,
-        ""gpu"": 2,
-    },
-)
-
-jax_test(
-    name = ""random_test"",
-    srcs = [""tests/random_test.py""],
-)
-
-jax_test(
-    name = ""api_test"",
-    srcs = [""tests/api_test.py""],
-)
-
-jax_test(
-    name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
-)
-
-py_binary(
-    name = ""interactive"",
-    srcs = [""examples/interactive.py""],
-    deps = ["":libjax""],
-)
-
 py_library(
     name = ""stax"",
     srcs = [""experimental/stax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
-)
-
 py_library(
     name = ""minmax"",
     srcs = [""experimental/minmax.py""],
     deps = ["":libjax""],
 )
 
-jax_test(
-    name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
-)
-
 py_library(
     name = ""lapax"",
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
-
-jax_test(
-    name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
-)
-
-py_library(
-    name = ""datasets"",
-    srcs = [""examples/datasets.py""],
-)
-
-py_binary(
-    name = ""mnist_classifier"",
-    srcs = [""examples/mnist_classifier.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-    ],
-)
-
-py_binary(
-    name = ""mnist_vae"",
-    srcs = [""examples/mnist_vae.py""],
-    deps = [
-        "":datasets"",
-        "":libjax"",
-        "":minmax"",
-        "":stax"",
-    ],
-)",No
jax/build_defs.bzl,tests/build_defs.bzl,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,0dfa736ece742445721d3e031964ffb6695b933a,"split BUILD file, move up license files","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
new file mode 100644
index 000000000..1e1052687
--- /dev/null
+++ b/tests/build_defs.bzl
@@ -0,0 +1,77 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Helpers for defining multi-platform JAX binary/test targets.""""""
+
+def jax_test(
+        name,
+        srcs,
+        deps = [],
+        args = [],
+        disable = [],
+        data = [],
+        main = None,
+        shard_count = None):
+    """"""Defines a set of per-platform JAX test targets.""""""
+    if main == None:
+        if len(srcs) == 1:
+            main = srcs[0]
+        else:
+            fail(""Only one test source file is currently supported."")
+
+    # Deps that are linked into all test target variants.
+    all_test_deps = [
+        "":libjax"",
+    ]
+    disabled_tags = [""manual"", ""notap"", ""disabled""]
+    native.py_test(
+        name = name + ""_cpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_cpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_gpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )
+    native.py_test(
+        name = name + ""_gpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )","diff --git a/tests/build_defs.bzl b/tests/build_defs.bzl
new file mode 100644
index 000000000..1e1052687
--- /dev/null
+++ b/tests/build_defs.bzl
@@ -0,0 +1,77 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+""""""Helpers for defining multi-platform JAX binary/test targets.""""""
+
+def jax_test(
+        name,
+        srcs,
+        deps = [],
+        args = [],
+        disable = [],
+        data = [],
+        main = None,
+        shard_count = None):
+    """"""Defines a set of per-platform JAX test targets.""""""
+    if main == None:
+        if len(srcs) == 1:
+            main = srcs[0]
+        else:
+            fail(""Only one test source file is currently supported."")
+
+    # Deps that are linked into all test target variants.
+    all_test_deps = [
+        "":libjax"",
+    ]
+    disabled_tags = [""manual"", ""notap"", ""disabled""]
+    native.py_test(
+        name = name + ""_cpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=true --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_cpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        deps = deps + all_test_deps,
+        shard_count = shard_count.get(""cpu"") if shard_count else None,
+        args = args + [""--jax_enable_x64=false --jax_test_dut=cpu --jax_platform_name=Host""],
+        tags = (disabled_tags if ""cpu"" in disable else []),
+    )
+    native.py_test(
+        name = name + ""_gpu"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=true --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )
+    native.py_test(
+        name = name + ""_gpu_x32"",
+        main = main,
+        srcs = srcs,
+        data = data,
+        args = args + [""--jax_test_dut=gpu --jax_enable_x64=false --jax_platform_name=CUDA""],
+        deps = deps + all_test_deps,
+        tags = [""requires-gpu-sm35""] + (disabled_tags if ""gpu"" in disable else []),
+        shard_count = shard_count.get(""gpu"") if shard_count else None,
+    )",No
build/build_jax.sh,build/build_jax.sh,c03e5e80c5dab33907ef9594543810073f51f492,9ae0f3a610c08ad1355ca92cda4906b4474b6fa3,tweak build script,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index 708f78e16..776c972a1 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -65,14 +65,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 708f78e16..776c972a1 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -65,14 +65,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} jax:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/",No
build/WORKSPACE,WORKSPACE,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,c03e5e80c5dab33907ef9594543810073f51f492,"add dummy binary build target, move WORKSPACE up","diff --git a/WORKSPACE b/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)","diff --git a/WORKSPACE b/WORKSPACE
new file mode 100644
index 000000000..f49ede9a4
--- /dev/null
+++ b/WORKSPACE
@@ -0,0 +1,21 @@
+local_repository(
+    name = ""org_tensorflow"",
+    path = ""tensorflow"",
+)
+
+http_archive(
+    name = ""io_bazel_rules_closure"",
+    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
+    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
+    urls = [
+        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
+    ],
+)
+
+load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")
+
+tf_workspace(
+    path_prefix = """",
+    tf_repo_name = ""org_tensorflow"",
+)",No
build/build_jax.sh,build/build_jax.sh,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,c03e5e80c5dab33907ef9594543810073f51f492,"add dummy binary build target, move WORKSPACE up","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 776c972a1..dbab5d69c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -8,8 +8,6 @@ then
   exit 1
 fi
 
-cp build/WORKSPACE .
-
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -28,7 +26,10 @@ export PATH=""${bazel_dir}/bin:$PATH""
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla with gpu support
-git clone https://github.com/tensorflow/tensorflow.git
+if [[ ! -d tensorflow ]]
+then
+  git clone https://github.com/tensorflow/tensorflow.git
+fi
 pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
@@ -65,14 +66,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
@@ -84,6 +85,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
-rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 776c972a1..dbab5d69c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -8,8 +8,6 @@ then
   exit 1
 fi
 
-cp build/WORKSPACE .
-
 tmp=/tmp/jax-build  # could mktemp -d but this way we can cache results
 mkdir -p ${tmp}
 
@@ -28,7 +26,10 @@ export PATH=""${bazel_dir}/bin:$PATH""
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla with gpu support
-git clone https://github.com/tensorflow/tensorflow.git
+if [[ ! -d tensorflow ]]
+then
+  git clone https://github.com/tensorflow/tensorflow.git
+fi
 pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
@@ -65,14 +66,14 @@ bazel_build_opt=""-c opt --config=cuda""
 if [ -n $handle_temporary_bazel_0_19_1_bug ]
 then
   set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive 2> /dev/null
+  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
   sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
   set -e
 fi
-bazel ${bazel_opt} build ${bazel_build_opt} examples:interactive
+bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
-runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/examples/interactive.runfiles/org_tensorflow/tensorflow""
+runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
 cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
 cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
@@ -84,6 +85,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 
 ## clean up
 rm -f bazel-*  # symlinks
-rm -f WORKSPACE
 rm -rf tensorflow
 # rm -rf ${tmp}",No
jax/BUILD,jax/BUILD,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,c03e5e80c5dab33907ef9594543810073f51f492,"add dummy binary build target, move WORKSPACE up","diff --git a/jax/BUILD b/jax/BUILD
index c5155e6e7..310729498 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,6 @@
 # JAX is Autograd and XLA
+package(default_visibility = [""//visibility:public""])
+
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -36,3 +38,10 @@ py_library(
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
+
+# this is a dummy target for building purposes
+py_binary(
+    name = ""build_jax"",
+    srcs = [""core.py""],
+    deps = ["":libjax""],
+)","diff --git a/jax/BUILD b/jax/BUILD
index c5155e6e7..310729498 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,6 @@
 # JAX is Autograd and XLA
+package(default_visibility = [""//visibility:public""])
+
 py_library(
     name = ""libjax"",
     srcs = glob(
@@ -36,3 +38,10 @@ py_library(
     srcs = [""experimental/lapax.py""],
     deps = ["":libjax""],
 )
+
+# this is a dummy target for building purposes
+py_binary(
+    name = ""build_jax"",
+    srcs = [""core.py""],
+    deps = ["":libjax""],
+)",No
jax/BUILD,jax/BUILD,ae641fdaeca7a12f98e9554faddd92580ed09a3e,d347d65c5c6c6a97f1a8544dcdfe905a3e40fcc2,add 'main' to build_jax dummy py_binary target,"diff --git a/jax/BUILD b/jax/BUILD
index 310729498..d03b01984 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -42,6 +42,7 @@ py_library(
 # this is a dummy target for building purposes
 py_binary(
     name = ""build_jax"",
-    srcs = [""core.py""],
+    srcs = [""__init__.py""],
+    main = ""__init__.py"",
     deps = ["":libjax""],
 )","diff --git a/jax/BUILD b/jax/BUILD
index 310729498..d03b01984 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -42,6 +42,7 @@ py_library(
 # this is a dummy target for building purposes
 py_binary(
     name = ""build_jax"",
-    srcs = [""core.py""],
+    srcs = [""__init__.py""],
+    main = ""__init__.py"",
     deps = ["":libjax""],
 )",No
examples/interactive.py,examples/interactive.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/examples/interactive.py b/examples/interactive.py
index e0c061aed..ca111b703 100644
--- a/examples/interactive.py
+++ b/examples/interactive.py
@@ -20,9 +20,11 @@ from absl import app
 import IPython
 import numpy as onp
 
+import jax
+import jax.numpy as np
 from jax import lax
-from jax import numpy as np
-from jax import jit, grad, vmap
+from jax import random
+from jax import jit, grad, vmap, jacfwd, jacrev, hessian
 
 
 def main(unused_argv):","diff --git a/examples/interactive.py b/examples/interactive.py
index e0c061aed..ca111b703 100644
--- a/examples/interactive.py
+++ b/examples/interactive.py
@@ -20,9 +20,11 @@ from absl import app
 import IPython
 import numpy as onp
 
+import jax
+import jax.numpy as np
 from jax import lax
-from jax import numpy as np
-from jax import jit, grad, vmap
+from jax import random
+from jax import jit, grad, vmap, jacfwd, jacrev, hessian
 
 
 def main(unused_argv):",No
examples/mnist_classifier.py,examples/mnist_classifier.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 58b660024..c97b18e15 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -12,9 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-""""""A basic MNIST example using Numpy and JAX.
-
-The primary aim here is simplicity and minimal dependencies.
+""""""A basic MNIST example using JAX together with the mini-libraries stax, for
+neural network building, and minmax, for first-order stochastic optimization.
 """"""
 
 from __future__ import absolute_import
@@ -22,26 +21,19 @@ from __future__ import division
 from __future__ import print_function
 
 import time
+import itertools
 
 from absl import app
 import numpy.random as npr
 
-from jax.api import jit, grad
-from jax.examples import datasets
-from jax.scipy.misc import logsumexp
 import jax.numpy as np
+from jax import jit, grad
+from jax.experimental import minmax
+from jax.examples import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
-def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
-  return [(scale * rng.randn(m, n), scale * rng.randn(n))
-          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
-
-def predict(params, inputs):
-  for w, b in params:
-    outputs = np.dot(inputs, w) + b
-    inputs = np.tanh(outputs)
-  return outputs - logsumexp(outputs, axis=1, keepdims=True)
-
 def loss(params, batch):
   inputs, targets = batch
   preds = predict(params, inputs)
@@ -53,13 +45,16 @@ def accuracy(params, batch):
   predicted_class = np.argmax(predict(params, inputs), axis=1)
   return np.mean(predicted_class == target_class)
 
+init_random_params, predict = stax.serial(
+    Dense(1024), Relu,
+    Dense(1024), Relu,
+    Dense(10), Softmax)
 
 def main(unused_argv):
-  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
-  param_scale = 0.1
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
+  momentum_mass = 0.9
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()
   num_train = train_images.shape[0]
@@ -75,19 +70,24 @@ def main(unused_argv):
         yield train_images[batch_idx], train_labels[batch_idx]
   batches = data_stream()
 
+  opt_init, opt_update = minmax.momentum(step_size, mass=momentum_mass)
+
   @jit
-  def update(params, batch):
-    grads = grad(loss)(params, batch)
-    return [(w - step_size * dw, b - step_size * db)
-            for (w, b), (dw, db) in zip(params, grads)]
+  def update(i, opt_state, batch):
+    params = minmax.get_params(opt_state)
+    return opt_update(i, grad(loss)(params, batch), opt_state)
+
+  _, init_params = init_random_params((-1, 28 * 28))
+  opt_state = opt_init(init_params)
+  itercount = itertools.count()
 
-  params = init_random_params(param_scale, layer_sizes)
   for epoch in range(num_epochs):
     start_time = time.time()
     for _ in range(num_batches):
-      params = update(params, next(batches))
+      opt_state = update(next(itercount), opt_state, next(batches))
     epoch_time = time.time() - start_time
 
+    params = minmax.get_params(opt_state)
     train_acc = accuracy(params, (train_images, train_labels))
     test_acc = accuracy(params, (test_images, test_labels))
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 58b660024..c97b18e15 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -12,9 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-""""""A basic MNIST example using Numpy and JAX.
-
-The primary aim here is simplicity and minimal dependencies.
+""""""A basic MNIST example using JAX together with the mini-libraries stax, for
+neural network building, and minmax, for first-order stochastic optimization.
 """"""
 
 from __future__ import absolute_import
@@ -22,26 +21,19 @@ from __future__ import division
 from __future__ import print_function
 
 import time
+import itertools
 
 from absl import app
 import numpy.random as npr
 
-from jax.api import jit, grad
-from jax.examples import datasets
-from jax.scipy.misc import logsumexp
 import jax.numpy as np
+from jax import jit, grad
+from jax.experimental import minmax
+from jax.examples import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
-def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
-  return [(scale * rng.randn(m, n), scale * rng.randn(n))
-          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]
-
-def predict(params, inputs):
-  for w, b in params:
-    outputs = np.dot(inputs, w) + b
-    inputs = np.tanh(outputs)
-  return outputs - logsumexp(outputs, axis=1, keepdims=True)
-
 def loss(params, batch):
   inputs, targets = batch
   preds = predict(params, inputs)
@@ -53,13 +45,16 @@ def accuracy(params, batch):
   predicted_class = np.argmax(predict(params, inputs), axis=1)
   return np.mean(predicted_class == target_class)
 
+init_random_params, predict = stax.serial(
+    Dense(1024), Relu,
+    Dense(1024), Relu,
+    Dense(10), Softmax)
 
 def main(unused_argv):
-  layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
-  param_scale = 0.1
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
+  momentum_mass = 0.9
 
   train_images, train_labels, test_images, test_labels = datasets.mnist()
   num_train = train_images.shape[0]
@@ -75,19 +70,24 @@ def main(unused_argv):
         yield train_images[batch_idx], train_labels[batch_idx]
   batches = data_stream()
 
-  @jit
-  def update(params, batch):
-    grads = grad(loss)(params, batch)
-    return [(w - step_size * dw, b - step_size * db)
-            for (w, b), (dw, db) in zip(params, grads)]
+  opt_init, opt_update = minmax.momentum(step_size, mass=momentum_mass)
+
+  @jit
+  def update(i, opt_state, batch):
+    params = minmax.get_params(opt_state)
+    return opt_update(i, grad(loss)(params, batch), opt_state)
+
+  _, init_params = init_random_params((-1, 28 * 28))
+  opt_state = opt_init(init_params)
+  itercount = itertools.count()
 
-  params = init_random_params(param_scale, layer_sizes)
   for epoch in range(num_epochs):
     start_time = time.time()
     for _ in range(num_batches):
-      params = update(params, next(batches))
+      opt_state = update(next(itercount), opt_state, next(batches))
     epoch_time = time.time() - start_time
 
+    params = minmax.get_params(opt_state)
     train_acc = accuracy(params, (train_images, train_labels))
     test_acc = accuracy(params, (test_images, test_labels))
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))",Yes
examples/mnist_vae.py,examples/mnist_vae.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 163e59b38..7e9f719bb 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -28,13 +28,12 @@ import time
 from absl import app
 import matplotlib.pyplot as plt
 
-from jax import lax, random
-from jax.api import jit, grad
+import jax.numpy as np
+from jax import jit, grad, lax, random
 from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import jax.numpy as np
 
 
 def gaussian_kl(mu, sigmasq):","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 163e59b38..7e9f719bb 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -28,13 +28,12 @@ import time
 from absl import app
 import matplotlib.pyplot as plt
 
-from jax import lax, random
-from jax.api import jit, grad
+import jax.numpy as np
+from jax import jit, grad, lax, random
 from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import jax.numpy as np
 
 
 def gaussian_kl(mu, sigmasq):",No
jax/core.py,jax/core.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/core.py b/jax/core.py
index 9526bdd77..2d6081827 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -428,9 +428,9 @@ pytype_aval_mappings = {}
 # ------------------- Products -------------------
 
 class JaxTuple(tuple):
-  def __new__(self, xs):
-    assert all(map(valid_jaxtype, xs)), xs
-    return tuple.__new__(JaxTuple, xs)
+  def __new__(cls, xs):
+    assert skip_checks or all(map(valid_jaxtype, xs)), xs
+    return tuple.__new__(cls, xs)
 
   def __repr__(self):
     if self is unit:","diff --git a/jax/core.py b/jax/core.py
index 9526bdd77..2d6081827 100644
--- a/jax/core.py
+++ b/jax/core.py
@@ -428,9 +428,9 @@ pytype_aval_mappings = {}
 # ------------------- Products -------------------
 
 class JaxTuple(tuple):
-  def __new__(self, xs):
-    assert all(map(valid_jaxtype, xs)), xs
-    return tuple.__new__(JaxTuple, xs)
+  def __new__(cls, xs):
+    assert skip_checks or all(map(valid_jaxtype, xs)), xs
+    return tuple.__new__(cls, xs)
 
   def __repr__(self):
     if self is unit:",No
jax/experimental/minmax.py,jax/experimental/minmax.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index e24662325..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -23,21 +23,28 @@ from __future__ import division
 from __future__ import print_function
 
 import operator
+import functools
 
+import jax.numpy as np
 from jax.core import pack
 from jax.tree_util import tree_map, tree_multimap
-import jax.numpy as np
 
 
 def optimizer(opt_maker):
   """"""Decorator to make an optimizer map over tuple/list/dict containers.""""""
+  @functools.wraps(opt_maker)
   def tree_opt_maker(*args, **kwargs):
     init_fun, update_fun = opt_maker(*args, **kwargs)
+
+    @functools.wraps(init_fun)
     def fmapped_init_fun(x0_tree):
       return tree_map(lambda x0: pack(init_fun(x0)), x0_tree)
+
+    @functools.wraps(update_fun)
     def fmapped_update_fun(i, grad_tree, state_tree):
       update = lambda g, state: pack(update_fun(i, g, *state))
       return tree_multimap(update, grad_tree, state_tree)
+
     return fmapped_init_fun, fmapped_update_fun
   return tree_opt_maker
 
@@ -46,42 +53,86 @@ def iterate(state_tree):
   return tree_map(lambda state: tuple(state)[0], state_tree)
 get_params = iterate
 
+# optimizers
+
 @optimizer
 def sgd(step_size):
-  """"""Init and update step functions for stochastic gradient descent.""""""
+  """"""Construct init and update step functions for stochastic gradient descent.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     return (x0,)
   def update_fun(i, g, x):
-    return (x - step_size * g,)
+    return (x - step_size(i) * g,)
   return init_fun, update_fun
 
 @optimizer
 def momentum(step_size, mass):
-  """"""Init and update step functions for SGD with Nesterov momentum.""""""
+  """"""Construct init and update step functions for SGD with Nesterov momentum.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     v0 = np.zeros_like(x0)
     return x0, v0
   def update_fun(i, g, x, velocity):
     velocity = mass * velocity - (1. - mass) * g
-    x = x + step_size * velocity
+    x = x + step_size(i) * velocity
     return x, velocity
   return init_fun, update_fun
 
 @optimizer
 def rmsprop(step_size, gamma=0.9, eps=1e-8):
-  """"""Init and update step functions for RMSProp.""""""
+  """"""Construct init and update step functions for RMSProp.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     avg_sq_grad = np.ones_like(x0)
     return x0, avg_sq_grad
   def update_fun(i, g, x, avg_sq_grad):
     avg_sq_grad = avg_sq_grad * gamma + g**2 * (1. - gamma)
-    x = x - step_size * g / (np.sqrt(avg_sq_grad) + eps)
+    x = x - step_size(i) * g / (np.sqrt(avg_sq_grad) + eps)
     return x, avg_sq_grad
   return init_fun, update_fun
 
 @optimizer
 def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
-  """"""Init and update step functions for Adam.""""""
+  """"""Construct init and update step functions for Adam.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+    b1: optional, a positive scalar value for beta_1, the exponential decay rate
+      for the first moment estimates (default 0.9).
+    b2: optional, a positive scalar value for beta_2, the exponential decay rate
+      for the second moment estimates (default 0.999).
+    eps: optional, a positive scalar value for epsilon, a small constant for
+      numerical stability (default 1e-8).
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     m0 = np.zeros_like(x0)
     v0 = np.zeros_like(x0)
@@ -91,26 +142,47 @@ def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
     v = (1 - b2) * (g ** 2) + b2 * v  # Second moment estimate.
     mhat = m / (1 - b1 ** (i + 1))  # Bias correction.
     vhat = v / (1 - b2 ** (i + 1))
-    x = x - step_size*mhat / (np.sqrt(vhat) + eps)
+    x = x - step_size(i) * mhat / (np.sqrt(vhat) + eps)
     return x, m, v
   return init_fun, update_fun
 
-def run_optimizer(loss, infeed, update_fun, state):
-  """"""A convenience function for running optimizers with iterated map-reduce.
+# learning rate schedules
 
-  Args:
-    loss: a scalar-valued loss function taking two aguments, the current iterate
-      and a data value.
-    infeed: an infeed instance supplying the data stream.
-    update_fun: a function that has signature update_fun(i, grad, state) where
-      i is the integer iteration count, grad is the gradient of the loss at the
-      current iterate, and state is the current optimizer state.
-    state: the initial optimizer state.
+def constant(step_size):
+  def schedule(i):
+    return step_size
+  return schedule
 
-  Returns:
-    A pair (x, state) where is the final iterate and state represents the final
-    optimizer state.
-  """"""
-  map_fun = lambda _, state, batch: grad(loss)(iterate(state), batch)
-  state = fax.iterated_map_reduce(state, map_fun, update_fun, infeed)
-  return iterate(state), state
+def exponential_decay(step_size, decay_steps, decay_rate):
+  def schedule(i):
+    return step_size * decay_rate ** (i / decay_steps)
+  return schedule
+
+def inverse_time_decay(step_size, decay_steps, decay_rate, staircase=False):
+  if staircase:
+    def schedule(i):
+      return step_size / (1 + decay_rate * np.floor(i / decay_steps))
+  else:
+    def schedule(i):
+      return step_size / (1 + decay_rate * i / decay_steps)
+  return schedule
+
+def piecewise_constant(boundaries, values):
+  boundaries = np.array(boundaries)
+  values = np.array(values)
+  if not boundaries.ndim == values.ndim == 1:
+    raise ValueError(""boundaries and values must be sequences"")
+  if not boundaries.shape[0] == values.shape[0] - 1:
+    raise ValueError(""boundaries length must be one longer than values length"")
+
+  def schedule(i):
+    return values[np.sum(i > boundaries)]
+  return schedule
+
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
+  else:
+    raise TypeError, type(constant_scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index e24662325..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -23,21 +23,28 @@ from __future__ import division
 from __future__ import print_function
 
 import operator
+import functools
 
+import jax.numpy as np
 from jax.core import pack
 from jax.tree_util import tree_map, tree_multimap
-import jax.numpy as np
 
 
 def optimizer(opt_maker):
   """"""Decorator to make an optimizer map over tuple/list/dict containers.""""""
+  @functools.wraps(opt_maker)
   def tree_opt_maker(*args, **kwargs):
     init_fun, update_fun = opt_maker(*args, **kwargs)
+
+    @functools.wraps(init_fun)
     def fmapped_init_fun(x0_tree):
       return tree_map(lambda x0: pack(init_fun(x0)), x0_tree)
+
+    @functools.wraps(update_fun)
     def fmapped_update_fun(i, grad_tree, state_tree):
       update = lambda g, state: pack(update_fun(i, g, *state))
       return tree_multimap(update, grad_tree, state_tree)
+
     return fmapped_init_fun, fmapped_update_fun
   return tree_opt_maker
 
@@ -46,42 +53,86 @@ def iterate(state_tree):
   return tree_map(lambda state: tuple(state)[0], state_tree)
 get_params = iterate
 
+# optimizers
+
 @optimizer
 def sgd(step_size):
-  """"""Init and update step functions for stochastic gradient descent.""""""
+  """"""Construct init and update step functions for stochastic gradient descent.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     return (x0,)
   def update_fun(i, g, x):
-    return (x - step_size * g,)
+    return (x - step_size(i) * g,)
   return init_fun, update_fun
 
 @optimizer
 def momentum(step_size, mass):
-  """"""Init and update step functions for SGD with Nesterov momentum.""""""
+  """"""Construct init and update step functions for SGD with Nesterov momentum.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     v0 = np.zeros_like(x0)
     return x0, v0
   def update_fun(i, g, x, velocity):
     velocity = mass * velocity - (1. - mass) * g
-    x = x + step_size * velocity
+    x = x + step_size(i) * velocity
     return x, velocity
   return init_fun, update_fun
 
 @optimizer
 def rmsprop(step_size, gamma=0.9, eps=1e-8):
-  """"""Init and update step functions for RMSProp.""""""
+  """"""Construct init and update step functions for RMSProp.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     avg_sq_grad = np.ones_like(x0)
     return x0, avg_sq_grad
   def update_fun(i, g, x, avg_sq_grad):
     avg_sq_grad = avg_sq_grad * gamma + g**2 * (1. - gamma)
-    x = x - step_size * g / (np.sqrt(avg_sq_grad) + eps)
+    x = x - step_size(i) * g / (np.sqrt(avg_sq_grad) + eps)
     return x, avg_sq_grad
   return init_fun, update_fun
 
 @optimizer
 def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
-  """"""Init and update step functions for Adam.""""""
+  """"""Construct init and update step functions for Adam.
+
+  Args:
+    step_size: positive scalar, or a callable representing a step size schedule
+      that maps the iteration index to positive scalar.
+    b1: optional, a positive scalar value for beta_1, the exponential decay rate
+      for the first moment estimates (default 0.9).
+    b2: optional, a positive scalar value for beta_2, the exponential decay rate
+      for the second moment estimates (default 0.999).
+    eps: optional, a positive scalar value for epsilon, a small constant for
+      numerical stability (default 1e-8).
+
+  Returns:
+    An (init_fun, update_fun) pair.
+  """"""
+  step_size = make_schedule(step_size)
   def init_fun(x0):
     m0 = np.zeros_like(x0)
     v0 = np.zeros_like(x0)
@@ -91,26 +142,47 @@ def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):
     v = (1 - b2) * (g ** 2) + b2 * v  # Second moment estimate.
     mhat = m / (1 - b1 ** (i + 1))  # Bias correction.
     vhat = v / (1 - b2 ** (i + 1))
-    x = x - step_size*mhat / (np.sqrt(vhat) + eps)
+    x = x - step_size(i) * mhat / (np.sqrt(vhat) + eps)
     return x, m, v
   return init_fun, update_fun
 
-def run_optimizer(loss, infeed, update_fun, state):
-  """"""A convenience function for running optimizers with iterated map-reduce.
+# learning rate schedules
 
-  Args:
-    loss: a scalar-valued loss function taking two aguments, the current iterate
-      and a data value.
-    infeed: an infeed instance supplying the data stream.
-    update_fun: a function that has signature update_fun(i, grad, state) where
-      i is the integer iteration count, grad is the gradient of the loss at the
-      current iterate, and state is the current optimizer state.
-    state: the initial optimizer state.
+def constant(step_size):
+  def schedule(i):
+    return step_size
+  return schedule
 
-  Returns:
-    A pair (x, state) where is the final iterate and state represents the final
-    optimizer state.
-  """"""
-  map_fun = lambda _, state, batch: grad(loss)(iterate(state), batch)
-  state = fax.iterated_map_reduce(state, map_fun, update_fun, infeed)
-  return iterate(state), state
+def exponential_decay(step_size, decay_steps, decay_rate):
+  def schedule(i):
+    return step_size * decay_rate ** (i / decay_steps)
+  return schedule
+
+def inverse_time_decay(step_size, decay_steps, decay_rate, staircase=False):
+  if staircase:
+    def schedule(i):
+      return step_size / (1 + decay_rate * np.floor(i / decay_steps))
+  else:
+    def schedule(i):
+      return step_size / (1 + decay_rate * i / decay_steps)
+  return schedule
+
+def piecewise_constant(boundaries, values):
+  boundaries = np.array(boundaries)
+  values = np.array(values)
+  if not boundaries.ndim == values.ndim == 1:
+    raise ValueError(""boundaries and values must be sequences"")
+  if not boundaries.shape[0] == values.shape[0] - 1:
+    raise ValueError(""boundaries length must be one longer than values length"")
+
+  def schedule(i):
+    return values[np.sum(i > boundaries)]
+  return schedule
+
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
+  else:
+    raise TypeError, type(constant_scalar_or_schedule_fun)",No
jax/interpreters/partial_eval.py,jax/interpreters/partial_eval.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index eb311fd6f..74ab93d60 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -173,12 +173,12 @@ class JaxprTracerTuple(tuple): pass
 Destructuring = namedtuple('Destructuring', ['i', 'eqn', 'key'])
 
 class PartialVal(tuple):
-  def __init__(self, xs):
+  def __new__(cls, xs):
     assert core.skip_checks or (
         isinstance(xs[0], valid_pv_types)
         and isinstance(xs[1], core.Tracer) or core.valid_jaxtype(xs[1])
     ), xs
-    super(PartialVal, self).__init__(xs)
+    return tuple.__new__(cls, xs)
 
 valid_pv_types = (AbstractValue, JaxprTracerTuple, type(None))
 ","diff --git a/jax/interpreters/partial_eval.py b/jax/interpreters/partial_eval.py
index eb311fd6f..74ab93d60 100644
--- a/jax/interpreters/partial_eval.py
+++ b/jax/interpreters/partial_eval.py
@@ -173,12 +173,12 @@ class JaxprTracerTuple(tuple): pass
 Destructuring = namedtuple('Destructuring', ['i', 'eqn', 'key'])
 
 class PartialVal(tuple):
-  def __init__(self, xs):
+  def __new__(cls, xs):
     assert core.skip_checks or (
         isinstance(xs[0], valid_pv_types)
         and isinstance(xs[1], core.Tracer) or core.valid_jaxtype(xs[1])
     ), xs
-    super(PartialVal, self).__init__(xs)
+    return tuple.__new__(cls, xs)
 
 valid_pv_types = (AbstractValue, JaxprTracerTuple, type(None))
 ",No
jax/lax.py,jax/lax.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/lax.py b/jax/lax.py
index 5aea18e03..367555659 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1566,9 +1566,30 @@ def slice_transpose_rule(t, start_indices, limit_indices, strides,
   assert result.shape == operand_shape
   return [result]
 
+def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
+                        strides, **unused_kwargs):
+  operand, = batched_args
+  bdim, = batch_dims
+
+  new_start_indices = list(start_indices)
+  new_start_indices.insert(bdim, 0)
+
+  new_limit_indices = list(limit_indices)
+  new_limit_indices.insert(bdim, operand.shape[bdim])
+
+  if strides is None:
+    new_strides = None
+  else:
+    new_strides = list(strides)
+    new_strides.insert(bdim, 1)
+
+  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
+  return out, bdim
+
 slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                              slice_translation_rule)
 ad.deflinear(slice_p, slice_transpose_rule)
+batching.primitive_batchers[slice_p] = slice_batching_rule
 
 
 def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,","diff --git a/jax/lax.py b/jax/lax.py
index 5aea18e03..367555659 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1566,9 +1566,30 @@ def slice_transpose_rule(t, start_indices, limit_indices, strides,
   assert result.shape == operand_shape
   return [result]
 
+def slice_batching_rule(batched_args, batch_dims, start_indices, limit_indices,
+                        strides, **unused_kwargs):
+  operand, = batched_args
+  bdim, = batch_dims
+
+  new_start_indices = list(start_indices)
+  new_start_indices.insert(bdim, 0)
+
+  new_limit_indices = list(limit_indices)
+  new_limit_indices.insert(bdim, operand.shape[bdim])
+
+  if strides is None:
+    new_strides = None
+  else:
+    new_strides = list(strides)
+    new_strides.insert(bdim, 1)
+
+  out = slice(operand, new_start_indices, new_limit_indices, new_strides)
+  return out, bdim
+
 slice_p = standard_primitive(slice_shape_rule, _input_dtype, 'slice',
                              slice_translation_rule)
 ad.deflinear(slice_p, slice_transpose_rule)
+batching.primitive_batchers[slice_p] = slice_batching_rule
 
 
 def dynamic_slice_shape_rule(operand, start_indices, slice_sizes,",No
jax/numpy/lax_numpy.py,jax/numpy/lax_numpy.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 21717ade1..71019b420 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -171,7 +171,7 @@ def _constant_like(x, const):
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Numpy docstring below.
+  LAX-backed implementation of {fun}. Original docstring below.
 
   {np_doc}
   """""".format(fun=fun.__name__, np_doc=fun.__doc__)","diff --git a/jax/numpy/lax_numpy.py b/jax/numpy/lax_numpy.py
index 21717ade1..71019b420 100644
--- a/jax/numpy/lax_numpy.py
+++ b/jax/numpy/lax_numpy.py
@@ -171,7 +171,7 @@ def _constant_like(x, const):
 def _wraps(fun):
   """"""Like functools.wraps but works with numpy.ufuncs.""""""
   docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Numpy docstring below.
+  LAX-backed implementation of {fun}. Original docstring below.
 
   {np_doc}
   """""".format(fun=fun.__name__, np_doc=fun.__doc__)",No
jax/random.py,jax/random.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/random.py b/jax/random.py
index 69fdea8da..c626664c9 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -309,7 +309,7 @@ def shuffle(key, x, axis=0):
   for _ in range(num_rounds):
     key, subkey = split(key)
     sort_keys = _random_bits(subkey, 32, x.shape)
-    _, x = lax.sort_keyval(sort_keys, x, axis)
+    _, x = lax.sort_key_val(sort_keys, x, axis)
 
   return x
 ","diff --git a/jax/random.py b/jax/random.py
index 69fdea8da..c626664c9 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -309,7 +309,7 @@ def shuffle(key, x, axis=0):
   for _ in range(num_rounds):
     key, subkey = split(key)
     sort_keys = _random_bits(subkey, 32, x.shape)
-    _, x = lax.sort_keyval(sort_keys, x, axis)
+    _, x = lax.sort_key_val(sort_keys, x, axis)
 
   return x
 ",No
jax/scipy/misc.py,jax/scipy/misc.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/scipy/misc.py b/jax/scipy/misc.py
index f8f9591a6..57c237c4c 100644
--- a/jax/scipy/misc.py
+++ b/jax/scipy/misc.py
@@ -20,22 +20,7 @@ import numpy as onp
 import scipy.misc as osp_misc
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps, _reduction_dims, _constant_like
 
 
 @_wraps(osp_misc.logsumexp)
@@ -50,19 +35,3 @@ def logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False):
   out = lax.add(lax.log(lax.reduce(lax.exp(lax.sub(a, amax_singletons)),
                                    _constant_like(a, 0), lax.add, dims)), amax)
   return dimadd(out) if keepdims else out
-
-
-# TODO(mattjj): this is duplicated from lax_numpy.py
-def _reduction_dims(a, axis):
-  if axis is None:
-    return onp.arange(onp.ndim(a))
-  elif isinstance(axis, (onp.ndarray, tuple, list)):
-    return onp.mod(onp.asarray(axis), onp.ndim(a))
-  elif isinstance(axis, int):
-    return onp.mod([axis], onp.ndim(a))
-  else:
-    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))
-
-
-def _constant_like(x, const):
-  return onp.array(const, dtype=lax._dtype(x))","diff --git a/jax/scipy/misc.py b/jax/scipy/misc.py
index f8f9591a6..57c237c4c 100644
--- a/jax/scipy/misc.py
+++ b/jax/scipy/misc.py
@@ -20,22 +20,7 @@ import numpy as onp
 import scipy.misc as osp_misc
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps, _reduction_dims, _constant_like
 
 
 @_wraps(osp_misc.logsumexp)
@@ -50,19 +35,3 @@ def logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False):
   out = lax.add(lax.log(lax.reduce(lax.exp(lax.sub(a, amax_singletons)),
                                    _constant_like(a, 0), lax.add, dims)), amax)
   return dimadd(out) if keepdims else out
-
-
-# TODO(mattjj): this is duplicated from lax_numpy.py
-def _reduction_dims(a, axis):
-  if axis is None:
-    return onp.arange(onp.ndim(a))
-  elif isinstance(axis, (onp.ndarray, tuple, list)):
-    return onp.mod(onp.asarray(axis), onp.ndim(a))
-  elif isinstance(axis, int):
-    return onp.mod([axis], onp.ndim(a))
-  else:
-    raise TypeError(""Unexpected type of axis argument: {}"".format(type(axis)))
-
-
-def _constant_like(x, const):
-  return onp.array(const, dtype=lax._dtype(x))",No
jax/scipy/special.py,jax/scipy/special.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/jax/scipy/special.py b/jax/scipy/special.py
index b158276de..66cd9defa 100644
--- a/jax/scipy/special.py
+++ b/jax/scipy/special.py
@@ -19,22 +19,7 @@ from __future__ import print_function
 import scipy.special as osp_special
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps
 
 
 gammaln = _wraps(osp_special.gammaln)(lax.lgamma)","diff --git a/jax/scipy/special.py b/jax/scipy/special.py
index b158276de..66cd9defa 100644
--- a/jax/scipy/special.py
+++ b/jax/scipy/special.py
@@ -19,22 +19,7 @@ from __future__ import print_function
 import scipy.special as osp_special
 
 from .. import lax
-
-
-def _wraps(fun):
-  """"""Like functools.wraps but works with numpy.ufuncs.""""""
-  docstr = """"""
-  LAX-backed implementation of {fun}. Corresponding Scipy docstring below.
-
-  {np_doc}
-  """""".format(fun=fun.__name__, np_doc=fun.__doc__)
-  def wrap(op):
-    try:
-      op.__name__ = fun.__name__
-      op.__doc__ = docstr
-    finally:
-      return op
-  return wrap
+from ..numpy.lax_numpy import _wraps
 
 
 gammaln = _wraps(osp_special.gammaln)(lax.lgamma)",No
tests/batching_test.py,tests/batching_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/batching_test.py b/tests/batching_test.py
index 6020079ef..edc3c2672 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -21,6 +21,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
+from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
 from jax.api import vmap
 from jax.core import unit
@@ -135,6 +136,24 @@ class BatchingTest(jtu.JaxTestCase):
                         check_dtypes=False)
     self.assertEqual(len(side), 1)
 
+  def testSliceLax(self):
+    fun = lambda x: lax.slice(x, (2,), (4,))
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, 2:4]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+  def testSliceNumpy(self):
+    fun = lambda x: x[:, 2]
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, :, 2]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/batching_test.py b/tests/batching_test.py
index 6020079ef..edc3c2672 100644
--- a/tests/batching_test.py
+++ b/tests/batching_test.py
@@ -21,6 +21,7 @@ from absl.testing import parameterized
 import jax.numpy as np
 from jax import test_util as jtu
 from jax.abstract_arrays import ShapedArray
+from jax import lax
 from jax.api import jit, grad, jvp, vjp, trace_to_jaxpr
 from jax.api import vmap
 from jax.core import unit
@@ -135,6 +136,24 @@ class BatchingTest(jtu.JaxTestCase):
                         check_dtypes=False)
     self.assertEqual(len(side), 1)
 
+  def testSliceLax(self):
+    fun = lambda x: lax.slice(x, (2,), (4,))
+    R = onp.random.RandomState(0).randn
+    x = R(5, 10)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, 2:4]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
+  def testSliceNumpy(self):
+    fun = lambda x: x[:, 2]
+    R = onp.random.RandomState(0).randn
+    x = R(10, 5, 3, 7)
+
+    ans = vmap(fun, x)
+    expected_ans = x[:, :, 2]
+    self.assertAllClose(ans, expected_ans, check_dtypes=False)
+
 
 if __name__ == '__main__':
   absltest.main()",No
tests/lax_scipy_test.py,tests/lax_scipy_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 21a5ac4b3..4b7c95ed6 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -27,15 +27,17 @@ from absl.testing import parameterized
 import numpy as onp
 import scipy.misc as osp_misc
 import scipy.special as osp_special
+import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
+from jax.scipy import stats as lsp_stats
 
 FLAGS = flags.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -79,6 +81,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True])
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    # TODO(mattjj): test autodiff
     def scipy_fun(array_to_reduce):
       return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
 
@@ -101,6 +104,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
   def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
     # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    # TODO(mattjj): test autodiff
     if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
         and not shapes[0]):
       return absltest.unittest.skip(""real() on scalar not supported on tpu"")
@@ -111,7 +115,40 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 3)
+      for dtypes in CombosWithReplacement(default_dtypes, 3)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scipy_fun = osp_stats.norm.logpdf
+    lax_fun = lsp_stats.norm.logpdf
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = 0.5 + onp.abs(scale)
+      return [x, loc, scale]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 2)
+      for dtypes in CombosWithReplacement(default_dtypes, 2)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scale = 0.5
+    scipy_fun = functools.partial(osp_stats.norm.logpdf, scale=scale)
+    lax_fun = functools.partial(lsp_stats.norm.logpdf, scale=scale)
+    def args_maker():
+      return list(map(rng, shapes, dtypes))
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
 
 if __name__ == ""__main__"":","diff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py
index 21a5ac4b3..4b7c95ed6 100644
--- a/tests/lax_scipy_test.py
+++ b/tests/lax_scipy_test.py
@@ -27,15 +27,17 @@ from absl.testing import parameterized
 import numpy as onp
 import scipy.misc as osp_misc
 import scipy.special as osp_special
+import scipy.stats as osp_stats
 
 from jax import api
 from jax import test_util as jtu
 from jax.scipy import misc as lsp_misc
 from jax.scipy import special as lsp_special
+from jax.scipy import stats as lsp_stats
 
 FLAGS = flags.FLAGS
 
-all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4), (2, 3, 4)]
+all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]
 
 float_dtypes = [onp.float32, onp.float64]
 complex_dtypes = [onp.complex64]
@@ -79,6 +81,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for axis in range(-len(shape), len(shape))
       for keepdims in [False, True])
   def testLogSumExp(self, rng, shape, dtype, axis, keepdims):
+    # TODO(mattjj): test autodiff
     def scipy_fun(array_to_reduce):
       return osp_misc.logsumexp(array_to_reduce, axis, keepdims=keepdims)
 
@@ -101,6 +104,7 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
       for dtypes in CombosWithReplacement(rec.dtypes, rec.nargs))
   def testScipySpecialFun(self, scipy_op, lax_op, rng, shapes, dtypes, modes):
     # TODO(mattjj): unskip this test combination when real() on tpu is improved
+    # TODO(mattjj): test autodiff
     if (FLAGS.jax_test_dut and FLAGS.jax_test_dut.startswith(""tpu"")
         and not shapes[0]):
       return absltest.unittest.skip(""real() on scalar not supported on tpu"")
@@ -111,7 +115,40 @@ class LaxBackedScipyTests(jtu.JaxTestCase):
                         check_dtypes=False)
     self._CompileAndCheck(lax_op, args_maker, check_dtypes=True)
 
-  # TODO(mattjj): add test for lsp_stats.norm_logpdf
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 3)
+      for dtypes in CombosWithReplacement(default_dtypes, 3)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfThreeArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scipy_fun = osp_stats.norm.logpdf
+    lax_fun = lsp_stats.norm.logpdf
+    def args_maker():
+      x, loc, scale = map(rng, shapes, dtypes)
+      scale = 0.5 + onp.abs(scale)
+      return [x, loc, scale]
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
+
+  @parameterized.named_parameters(
+      {""testcase_name"": jtu.format_test_name_suffix(
+          """", shapes, dtypes),
+       ""rng"": rng, ""shapes"": shapes, ""dtypes"": dtypes}
+      for shapes in CombosWithReplacement(all_shapes, 2)
+      for dtypes in CombosWithReplacement(default_dtypes, 2)
+      for rng in [jtu.rand_default()])
+  def testNormLogPdfTwoArgs(self, rng, shapes, dtypes):
+    # TODO(mattjj): test autodiff
+    scale = 0.5
+    scipy_fun = functools.partial(osp_stats.norm.logpdf, scale=scale)
+    lax_fun = functools.partial(lsp_stats.norm.logpdf, scale=scale)
+    def args_maker():
+      return list(map(rng, shapes, dtypes))
+    self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=True)
+    self._CompileAndCheck(lax_fun, args_maker, check_dtypes=True)
 
 
 if __name__ == ""__main__"":",No
tests/minmax_test.py,tests/minmax_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 63436c6fd..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -114,5 +114,41 @@ class OptimizerTests(jtu.JaxTestCase):
     partial_loss = functools.partial(loss, y)
     self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
 
+  def testSgdVectorExponentialDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.exponential_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testSgdVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testAdamVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_schedule)
+
+  def testMomentumVectorInverseTimeDecayStaircaseSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_sched = minmax.inverse_time_decay(0.1, 3, 2., staircase=True)
+    mass = 0.9
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_sched, mass)
+
+  def testRmspropVectorPiecewiseConstantSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 63436c6fd..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -114,5 +114,41 @@ class OptimizerTests(jtu.JaxTestCase):
     partial_loss = functools.partial(loss, y)
     self._CheckRun(minmax.sgd, partial_loss, x0, num_iters, step_size)
 
+  def testSgdVectorExponentialDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.exponential_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testSgdVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.sgd, loss, x0, num_iters, step_schedule)
+
+  def testAdamVectorInverseTimeDecaySchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.inverse_time_decay(0.1, 3, 2.)
+    self._CheckOptimizer(minmax.adam, loss, x0, num_iters, step_schedule)
+
+  def testMomentumVectorInverseTimeDecayStaircaseSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_sched = minmax.inverse_time_decay(0.1, 3, 2., staircase=True)
+    mass = 0.9
+    self._CheckOptimizer(minmax.momentum, loss, x0, num_iters, step_sched, mass)
+
+  def testRmspropVectorPiecewiseConstantSchedule(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
+    self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
+
 if __name__ == '__main__':
   absltest.main()",No
tests/random_test.py,tests/random_test.py,46c6a9170f5829e9ff52a970b7d8933a99d2216d,ae641fdaeca7a12f98e9554faddd92580ed09a3e,sync updates,"diff --git a/tests/random_test.py b/tests/random_test.py
index 9d3ae7239..128b599b4 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -127,6 +127,23 @@ class LaxRandomTest(jtu.JaxTestCase):
     for samples in [uncompiled_samples, compiled_samples]:
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64])
+  def testShuffle(self, dtype):
+    key = random.PRNGKey(0)
+    x = onp.arange(100).astype(dtype)
+    rand = lambda key: random.shuffle(key, x)
+    crand = api.jit(rand)
+
+    perm1 = rand(key)
+    perm2 = crand(key)
+
+    self.assertTrue(onp.all(perm1 == perm2))
+    self.assertTrue(onp.all(perm1.dtype == perm2.dtype))
+    self.assertFalse(onp.all(perm1 == x))  # seems unlikely!
+    self.assertTrue(onp.all(onp.sort(perm1) == x))
+
 
 if __name__ == ""__main__"":
   absltest.main()","diff --git a/tests/random_test.py b/tests/random_test.py
index 9d3ae7239..128b599b4 100644
--- a/tests/random_test.py
+++ b/tests/random_test.py
@@ -127,6 +127,23 @@ class LaxRandomTest(jtu.JaxTestCase):
     for samples in [uncompiled_samples, compiled_samples]:
       self._CheckKolmogorovSmirnovCDF(samples, scipy.stats.norm().cdf)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(dtype), ""dtype"": onp.dtype(dtype).name}
+      for dtype in [onp.float32, onp.float64, onp.int32, onp.int64])
+  def testShuffle(self, dtype):
+    key = random.PRNGKey(0)
+    x = onp.arange(100).astype(dtype)
+    rand = lambda key: random.shuffle(key, x)
+    crand = api.jit(rand)
+
+    perm1 = rand(key)
+    perm2 = crand(key)
+
+    self.assertTrue(onp.all(perm1 == perm2))
+    self.assertTrue(onp.all(perm1.dtype == perm2.dtype))
+    self.assertFalse(onp.all(perm1 == x))  # seems unlikely!
+    self.assertTrue(onp.all(onp.sort(perm1) == x))
+
 
 if __name__ == ""__main__"":
   absltest.main()",No
build/build_jax.sh,build/build_jax.sh,77731679954a4ae24d5c3b8250b5cf536e9bb604,46c6a9170f5829e9ff52a970b7d8933a99d2216d,add CPU-only build option,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index dbab5d69c..be093a636 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,17 @@
 #!/bin/bash
 set -exv
 
+# For a build with CUDA, from the repo root run:
+#   bash build/build_jax.sh
+# For building without CUDA (CPU-only), instead run:
+#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
+# To clean intermediate results, run
+#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
+# To clean everything, run
+#   rm -rf /tmp/jax-build
+
+JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}
+
 init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
 if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
 then
@@ -25,7 +36,7 @@ export PATH=""${bazel_dir}/bin:$PATH""
 # BUG: https://github.com/bazelbuild/bazel/issues/6665
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
-## get and configure tensorflow for building xla with gpu support
+## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
 then
   git clone https://github.com/tensorflow/tensorflow.git
@@ -34,12 +45,17 @@ pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
 export USE_DEFAULT_PYTHON_LIB_PATH=1
-export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
-export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
-export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
-export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
-export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-export TF_NEED_CUDA=1
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
+  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
+  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
+  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
+  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NEED_CUDA=1
+else
+  export TF_NEED_CUDA=0
+fi
 export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
 export TF_ENABLE_XLA=1
 export TF_NEED_MKL=0
@@ -62,8 +78,14 @@ mkdir -p ${PYTHON_LIB_PATH}
 bazel_output_user_root=${tmp}/jax-bazel-output-user-root
 bazel_output_base=${bazel_output_user_root}/output-base
 bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
-bazel_build_opt=""-c opt --config=cuda""
-if [ -n $handle_temporary_bazel_0_19_1_bug ]
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  bazel_build_opt=""-c opt --config=cuda""
+else
+  bazel_build_opt=""-c opt""
+fi
+# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
+if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
 then
   set +e
   bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
@@ -74,9 +96,9 @@ bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
 runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
-cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
 
 ## rewrite some imports
 sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
@@ -86,4 +108,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 ## clean up
 rm -f bazel-*  # symlinks
 rm -rf tensorflow
-# rm -rf ${tmp}
+rm -rf ${bazel_output_user_root}  # clean build results
+# rm -rf ${tmp}  # clean everything, including the bazel binary","diff --git a/build/build_jax.sh b/build/build_jax.sh
index dbab5d69c..be093a636 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -1,6 +1,17 @@
 #!/bin/bash
 set -exv
 
+# For a build with CUDA, from the repo root run:
+#   bash build/build_jax.sh
+# For building without CUDA (CPU-only), instead run:
+#   JAX_BUILD_WITH_CUDA=0 bash build/build_jax.sh
+# To clean intermediate results, run
+#   rm -rf /tmp/jax-build/jax-bazel-output-user-root
+# To clean everything, run
+#   rm -rf /tmp/jax-build
+
+JAX_BUILD_WITH_CUDA=${JAX_BUILD_WITH_CUDA:-1}
+
 init_commit=a30e858e59d7184b9e54dc3f3955238221d70439
 if [[ ! -d .git || $(git rev-list --parents HEAD | tail -1) != ${init_commit} ]]
 then
@@ -25,7 +36,7 @@ export PATH=""${bazel_dir}/bin:$PATH""
 # BUG: https://github.com/bazelbuild/bazel/issues/6665
 handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
-## get and configure tensorflow for building xla with gpu support
+## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
 then
   git clone https://github.com/tensorflow/tensorflow.git
@@ -34,12 +45,17 @@ pushd tensorflow
 export PYTHON_BIN_PATH=${PYTHON_BIN_PATH:-$(which python)}
 export PYTHON_LIB_PATH=${SP_DIR:-$(python -m site --user-site)}
 export USE_DEFAULT_PYTHON_LIB_PATH=1
-export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
-export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
-export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
-export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
-export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-export TF_NEED_CUDA=1
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  export CUDA_TOOLKIT_PATH=${CUDA_PATH:-/usr/local/cuda}
+  export CUDNN_INSTALL_PATH=${CUDA_TOOLKIT_PATH}
+  export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
+  export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
+  export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NEED_CUDA=1
+else
+  export TF_NEED_CUDA=0
+fi
 export GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
 export TF_ENABLE_XLA=1
 export TF_NEED_MKL=0
@@ -62,8 +78,14 @@ mkdir -p ${PYTHON_LIB_PATH}
 bazel_output_user_root=${tmp}/jax-bazel-output-user-root
 bazel_output_base=${bazel_output_user_root}/output-base
 bazel_opt=""--output_user_root=${bazel_output_user_root} --output_base=${bazel_output_base} --bazelrc=tensorflow/tools/bazel.rc""
-bazel_build_opt=""-c opt --config=cuda""
-if [ -n $handle_temporary_bazel_0_19_1_bug ]
+if [[ ${JAX_BUILD_WITH_CUDA} != 0 ]]
+then
+  bazel_build_opt=""-c opt --config=cuda""
+else
+  bazel_build_opt=""-c opt""
+fi
+# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
+if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
 then
   set +e
   bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
@@ -74,9 +96,9 @@ bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need
 runfiles_prefix=""execroot/__main__/bazel-out/k8-opt/bin/jax/build_jax.runfiles/org_tensorflow/tensorflow""
-cp ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
-cp ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/libtensorflow_framework.so jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/xla_data_pb2.py jax/lib/
+cp -f ${bazel_output_base}/${runfiles_prefix}/compiler/xla/python/{xla_client.py,pywrap_xla.py,_pywrap_xla.so} jax/lib/
 
 ## rewrite some imports
 sed -i 's/from tensorflow.compiler.xla.python import pywrap_xla as c_api/from . import pywrap_xla as c_api/' jax/lib/xla_client.py
@@ -86,4 +108,5 @@ sed -i '/from tensorflow.compiler.xla.service import hlo_pb2/d' jax/lib/xla_clie
 ## clean up
 rm -f bazel-*  # symlinks
 rm -rf tensorflow
-# rm -rf ${tmp}
+rm -rf ${bazel_output_user_root}  # clean build results
+# rm -rf ${tmp}  # clean everything, including the bazel binary",No
build/build_jax.sh,build/build_jax.sh,af62fdc17dd49279e3bfb26ff066785954716099,77731679954a4ae24d5c3b8250b5cf536e9bb604,update build script: bazel-0.19.2 and darwin,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index be093a636..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -27,14 +27,17 @@ bazel_dir=${tmp}/jax-bazel
 if [ ! -d ${bazel_dir}/bin ]
 then
     mkdir -p ${bazel_dir}
-    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.1/bazel-0.19.1-installer-linux-x86_64.sh
-    chmod +x bazel-0.19.1-installer-linux-x86_64.sh
-    ./bazel-0.19.1-installer-linux-x86_64.sh --prefix=${bazel_dir}
-    rm bazel-0.19.1-installer-linux-x86_64.sh
+    case ""$(uname -s)"" in
+      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
+      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
+      *) exit 1;;
+    esac
+    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
+    chmod +x ${installer}
+    bash ${installer} --prefix=${bazel_dir}
+    rm ${installer}
 fi
 export PATH=""${bazel_dir}/bin:$PATH""
-# BUG: https://github.com/bazelbuild/bazel/issues/6665
-handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
@@ -84,14 +87,6 @@ then
 else
   bazel_build_opt=""-c opt""
 fi
-# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
-if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
-then
-  set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
-  sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
-  set -e
-fi
 bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need","diff --git a/build/build_jax.sh b/build/build_jax.sh
index be093a636..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -27,14 +27,17 @@ bazel_dir=${tmp}/jax-bazel
 if [ ! -d ${bazel_dir}/bin ]
 then
     mkdir -p ${bazel_dir}
-    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.1/bazel-0.19.1-installer-linux-x86_64.sh
-    chmod +x bazel-0.19.1-installer-linux-x86_64.sh
-    ./bazel-0.19.1-installer-linux-x86_64.sh --prefix=${bazel_dir}
-    rm bazel-0.19.1-installer-linux-x86_64.sh
+    case ""$(uname -s)"" in
+      Linux*) installer=bazel-0.19.2-installer-linux-x86_64.sh;;
+      Darwin*) installer=bazel-0.19.2-installer-darwin-x86_64.sh;;
+      *) exit 1;;
+    esac
+    curl -OL https://github.com/bazelbuild/bazel/releases/download/0.19.2/${installer}
+    chmod +x ${installer}
+    bash ${installer} --prefix=${bazel_dir}
+    rm ${installer}
 fi
 export PATH=""${bazel_dir}/bin:$PATH""
-# BUG: https://github.com/bazelbuild/bazel/issues/6665
-handle_temporary_bazel_0_19_1_bug=1  # TODO(mattjj): remove with bazel 0.19.2
 
 ## get and configure tensorflow for building xla
 if [[ ! -d tensorflow ]]
@@ -84,14 +87,6 @@ then
 else
   bazel_build_opt=""-c opt""
 fi
-# TODO(mattjj): remove this if/else clause with bazel 0.19.2 release
-if [[ -n $handle_temporary_bazel_0_19_1_bug && ${JAX_BUILD_WITH_CUDA} != 0 ]]
-then
-  set +e
-  bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax 2> /dev/null
-  sed -i 's/toolchain_identifier = ""local""/toolchain_identifier = ""local_linux""/' ${bazel_output_base}/external/local_config_cc/BUILD
-  set -e
-fi
 bazel ${bazel_opt} build ${bazel_build_opt} jax:build_jax
 
 ## extract the pieces we need",No
examples/BUILD,examples/BUILD,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/examples/BUILD b/examples/BUILD
index 227c1bce7..714100038 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],","diff --git a/examples/BUILD b/examples/BUILD
index 227c1bce7..714100038 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],",No
jax/BUILD,jax/BUILD,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index d03b01984..ba7c51848 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,19 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # JAX is Autograd and XLA
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index d03b01984..ba7c51848 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -1,4 +1,19 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # JAX is Autograd and XLA
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(",No
setup.py,setup.py,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/setup.py b/setup.py
index 54b9e9c46..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from setuptools import setup
 from glob import glob
 ","diff --git a/setup.py b/setup.py
index 54b9e9c46..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from setuptools import setup
 from glob import glob
 ",No
tests/BUILD,tests/BUILD,da2d53ad33dd5cb988e457ad49da422e440ed663,af62fdc17dd49279e3bfb26ff066785954716099,source sync,"diff --git a/tests/BUILD b/tests/BUILD
index 3d4a13e3b..f343cb634 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(","diff --git a/tests/BUILD b/tests/BUILD
index 3d4a13e3b..f343cb634 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -1,3 +1,17 @@
+# Copyright 2018 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(",No
jax/BUILD,jax/BUILD,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,da2d53ad33dd5cb988e457ad49da422e440ed663,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index ba7c51848..a9132e364 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -31,9 +31,7 @@ py_library(
             ""**/*_test.py"",
         ],
     ),
-    deps = [
-        ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
-    ],
+    deps = [""@org_tensorflow//tensorflow/compiler/xla/python:xla_client""],
 )
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index ba7c51848..a9132e364 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -31,9 +31,7 @@ py_library(
             ""**/*_test.py"",
         ],
     ),
-    deps = [
-        ""@org_tensorflow//tensorflow/compiler/xla/python:xla_client"",
-    ],
+    deps = [""@org_tensorflow//tensorflow/compiler/xla/python:xla_client""],
 )
 
 py_library(",No
examples/BUILD,examples/BUILD,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,source sync,"diff --git a/examples/BUILD b/examples/BUILD
index 714100038..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],","diff --git a/examples/BUILD b/examples/BUILD
index 714100038..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 py_binary(
     name = ""interactive"",
     srcs = [""interactive.py""],",No
jax/BUILD,jax/BUILD,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index a9132e364..281b87482 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -14,6 +14,8 @@
 
 # JAX is Autograd and XLA
 
+licenses([""notice""])  # Apache 2
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index a9132e364..281b87482 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -14,6 +14,8 @@
 
 # JAX is Autograd and XLA
 
+licenses([""notice""])  # Apache 2
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(",No
tests/BUILD,tests/BUILD,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,24f7f35e16bea1154b1b6530b73f0e8dbce91c6c,source sync,"diff --git a/tests/BUILD b/tests/BUILD
index f343cb634..310d71c90 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(","diff --git a/tests/BUILD b/tests/BUILD
index f343cb634..310d71c90 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -12,6 +12,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+licenses([""notice""])  # Apache 2
+
 load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(",No
jax/BUILD,jax/BUILD,3731ca2299d7a1f55bd43a21cdec0c7249bacb32,99f98f8e8c659599eed0682f44c0a9eeeb9f9f98,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index 281b87482..a339ebb8e 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,6 +16,8 @@
 
 licenses([""notice""])  # Apache 2
 
+# top-level EF placeholder
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(","diff --git a/jax/BUILD b/jax/BUILD
index 281b87482..a339ebb8e 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,6 +16,8 @@
 
 licenses([""notice""])  # Apache 2
 
+# top-level EF placeholder
+
 package(default_visibility = [""//visibility:public""])
 
 py_library(",No
jax/BUILD,jax/BUILD,fe11b19e46c8d92817dd7b8c710b644b98845a5a,3731ca2299d7a1f55bd43a21cdec0c7249bacb32,source sync,"diff --git a/jax/BUILD b/jax/BUILD
index a339ebb8e..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,10 +16,10 @@
 
 licenses([""notice""])  # Apache 2
 
-# top-level EF placeholder
-
 package(default_visibility = [""//visibility:public""])
 
+# top-level EF placeholder
+
 py_library(
     name = ""libjax"",
     srcs = glob(","diff --git a/jax/BUILD b/jax/BUILD
index a339ebb8e..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -16,10 +16,10 @@
 
 licenses([""notice""])  # Apache 2
 
-# top-level EF placeholder
-
 package(default_visibility = [""//visibility:public""])
 
+# top-level EF placeholder
+
 py_library(
     name = ""libjax"",
     srcs = glob(",No
build/build_jax.sh,build/build_jax.sh,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ",No
examples/BUILD,examples/BUILD,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )",No
jax/BUILD,jax/BUILD,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",",No
setup.py,setup.py,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},","diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},",No
tests/BUILD,tests/BUILD,50038c07c815b82d412af43996a122e33eecd385,fe11b19e46c8d92817dd7b8c710b644b98845a5a,fix build file issues,"diff --git a/tests/BUILD b/tests/BUILD
index 310d71c90..95d6ee29e 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index 310d71c90..95d6ee29e 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load("":build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )",No
jax/experimental/minmax.py,jax/experimental/minmax.py,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,50038c07c815b82d412af43996a122e33eecd385,source sync,"diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)",No
jax/experimental/stax.py,jax/experimental/stax.py,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,50038c07c815b82d412af43996a122e33eecd385,source sync,"diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ",No
examples/mnist_classifier.py,examples/mnist_classifier.py,5e9fcbb6e8426a20a24bdba2a78eb3900d87d593,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,source sync,"diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c97b18e15..f88624f97 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 from jax.examples import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 def main(unused_argv):
   step_size = 0.001","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c97b18e15..f88624f97 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 from jax.examples import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 def main(unused_argv):
   step_size = 0.001",No
examples/resnet50.py,examples/resnet50.py,5e9fcbb6e8426a20a24bdba2a78eb3900d87d593,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,source sync,"diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):",No
tests/minmax_test.py,tests/minmax_test.py,5e9fcbb6e8426a20a24bdba2a78eb3900d87d593,b2b1e8d70cff6739e94e4070b88d84ae0ff6271e,source sync,"diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()",No
README.md,README.md,b58ccb430aeec58e3597077a25fb22d525a57ef3,76346a87b5d43d0c1f499fe78e8e654ae9d18d83,update readme with 'not a Google product',"diff --git a/README.md b/README.md
index 2c4adae2f..d05bf570b 100644
--- a/README.md
+++ b/README.md
@@ -1 +1,3 @@
 JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+
+This is not an official Google product.","diff --git a/README.md b/README.md
index 2c4adae2f..d05bf570b 100644
--- a/README.md
+++ b/README.md
@@ -1 +1,3 @@
 JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+
+This is not an official Google product.",No
README.md,README.md,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,b58ccb430aeec58e3597077a25fb22d525a57ef3,Add two-pager link to README.,"diff --git a/README.md b/README.md
index d05bf570b..22e31d080 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,5 @@
-JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd).
+Here's a [two-page abstract](https://www.sysml.cc/doc/146.pdf) about an early version.
+Watch this space for updates!
 
 This is not an official Google product.","diff --git a/README.md b/README.md
index d05bf570b..22e31d080 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,5 @@
-JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd). Watch this space for updates!
+JAX is a research project that grew out of [Autograd](https://github.com/hips/autograd).
+Here's a [two-page abstract](https://www.sysml.cc/doc/146.pdf) about an early version.
+Watch this space for updates!
 
 This is not an official Google product.",No
examples/mnist_classifier.py,examples/mnist_classifier.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index f88624f97..7eba6b960 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,15 +23,14 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.examples import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):
@@ -50,7 +49,7 @@ init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(10), LogSoftmax)
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index f88624f97..7eba6b960 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,15 +23,14 @@ from __future__ import print_function
 import time
 import itertools
 
-from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.examples import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):
@@ -50,7 +49,7 @@ init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(10), LogSoftmax)
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 10
   batch_size = 32
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 58b660024..2910b1c3b 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,13 +23,12 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-from jax.examples import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -54,7 +53,7 @@ def accuracy(params, batch):
   return np.mean(predicted_class == target_class)
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
   param_scale = 0.1
   step_size = 0.001
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 58b660024..2910b1c3b 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,13 +23,12 @@ from __future__ import print_function
 
 import time
 
-from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
-from jax.examples import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
+import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -54,7 +53,7 @@ def accuracy(params, batch):
   return np.mean(predicted_class == target_class)
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   layer_sizes = [784, 1024, 1024, 10]  # TODO(mattjj): revise to standard arch
   param_scale = 0.1
   step_size = 0.001
@@ -93,7 +92,3 @@ def main(unused_argv):
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 7e9f719bb..e97c181e4 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,15 +25,14 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -84,7 +83,7 @@ decoder_init, decode = stax.serial(
 )
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 100
   batch_size = 32
@@ -138,7 +137,3 @@ def main(unused_argv):
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index 7e9f719bb..e97c181e4 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,15 +25,14 @@ from __future__ import print_function
 import os
 import time
 
-from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
-from jax.examples import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
+import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -84,7 +83,7 @@ decoder_init, decode = stax.serial(
 )
 
 
-def main(unused_argv):
+if __name__ == ""__main__"":
   step_size = 0.001
   num_epochs = 100
   batch_size = 32
@@ -138,7 +137,3 @@ def main(unused_argv):
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
-
-
-if __name__ == ""__main__"":
-  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,51fc713089f8e456d95f79e4ed58687eb72edbfb,7cf6babc78c72702aa2969ca4a0d4a83f410a1bf,"remove absl from examples, fix import statements","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..d44f40e56 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 0b550b24a..d44f40e56 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,8 +21,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl import app
-
 import numpy.random as npr
 
 import jax.numpy as np
@@ -85,9 +83,7 @@ def ResNet50(num_classes):
       AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
-def main(argv):
-  del argv  # Unused.
-
+if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -128,7 +124,3 @@ def main(argv):
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
-
-
-if __name__ == '__main__':
-  app.run(main)",No
build/build_jax.sh,build/build_jax.sh,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 0bf8ae06c..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,7 +55,6 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -73,6 +72,7 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
+export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 0bf8ae06c..63cb0b229 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,7 +55,6 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
-  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -73,6 +72,7 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
+export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ",No
examples/BUILD,examples/BUILD,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
+        "":minmax"",
+        "":stax"",
         ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..463f244df 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
+        "":minmax"",
+        "":stax"",
         ""//jax:libjax"",
-        ""//jax:minmax"",
-        ""//jax:stax"",
     ],
 )",No
examples/mnist_classifier.py,examples/mnist_classifier.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 7eba6b960..c17f5fc1f 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,14 +23,15 @@ from __future__ import print_function
 import time
 import itertools
 
+from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, LogSoftmax
 import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
 def loss(params, batch):
@@ -47,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), LogSoftmax)
+    Dense(10), Softmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 7eba6b960..c17f5fc1f 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -23,14 +23,15 @@ from __future__ import print_function
 import time
 import itertools
 
+from absl import app
 import numpy.random as npr
 
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, LogSoftmax
 import datasets
+from jax.experimental import stax
+from jax.experimental.stax import Dense, Relu, Softmax
 
 
 def loss(params, batch):
@@ -47,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), LogSoftmax)
+    Dense(10), Softmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/mnist_classifier_fromscratch.py,examples/mnist_classifier_fromscratch.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 2910b1c3b..03d21538a 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,12 +23,13 @@ from __future__ import print_function
 
 import time
 
+from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
+import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
-import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_classifier_fromscratch.py b/examples/mnist_classifier_fromscratch.py
index 2910b1c3b..03d21538a 100644
--- a/examples/mnist_classifier_fromscratch.py
+++ b/examples/mnist_classifier_fromscratch.py
@@ -23,12 +23,13 @@ from __future__ import print_function
 
 import time
 
+from absl import app
 import numpy.random as npr
 
 from jax.api import jit, grad
+import datasets
 from jax.scipy.misc import logsumexp
 import jax.numpy as np
-import datasets
 
 
 def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):
@@ -92,3 +93,7 @@ if __name__ == ""__main__"":
     print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     print(""Training set accuracy {}"".format(train_acc))
     print(""Test set accuracy {}"".format(test_acc))
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/mnist_vae.py,examples/mnist_vae.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index e97c181e4..f15e3dd30 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,14 +25,15 @@ from __future__ import print_function
 import os
 import time
 
+from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
+import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -137,3 +138,7 @@ if __name__ == ""__main__"":
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)","diff --git a/examples/mnist_vae.py b/examples/mnist_vae.py
index e97c181e4..f15e3dd30 100644
--- a/examples/mnist_vae.py
+++ b/examples/mnist_vae.py
@@ -25,14 +25,15 @@ from __future__ import print_function
 import os
 import time
 
+from absl import app
 import matplotlib.pyplot as plt
 
 import jax.numpy as np
 from jax import jit, grad, lax, random
+import datasets
 from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import Dense, FanOut, Relu, Softplus
-import datasets
 
 
 def gaussian_kl(mu, sigmasq):
@@ -137,3 +138,7 @@ if __name__ == ""__main__"":
     test_elbo, images = evaluate(opt_state, test_images)
     print(""{: 3d} {} ({:.3f} sec)"".format(epoch, test_elbo, time.time() - tic))
     plt.imsave(imfile.format(epoch), images, cmap=plt.cm.gray)
+
+
+if __name__ == ""__main__"":
+  app.run(main)",No
examples/resnet50.py,examples/resnet50.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..9c958c131 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import app
+
 import numpy.random as npr
 
 import jax.numpy as np
@@ -29,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, LogSoftmax)
+                                   MaxPool, Relu, Softmax)
 
 
 # ResNet blocks compose other layers
@@ -80,10 +82,12 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+
 
+def main(argv):
+  del argv  # Unused.
 
-if __name__ == ""__main__"":
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -124,3 +128,7 @@ if __name__ == ""__main__"":
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
+
+
+if __name__ == '__main__':
+  app.run(main)","diff --git a/examples/resnet50.py b/examples/resnet50.py
index d44f40e56..9c958c131 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from absl import app
+
 import numpy.random as npr
 
 import jax.numpy as np
@@ -29,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, LogSoftmax)
+                                   MaxPool, Relu, Softmax)
 
 
 # ResNet blocks compose other layers
@@ -80,10 +82,12 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
 
 
-if __name__ == ""__main__"":
+def main(argv):
+  del argv  # Unused.
+
   batch_size = 8
   num_classes = 1001
   input_shape = (224, 224, 3, batch_size)
@@ -124,3 +128,7 @@ if __name__ == ""__main__"":
   for i in xrange(num_steps):
     opt_state = update(i, opt_state, next(batches))
   trained_params = minmax.get_params(opt_state)
+
+
+if __name__ == '__main__':
+  app.run(main)",Yes
jax/BUILD,jax/BUILD,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/jax/BUILD b/jax/BUILD
index c9754372f..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,7 +29,6 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
-            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",","diff --git a/jax/BUILD b/jax/BUILD
index c9754372f..cddf993d6 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,7 +29,6 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
-            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",",No
jax/experimental/minmax.py,jax/experimental/minmax.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index 92caefdeb..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(scalar_or_schedule_fun):
-  if callable(scalar_or_schedule_fun):
-    return scalar_or_schedule_fun
-  elif np.ndim(scalar_or_schedule_fun) == 0:
-    return constant(scalar_or_schedule_fun)
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
   else:
-    raise TypeError, type(scalar_or_schedule_fun)
+    raise TypeError, type(constant_scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index 92caefdeb..bbb08f96d 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(scalar_or_schedule_fun):
-  if callable(scalar_or_schedule_fun):
-    return scalar_or_schedule_fun
-  elif np.ndim(scalar_or_schedule_fun) == 0:
-    return constant(scalar_or_schedule_fun)
+def make_schedule(constant_scalar_or_schedule_fun):
+  if np.isscalar(constant_scalar_or_schedule_fun):
+    return constant(constant_scalar_or_schedule_fun)
+  elif callable(constant_scalar_or_schedule_fun):
+    return constant_scalar_or_schedule_fun
   else:
-    raise TypeError, type(scalar_or_schedule_fun)
+    raise TypeError, type(constant_scalar_or_schedule_fun)",No
jax/experimental/stax.py,jax/experimental/stax.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 6db50cc48..874aaa735 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,8 +42,9 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-def logsoftmax(x, axis=-1):
-  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
+# TODO(mattjj): change this name to better fit convention
+def softmax(x, axis=-1):
+  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -145,7 +146,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
+Softmax = _elemwise_no_params(softmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 6db50cc48..874aaa735 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,8 +42,9 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-def logsoftmax(x, axis=-1):
-  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
+# TODO(mattjj): change this name to better fit convention
+def softmax(x, axis=-1):
+  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -145,7 +146,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
+Softmax = _elemwise_no_params(softmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ",No
setup.py,setup.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/setup.py b/setup.py
index b634177d3..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},","diff --git a/setup.py b/setup.py
index b634177d3..2b2ebc68d 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},",No
tests/BUILD,tests/BUILD,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/tests/BUILD b/tests/BUILD
index 95d6ee29e..ec4e62f73 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -14,11 +14,11 @@
 
 licenses([""notice""])  # Apache 2
 
-load("":build_defs.bzl"", ""jax_test"")
+load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""core_test.py""],
+    srcs = [""tests/core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""lax_test.py""],
+    srcs = [""tests/lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""lax_numpy_test.py""],
+    srcs = [""tests/lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""lax_numpy_indexing_test.py""],
+    srcs = [""tests/lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""lax_scipy_test.py""],
+    srcs = [""tests/lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""random_test.py""],
+    srcs = [""tests/random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""api_test.py""],
+    srcs = [""tests/api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""batching_test.py""],
+    srcs = [""tests/batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""stax_test.py""],
-    deps = [""//jax:stax""],
+    srcs = [""tests/stax_test.py""],
+    deps = ["":stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""minmax_test.py""],
-    deps = [""//jax:minmax""],
+    srcs = [""tests/minmax_test.py""],
+    deps = ["":minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""lapax_test.py""],
-    deps = [""//jax:lapax""],
+    srcs = [""tests/lapax_test.py""],
+    deps = ["":lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index 95d6ee29e..ec4e62f73 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -14,11 +14,11 @@
 
 licenses([""notice""])  # Apache 2
 
-load("":build_defs.bzl"", ""jax_test"")
+load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""core_test.py""],
+    srcs = [""tests/core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""lax_test.py""],
+    srcs = [""tests/lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""lax_numpy_test.py""],
+    srcs = [""tests/lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""lax_numpy_indexing_test.py""],
+    srcs = [""tests/lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""lax_scipy_test.py""],
+    srcs = [""tests/lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""random_test.py""],
+    srcs = [""tests/random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""api_test.py""],
+    srcs = [""tests/api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""batching_test.py""],
+    srcs = [""tests/batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""stax_test.py""],
-    deps = [""//jax:stax""],
+    srcs = [""tests/stax_test.py""],
+    deps = ["":stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""minmax_test.py""],
-    deps = [""//jax:minmax""],
+    srcs = [""tests/minmax_test.py""],
+    deps = ["":minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""lapax_test.py""],
-    deps = [""//jax:lapax""],
+    srcs = [""tests/lapax_test.py""],
+    deps = ["":lapax""],
 )",No
tests/minmax_test.py,tests/minmax_test.py,a3619ca89d643502b5f958c9f69685da5edbc4cc,51fc713089f8e456d95f79e4ed58687eb72edbfb,"source sync

PiperOrigin-RevId: 222153576","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 5eee658ce..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.numpy as np
 import jax.test_util as jtu
-from jax import jit, grad
+import jax.numpy as np
+from jax.api import grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,24 +150,5 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
-  def testTracedStepSize(self):
-    def loss(x, _): return np.dot(x, x)
-    x0 = np.ones(2)
-    num_iters = 100
-    step_size = 0.1
-
-    init_fun, _ = minmax.sgd(step_size)
-    opt_state = init_fun(x0)
-
-    @jit
-    def update(opt_state, step_size):
-      _, update_fun = minmax.sgd(step_size)
-      x = minmax.get_params(opt_state)
-      g = grad(loss)(x, None)
-      return update_fun(0, g, opt_state)
-
-    update(opt_state, 0.9)  # doesn't crash
-
-
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 5eee658ce..6d39a0b14 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.numpy as np
 import jax.test_util as jtu
-from jax import jit, grad
+import jax.numpy as np
+from jax.api import grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,24 +150,5 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
-  def testTracedStepSize(self):
-    def loss(x, _): return np.dot(x, x)
-    x0 = np.ones(2)
-    num_iters = 100
-    step_size = 0.1
-
-    init_fun, _ = minmax.sgd(step_size)
-    opt_state = init_fun(x0)
-
-    @jit
-    def update(opt_state, step_size):
-      _, update_fun = minmax.sgd(step_size)
-      x = minmax.get_params(opt_state)
-      g = grad(loss)(x, None)
-      return update_fun(0, g, opt_state)
-
-    update(opt_state, 0.9)  # doesn't crash
-
-
 if __name__ == '__main__':
   absltest.main()",No
examples/BUILD,examples/BUILD,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )","diff --git a/examples/BUILD b/examples/BUILD
index 463f244df..230e9ceb7 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -39,8 +39,8 @@ py_binary(
     srcs = [""mnist_vae.py""],
     deps = [
         "":datasets"",
-        "":minmax"",
-        "":stax"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )",No
examples/mnist_classifier.py,examples/mnist_classifier.py,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c17f5fc1f..52f1e3df2 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index c17f5fc1f..52f1e3df2 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -31,7 +31,7 @@ from jax import jit, grad
 from jax.experimental import minmax
 import datasets
 from jax.experimental import stax
-from jax.experimental.stax import Dense, Relu, Softmax
+from jax.experimental.stax import Dense, Relu, LogSoftmax
 
 
 def loss(params, batch):
@@ -48,7 +48,7 @@ def accuracy(params, batch):
 init_random_params, predict = stax.serial(
     Dense(1024), Relu,
     Dense(1024), Relu,
-    Dense(10), Softmax)
+    Dense(10), LogSoftmax)
 
 if __name__ == ""__main__"":
   step_size = 0.001",No
examples/resnet50.py,examples/resnet50.py,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):","diff --git a/examples/resnet50.py b/examples/resnet50.py
index 9c958c131..0b550b24a 100644
--- a/examples/resnet50.py
+++ b/examples/resnet50.py
@@ -31,7 +31,7 @@ from jax.experimental import minmax
 from jax.experimental import stax
 from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,
                                    FanOut, Flatten, GeneralConv, Identity,
-                                   MaxPool, Relu, Softmax)
+                                   MaxPool, Relu, LogSoftmax)
 
 
 # ResNet blocks compose other layers
@@ -82,7 +82,7 @@ def ResNet50(num_classes):
       ConvBlock(3, [512, 512, 2048]),
       IdentityBlock(3, [512, 512]),
       IdentityBlock(3, [512, 512]),
-      AvgPool((7, 7)), Flatten, Dense(num_classes), Softmax)
+      AvgPool((7, 7)), Flatten, Dense(num_classes), LogSoftmax)
 
 
 def main(argv):",No
jax/BUILD,jax/BUILD,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",","diff --git a/jax/BUILD b/jax/BUILD
index cddf993d6..c9754372f 100644
--- a/jax/BUILD
+++ b/jax/BUILD
@@ -29,6 +29,7 @@ py_library(
             ""interpreters/*.py"",
             ""numpy/*.py"",
             ""scipy/*.py"",
+            ""scipy/stats/*.py"",
         ],
         exclude = [
             ""*_test.py"",",No
jax/experimental/stax.py,jax/experimental/stax.py,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ","diff --git a/jax/experimental/stax.py b/jax/experimental/stax.py
index 874aaa735..6db50cc48 100644
--- a/jax/experimental/stax.py
+++ b/jax/experimental/stax.py
@@ -42,9 +42,8 @@ import jax.numpy as np
 def relu(x): return np.maximum(x, 0.)
 def softplus(x): return np.logaddexp(x, 0.)
 
-# TODO(mattjj): change this name to better fit convention
-def softmax(x, axis=-1):
-  """"""Apply a softmax to an array of logits, log-normalizing along an axis.""""""
+def logsoftmax(x, axis=-1):
+  """"""Apply log softmax to an array of logits, log-normalizing along an axis.""""""
   return x - logsumexp(x, axis, keepdims=True)
 
 def fastvar(x, axis, keepdims):
@@ -146,7 +145,7 @@ def _elemwise_no_params(fun, **kwargs):
   return init_fun, apply_fun
 Tanh = _elemwise_no_params(np.tanh)
 Relu = _elemwise_no_params(relu)
-Softmax = _elemwise_no_params(softmax, axis=-1)
+LogSoftmax = _elemwise_no_params(logsoftmax, axis=-1)
 Softplus = _elemwise_no_params(softplus)
 
 ",No
tests/BUILD,tests/BUILD,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,a3619ca89d643502b5f958c9f69685da5edbc4cc,"source sync

PiperOrigin-RevId: 222170151","diff --git a/tests/BUILD b/tests/BUILD
index ec4e62f73..3317f5017 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )","diff --git a/tests/BUILD b/tests/BUILD
index ec4e62f73..3317f5017 100644
--- a/tests/BUILD
+++ b/tests/BUILD
@@ -18,7 +18,7 @@ load(""//third_party/py/jax/tests:build_defs.bzl"", ""jax_test"")
 
 jax_test(
     name = ""core_test"",
-    srcs = [""tests/core_test.py""],
+    srcs = [""core_test.py""],
     shard_count = {
         ""cpu"": 5,
     },
@@ -26,7 +26,7 @@ jax_test(
 
 jax_test(
     name = ""lax_test"",
-    srcs = [""tests/lax_test.py""],
+    srcs = [""lax_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -35,7 +35,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_test"",
-    srcs = [""tests/lax_numpy_test.py""],
+    srcs = [""lax_numpy_test.py""],
     shard_count = {
         ""cpu"": 40,
         ""gpu"": 20,
@@ -44,7 +44,7 @@ jax_test(
 
 jax_test(
     name = ""lax_numpy_indexing_test"",
-    srcs = [""tests/lax_numpy_indexing_test.py""],
+    srcs = [""lax_numpy_indexing_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -53,7 +53,7 @@ jax_test(
 
 jax_test(
     name = ""lax_scipy_test"",
-    srcs = [""tests/lax_scipy_test.py""],
+    srcs = [""lax_scipy_test.py""],
     shard_count = {
         ""cpu"": 10,
         ""gpu"": 2,
@@ -62,33 +62,33 @@ jax_test(
 
 jax_test(
     name = ""random_test"",
-    srcs = [""tests/random_test.py""],
+    srcs = [""random_test.py""],
 )
 
 jax_test(
     name = ""api_test"",
-    srcs = [""tests/api_test.py""],
+    srcs = [""api_test.py""],
 )
 
 jax_test(
     name = ""batching_test"",
-    srcs = [""tests/batching_test.py""],
+    srcs = [""batching_test.py""],
 )
 
 jax_test(
     name = ""stax_test"",
-    srcs = [""tests/stax_test.py""],
-    deps = ["":stax""],
+    srcs = [""stax_test.py""],
+    deps = [""//jax:stax""],
 )
 
 jax_test(
     name = ""minmax_test"",
-    srcs = [""tests/minmax_test.py""],
-    deps = ["":minmax""],
+    srcs = [""minmax_test.py""],
+    deps = [""//jax:minmax""],
 )
 
 jax_test(
     name = ""lapax_test"",
-    srcs = [""tests/lapax_test.py""],
-    deps = ["":lapax""],
+    srcs = [""lapax_test.py""],
+    deps = [""//jax:lapax""],
 )",No
jax/experimental/minmax.py,jax/experimental/minmax.py,7f546b8c02553f392e423e06bde5991bfa066b18,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,"source sync

PiperOrigin-RevId: 222175432","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)","diff --git a/jax/experimental/minmax.py b/jax/experimental/minmax.py
index bbb08f96d..92caefdeb 100644
--- a/jax/experimental/minmax.py
+++ b/jax/experimental/minmax.py
@@ -179,10 +179,10 @@ def piecewise_constant(boundaries, values):
     return values[np.sum(i > boundaries)]
   return schedule
 
-def make_schedule(constant_scalar_or_schedule_fun):
-  if np.isscalar(constant_scalar_or_schedule_fun):
-    return constant(constant_scalar_or_schedule_fun)
-  elif callable(constant_scalar_or_schedule_fun):
-    return constant_scalar_or_schedule_fun
+def make_schedule(scalar_or_schedule_fun):
+  if callable(scalar_or_schedule_fun):
+    return scalar_or_schedule_fun
+  elif np.ndim(scalar_or_schedule_fun) == 0:
+    return constant(scalar_or_schedule_fun)
   else:
-    raise TypeError, type(constant_scalar_or_schedule_fun)
+    raise TypeError, type(scalar_or_schedule_fun)",No
tests/minmax_test.py,tests/minmax_test.py,7f546b8c02553f392e423e06bde5991bfa066b18,25fb9b421d15c486ffa7dbb032a5a7522f5e5bd8,"source sync

PiperOrigin-RevId: 222175432","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()","diff --git a/tests/minmax_test.py b/tests/minmax_test.py
index 6d39a0b14..5eee658ce 100644
--- a/tests/minmax_test.py
+++ b/tests/minmax_test.py
@@ -21,9 +21,9 @@ import functools
 
 from absl.testing import absltest
 
-import jax.test_util as jtu
 import jax.numpy as np
-from jax.api import grad
+import jax.test_util as jtu
+from jax import jit, grad
 from jax.experimental import minmax
 from jax.lib import xla_bridge as xla
 
@@ -150,5 +150,24 @@ class OptimizerTests(jtu.JaxTestCase):
     step_schedule = minmax.piecewise_constant([25, 75], [1.0, 0.5, 0.1])
     self._CheckOptimizer(minmax.rmsprop, loss, x0, num_iters, step_schedule)
 
+  def testTracedStepSize(self):
+    def loss(x, _): return np.dot(x, x)
+    x0 = np.ones(2)
+    num_iters = 100
+    step_size = 0.1
+
+    init_fun, _ = minmax.sgd(step_size)
+    opt_state = init_fun(x0)
+
+    @jit
+    def update(opt_state, step_size):
+      _, update_fun = minmax.sgd(step_size)
+      x = minmax.get_params(opt_state)
+      g = grad(loss)(x, None)
+      return update_fun(0, g, opt_state)
+
+    update(opt_state, 0.9)  # doesn't crash
+
+
 if __name__ == '__main__':
   absltest.main()",No
build/build_jax.sh,build/build_jax.sh,a954389d06a3268919254093b192e159e16f7ccf,7f546b8c02553f392e423e06bde5991bfa066b18,"source sync

PiperOrigin-RevId: 222189611","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ","diff --git a/build/build_jax.sh b/build/build_jax.sh
index 63cb0b229..0bf8ae06c 100755
--- a/build/build_jax.sh
+++ b/build/build_jax.sh
@@ -55,6 +55,7 @@ then
   export TF_CUDA_VERSION=$(readlink -f ${CUDA_TOOLKIT_PATH}/lib64/libcudart.so | cut -d '.' -f4-5)
   export TF_CUDNN_VERSION=$(readlink -f ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so | cut -d '.' -f4-5)
   export TF_CUDA_COMPUTE_CAPABILITIES=""3.0,3.5,5.2,6.0,6.1,7.0""
+  export TF_NCCL_VERSION=2
   export TF_NEED_CUDA=1
 else
   export TF_NEED_CUDA=0
@@ -72,7 +73,6 @@ export TF_DOWNLOAD_CLANG=0
 export TF_SET_ANDROID_WORKSPACE=0
 export TF_CUDA_CLANG=0
 export TF_NEED_TENSORRT=0
-export TF_NCCL_VERSION=""2""
 ./configure
 popd
 ",No
setup.py,setup.py,a954389d06a3268919254093b192e159e16f7ccf,7f546b8c02553f392e423e06bde5991bfa066b18,"source sync

PiperOrigin-RevId: 222189611","diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},","diff --git a/setup.py b/setup.py
index 2b2ebc68d..b634177d3 100644
--- a/setup.py
+++ b/setup.py
@@ -23,7 +23,7 @@ setup(
     author_email='jax-team@google.com',
     packages=['jax', 'jax.lib', 'jax.interpreters', 'jax.numpy', 'jax.scipy',
               'jax.experimental'],
-    install_requires=['numpy>=1.12', 'six', 'protobuf'],
+    install_requires=['numpy>=1.12', 'six', 'protobuf', 'absl-py'],
     url='https://github.com/google/jax',
     license='Apache-2.0',
     package_data={'jax.lib': glob('jax/lib/*.so')},",No
jax/random.py,jax/random.py,065bb0baa2a397c6bac0eb844e7f9c20aa6f0da7,a954389d06a3268919254093b192e159e16f7ccf,"source sync

PiperOrigin-RevId: 222291726","diff --git a/jax/random.py b/jax/random.py
index c626664c9..e458516de 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -325,7 +325,7 @@ def normal(key, shape, dtype=onp.float32):
   Returns:
     A random array with the specified shape and dtype.
   """"""
-  lo = onp.nextafter(onp.array(-1., dtype), 0.)
+  lo = onp.nextafter(onp.array(-1., dtype), 0., dtype=dtype)
   hi = onp.array(1., dtype)
   u = uniform(key, shape, dtype, lo, hi)
   return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)","diff --git a/jax/random.py b/jax/random.py
index c626664c9..e458516de 100644
--- a/jax/random.py
+++ b/jax/random.py
@@ -325,7 +325,7 @@ def normal(key, shape, dtype=onp.float32):
   Returns:
     A random array with the specified shape and dtype.
   """"""
-  lo = onp.nextafter(onp.array(-1., dtype), 0.)
+  lo = onp.nextafter(onp.array(-1., dtype), 0., dtype=dtype)
   hi = onp.array(1., dtype)
   u = uniform(key, shape, dtype, lo, hi)
   return onp.array(onp.sqrt(2), dtype) * lax.erf_inv(u)",No
jax/lax.py,jax/lax.py,e5b76f4fdea1205aa78690894f68be3ce57717bc,065bb0baa2a397c6bac0eb844e7f9c20aa6f0da7,"source sync

PiperOrigin-RevId: 222340967","diff --git a/jax/lax.py b/jax/lax.py
index 367555659..28b580018 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1347,7 +1347,7 @@ def concatenate_transpose_rule(t, *operands, **kwargs):
   operand_shapes = kwargs.pop('operand_shapes')
   limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim))
+  starts = onp.zeros((len(operands), t.ndim), dtype=int)
   starts[1:, dimension] = limit_points[:-1]
   limits = onp.tile(t.shape, (len(operands), 1))
   limits[:, dimension] = limit_points","diff --git a/jax/lax.py b/jax/lax.py
index 367555659..28b580018 100644
--- a/jax/lax.py
+++ b/jax/lax.py
@@ -1347,7 +1347,7 @@ def concatenate_transpose_rule(t, *operands, **kwargs):
   operand_shapes = kwargs.pop('operand_shapes')
   limit_points = onp.cumsum([shape[dimension] for shape in operand_shapes])
 
-  starts = onp.zeros((len(operands), t.ndim))
+  starts = onp.zeros((len(operands), t.ndim), dtype=int)
   starts[1:, dimension] = limit_points[:-1]
   limits = onp.tile(t.shape, (len(operands), 1))
   limits[:, dimension] = limit_points",No
tests/lax_numpy_test.py,tests/lax_numpy_test.py,e5b76f4fdea1205aa78690894f68be3ce57717bc,065bb0baa2a397c6bac0eb844e7f9c20aa6f0da7,"source sync

PiperOrigin-RevId: 222340967","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 7b0c918fd..70280c0c6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -288,6 +288,23 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(
+          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
+       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
+      for dtypes in [
+        [onp.float32],
+        [onp.float32, onp.float32],
+        [onp.float32, onp.int32, onp.float32],
+        [onp.float32, onp.int64, onp.float32],
+        [onp.float32, onp.int32, onp.float64],
+      ]
+      for shape in [(), (2,), (3, 4), (1, 100)]
+      for rng in [jtu.rand_default()])
+  def testStack(self, shape, dtypes, rng):
+    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
+    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
           ""_"".join(str(d) for d in shape),","diff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py
index 7b0c918fd..70280c0c6 100644
--- a/tests/lax_numpy_test.py
+++ b/tests/lax_numpy_test.py
@@ -288,6 +288,23 @@ class LaxBackedNumpyTests(jtu.JaxTestCase):
     self._CheckAgainstNumpy(onp_fun, lnp_fun, args_maker, check_dtypes=True)
     self._CompileAndCheck(lnp_fun, args_maker, check_dtypes=True)
 
+  @parameterized.named_parameters(
+      {""testcase_name"": ""_{}"".format(
+          jtu.format_test_name_suffix("""", [shape] * len(dtypes), dtypes)),
+       ""shape"": shape, ""dtypes"": dtypes, ""rng"": rng}
+      for dtypes in [
+        [onp.float32],
+        [onp.float32, onp.float32],
+        [onp.float32, onp.int32, onp.float32],
+        [onp.float32, onp.int64, onp.float32],
+        [onp.float32, onp.int32, onp.float64],
+      ]
+      for shape in [(), (2,), (3, 4), (1, 100)]
+      for rng in [jtu.rand_default()])
+  def testStack(self, shape, dtypes, rng):
+    args_maker = lambda: [[rng(shape, dtype) for dtype in dtypes]]
+    self._CheckAgainstNumpy(lnp.stack, onp.stack, args_maker, check_dtypes=True)
+
   @parameterized.named_parameters(
       {""testcase_name"": ""_inshape=[{}]_indtype={}_outdtype={}"".format(
           ""_"".join(str(d) for d in shape),",No
examples/BUILD,examples/BUILD,323be694a7cd5e6497b27769f8a302e37fa9a514,e5b76f4fdea1205aa78690894f68be3ce57717bc,"source sync

PiperOrigin-RevId: 222448341","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..63c44f98f 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -25,12 +25,23 @@ py_library(
     srcs = [""datasets.py""],
 )
 
+py_binary(
+    name = ""mnist_classifier_fromscratch"",
+    srcs = [""mnist_classifier_fromscratch.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+    ],
+)
+
 py_binary(
     name = ""mnist_classifier"",
     srcs = [""mnist_classifier.py""],
     deps = [
         "":datasets"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )
 
@@ -44,3 +55,14 @@ py_binary(
         ""//jax:stax"",
     ],
 )
+
+py_binary(
+    name = ""resnet50"",
+    srcs = [""resnet50.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
+    ],
+)","diff --git a/examples/BUILD b/examples/BUILD
index 230e9ceb7..63c44f98f 100644
--- a/examples/BUILD
+++ b/examples/BUILD
@@ -25,12 +25,23 @@ py_library(
     srcs = [""datasets.py""],
 )
 
+py_binary(
+    name = ""mnist_classifier_fromscratch"",
+    srcs = [""mnist_classifier_fromscratch.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+    ],
+)
+
 py_binary(
     name = ""mnist_classifier"",
     srcs = [""mnist_classifier.py""],
     deps = [
         "":datasets"",
         ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
     ],
 )
 
@@ -44,3 +55,14 @@ py_binary(
         ""//jax:stax"",
     ],
 )
+
+py_binary(
+    name = ""resnet50"",
+    srcs = [""resnet50.py""],
+    deps = [
+        "":datasets"",
+        ""//jax:libjax"",
+        ""//jax:minmax"",
+        ""//jax:stax"",
+    ],
+)",No
examples/mnist_classifier.py,examples/mnist_classifier.py,323be694a7cd5e6497b27769f8a302e37fa9a514,e5b76f4fdea1205aa78690894f68be3ce57717bc,"source sync

PiperOrigin-RevId: 222448341","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 52f1e3df2..544d58b4d 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -29,9 +29,9 @@ import numpy.random as npr
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):","diff --git a/examples/mnist_classifier.py b/examples/mnist_classifier.py
index 52f1e3df2..544d58b4d 100644
--- a/examples/mnist_classifier.py
+++ b/examples/mnist_classifier.py
@@ -29,9 +29,9 @@ import numpy.random as npr
 import jax.numpy as np
 from jax import jit, grad
 from jax.experimental import minmax
-import datasets
 from jax.experimental import stax
 from jax.experimental.stax import Dense, Relu, LogSoftmax
+import datasets
 
 
 def loss(params, batch):",No
